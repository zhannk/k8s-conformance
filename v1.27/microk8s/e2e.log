  I0426 06:05:42.415119      22 e2e.go:117] Starting e2e run "f65d482f-32e7-4e21-bfa7-73a160598777" on Ginkgo node 1
  Apr 26 06:05:42.444: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682489142 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 26 06:05:42.626: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:05:42.627: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 26 06:05:42.654: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 26 06:05:42.658: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
  Apr 26 06:05:42.658: INFO: e2e test version: v1.27.0
  Apr 26 06:05:42.659: INFO: kube-apiserver version: v1.27.0
  Apr 26 06:05:42.660: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:05:42.663: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 04/26/23 06:05:42.899
  Apr 26 06:05:42.900: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename field-validation @ 04/26/23 06:05:42.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:05:42.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:05:42.927
  Apr 26 06:05:42.933: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  W0426 06:05:45.467360      22 warnings.go:70] unknown field "alpha"
  W0426 06:05:45.467399      22 warnings.go:70] unknown field "beta"
  W0426 06:05:45.467406      22 warnings.go:70] unknown field "delta"
  W0426 06:05:45.467413      22 warnings.go:70] unknown field "epsilon"
  W0426 06:05:45.467419      22 warnings.go:70] unknown field "gamma"
  Apr 26 06:05:45.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1572" for this suite. @ 04/26/23 06:05:45.495
• [2.606 seconds]
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/26/23 06:05:45.506
  Apr 26 06:05:45.506: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/26/23 06:05:45.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:05:45.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:05:45.533
  Apr 26 06:05:45.538: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 26 06:06:45.555: INFO: Waiting for terminating namespaces to be deleted...
  Apr 26 06:06:45.558: INFO: Starting informer...
  STEP: Starting pods... @ 04/26/23 06:06:45.558
  Apr 26 06:06:45.784: INFO: Pod1 is running on ip-172-31-3-127. Tainting Node
  Apr 26 06:06:48.005: INFO: Pod2 is running on ip-172-31-3-127. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/26/23 06:06:48.005
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/26/23 06:06:48.044
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/26/23 06:06:48.048
  Apr 26 06:06:53.724: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Apr 26 06:07:13.700: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 26 06:07:13.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/26/23 06:07:13.715
  STEP: Destroying namespace "taint-multiple-pods-9938" for this suite. @ 04/26/23 06:07:13.718
• [88.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/26/23 06:07:13.728
  Apr 26 06:07:13.728: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 06:07:13.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:07:13.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:07:13.746
  Apr 26 06:08:13.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6610" for this suite. @ 04/26/23 06:08:13.765
• [60.045 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/26/23 06:08:13.773
  Apr 26 06:08:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:08:13.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:08:13.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:08:13.792
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:08:13.795
  STEP: Saw pod success @ 04/26/23 06:08:17.814
  Apr 26 06:08:17.817: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-251a384d-8c34-4e88-80e4-5d4fd7fa5e10 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:08:17.835
  Apr 26 06:08:17.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7702" for this suite. @ 04/26/23 06:08:17.854
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/26/23 06:08:17.863
  Apr 26 06:08:17.863: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:08:17.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:08:17.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:08:17.882
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:08:17.884
  STEP: Saw pod success @ 04/26/23 06:08:21.907
  Apr 26 06:08:21.910: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-af832235-1a11-415b-963d-f352e4884a3e container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:08:21.914
  Apr 26 06:08:21.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-697" for this suite. @ 04/26/23 06:08:21.969
• [4.113 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/26/23 06:08:21.976
  Apr 26 06:08:21.976: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename deployment @ 04/26/23 06:08:21.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:08:21.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:08:21.994
  Apr 26 06:08:21.996: INFO: Creating deployment "test-recreate-deployment"
  Apr 26 06:08:22.002: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 26 06:08:22.009: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  Apr 26 06:08:24.015: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 26 06:08:24.018: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 26 06:08:24.029: INFO: Updating deployment test-recreate-deployment
  Apr 26 06:08:24.029: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 26 06:08:24.215: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3683  5512a105-8f44-43a7-9241-3f2fb06112b0 1768 2 2023-04-26 06:08:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 06:08:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:08:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054bfeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-26 06:08:24 +0000 UTC,LastTransitionTime:2023-04-26 06:08:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-04-26 06:08:24 +0000 UTC,LastTransitionTime:2023-04-26 06:08:22 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 26 06:08:24.218: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-3683  1ecc2af8-997f-4714-bfe7-106bb0975d38 1767 1 2023-04-26 06:08:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5512a105-8f44-43a7-9241-3f2fb06112b0 0xc004eecab7 0xc004eecab8}] [] [{kubelite Update apps/v1 2023-04-26 06:08:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5512a105-8f44-43a7-9241-3f2fb06112b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:08:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eecb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 06:08:24.218: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 26 06:08:24.218: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-3683  3fcb7215-766f-4f05-8bb1-c36c6551c3e1 1756 2 2023-04-26 06:08:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5512a105-8f44-43a7-9241-3f2fb06112b0 0xc004eec997 0xc004eec998}] [] [{kubelite Update apps/v1 2023-04-26 06:08:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5512a105-8f44-43a7-9241-3f2fb06112b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:08:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eeca58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 06:08:24.221: INFO: Pod "test-recreate-deployment-54757ffd6c-9fhsg" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-9fhsg test-recreate-deployment-54757ffd6c- deployment-3683  3c6303f9-9490-4200-9a2b-45d0702e6cb9 1766 0 2023-04-26 06:08:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 1ecc2af8-997f-4714-bfe7-106bb0975d38 0xc0043a56e7 0xc0043a56e8}] [] [{kubelite Update v1 2023-04-26 06:08:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ecc2af8-997f-4714-bfe7-106bb0975d38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:08:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddzc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddzc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:08:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:08:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:08:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:08:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:08:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:08:24.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3683" for this suite. @ 04/26/23 06:08:24.224
• [2.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/26/23 06:08:24.233
  Apr 26 06:08:24.233: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replication-controller @ 04/26/23 06:08:24.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:08:24.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:08:24.253
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/26/23 06:08:24.256
  STEP: When a replication controller with a matching selector is created @ 04/26/23 06:08:30.34
  STEP: Then the orphan pod is adopted @ 04/26/23 06:08:30.346
  Apr 26 06:08:31.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7632" for this suite. @ 04/26/23 06:08:31.355
• [7.129 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/26/23 06:08:31.362
  Apr 26 06:08:31.362: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pod-network-test @ 04/26/23 06:08:31.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:08:31.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:08:31.407
  STEP: Performing setup for networking test in namespace pod-network-test-4269 @ 04/26/23 06:08:31.409
  STEP: creating a selector @ 04/26/23 06:08:31.409
  STEP: Creating the service pods in kubernetes @ 04/26/23 06:08:31.409
  Apr 26 06:08:31.409: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/26/23 06:08:53.499
  Apr 26 06:08:55.526: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Apr 26 06:08:55.526: INFO: Breadth first check of 10.1.93.197 on host 172.31.1.91...
  Apr 26 06:08:55.529: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.96.11:9080/dial?request=hostname&protocol=http&host=10.1.93.197&port=8083&tries=1'] Namespace:pod-network-test-4269 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:08:55.529: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:08:55.529: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:08:55.529: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4269/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.96.11%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.1.93.197%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 26 06:08:55.595: INFO: Waiting for responses: map[]
  Apr 26 06:08:55.595: INFO: reached 10.1.93.197 after 0/1 tries
  Apr 26 06:08:55.595: INFO: Breadth first check of 10.1.96.10 on host 172.31.3.127...
  Apr 26 06:08:55.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.96.11:9080/dial?request=hostname&protocol=http&host=10.1.96.10&port=8083&tries=1'] Namespace:pod-network-test-4269 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:08:55.598: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:08:55.598: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:08:55.598: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4269/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.96.11%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.1.96.10%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 26 06:08:55.665: INFO: Waiting for responses: map[]
  Apr 26 06:08:55.665: INFO: reached 10.1.96.10 after 0/1 tries
  Apr 26 06:08:55.665: INFO: Going to retry 0 out of 2 pods....
  Apr 26 06:08:55.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4269" for this suite. @ 04/26/23 06:08:55.669
• [24.316 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/26/23 06:08:55.678
  Apr 26 06:08:55.679: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename var-expansion @ 04/26/23 06:08:55.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:08:55.698
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:08:55.7
  STEP: creating the pod @ 04/26/23 06:08:55.705
  STEP: waiting for pod running @ 04/26/23 06:08:55.715
  STEP: creating a file in subpath @ 04/26/23 06:08:59.725
  Apr 26 06:08:59.727: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2057 PodName:var-expansion-9b23a373-a0d7-4f29-a9f7-5e06b7f01461 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:08:59.727: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:08:59.728: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:08:59.728: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-2057/pods/var-expansion-9b23a373-a0d7-4f29-a9f7-5e06b7f01461/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/26/23 06:08:59.796
  Apr 26 06:08:59.799: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2057 PodName:var-expansion-9b23a373-a0d7-4f29-a9f7-5e06b7f01461 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:08:59.799: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:08:59.799: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:08:59.800: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-2057/pods/var-expansion-9b23a373-a0d7-4f29-a9f7-5e06b7f01461/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/26/23 06:08:59.861
  Apr 26 06:09:00.373: INFO: Successfully updated pod "var-expansion-9b23a373-a0d7-4f29-a9f7-5e06b7f01461"
  STEP: waiting for annotated pod running @ 04/26/23 06:09:00.373
  STEP: deleting the pod gracefully @ 04/26/23 06:09:00.379
  Apr 26 06:09:00.379: INFO: Deleting pod "var-expansion-9b23a373-a0d7-4f29-a9f7-5e06b7f01461" in namespace "var-expansion-2057"
  Apr 26 06:09:00.386: INFO: Wait up to 5m0s for pod "var-expansion-9b23a373-a0d7-4f29-a9f7-5e06b7f01461" to be fully deleted
  Apr 26 06:09:32.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2057" for this suite. @ 04/26/23 06:09:32.455
• [36.783 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/26/23 06:09:32.463
  Apr 26 06:09:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 06:09:32.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:09:32.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:09:32.486
  STEP: Creating configMap with name configmap-test-volume-map-8e239688-6bef-4f0e-94b0-caae4507b9bd @ 04/26/23 06:09:32.488
  STEP: Creating a pod to test consume configMaps @ 04/26/23 06:09:32.494
  STEP: Saw pod success @ 04/26/23 06:09:36.511
  Apr 26 06:09:36.514: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-0c684e7b-cfdc-4019-bfca-46dad1a16758 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 06:09:36.519
  Apr 26 06:09:36.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5764" for this suite. @ 04/26/23 06:09:36.538
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/26/23 06:09:36.547
  Apr 26 06:09:36.547: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 06:09:36.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:09:36.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:09:36.567
  Apr 26 06:09:36.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3645" for this suite. @ 04/26/23 06:09:36.611
• [0.071 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/26/23 06:09:36.618
  Apr 26 06:09:36.618: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename subpath @ 04/26/23 06:09:36.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:09:36.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:09:36.637
  STEP: Setting up data @ 04/26/23 06:09:36.64
  STEP: Creating pod pod-subpath-test-configmap-8l47 @ 04/26/23 06:09:36.653
  STEP: Creating a pod to test atomic-volume-subpath @ 04/26/23 06:09:36.653
  STEP: Saw pod success @ 04/26/23 06:10:00.711
  Apr 26 06:10:00.714: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-subpath-test-configmap-8l47 container test-container-subpath-configmap-8l47: <nil>
  STEP: delete the pod @ 04/26/23 06:10:00.719
  STEP: Deleting pod pod-subpath-test-configmap-8l47 @ 04/26/23 06:10:00.74
  Apr 26 06:10:00.740: INFO: Deleting pod "pod-subpath-test-configmap-8l47" in namespace "subpath-5000"
  Apr 26 06:10:00.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5000" for this suite. @ 04/26/23 06:10:00.746
• [24.136 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/26/23 06:10:00.755
  Apr 26 06:10:00.755: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 06:10:00.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:10:00.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:10:00.775
  STEP: creating service in namespace services-3610 @ 04/26/23 06:10:00.777
  STEP: creating service affinity-clusterip-transition in namespace services-3610 @ 04/26/23 06:10:00.777
  STEP: creating replication controller affinity-clusterip-transition in namespace services-3610 @ 04/26/23 06:10:00.788
  I0426 06:10:00.797662      22 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-3610, replica count: 3
  I0426 06:10:03.848975      22 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 06:10:03.854: INFO: Creating new exec pod
  Apr 26 06:10:06.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-3610 exec execpod-affinityz7bl5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Apr 26 06:10:07.009: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 26 06:10:07.009: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:10:07.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-3610 exec execpod-affinityz7bl5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.241 80'
  Apr 26 06:10:07.140: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.241 80\nConnection to 10.152.183.241 80 port [tcp/http] succeeded!\n"
  Apr 26 06:10:07.140: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:10:07.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-3610 exec execpod-affinityz7bl5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.241:80/ ; done'
  Apr 26 06:10:07.329: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n"
  Apr 26 06:10:07.329: INFO: stdout: "\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-knhkv\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-bbh7f\naffinity-clusterip-transition-knhkv\naffinity-clusterip-transition-bbh7f"
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-knhkv
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-knhkv
  Apr 26 06:10:07.329: INFO: Received response from host: affinity-clusterip-transition-bbh7f
  Apr 26 06:10:07.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-3610 exec execpod-affinityz7bl5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.241:80/ ; done'
  Apr 26 06:10:07.529: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.241:80/\n"
  Apr 26 06:10:07.529: INFO: stdout: "\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx\naffinity-clusterip-transition-64ljx"
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Received response from host: affinity-clusterip-transition-64ljx
  Apr 26 06:10:07.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 06:10:07.533: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3610, will wait for the garbage collector to delete the pods @ 04/26/23 06:10:07.559
  Apr 26 06:10:07.622: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.439983ms
  Apr 26 06:10:07.723: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.953288ms
  STEP: Destroying namespace "services-3610" for this suite. @ 04/26/23 06:10:10.217
• [9.469 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/26/23 06:10:10.225
  Apr 26 06:10:10.225: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename security-context @ 04/26/23 06:10:10.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:10:10.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:10:10.244
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/26/23 06:10:10.246
  STEP: Saw pod success @ 04/26/23 06:10:14.279
  Apr 26 06:10:14.281: INFO: Trying to get logs from node ip-172-31-3-127 pod security-context-d0da9c93-1f20-47d0-9ae1-297f5157290b container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:10:14.286
  Apr 26 06:10:14.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-281" for this suite. @ 04/26/23 06:10:14.306
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/26/23 06:10:14.315
  Apr 26 06:10:14.315: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename dns @ 04/26/23 06:10:14.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:10:14.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:10:14.338
  STEP: Creating a test externalName service @ 04/26/23 06:10:14.341
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4994.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4994.svc.cluster.local; sleep 1; done
   @ 04/26/23 06:10:14.346
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4994.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4994.svc.cluster.local; sleep 1; done
   @ 04/26/23 06:10:14.346
  STEP: creating a pod to probe DNS @ 04/26/23 06:10:14.346
  STEP: submitting the pod to kubernetes @ 04/26/23 06:10:14.346
  STEP: retrieving the pod @ 04/26/23 06:10:22.378
  STEP: looking for the results for each expected name from probers @ 04/26/23 06:10:22.38
  Apr 26 06:10:22.386: INFO: DNS probes using dns-test-82317b66-f43c-4848-9bb3-4c984fa2295e succeeded

  STEP: changing the externalName to bar.example.com @ 04/26/23 06:10:22.386
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4994.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4994.svc.cluster.local; sleep 1; done
   @ 04/26/23 06:10:22.421
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4994.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4994.svc.cluster.local; sleep 1; done
   @ 04/26/23 06:10:22.421
  STEP: creating a second pod to probe DNS @ 04/26/23 06:10:22.421
  STEP: submitting the pod to kubernetes @ 04/26/23 06:10:22.421
  STEP: retrieving the pod @ 04/26/23 06:10:24.438
  STEP: looking for the results for each expected name from probers @ 04/26/23 06:10:24.44
  Apr 26 06:10:24.443: INFO: File wheezy_udp@dns-test-service-3.dns-4994.svc.cluster.local from pod  dns-4994/dns-test-f928285b-50b1-4635-b36c-fd4e317efdf0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 26 06:10:24.446: INFO: File jessie_udp@dns-test-service-3.dns-4994.svc.cluster.local from pod  dns-4994/dns-test-f928285b-50b1-4635-b36c-fd4e317efdf0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 26 06:10:24.446: INFO: Lookups using dns-4994/dns-test-f928285b-50b1-4635-b36c-fd4e317efdf0 failed for: [wheezy_udp@dns-test-service-3.dns-4994.svc.cluster.local jessie_udp@dns-test-service-3.dns-4994.svc.cluster.local]

  Apr 26 06:10:29.452: INFO: DNS probes using dns-test-f928285b-50b1-4635-b36c-fd4e317efdf0 succeeded

  STEP: changing the service to type=ClusterIP @ 04/26/23 06:10:29.452
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4994.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4994.svc.cluster.local; sleep 1; done
   @ 04/26/23 06:10:29.466
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4994.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4994.svc.cluster.local; sleep 1; done
   @ 04/26/23 06:10:29.466
  STEP: creating a third pod to probe DNS @ 04/26/23 06:10:29.466
  STEP: submitting the pod to kubernetes @ 04/26/23 06:10:29.469
  STEP: retrieving the pod @ 04/26/23 06:10:39.507
  STEP: looking for the results for each expected name from probers @ 04/26/23 06:10:39.509
  Apr 26 06:10:39.516: INFO: DNS probes using dns-test-6973ac20-1912-4705-afbd-b213f6bb3738 succeeded

  Apr 26 06:10:39.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 06:10:39.519
  STEP: deleting the pod @ 04/26/23 06:10:39.56
  STEP: deleting the pod @ 04/26/23 06:10:39.59
  STEP: deleting the test externalName service @ 04/26/23 06:10:39.648
  STEP: Destroying namespace "dns-4994" for this suite. @ 04/26/23 06:10:39.694
• [25.398 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/26/23 06:10:39.715
  Apr 26 06:10:39.715: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 06:10:39.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:10:39.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:10:39.757
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6262 @ 04/26/23 06:10:39.772
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/26/23 06:10:39.819
  STEP: creating service externalsvc in namespace services-6262 @ 04/26/23 06:10:39.819
  STEP: creating replication controller externalsvc in namespace services-6262 @ 04/26/23 06:10:39.94
  I0426 06:10:39.951745      22 runners.go:194] Created replication controller with name: externalsvc, namespace: services-6262, replica count: 2
  I0426 06:10:43.003012      22 runners.go:194] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0426 06:10:46.004069      22 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/26/23 06:10:46.007
  Apr 26 06:10:46.021: INFO: Creating new exec pod
  Apr 26 06:10:48.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-6262 exec execpodqncwd -- /bin/sh -x -c nslookup clusterip-service.services-6262.svc.cluster.local'
  Apr 26 06:10:48.183: INFO: stderr: "+ nslookup clusterip-service.services-6262.svc.cluster.local\n"
  Apr 26 06:10:48.183: INFO: stdout: "Server:\t\t10.152.183.10\nAddress:\t10.152.183.10#53\n\nclusterip-service.services-6262.svc.cluster.local\tcanonical name = externalsvc.services-6262.svc.cluster.local.\nName:\texternalsvc.services-6262.svc.cluster.local\nAddress: 10.152.183.133\n\n"
  Apr 26 06:10:48.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-6262, will wait for the garbage collector to delete the pods @ 04/26/23 06:10:48.187
  Apr 26 06:10:48.272: INFO: Deleting ReplicationController externalsvc took: 32.41003ms
  Apr 26 06:10:48.373: INFO: Terminating ReplicationController externalsvc pods took: 100.818829ms
  Apr 26 06:10:51.294: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-6262" for this suite. @ 04/26/23 06:10:51.308
• [11.600 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/26/23 06:10:51.315
  Apr 26 06:10:51.316: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:10:51.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:10:51.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:10:51.334
  STEP: Counting existing ResourceQuota @ 04/26/23 06:10:51.337
  STEP: Creating a ResourceQuota @ 04/26/23 06:10:56.341
  STEP: Ensuring resource quota status is calculated @ 04/26/23 06:10:56.348
  STEP: Creating a ReplicationController @ 04/26/23 06:10:58.351
  STEP: Ensuring resource quota status captures replication controller creation @ 04/26/23 06:10:58.365
  STEP: Deleting a ReplicationController @ 04/26/23 06:11:00.369
  STEP: Ensuring resource quota status released usage @ 04/26/23 06:11:00.378
  Apr 26 06:11:02.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7273" for this suite. @ 04/26/23 06:11:02.386
• [11.077 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/26/23 06:11:02.393
  Apr 26 06:11:02.393: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/26/23 06:11:02.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:11:02.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:11:02.417
  STEP: fetching the /apis discovery document @ 04/26/23 06:11:02.42
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/26/23 06:11:02.421
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/26/23 06:11:02.421
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/26/23 06:11:02.421
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/26/23 06:11:02.422
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/26/23 06:11:02.422
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/26/23 06:11:02.423
  Apr 26 06:11:02.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1738" for this suite. @ 04/26/23 06:11:02.426
• [0.041 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/26/23 06:11:02.434
  Apr 26 06:11:02.434: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 06:11:02.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:11:02.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:11:02.481
  STEP: Creating pod busybox-b5a791aa-803f-4c98-8d48-c007eb712f89 in namespace container-probe-8790 @ 04/26/23 06:11:02.484
  Apr 26 06:11:04.501: INFO: Started pod busybox-b5a791aa-803f-4c98-8d48-c007eb712f89 in namespace container-probe-8790
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/26/23 06:11:04.501
  Apr 26 06:11:04.503: INFO: Initial restart count of pod busybox-b5a791aa-803f-4c98-8d48-c007eb712f89 is 0
  Apr 26 06:11:54.619: INFO: Restart count of pod container-probe-8790/busybox-b5a791aa-803f-4c98-8d48-c007eb712f89 is now 1 (50.115782634s elapsed)
  Apr 26 06:11:54.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 06:11:54.622
  STEP: Destroying namespace "container-probe-8790" for this suite. @ 04/26/23 06:11:54.674
• [52.250 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/26/23 06:11:54.687
  Apr 26 06:11:54.687: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 06:11:54.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:11:54.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:11:54.708
  STEP: Creating pod busybox-0341c8bd-6535-4041-97da-a2ba5919b750 in namespace container-probe-2394 @ 04/26/23 06:11:54.711
  Apr 26 06:11:56.725: INFO: Started pod busybox-0341c8bd-6535-4041-97da-a2ba5919b750 in namespace container-probe-2394
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/26/23 06:11:56.725
  Apr 26 06:11:56.728: INFO: Initial restart count of pod busybox-0341c8bd-6535-4041-97da-a2ba5919b750 is 0
  Apr 26 06:15:57.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 06:15:57.303
  STEP: Destroying namespace "container-probe-2394" for this suite. @ 04/26/23 06:15:57.323
• [242.646 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/26/23 06:15:57.333
  Apr 26 06:15:57.333: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename events @ 04/26/23 06:15:57.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:15:57.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:15:57.352
  STEP: Create set of events @ 04/26/23 06:15:57.376
  STEP: get a list of Events with a label in the current namespace @ 04/26/23 06:15:57.392
  STEP: delete a list of events @ 04/26/23 06:15:57.394
  Apr 26 06:15:57.394: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/26/23 06:15:57.413
  Apr 26 06:15:57.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6576" for this suite. @ 04/26/23 06:15:57.42
• [0.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/26/23 06:15:57.429
  Apr 26 06:15:57.429: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename svc-latency @ 04/26/23 06:15:57.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:15:57.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:15:57.446
  Apr 26 06:15:57.449: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7621 @ 04/26/23 06:15:57.449
  I0426 06:15:57.457912      22 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7621, replica count: 1
  I0426 06:15:58.509254      22 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0426 06:15:59.510089      22 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 06:15:59.623: INFO: Created: latency-svc-twg6b
  Apr 26 06:15:59.631: INFO: Got endpoints: latency-svc-twg6b [20.957221ms]
  Apr 26 06:15:59.645: INFO: Created: latency-svc-6w75s
  Apr 26 06:15:59.656: INFO: Got endpoints: latency-svc-6w75s [25.021821ms]
  Apr 26 06:15:59.660: INFO: Created: latency-svc-pvwbl
  Apr 26 06:15:59.672: INFO: Got endpoints: latency-svc-pvwbl [40.646481ms]
  Apr 26 06:15:59.675: INFO: Created: latency-svc-wpwkv
  Apr 26 06:15:59.743: INFO: Got endpoints: latency-svc-wpwkv [111.879561ms]
  Apr 26 06:15:59.750: INFO: Created: latency-svc-xfbg8
  Apr 26 06:15:59.760: INFO: Got endpoints: latency-svc-xfbg8 [129.051606ms]
  Apr 26 06:15:59.763: INFO: Created: latency-svc-l9dmp
  Apr 26 06:15:59.770: INFO: Got endpoints: latency-svc-l9dmp [138.22036ms]
  Apr 26 06:15:59.777: INFO: Created: latency-svc-rxlk6
  Apr 26 06:15:59.783: INFO: Got endpoints: latency-svc-rxlk6 [151.298848ms]
  Apr 26 06:15:59.789: INFO: Created: latency-svc-25wgd
  Apr 26 06:15:59.801: INFO: Got endpoints: latency-svc-25wgd [169.200178ms]
  Apr 26 06:15:59.804: INFO: Created: latency-svc-gklfd
  Apr 26 06:15:59.811: INFO: Got endpoints: latency-svc-gklfd [179.522082ms]
  Apr 26 06:15:59.820: INFO: Created: latency-svc-6nknz
  Apr 26 06:15:59.830: INFO: Got endpoints: latency-svc-6nknz [198.170991ms]
  Apr 26 06:15:59.837: INFO: Created: latency-svc-4n7j9
  Apr 26 06:15:59.845: INFO: Got endpoints: latency-svc-4n7j9 [213.954774ms]
  Apr 26 06:15:59.853: INFO: Created: latency-svc-l8dfk
  Apr 26 06:15:59.865: INFO: Got endpoints: latency-svc-l8dfk [233.201621ms]
  Apr 26 06:15:59.868: INFO: Created: latency-svc-w8mn9
  Apr 26 06:15:59.881: INFO: Got endpoints: latency-svc-w8mn9 [248.917149ms]
  Apr 26 06:15:59.883: INFO: Created: latency-svc-7vgvp
  Apr 26 06:15:59.892: INFO: Got endpoints: latency-svc-7vgvp [260.599438ms]
  Apr 26 06:15:59.898: INFO: Created: latency-svc-qrg9m
  Apr 26 06:15:59.911: INFO: Got endpoints: latency-svc-qrg9m [279.523569ms]
  Apr 26 06:15:59.914: INFO: Created: latency-svc-5q9hk
  Apr 26 06:15:59.922: INFO: Got endpoints: latency-svc-5q9hk [290.299754ms]
  Apr 26 06:15:59.929: INFO: Created: latency-svc-wgzhk
  Apr 26 06:15:59.962: INFO: Got endpoints: latency-svc-wgzhk [305.664964ms]
  Apr 26 06:15:59.968: INFO: Created: latency-svc-4wg6b
  Apr 26 06:15:59.975: INFO: Got endpoints: latency-svc-4wg6b [303.519895ms]
  Apr 26 06:15:59.982: INFO: Created: latency-svc-g7fzr
  Apr 26 06:15:59.989: INFO: Got endpoints: latency-svc-g7fzr [245.679918ms]
  Apr 26 06:15:59.994: INFO: Created: latency-svc-sxwkm
  Apr 26 06:16:00.001: INFO: Got endpoints: latency-svc-sxwkm [240.907556ms]
  Apr 26 06:16:00.008: INFO: Created: latency-svc-4cx5j
  Apr 26 06:16:00.017: INFO: Got endpoints: latency-svc-4cx5j [246.969537ms]
  Apr 26 06:16:00.031: INFO: Created: latency-svc-8jzhq
  Apr 26 06:16:00.039: INFO: Got endpoints: latency-svc-8jzhq [256.395426ms]
  Apr 26 06:16:00.042: INFO: Created: latency-svc-plfbg
  Apr 26 06:16:00.161: INFO: Got endpoints: latency-svc-plfbg [360.241627ms]
  Apr 26 06:16:00.170: INFO: Created: latency-svc-78pgz
  Apr 26 06:16:00.185: INFO: Got endpoints: latency-svc-78pgz [374.107564ms]
  Apr 26 06:16:00.188: INFO: Created: latency-svc-252hm
  Apr 26 06:16:00.196: INFO: Got endpoints: latency-svc-252hm [366.24292ms]
  Apr 26 06:16:00.204: INFO: Created: latency-svc-2xstp
  Apr 26 06:16:00.211: INFO: Got endpoints: latency-svc-2xstp [365.854204ms]
  Apr 26 06:16:00.219: INFO: Created: latency-svc-9qf9s
  Apr 26 06:16:00.227: INFO: Got endpoints: latency-svc-9qf9s [361.913688ms]
  Apr 26 06:16:00.258: INFO: Created: latency-svc-c5qwp
  Apr 26 06:16:00.270: INFO: Got endpoints: latency-svc-c5qwp [389.158922ms]
  Apr 26 06:16:00.274: INFO: Created: latency-svc-hfpvs
  Apr 26 06:16:00.300: INFO: Got endpoints: latency-svc-hfpvs [407.480563ms]
  Apr 26 06:16:00.306: INFO: Created: latency-svc-mpzrn
  Apr 26 06:16:00.319: INFO: Got endpoints: latency-svc-mpzrn [407.543815ms]
  Apr 26 06:16:00.322: INFO: Created: latency-svc-5cjcl
  Apr 26 06:16:00.329: INFO: Got endpoints: latency-svc-5cjcl [406.747145ms]
  Apr 26 06:16:00.334: INFO: Created: latency-svc-5vrj6
  Apr 26 06:16:00.341: INFO: Got endpoints: latency-svc-5vrj6 [379.281607ms]
  Apr 26 06:16:00.348: INFO: Created: latency-svc-g5bsd
  Apr 26 06:16:00.355: INFO: Got endpoints: latency-svc-g5bsd [379.620984ms]
  Apr 26 06:16:00.398: INFO: Created: latency-svc-9tzzf
  Apr 26 06:16:00.405: INFO: Got endpoints: latency-svc-9tzzf [416.267993ms]
  Apr 26 06:16:00.411: INFO: Created: latency-svc-q9lst
  Apr 26 06:16:00.420: INFO: Got endpoints: latency-svc-q9lst [418.817913ms]
  Apr 26 06:16:00.423: INFO: Created: latency-svc-2vjn6
  Apr 26 06:16:00.450: INFO: Got endpoints: latency-svc-2vjn6 [433.80668ms]
  Apr 26 06:16:00.456: INFO: Created: latency-svc-p67bl
  Apr 26 06:16:00.463: INFO: Got endpoints: latency-svc-p67bl [423.643314ms]
  Apr 26 06:16:00.470: INFO: Created: latency-svc-hsjj6
  Apr 26 06:16:00.476: INFO: Got endpoints: latency-svc-hsjj6 [315.017919ms]
  Apr 26 06:16:00.484: INFO: Created: latency-svc-dnp8s
  Apr 26 06:16:00.491: INFO: Got endpoints: latency-svc-dnp8s [306.101189ms]
  Apr 26 06:16:00.497: INFO: Created: latency-svc-fc5mf
  Apr 26 06:16:00.556: INFO: Got endpoints: latency-svc-fc5mf [360.033991ms]
  Apr 26 06:16:00.563: INFO: Created: latency-svc-nf7tv
  Apr 26 06:16:00.570: INFO: Got endpoints: latency-svc-nf7tv [358.789814ms]
  Apr 26 06:16:00.576: INFO: Created: latency-svc-lb9tg
  Apr 26 06:16:00.585: INFO: Got endpoints: latency-svc-lb9tg [358.045367ms]
  Apr 26 06:16:00.591: INFO: Created: latency-svc-8jcg8
  Apr 26 06:16:00.599: INFO: Got endpoints: latency-svc-8jcg8 [328.870752ms]
  Apr 26 06:16:00.605: INFO: Created: latency-svc-bs5qj
  Apr 26 06:16:00.612: INFO: Got endpoints: latency-svc-bs5qj [312.225876ms]
  Apr 26 06:16:00.621: INFO: Created: latency-svc-5q9b8
  Apr 26 06:16:00.676: INFO: Created: latency-svc-rbpbs
  Apr 26 06:16:00.685: INFO: Got endpoints: latency-svc-5q9b8 [366.190788ms]
  Apr 26 06:16:00.695: INFO: Got endpoints: latency-svc-rbpbs [366.172855ms]
  Apr 26 06:16:00.698: INFO: Created: latency-svc-8jxzd
  Apr 26 06:16:00.705: INFO: Got endpoints: latency-svc-8jxzd [363.83768ms]
  Apr 26 06:16:00.712: INFO: Created: latency-svc-bkv2n
  Apr 26 06:16:00.723: INFO: Got endpoints: latency-svc-bkv2n [368.230284ms]
  Apr 26 06:16:00.726: INFO: Created: latency-svc-fvtrk
  Apr 26 06:16:00.734: INFO: Got endpoints: latency-svc-fvtrk [328.515254ms]
  Apr 26 06:16:00.740: INFO: Created: latency-svc-5gckh
  Apr 26 06:16:00.749: INFO: Got endpoints: latency-svc-5gckh [328.375689ms]
  Apr 26 06:16:00.755: INFO: Created: latency-svc-vsmbt
  Apr 26 06:16:00.791: INFO: Got endpoints: latency-svc-vsmbt [340.731145ms]
  Apr 26 06:16:00.799: INFO: Created: latency-svc-bqn77
  Apr 26 06:16:00.813: INFO: Got endpoints: latency-svc-bqn77 [350.489021ms]
  Apr 26 06:16:00.817: INFO: Created: latency-svc-sfck9
  Apr 26 06:16:00.825: INFO: Got endpoints: latency-svc-sfck9 [348.661547ms]
  Apr 26 06:16:00.834: INFO: Created: latency-svc-8p8mj
  Apr 26 06:16:00.841: INFO: Got endpoints: latency-svc-8p8mj [349.297524ms]
  Apr 26 06:16:00.847: INFO: Created: latency-svc-zc9k6
  Apr 26 06:16:00.857: INFO: Created: latency-svc-jdm46
  Apr 26 06:16:00.865: INFO: Created: latency-svc-gqfqm
  Apr 26 06:16:00.874: INFO: Created: latency-svc-gfn6j
  Apr 26 06:16:00.882: INFO: Got endpoints: latency-svc-zc9k6 [326.328718ms]
  Apr 26 06:16:00.890: INFO: Created: latency-svc-df8s4
  Apr 26 06:16:00.923: INFO: Created: latency-svc-wdpd5
  Apr 26 06:16:00.932: INFO: Got endpoints: latency-svc-jdm46 [361.561829ms]
  Apr 26 06:16:00.935: INFO: Created: latency-svc-jxs46
  Apr 26 06:16:00.952: INFO: Created: latency-svc-qxcbt
  Apr 26 06:16:00.961: INFO: Created: latency-svc-pr7jm
  Apr 26 06:16:00.970: INFO: Created: latency-svc-d77q7
  Apr 26 06:16:00.980: INFO: Created: latency-svc-4k6q2
  Apr 26 06:16:00.983: INFO: Got endpoints: latency-svc-gqfqm [398.615461ms]
  Apr 26 06:16:00.993: INFO: Created: latency-svc-h5zn7
  Apr 26 06:16:01.006: INFO: Created: latency-svc-5gvll
  Apr 26 06:16:01.014: INFO: Created: latency-svc-q4pn8
  Apr 26 06:16:01.023: INFO: Created: latency-svc-4zjl8
  Apr 26 06:16:01.031: INFO: Got endpoints: latency-svc-gfn6j [432.347827ms]
  Apr 26 06:16:01.063: INFO: Created: latency-svc-fnl7v
  Apr 26 06:16:01.073: INFO: Created: latency-svc-qrk98
  Apr 26 06:16:01.081: INFO: Got endpoints: latency-svc-df8s4 [469.350071ms]
  Apr 26 06:16:01.088: INFO: Created: latency-svc-6pvqq
  Apr 26 06:16:01.097: INFO: Created: latency-svc-fhdq5
  Apr 26 06:16:01.104: INFO: Created: latency-svc-prsf8
  Apr 26 06:16:01.130: INFO: Got endpoints: latency-svc-wdpd5 [444.782973ms]
  Apr 26 06:16:01.144: INFO: Created: latency-svc-p7gl4
  Apr 26 06:16:01.181: INFO: Got endpoints: latency-svc-jxs46 [486.467949ms]
  Apr 26 06:16:01.195: INFO: Created: latency-svc-sqfpl
  Apr 26 06:16:01.231: INFO: Got endpoints: latency-svc-qxcbt [525.647394ms]
  Apr 26 06:16:01.245: INFO: Created: latency-svc-l2f62
  Apr 26 06:16:01.281: INFO: Got endpoints: latency-svc-pr7jm [558.025878ms]
  Apr 26 06:16:01.320: INFO: Created: latency-svc-mm2hh
  Apr 26 06:16:01.334: INFO: Got endpoints: latency-svc-d77q7 [600.540412ms]
  Apr 26 06:16:01.348: INFO: Created: latency-svc-hgg4z
  Apr 26 06:16:01.384: INFO: Got endpoints: latency-svc-4k6q2 [635.170051ms]
  Apr 26 06:16:01.399: INFO: Created: latency-svc-ddlrt
  Apr 26 06:16:01.435: INFO: Got endpoints: latency-svc-h5zn7 [643.728275ms]
  Apr 26 06:16:01.449: INFO: Created: latency-svc-4kl5w
  Apr 26 06:16:01.481: INFO: Got endpoints: latency-svc-5gvll [667.894967ms]
  Apr 26 06:16:01.497: INFO: Created: latency-svc-9tv6f
  Apr 26 06:16:01.530: INFO: Got endpoints: latency-svc-q4pn8 [705.722059ms]
  Apr 26 06:16:01.545: INFO: Created: latency-svc-x5rdh
  Apr 26 06:16:01.636: INFO: Got endpoints: latency-svc-4zjl8 [794.988233ms]
  Apr 26 06:16:01.646: INFO: Got endpoints: latency-svc-fnl7v [763.296765ms]
  Apr 26 06:16:01.661: INFO: Created: latency-svc-sg5gc
  Apr 26 06:16:01.669: INFO: Created: latency-svc-v77vk
  Apr 26 06:16:01.684: INFO: Got endpoints: latency-svc-qrk98 [752.069347ms]
  Apr 26 06:16:01.698: INFO: Created: latency-svc-q299c
  Apr 26 06:16:01.730: INFO: Got endpoints: latency-svc-6pvqq [746.426514ms]
  Apr 26 06:16:01.744: INFO: Created: latency-svc-zdwsr
  Apr 26 06:16:01.781: INFO: Got endpoints: latency-svc-fhdq5 [749.629029ms]
  Apr 26 06:16:01.796: INFO: Created: latency-svc-8mdnz
  Apr 26 06:16:01.831: INFO: Got endpoints: latency-svc-prsf8 [749.894924ms]
  Apr 26 06:16:01.880: INFO: Created: latency-svc-269nn
  Apr 26 06:16:01.884: INFO: Got endpoints: latency-svc-p7gl4 [754.295915ms]
  Apr 26 06:16:01.899: INFO: Created: latency-svc-4djx9
  Apr 26 06:16:01.934: INFO: Got endpoints: latency-svc-sqfpl [752.661987ms]
  Apr 26 06:16:01.950: INFO: Created: latency-svc-5v7n8
  Apr 26 06:16:01.984: INFO: Got endpoints: latency-svc-l2f62 [753.522609ms]
  Apr 26 06:16:01.998: INFO: Created: latency-svc-xw7cz
  Apr 26 06:16:02.031: INFO: Got endpoints: latency-svc-mm2hh [750.050633ms]
  Apr 26 06:16:02.047: INFO: Created: latency-svc-5mmsq
  Apr 26 06:16:02.081: INFO: Got endpoints: latency-svc-hgg4z [746.629352ms]
  Apr 26 06:16:02.095: INFO: Created: latency-svc-zrr7g
  Apr 26 06:16:02.131: INFO: Got endpoints: latency-svc-ddlrt [747.14137ms]
  Apr 26 06:16:02.146: INFO: Created: latency-svc-dtdc5
  Apr 26 06:16:02.257: INFO: Got endpoints: latency-svc-4kl5w [822.373591ms]
  Apr 26 06:16:02.269: INFO: Got endpoints: latency-svc-9tv6f [787.489487ms]
  Apr 26 06:16:02.284: INFO: Created: latency-svc-xfs8l
  Apr 26 06:16:02.289: INFO: Got endpoints: latency-svc-x5rdh [758.702816ms]
  Apr 26 06:16:02.302: INFO: Created: latency-svc-p8cvt
  Apr 26 06:16:02.311: INFO: Created: latency-svc-zjjbw
  Apr 26 06:16:02.334: INFO: Got endpoints: latency-svc-sg5gc [697.840711ms]
  Apr 26 06:16:02.350: INFO: Created: latency-svc-j9zvx
  Apr 26 06:16:02.382: INFO: Got endpoints: latency-svc-v77vk [735.871203ms]
  Apr 26 06:16:02.424: INFO: Created: latency-svc-9v4sc
  Apr 26 06:16:02.432: INFO: Got endpoints: latency-svc-q299c [748.288359ms]
  Apr 26 06:16:02.456: INFO: Created: latency-svc-4mptv
  Apr 26 06:16:02.481: INFO: Got endpoints: latency-svc-zdwsr [750.666594ms]
  Apr 26 06:16:02.496: INFO: Created: latency-svc-dphsf
  Apr 26 06:16:02.544: INFO: Got endpoints: latency-svc-8mdnz [763.554364ms]
  Apr 26 06:16:02.560: INFO: Created: latency-svc-mmdbj
  Apr 26 06:16:02.581: INFO: Got endpoints: latency-svc-269nn [749.466046ms]
  Apr 26 06:16:02.597: INFO: Created: latency-svc-xhqx7
  Apr 26 06:16:02.669: INFO: Got endpoints: latency-svc-4djx9 [785.089904ms]
  Apr 26 06:16:02.689: INFO: Got endpoints: latency-svc-5v7n8 [755.342205ms]
  Apr 26 06:16:02.700: INFO: Created: latency-svc-sx6qj
  Apr 26 06:16:02.715: INFO: Created: latency-svc-5qf6n
  Apr 26 06:16:02.730: INFO: Got endpoints: latency-svc-xw7cz [746.010448ms]
  Apr 26 06:16:02.745: INFO: Created: latency-svc-qj5bs
  Apr 26 06:16:02.781: INFO: Got endpoints: latency-svc-5mmsq [749.7881ms]
  Apr 26 06:16:02.796: INFO: Created: latency-svc-pklcg
  Apr 26 06:16:02.830: INFO: Got endpoints: latency-svc-zrr7g [748.904505ms]
  Apr 26 06:16:02.845: INFO: Created: latency-svc-48lvw
  Apr 26 06:16:02.881: INFO: Got endpoints: latency-svc-dtdc5 [749.716275ms]
  Apr 26 06:16:02.895: INFO: Created: latency-svc-t6psh
  Apr 26 06:16:02.930: INFO: Got endpoints: latency-svc-xfs8l [672.23445ms]
  Apr 26 06:16:02.945: INFO: Created: latency-svc-5ngwg
  Apr 26 06:16:02.982: INFO: Got endpoints: latency-svc-p8cvt [712.876002ms]
  Apr 26 06:16:02.998: INFO: Created: latency-svc-c6sw8
  Apr 26 06:16:03.031: INFO: Got endpoints: latency-svc-zjjbw [741.987962ms]
  Apr 26 06:16:03.046: INFO: Created: latency-svc-cqrm4
  Apr 26 06:16:03.085: INFO: Got endpoints: latency-svc-j9zvx [751.442016ms]
  Apr 26 06:16:03.100: INFO: Created: latency-svc-nsjks
  Apr 26 06:16:03.130: INFO: Got endpoints: latency-svc-9v4sc [748.802642ms]
  Apr 26 06:16:03.166: INFO: Created: latency-svc-n5rsh
  Apr 26 06:16:03.180: INFO: Got endpoints: latency-svc-4mptv [747.6832ms]
  Apr 26 06:16:03.195: INFO: Created: latency-svc-x48pg
  Apr 26 06:16:03.231: INFO: Got endpoints: latency-svc-dphsf [750.044446ms]
  Apr 26 06:16:03.247: INFO: Created: latency-svc-nrnnj
  Apr 26 06:16:03.281: INFO: Got endpoints: latency-svc-mmdbj [736.852078ms]
  Apr 26 06:16:03.297: INFO: Created: latency-svc-2ptbx
  Apr 26 06:16:03.331: INFO: Got endpoints: latency-svc-xhqx7 [750.076953ms]
  Apr 26 06:16:03.348: INFO: Created: latency-svc-xx2kh
  Apr 26 06:16:03.384: INFO: Got endpoints: latency-svc-sx6qj [714.794441ms]
  Apr 26 06:16:03.399: INFO: Created: latency-svc-vnnll
  Apr 26 06:16:03.434: INFO: Got endpoints: latency-svc-5qf6n [744.093471ms]
  Apr 26 06:16:03.452: INFO: Created: latency-svc-xrbzl
  Apr 26 06:16:03.507: INFO: Got endpoints: latency-svc-qj5bs [776.986057ms]
  Apr 26 06:16:03.526: INFO: Created: latency-svc-bcwmk
  Apr 26 06:16:03.532: INFO: Got endpoints: latency-svc-pklcg [750.624408ms]
  Apr 26 06:16:03.550: INFO: Created: latency-svc-74j5b
  Apr 26 06:16:03.586: INFO: Got endpoints: latency-svc-48lvw [755.664839ms]
  Apr 26 06:16:03.606: INFO: Created: latency-svc-vgx6k
  Apr 26 06:16:03.632: INFO: Got endpoints: latency-svc-t6psh [750.807027ms]
  Apr 26 06:16:03.649: INFO: Created: latency-svc-sr6gd
  Apr 26 06:16:03.681: INFO: Got endpoints: latency-svc-5ngwg [751.336287ms]
  Apr 26 06:16:03.697: INFO: Created: latency-svc-r7m59
  Apr 26 06:16:03.731: INFO: Got endpoints: latency-svc-c6sw8 [749.457035ms]
  Apr 26 06:16:03.748: INFO: Created: latency-svc-nhdx2
  Apr 26 06:16:03.821: INFO: Got endpoints: latency-svc-cqrm4 [790.253444ms]
  Apr 26 06:16:03.839: INFO: Got endpoints: latency-svc-nsjks [753.637314ms]
  Apr 26 06:16:03.849: INFO: Created: latency-svc-c8rfj
  Apr 26 06:16:03.857: INFO: Created: latency-svc-8r45f
  Apr 26 06:16:03.886: INFO: Got endpoints: latency-svc-n5rsh [755.401054ms]
  Apr 26 06:16:03.902: INFO: Created: latency-svc-252sm
  Apr 26 06:16:03.931: INFO: Got endpoints: latency-svc-x48pg [750.422473ms]
  Apr 26 06:16:03.949: INFO: Created: latency-svc-4cnvj
  Apr 26 06:16:03.981: INFO: Got endpoints: latency-svc-nrnnj [750.683615ms]
  Apr 26 06:16:03.998: INFO: Created: latency-svc-9xdkb
  Apr 26 06:16:04.031: INFO: Got endpoints: latency-svc-2ptbx [749.926953ms]
  Apr 26 06:16:04.048: INFO: Created: latency-svc-2kr7p
  Apr 26 06:16:04.106: INFO: Got endpoints: latency-svc-xx2kh [774.950633ms]
  Apr 26 06:16:04.142: INFO: Got endpoints: latency-svc-vnnll [757.797367ms]
  Apr 26 06:16:04.153: INFO: Created: latency-svc-m7w2w
  Apr 26 06:16:04.162: INFO: Created: latency-svc-fqtx8
  Apr 26 06:16:04.180: INFO: Got endpoints: latency-svc-xrbzl [746.223221ms]
  Apr 26 06:16:04.197: INFO: Created: latency-svc-5b27l
  Apr 26 06:16:04.231: INFO: Got endpoints: latency-svc-bcwmk [723.813355ms]
  Apr 26 06:16:04.250: INFO: Created: latency-svc-lw5sk
  Apr 26 06:16:04.348: INFO: Got endpoints: latency-svc-74j5b [816.485788ms]
  Apr 26 06:16:04.418: INFO: Got endpoints: latency-svc-vgx6k [832.746061ms]
  Apr 26 06:16:04.434: INFO: Got endpoints: latency-svc-sr6gd [802.0015ms]
  Apr 26 06:16:04.437: INFO: Created: latency-svc-8fbpc
  Apr 26 06:16:04.442: INFO: Got endpoints: latency-svc-r7m59 [760.653698ms]
  Apr 26 06:16:04.456: INFO: Created: latency-svc-2cz65
  Apr 26 06:16:04.472: INFO: Created: latency-svc-5mrtk
  Apr 26 06:16:04.487: INFO: Got endpoints: latency-svc-nhdx2 [755.426033ms]
  Apr 26 06:16:04.492: INFO: Created: latency-svc-fx47d
  Apr 26 06:16:04.504: INFO: Created: latency-svc-rcbmv
  Apr 26 06:16:04.531: INFO: Got endpoints: latency-svc-c8rfj [709.142187ms]
  Apr 26 06:16:04.548: INFO: Created: latency-svc-4tnhq
  Apr 26 06:16:04.585: INFO: Got endpoints: latency-svc-8r45f [746.546635ms]
  Apr 26 06:16:04.649: INFO: Got endpoints: latency-svc-252sm [762.748404ms]
  Apr 26 06:16:04.659: INFO: Created: latency-svc-rcg8w
  Apr 26 06:16:04.668: INFO: Created: latency-svc-h8hwh
  Apr 26 06:16:04.681: INFO: Got endpoints: latency-svc-4cnvj [750.380648ms]
  Apr 26 06:16:04.699: INFO: Created: latency-svc-5ds9k
  Apr 26 06:16:04.731: INFO: Got endpoints: latency-svc-9xdkb [749.606623ms]
  Apr 26 06:16:04.748: INFO: Created: latency-svc-jlt56
  Apr 26 06:16:04.786: INFO: Got endpoints: latency-svc-2kr7p [754.670081ms]
  Apr 26 06:16:04.803: INFO: Created: latency-svc-j72pb
  Apr 26 06:16:04.830: INFO: Got endpoints: latency-svc-m7w2w [724.247627ms]
  Apr 26 06:16:04.847: INFO: Created: latency-svc-v8zrr
  Apr 26 06:16:04.880: INFO: Got endpoints: latency-svc-fqtx8 [738.31221ms]
  Apr 26 06:16:04.940: INFO: Got endpoints: latency-svc-5b27l [759.644412ms]
  Apr 26 06:16:04.949: INFO: Created: latency-svc-57x2d
  Apr 26 06:16:04.958: INFO: Created: latency-svc-pgcpd
  Apr 26 06:16:04.984: INFO: Got endpoints: latency-svc-lw5sk [752.763255ms]
  Apr 26 06:16:05.005: INFO: Created: latency-svc-zmvsr
  Apr 26 06:16:05.039: INFO: Got endpoints: latency-svc-8fbpc [690.776838ms]
  Apr 26 06:16:05.063: INFO: Created: latency-svc-zjw99
  Apr 26 06:16:05.091: INFO: Got endpoints: latency-svc-2cz65 [672.890435ms]
  Apr 26 06:16:05.111: INFO: Created: latency-svc-74bg4
  Apr 26 06:16:05.135: INFO: Got endpoints: latency-svc-5mrtk [701.375382ms]
  Apr 26 06:16:05.154: INFO: Created: latency-svc-z77sm
  Apr 26 06:16:05.185: INFO: Got endpoints: latency-svc-fx47d [743.402775ms]
  Apr 26 06:16:05.230: INFO: Created: latency-svc-rqjlh
  Apr 26 06:16:05.235: INFO: Got endpoints: latency-svc-rcbmv [748.064106ms]
  Apr 26 06:16:05.253: INFO: Created: latency-svc-rcznt
  Apr 26 06:16:05.284: INFO: Got endpoints: latency-svc-4tnhq [753.513857ms]
  Apr 26 06:16:05.303: INFO: Created: latency-svc-w5rsl
  Apr 26 06:16:05.331: INFO: Got endpoints: latency-svc-rcg8w [745.953741ms]
  Apr 26 06:16:05.350: INFO: Created: latency-svc-sbtpp
  Apr 26 06:16:05.386: INFO: Got endpoints: latency-svc-h8hwh [737.531364ms]
  Apr 26 06:16:05.405: INFO: Created: latency-svc-6k88k
  Apr 26 06:16:05.437: INFO: Got endpoints: latency-svc-5ds9k [756.019209ms]
  Apr 26 06:16:05.456: INFO: Created: latency-svc-wdr6h
  Apr 26 06:16:05.481: INFO: Got endpoints: latency-svc-jlt56 [749.969864ms]
  Apr 26 06:16:05.499: INFO: Created: latency-svc-ms4fn
  Apr 26 06:16:05.557: INFO: Got endpoints: latency-svc-j72pb [771.082078ms]
  Apr 26 06:16:05.575: INFO: Created: latency-svc-5t6xh
  Apr 26 06:16:05.591: INFO: Got endpoints: latency-svc-v8zrr [760.652155ms]
  Apr 26 06:16:05.609: INFO: Created: latency-svc-qwqrs
  Apr 26 06:16:05.630: INFO: Got endpoints: latency-svc-57x2d [749.851844ms]
  Apr 26 06:16:05.649: INFO: Created: latency-svc-nz5vh
  Apr 26 06:16:05.681: INFO: Got endpoints: latency-svc-pgcpd [741.096173ms]
  Apr 26 06:16:05.700: INFO: Created: latency-svc-vclbb
  Apr 26 06:16:05.735: INFO: Got endpoints: latency-svc-zmvsr [751.287909ms]
  Apr 26 06:16:05.755: INFO: Created: latency-svc-82fn9
  Apr 26 06:16:05.813: INFO: Got endpoints: latency-svc-zjw99 [773.99085ms]
  Apr 26 06:16:05.835: INFO: Got endpoints: latency-svc-74bg4 [743.193081ms]
  Apr 26 06:16:05.847: INFO: Created: latency-svc-zgmmr
  Apr 26 06:16:05.855: INFO: Created: latency-svc-w68v8
  Apr 26 06:16:05.886: INFO: Got endpoints: latency-svc-z77sm [750.777772ms]
  Apr 26 06:16:05.904: INFO: Created: latency-svc-plqsf
  Apr 26 06:16:05.934: INFO: Got endpoints: latency-svc-rqjlh [749.005786ms]
  Apr 26 06:16:05.952: INFO: Created: latency-svc-lfkxj
  Apr 26 06:16:05.984: INFO: Got endpoints: latency-svc-rcznt [749.467196ms]
  Apr 26 06:16:06.003: INFO: Created: latency-svc-lkbtz
  Apr 26 06:16:06.034: INFO: Got endpoints: latency-svc-w5rsl [750.194881ms]
  Apr 26 06:16:06.053: INFO: Created: latency-svc-ppb6z
  Apr 26 06:16:06.081: INFO: Got endpoints: latency-svc-sbtpp [749.557029ms]
  Apr 26 06:16:06.133: INFO: Created: latency-svc-4lkgj
  Apr 26 06:16:06.136: INFO: Got endpoints: latency-svc-6k88k [750.018532ms]
  Apr 26 06:16:06.155: INFO: Created: latency-svc-7gg6g
  Apr 26 06:16:06.182: INFO: Got endpoints: latency-svc-wdr6h [745.057949ms]
  Apr 26 06:16:06.201: INFO: Created: latency-svc-5k8wt
  Apr 26 06:16:06.230: INFO: Got endpoints: latency-svc-ms4fn [748.664439ms]
  Apr 26 06:16:06.249: INFO: Created: latency-svc-v2kw7
  Apr 26 06:16:06.280: INFO: Got endpoints: latency-svc-5t6xh [723.002139ms]
  Apr 26 06:16:06.299: INFO: Created: latency-svc-bg8qg
  Apr 26 06:16:06.330: INFO: Got endpoints: latency-svc-qwqrs [739.295647ms]
  Apr 26 06:16:06.351: INFO: Created: latency-svc-mrqgt
  Apr 26 06:16:06.450: INFO: Got endpoints: latency-svc-nz5vh [820.00456ms]
  Apr 26 06:16:06.487: INFO: Got endpoints: latency-svc-vclbb [806.461063ms]
  Apr 26 06:16:06.499: INFO: Got endpoints: latency-svc-82fn9 [763.953937ms]
  Apr 26 06:16:06.512: INFO: Created: latency-svc-fmpmv
  Apr 26 06:16:06.523: INFO: Created: latency-svc-kwwcf
  Apr 26 06:16:06.533: INFO: Created: latency-svc-bnqjz
  Apr 26 06:16:06.537: INFO: Got endpoints: latency-svc-zgmmr [723.917055ms]
  Apr 26 06:16:06.556: INFO: Created: latency-svc-bs5xf
  Apr 26 06:16:06.580: INFO: Got endpoints: latency-svc-w68v8 [745.46275ms]
  Apr 26 06:16:06.599: INFO: Created: latency-svc-dl4wk
  Apr 26 06:16:06.638: INFO: Got endpoints: latency-svc-plqsf [751.853368ms]
  Apr 26 06:16:06.657: INFO: Created: latency-svc-v9mgr
  Apr 26 06:16:06.681: INFO: Got endpoints: latency-svc-lfkxj [746.939345ms]
  Apr 26 06:16:06.701: INFO: Created: latency-svc-bhgf2
  Apr 26 06:16:06.730: INFO: Got endpoints: latency-svc-lkbtz [745.739177ms]
  Apr 26 06:16:06.750: INFO: Created: latency-svc-zz958
  Apr 26 06:16:06.781: INFO: Got endpoints: latency-svc-ppb6z [746.789752ms]
  Apr 26 06:16:06.802: INFO: Created: latency-svc-jxmdz
  Apr 26 06:16:06.830: INFO: Got endpoints: latency-svc-4lkgj [749.614684ms]
  Apr 26 06:16:06.851: INFO: Created: latency-svc-rdlc8
  Apr 26 06:16:06.882: INFO: Got endpoints: latency-svc-7gg6g [745.349384ms]
  Apr 26 06:16:06.921: INFO: Created: latency-svc-f59dx
  Apr 26 06:16:06.933: INFO: Got endpoints: latency-svc-5k8wt [750.85966ms]
  Apr 26 06:16:06.978: INFO: Created: latency-svc-f77nz
  Apr 26 06:16:06.982: INFO: Got endpoints: latency-svc-v2kw7 [751.847849ms]
  Apr 26 06:16:07.002: INFO: Created: latency-svc-phg5w
  Apr 26 06:16:07.030: INFO: Got endpoints: latency-svc-bg8qg [750.291825ms]
  Apr 26 06:16:07.050: INFO: Created: latency-svc-mrxm5
  Apr 26 06:16:07.081: INFO: Got endpoints: latency-svc-mrqgt [750.432496ms]
  Apr 26 06:16:07.101: INFO: Created: latency-svc-wp8n4
  Apr 26 06:16:07.135: INFO: Got endpoints: latency-svc-fmpmv [684.501105ms]
  Apr 26 06:16:07.154: INFO: Created: latency-svc-2nnvx
  Apr 26 06:16:07.182: INFO: Got endpoints: latency-svc-kwwcf [694.581451ms]
  Apr 26 06:16:07.204: INFO: Created: latency-svc-x8frk
  Apr 26 06:16:07.230: INFO: Got endpoints: latency-svc-bnqjz [730.891782ms]
  Apr 26 06:16:07.268: INFO: Created: latency-svc-6zmnt
  Apr 26 06:16:07.284: INFO: Got endpoints: latency-svc-bs5xf [746.91211ms]
  Apr 26 06:16:07.307: INFO: Created: latency-svc-mg5zx
  Apr 26 06:16:07.331: INFO: Got endpoints: latency-svc-dl4wk [750.945273ms]
  Apr 26 06:16:07.354: INFO: Created: latency-svc-vbhfn
  Apr 26 06:16:07.381: INFO: Got endpoints: latency-svc-v9mgr [743.286957ms]
  Apr 26 06:16:07.402: INFO: Created: latency-svc-45qvj
  Apr 26 06:16:07.430: INFO: Got endpoints: latency-svc-bhgf2 [749.189774ms]
  Apr 26 06:16:07.452: INFO: Created: latency-svc-ckdqv
  Apr 26 06:16:07.483: INFO: Got endpoints: latency-svc-zz958 [752.72559ms]
  Apr 26 06:16:07.531: INFO: Got endpoints: latency-svc-jxmdz [750.204353ms]
  Apr 26 06:16:07.584: INFO: Got endpoints: latency-svc-rdlc8 [753.157092ms]
  Apr 26 06:16:07.663: INFO: Got endpoints: latency-svc-f59dx [781.033342ms]
  Apr 26 06:16:07.684: INFO: Got endpoints: latency-svc-f77nz [750.819053ms]
  Apr 26 06:16:07.735: INFO: Got endpoints: latency-svc-phg5w [753.310025ms]
  Apr 26 06:16:07.781: INFO: Got endpoints: latency-svc-mrxm5 [750.334763ms]
  Apr 26 06:16:07.835: INFO: Got endpoints: latency-svc-wp8n4 [754.154077ms]
  Apr 26 06:16:07.884: INFO: Got endpoints: latency-svc-2nnvx [749.333631ms]
  Apr 26 06:16:07.933: INFO: Got endpoints: latency-svc-x8frk [751.110398ms]
  Apr 26 06:16:07.986: INFO: Got endpoints: latency-svc-6zmnt [756.229968ms]
  Apr 26 06:16:08.031: INFO: Got endpoints: latency-svc-mg5zx [746.282487ms]
  Apr 26 06:16:08.080: INFO: Got endpoints: latency-svc-vbhfn [748.889396ms]
  Apr 26 06:16:08.135: INFO: Got endpoints: latency-svc-45qvj [753.476209ms]
  Apr 26 06:16:08.207: INFO: Got endpoints: latency-svc-ckdqv [776.047193ms]
  Apr 26 06:16:08.207: INFO: Latencies: [25.021821ms 40.646481ms 111.879561ms 129.051606ms 138.22036ms 151.298848ms 169.200178ms 179.522082ms 198.170991ms 213.954774ms 233.201621ms 240.907556ms 245.679918ms 246.969537ms 248.917149ms 256.395426ms 260.599438ms 279.523569ms 290.299754ms 303.519895ms 305.664964ms 306.101189ms 312.225876ms 315.017919ms 326.328718ms 328.375689ms 328.515254ms 328.870752ms 340.731145ms 348.661547ms 349.297524ms 350.489021ms 358.045367ms 358.789814ms 360.033991ms 360.241627ms 361.561829ms 361.913688ms 363.83768ms 365.854204ms 366.172855ms 366.190788ms 366.24292ms 368.230284ms 374.107564ms 379.281607ms 379.620984ms 389.158922ms 398.615461ms 406.747145ms 407.480563ms 407.543815ms 416.267993ms 418.817913ms 423.643314ms 432.347827ms 433.80668ms 444.782973ms 469.350071ms 486.467949ms 525.647394ms 558.025878ms 600.540412ms 635.170051ms 643.728275ms 667.894967ms 672.23445ms 672.890435ms 684.501105ms 690.776838ms 694.581451ms 697.840711ms 701.375382ms 705.722059ms 709.142187ms 712.876002ms 714.794441ms 723.002139ms 723.813355ms 723.917055ms 724.247627ms 730.891782ms 735.871203ms 736.852078ms 737.531364ms 738.31221ms 739.295647ms 741.096173ms 741.987962ms 743.193081ms 743.286957ms 743.402775ms 744.093471ms 745.057949ms 745.349384ms 745.46275ms 745.739177ms 745.953741ms 746.010448ms 746.223221ms 746.282487ms 746.426514ms 746.546635ms 746.629352ms 746.789752ms 746.91211ms 746.939345ms 747.14137ms 747.6832ms 748.064106ms 748.288359ms 748.664439ms 748.802642ms 748.889396ms 748.904505ms 749.005786ms 749.189774ms 749.333631ms 749.457035ms 749.466046ms 749.467196ms 749.557029ms 749.606623ms 749.614684ms 749.629029ms 749.716275ms 749.7881ms 749.851844ms 749.894924ms 749.926953ms 749.969864ms 750.018532ms 750.044446ms 750.050633ms 750.076953ms 750.194881ms 750.204353ms 750.291825ms 750.334763ms 750.380648ms 750.422473ms 750.432496ms 750.624408ms 750.666594ms 750.683615ms 750.777772ms 750.807027ms 750.819053ms 750.85966ms 750.945273ms 751.110398ms 751.287909ms 751.336287ms 751.442016ms 751.847849ms 751.853368ms 752.069347ms 752.661987ms 752.72559ms 752.763255ms 753.157092ms 753.310025ms 753.476209ms 753.513857ms 753.522609ms 753.637314ms 754.154077ms 754.295915ms 754.670081ms 755.342205ms 755.401054ms 755.426033ms 755.664839ms 756.019209ms 756.229968ms 757.797367ms 758.702816ms 759.644412ms 760.652155ms 760.653698ms 762.748404ms 763.296765ms 763.554364ms 763.953937ms 771.082078ms 773.99085ms 774.950633ms 776.047193ms 776.986057ms 781.033342ms 785.089904ms 787.489487ms 790.253444ms 794.988233ms 802.0015ms 806.461063ms 816.485788ms 820.00456ms 822.373591ms 832.746061ms]
  Apr 26 06:16:08.207: INFO: 50 %ile: 746.282487ms
  Apr 26 06:16:08.207: INFO: 90 %ile: 762.748404ms
  Apr 26 06:16:08.207: INFO: 99 %ile: 822.373591ms
  Apr 26 06:16:08.207: INFO: Total sample count: 200
  Apr 26 06:16:08.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7621" for this suite. @ 04/26/23 06:16:08.211
• [10.790 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/26/23 06:16:08.219
  Apr 26 06:16:08.219: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 06:16:08.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:08.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:08.245
  STEP: creating service in namespace services-5930 @ 04/26/23 06:16:08.249
  STEP: creating service affinity-nodeport in namespace services-5930 @ 04/26/23 06:16:08.249
  STEP: creating replication controller affinity-nodeport in namespace services-5930 @ 04/26/23 06:16:08.276
  I0426 06:16:08.290431      22 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-5930, replica count: 3
  I0426 06:16:11.341930      22 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 06:16:11.350: INFO: Creating new exec pod
  Apr 26 06:16:14.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-5930 exec execpod-affinityc72ql -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Apr 26 06:16:14.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 26 06:16:14.512: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:16:14.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-5930 exec execpod-affinityc72ql -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.23 80'
  Apr 26 06:16:14.659: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.23 80\nConnection to 10.152.183.23 80 port [tcp/http] succeeded!\n"
  Apr 26 06:16:14.659: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:16:14.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-5930 exec execpod-affinityc72ql -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.127 31451'
  Apr 26 06:16:14.816: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.3.127 31451\nConnection to 172.31.3.127 31451 port [tcp/*] succeeded!\n"
  Apr 26 06:16:14.816: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:16:14.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-5930 exec execpod-affinityc72ql -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.1.91 31451'
  Apr 26 06:16:14.933: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.1.91 31451\nConnection to 172.31.1.91 31451 port [tcp/*] succeeded!\n"
  Apr 26 06:16:14.933: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:16:14.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-5930 exec execpod-affinityc72ql -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.127:31451/ ; done'
  Apr 26 06:16:15.202: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:31451/\n"
  Apr 26 06:16:15.202: INFO: stdout: "\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l\naffinity-nodeport-8xp8l"
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Received response from host: affinity-nodeport-8xp8l
  Apr 26 06:16:15.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 06:16:15.205: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-5930, will wait for the garbage collector to delete the pods @ 04/26/23 06:16:15.302
  Apr 26 06:16:15.401: INFO: Deleting ReplicationController affinity-nodeport took: 45.211568ms
  Apr 26 06:16:15.501: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.135463ms
  STEP: Destroying namespace "services-5930" for this suite. @ 04/26/23 06:16:17.994
• [9.798 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/26/23 06:16:18.018
  Apr 26 06:16:18.018: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 06:16:18.019
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:18.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:18.059
  STEP: Creating a pod to test downward api env vars @ 04/26/23 06:16:18.061
  STEP: Saw pod success @ 04/26/23 06:16:22.107
  Apr 26 06:16:22.109: INFO: Trying to get logs from node ip-172-31-3-127 pod downward-api-0cfe2787-1512-45e8-a14b-3a0c2c6f14c5 container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 06:16:22.123
  Apr 26 06:16:22.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8106" for this suite. @ 04/26/23 06:16:22.19
• [4.192 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/26/23 06:16:22.21
  Apr 26 06:16:22.210: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename disruption @ 04/26/23 06:16:22.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:22.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:22.253
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:16:22.263
  STEP: Updating PodDisruptionBudget status @ 04/26/23 06:16:24.271
  STEP: Waiting for all pods to be running @ 04/26/23 06:16:24.302
  Apr 26 06:16:24.310: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 04/26/23 06:16:26.314
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:16:26.326
  STEP: Patching PodDisruptionBudget status @ 04/26/23 06:16:26.33
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:16:26.34
  Apr 26 06:16:26.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4306" for this suite. @ 04/26/23 06:16:26.345
• [4.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/26/23 06:16:26.354
  Apr 26 06:16:26.354: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename server-version @ 04/26/23 06:16:26.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:26.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:26.372
  STEP: Request ServerVersion @ 04/26/23 06:16:26.375
  STEP: Confirm major version @ 04/26/23 06:16:26.376
  Apr 26 06:16:26.376: INFO: Major version: 1
  STEP: Confirm minor version @ 04/26/23 06:16:26.376
  Apr 26 06:16:26.376: INFO: cleanMinorVersion: 27
  Apr 26 06:16:26.376: INFO: Minor version: 27
  Apr 26 06:16:26.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-6291" for this suite. @ 04/26/23 06:16:26.379
• [0.032 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/26/23 06:16:26.387
  Apr 26 06:16:26.387: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:16:26.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:26.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:26.405
  STEP: Creating a ResourceQuota with best effort scope @ 04/26/23 06:16:26.408
  STEP: Ensuring ResourceQuota status is calculated @ 04/26/23 06:16:26.414
  STEP: Creating a ResourceQuota with not best effort scope @ 04/26/23 06:16:28.418
  STEP: Ensuring ResourceQuota status is calculated @ 04/26/23 06:16:28.424
  STEP: Creating a best-effort pod @ 04/26/23 06:16:30.427
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/26/23 06:16:30.476
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/26/23 06:16:32.479
  STEP: Deleting the pod @ 04/26/23 06:16:34.483
  STEP: Ensuring resource quota status released the pod usage @ 04/26/23 06:16:34.504
  STEP: Creating a not best-effort pod @ 04/26/23 06:16:36.508
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/26/23 06:16:36.52
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/26/23 06:16:38.525
  STEP: Deleting the pod @ 04/26/23 06:16:40.528
  STEP: Ensuring resource quota status released the pod usage @ 04/26/23 06:16:40.547
  Apr 26 06:16:42.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9211" for this suite. @ 04/26/23 06:16:42.553
• [16.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/26/23 06:16:42.564
  Apr 26 06:16:42.564: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename watch @ 04/26/23 06:16:42.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:42.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:42.588
  STEP: getting a starting resourceVersion @ 04/26/23 06:16:42.591
  STEP: starting a background goroutine to produce watch events @ 04/26/23 06:16:42.593
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/26/23 06:16:42.593
  Apr 26 06:16:45.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6854" for this suite. @ 04/26/23 06:16:45.419
• [2.909 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/26/23 06:16:45.474
  Apr 26 06:16:45.474: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 06:16:45.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:45.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:45.494
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/26/23 06:16:45.497
  STEP: Saw pod success @ 04/26/23 06:16:49.549
  Apr 26 06:16:49.552: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-3f46e71e-00f9-426e-bb5b-1c5374dfe3e2 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:16:49.557
  Apr 26 06:16:49.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5991" for this suite. @ 04/26/23 06:16:49.579
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/26/23 06:16:49.592
  Apr 26 06:16:49.592: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:16:49.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:49.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:49.617
  STEP: Creating secret with name s-test-opt-del-9d8821b8-d141-48a9-83c2-569a11fe0ebc @ 04/26/23 06:16:49.622
  STEP: Creating secret with name s-test-opt-upd-8ad8cf70-5719-4c9e-82f7-17bd294be0a2 @ 04/26/23 06:16:49.656
  STEP: Creating the pod @ 04/26/23 06:16:49.663
  STEP: Deleting secret s-test-opt-del-9d8821b8-d141-48a9-83c2-569a11fe0ebc @ 04/26/23 06:16:51.696
  STEP: Updating secret s-test-opt-upd-8ad8cf70-5719-4c9e-82f7-17bd294be0a2 @ 04/26/23 06:16:51.703
  STEP: Creating secret with name s-test-opt-create-d101e56a-4bc0-430c-8c75-75cc080edefd @ 04/26/23 06:16:51.709
  STEP: waiting to observe update in volume @ 04/26/23 06:16:51.715
  Apr 26 06:16:53.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1941" for this suite. @ 04/26/23 06:16:53.738
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/26/23 06:16:53.748
  Apr 26 06:16:53.748: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:16:53.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:53.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:53.767
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:16:53.769
  STEP: Saw pod success @ 04/26/23 06:16:57.8
  Apr 26 06:16:57.802: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-9735350b-ba31-4f95-b159-c19ac224fa81 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:16:57.807
  Apr 26 06:16:57.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8965" for this suite. @ 04/26/23 06:16:57.826
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/26/23 06:16:57.834
  Apr 26 06:16:57.834: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:16:57.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:16:57.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:16:57.852
  STEP: Creating configMap with name projected-configmap-test-volume-0250b915-5c11-4052-a617-3e7ef9bc3a57 @ 04/26/23 06:16:57.855
  STEP: Creating a pod to test consume configMaps @ 04/26/23 06:16:57.86
  STEP: Saw pod success @ 04/26/23 06:17:01.88
  Apr 26 06:17:01.883: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-configmaps-d19e9cb8-fdfe-4307-992b-9308aa98e90f container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:17:01.888
  Apr 26 06:17:01.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1574" for this suite. @ 04/26/23 06:17:01.908
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 04/26/23 06:17:01.917
  Apr 26 06:17:01.917: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename statefulset @ 04/26/23 06:17:01.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:17:01.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:17:01.941
  STEP: Creating service test in namespace statefulset-7224 @ 04/26/23 06:17:01.944
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/26/23 06:17:01.95
  STEP: Creating stateful set ss in namespace statefulset-7224 @ 04/26/23 06:17:01.955
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7224 @ 04/26/23 06:17:01.965
  Apr 26 06:17:01.968: INFO: Found 0 stateful pods, waiting for 1
  Apr 26 06:17:11.974: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/26/23 06:17:11.974
  Apr 26 06:17:11.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-7224 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 06:17:12.115: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 06:17:12.115: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 06:17:12.115: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 26 06:17:12.118: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 26 06:17:22.143: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 26 06:17:22.143: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 06:17:22.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998566s
  Apr 26 06:17:23.161: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997020808s
  Apr 26 06:17:24.197: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993294812s
  Apr 26 06:17:25.200: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.95744439s
  Apr 26 06:17:26.204: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.953666414s
  Apr 26 06:17:27.208: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.949849238s
  Apr 26 06:17:28.212: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.945242265s
  Apr 26 06:17:29.216: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.941606398s
  Apr 26 06:17:30.220: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.937758436s
  Apr 26 06:17:31.223: INFO: Verifying statefulset ss doesn't scale past 1 for another 934.31776ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7224 @ 04/26/23 06:17:32.223
  Apr 26 06:17:32.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-7224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 06:17:32.358: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 26 06:17:32.358: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 06:17:32.358: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 26 06:17:32.361: INFO: Found 1 stateful pods, waiting for 3
  Apr 26 06:17:42.368: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 06:17:42.368: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 06:17:42.368: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/26/23 06:17:42.368
  STEP: Scale down will halt with unhealthy stateful pod @ 04/26/23 06:17:42.368
  Apr 26 06:17:42.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-7224 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 06:17:42.511: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 06:17:42.511: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 06:17:42.511: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 26 06:17:42.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-7224 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 06:17:42.645: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 06:17:42.645: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 06:17:42.645: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 26 06:17:42.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-7224 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 06:17:42.786: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 06:17:42.787: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 06:17:42.787: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 26 06:17:42.787: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 06:17:42.791: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Apr 26 06:17:52.798: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 26 06:17:52.798: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 26 06:17:52.798: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 26 06:17:52.846: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998568s
  Apr 26 06:17:53.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996028102s
  Apr 26 06:17:54.855: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991720156s
  Apr 26 06:17:55.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987656864s
  Apr 26 06:17:56.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983341665s
  Apr 26 06:17:57.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978543176s
  Apr 26 06:17:58.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974433091s
  Apr 26 06:17:59.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970079267s
  Apr 26 06:18:00.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966372806s
  Apr 26 06:18:01.885: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.45257ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7224 @ 04/26/23 06:18:02.885
  Apr 26 06:18:02.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-7224 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 06:18:03.048: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 26 06:18:03.048: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 06:18:03.048: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 26 06:18:03.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-7224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 06:18:03.187: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 26 06:18:03.187: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 06:18:03.187: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 26 06:18:03.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-7224 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 06:18:03.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 26 06:18:03.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 06:18:03.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 26 06:18:03.332: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/26/23 06:18:13.369
  Apr 26 06:18:13.369: INFO: Deleting all statefulset in ns statefulset-7224
  Apr 26 06:18:13.372: INFO: Scaling statefulset ss to 0
  Apr 26 06:18:13.381: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 06:18:13.383: INFO: Deleting statefulset ss
  Apr 26 06:18:13.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7224" for this suite. @ 04/26/23 06:18:13.399
• [71.489 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/26/23 06:18:13.407
  Apr 26 06:18:13.407: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 06:18:13.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:13.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:13.426
  STEP: Setting up server cert @ 04/26/23 06:18:13.449
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 06:18:13.834
  STEP: Deploying the webhook pod @ 04/26/23 06:18:13.845
  STEP: Wait for the deployment to be ready @ 04/26/23 06:18:13.862
  Apr 26 06:18:13.867: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/26/23 06:18:15.877
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:18:15.923
  Apr 26 06:18:16.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/26/23 06:18:16.926
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/26/23 06:18:16.942
  STEP: Creating a dummy validating-webhook-configuration object @ 04/26/23 06:18:16.955
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/26/23 06:18:16.963
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/26/23 06:18:16.97
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/26/23 06:18:16.979
  Apr 26 06:18:16.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3543" for this suite. @ 04/26/23 06:18:17.141
  STEP: Destroying namespace "webhook-markers-2252" for this suite. @ 04/26/23 06:18:17.147
• [3.746 seconds]
------------------------------
S
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/26/23 06:18:17.153
  Apr 26 06:18:17.154: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename prestop @ 04/26/23 06:18:17.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:17.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:17.173
  STEP: Creating server pod server in namespace prestop-6027 @ 04/26/23 06:18:17.175
  STEP: Waiting for pods to come up. @ 04/26/23 06:18:17.184
  STEP: Creating tester pod tester in namespace prestop-6027 @ 04/26/23 06:18:19.192
  STEP: Deleting pre-stop pod @ 04/26/23 06:18:21.239
  Apr 26 06:18:26.253: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 26 06:18:26.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/26/23 06:18:26.257
  STEP: Destroying namespace "prestop-6027" for this suite. @ 04/26/23 06:18:26.278
• [9.170 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/26/23 06:18:26.325
  Apr 26 06:18:26.325: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replicaset @ 04/26/23 06:18:26.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:26.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:26.345
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/26/23 06:18:26.347
  Apr 26 06:18:26.357: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 26 06:18:31.362: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/26/23 06:18:31.362
  STEP: getting scale subresource @ 04/26/23 06:18:31.362
  STEP: updating a scale subresource @ 04/26/23 06:18:31.365
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/26/23 06:18:31.372
  STEP: Patch a scale subresource @ 04/26/23 06:18:31.375
  Apr 26 06:18:31.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5427" for this suite. @ 04/26/23 06:18:31.39
• [5.083 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/26/23 06:18:31.407
  Apr 26 06:18:31.407: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:18:31.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:31.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:31.432
  STEP: Creating configMap with name projected-configmap-test-volume-map-9fdaf04f-9401-480e-9f22-ed156ccc7776 @ 04/26/23 06:18:31.434
  STEP: Creating a pod to test consume configMaps @ 04/26/23 06:18:31.44
  STEP: Saw pod success @ 04/26/23 06:18:35.461
  Apr 26 06:18:35.464: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-configmaps-8f5de75f-99bc-4a22-b3cc-49bb012cf01e container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 06:18:35.477
  Apr 26 06:18:35.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3884" for this suite. @ 04/26/23 06:18:35.497
• [4.097 seconds]
------------------------------
S
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/26/23 06:18:35.505
  Apr 26 06:18:35.505: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename events @ 04/26/23 06:18:35.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:35.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:35.557
  STEP: creating a test event @ 04/26/23 06:18:35.56
  STEP: listing events in all namespaces @ 04/26/23 06:18:35.566
  STEP: listing events in test namespace @ 04/26/23 06:18:35.571
  STEP: listing events with field selection filtering on source @ 04/26/23 06:18:35.573
  STEP: listing events with field selection filtering on reportingController @ 04/26/23 06:18:35.575
  STEP: getting the test event @ 04/26/23 06:18:35.577
  STEP: patching the test event @ 04/26/23 06:18:35.579
  STEP: getting the test event @ 04/26/23 06:18:35.589
  STEP: updating the test event @ 04/26/23 06:18:35.591
  STEP: getting the test event @ 04/26/23 06:18:35.598
  STEP: deleting the test event @ 04/26/23 06:18:35.6
  STEP: listing events in all namespaces @ 04/26/23 06:18:35.607
  STEP: listing events in test namespace @ 04/26/23 06:18:35.611
  Apr 26 06:18:35.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4201" for this suite. @ 04/26/23 06:18:35.617
• [0.118 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/26/23 06:18:35.623
  Apr 26 06:18:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/26/23 06:18:35.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:35.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:35.683
  Apr 26 06:18:37.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/26/23 06:18:37.72
  STEP: Cleaning up the configmap @ 04/26/23 06:18:37.726
  STEP: Cleaning up the pod @ 04/26/23 06:18:37.733
  STEP: Destroying namespace "emptydir-wrapper-132" for this suite. @ 04/26/23 06:18:37.749
• [2.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/26/23 06:18:37.76
  Apr 26 06:18:37.760: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 06:18:37.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:37.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:37.78
  STEP: Setting up server cert @ 04/26/23 06:18:37.808
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 06:18:38.072
  STEP: Deploying the webhook pod @ 04/26/23 06:18:38.081
  STEP: Wait for the deployment to be ready @ 04/26/23 06:18:38.121
  Apr 26 06:18:38.127: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/26/23 06:18:40.136
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:18:40.194
  Apr 26 06:18:41.195: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/26/23 06:18:41.198
  STEP: create a namespace for the webhook @ 04/26/23 06:18:41.212
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/26/23 06:18:41.228
  Apr 26 06:18:41.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2570" for this suite. @ 04/26/23 06:18:41.364
  STEP: Destroying namespace "webhook-markers-2401" for this suite. @ 04/26/23 06:18:41.372
  STEP: Destroying namespace "fail-closed-namespace-2039" for this suite. @ 04/26/23 06:18:41.379
• [3.628 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/26/23 06:18:41.389
  Apr 26 06:18:41.389: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 06:18:41.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:41.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:41.409
  STEP: Creating configMap with name cm-test-opt-del-ceba2d91-c27b-4f42-876f-b32ad179a578 @ 04/26/23 06:18:41.415
  STEP: Creating configMap with name cm-test-opt-upd-46409cc4-11a5-4cec-8bdc-b00af01a0af4 @ 04/26/23 06:18:41.422
  STEP: Creating the pod @ 04/26/23 06:18:41.427
  STEP: Deleting configmap cm-test-opt-del-ceba2d91-c27b-4f42-876f-b32ad179a578 @ 04/26/23 06:18:43.459
  STEP: Updating configmap cm-test-opt-upd-46409cc4-11a5-4cec-8bdc-b00af01a0af4 @ 04/26/23 06:18:43.466
  STEP: Creating configMap with name cm-test-opt-create-d9a8130f-68d4-4e91-b09c-25435671747e @ 04/26/23 06:18:43.471
  STEP: waiting to observe update in volume @ 04/26/23 06:18:43.477
  Apr 26 06:18:47.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9293" for this suite. @ 04/26/23 06:18:47.505
• [6.124 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/26/23 06:18:47.514
  Apr 26 06:18:47.514: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename job @ 04/26/23 06:18:47.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:47.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:47.532
  STEP: Creating a job @ 04/26/23 06:18:47.534
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/26/23 06:18:47.541
  STEP: patching /status @ 04/26/23 06:18:49.546
  STEP: updating /status @ 04/26/23 06:18:49.556
  STEP: get /status @ 04/26/23 06:18:49.562
  Apr 26 06:18:49.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-239" for this suite. @ 04/26/23 06:18:49.568
• [2.061 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/26/23 06:18:49.577
  Apr 26 06:18:49.577: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 06:18:49.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:49.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:49.599
  Apr 26 06:18:49.602: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/26/23 06:18:50.995
  Apr 26 06:18:50.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-3717 --namespace=crd-publish-openapi-3717 create -f -'
  Apr 26 06:18:51.775: INFO: stderr: ""
  Apr 26 06:18:51.775: INFO: stdout: "e2e-test-crd-publish-openapi-7069-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 26 06:18:51.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-3717 --namespace=crd-publish-openapi-3717 delete e2e-test-crd-publish-openapi-7069-crds test-cr'
  Apr 26 06:18:51.846: INFO: stderr: ""
  Apr 26 06:18:51.846: INFO: stdout: "e2e-test-crd-publish-openapi-7069-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 26 06:18:51.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-3717 --namespace=crd-publish-openapi-3717 apply -f -'
  Apr 26 06:18:52.589: INFO: stderr: ""
  Apr 26 06:18:52.589: INFO: stdout: "e2e-test-crd-publish-openapi-7069-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 26 06:18:52.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-3717 --namespace=crd-publish-openapi-3717 delete e2e-test-crd-publish-openapi-7069-crds test-cr'
  Apr 26 06:18:52.668: INFO: stderr: ""
  Apr 26 06:18:52.668: INFO: stdout: "e2e-test-crd-publish-openapi-7069-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/26/23 06:18:52.668
  Apr 26 06:18:52.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-3717 explain e2e-test-crd-publish-openapi-7069-crds'
  Apr 26 06:18:52.959: INFO: stderr: ""
  Apr 26 06:18:52.959: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-7069-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Apr 26 06:18:54.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3717" for this suite. @ 04/26/23 06:18:54.393
• [4.826 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/26/23 06:18:54.403
  Apr 26 06:18:54.403: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-runtime @ 04/26/23 06:18:54.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:54.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:54.741
  STEP: create the container @ 04/26/23 06:18:54.746
  W0426 06:18:54.758707      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/26/23 06:18:54.759
  STEP: get the container status @ 04/26/23 06:18:57.773
  STEP: the container should be terminated @ 04/26/23 06:18:57.776
  STEP: the termination message should be set @ 04/26/23 06:18:57.776
  Apr 26 06:18:57.776: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/26/23 06:18:57.776
  Apr 26 06:18:57.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9661" for this suite. @ 04/26/23 06:18:57.798
• [3.402 seconds]
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/26/23 06:18:57.806
  Apr 26 06:18:57.806: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 06:18:57.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:18:57.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:18:57.904
  STEP: Creating secret with name secret-test-0b9db6b6-db48-4889-8913-7221618c6550 @ 04/26/23 06:18:58.178
  STEP: Creating a pod to test consume secrets @ 04/26/23 06:18:58.185
  STEP: Saw pod success @ 04/26/23 06:19:02.258
  Apr 26 06:19:02.269: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-secrets-4e86ad6e-070a-4953-8ea9-1b247244a967 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:19:02.286
  Apr 26 06:19:02.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6914" for this suite. @ 04/26/23 06:19:02.307
  STEP: Destroying namespace "secret-namespace-4889" for this suite. @ 04/26/23 06:19:02.315
• [4.517 seconds]
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/26/23 06:19:02.323
  Apr 26 06:19:02.323: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 06:19:02.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:02.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:02.905
  STEP: Creating projection with secret that has name secret-emptykey-test-b23750c3-e86c-45c3-80b0-bb134a09ecf6 @ 04/26/23 06:19:02.907
  Apr 26 06:19:02.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3209" for this suite. @ 04/26/23 06:19:02.912
• [0.602 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/26/23 06:19:02.925
  Apr 26 06:19:02.925: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 06:19:02.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:03.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:03.808
  STEP: Creating secret with name s-test-opt-del-c1110bd8-6e19-4cde-8108-f98501000b96 @ 04/26/23 06:19:03.813
  STEP: Creating secret with name s-test-opt-upd-b1f8fcc2-6b6e-4a7c-b491-906e7b1b6d70 @ 04/26/23 06:19:03.82
  STEP: Creating the pod @ 04/26/23 06:19:03.826
  STEP: Deleting secret s-test-opt-del-c1110bd8-6e19-4cde-8108-f98501000b96 @ 04/26/23 06:19:05.867
  STEP: Updating secret s-test-opt-upd-b1f8fcc2-6b6e-4a7c-b491-906e7b1b6d70 @ 04/26/23 06:19:05.876
  STEP: Creating secret with name s-test-opt-create-934c8e4b-682c-47a0-9aee-9c14aa6f3748 @ 04/26/23 06:19:05.883
  STEP: waiting to observe update in volume @ 04/26/23 06:19:05.889
  Apr 26 06:19:07.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4257" for this suite. @ 04/26/23 06:19:07.92
• [5.003 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/26/23 06:19:07.929
  Apr 26 06:19:07.929: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/26/23 06:19:07.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:08.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:08.391
  STEP: mirroring a new custom Endpoint @ 04/26/23 06:19:08.443
  Apr 26 06:19:08.454: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 04/26/23 06:19:10.459
  Apr 26 06:19:10.469: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 04/26/23 06:19:12.472
  Apr 26 06:19:12.482: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Apr 26 06:19:14.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-8063" for this suite. @ 04/26/23 06:19:14.491
• [6.570 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 04/26/23 06:19:14.5
  Apr 26 06:19:14.500: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename daemonsets @ 04/26/23 06:19:14.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:15.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:15.254
  Apr 26 06:19:15.277: INFO: Create a RollingUpdate DaemonSet
  Apr 26 06:19:15.284: INFO: Check that daemon pods launch on every node of the cluster
  Apr 26 06:19:15.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 06:19:15.291: INFO: Node ip-172-31-3-127 is running 0 daemon pod, expected 1
  Apr 26 06:19:16.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 06:19:16.298: INFO: Node ip-172-31-3-127 is running 0 daemon pod, expected 1
  Apr 26 06:19:17.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 06:19:17.298: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  Apr 26 06:19:17.298: INFO: Update the DaemonSet to trigger a rollout
  Apr 26 06:19:17.310: INFO: Updating DaemonSet daemon-set
  Apr 26 06:19:20.362: INFO: Roll back the DaemonSet before rollout is complete
  Apr 26 06:19:20.373: INFO: Updating DaemonSet daemon-set
  Apr 26 06:19:20.373: INFO: Make sure DaemonSet rollback is complete
  Apr 26 06:19:20.376: INFO: Wrong image for pod: daemon-set-qbtv9. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 26 06:19:20.376: INFO: Pod daemon-set-qbtv9 is not available
  Apr 26 06:19:23.383: INFO: Pod daemon-set-tr5vr is not available
  STEP: Deleting DaemonSet "daemon-set" @ 04/26/23 06:19:23.392
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-893, will wait for the garbage collector to delete the pods @ 04/26/23 06:19:23.392
  Apr 26 06:19:23.455: INFO: Deleting DaemonSet.extensions daemon-set took: 9.819344ms
  Apr 26 06:19:24.056: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.45967ms
  Apr 26 06:19:25.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 06:19:25.462: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 26 06:19:25.467: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6953"},"items":null}

  Apr 26 06:19:25.472: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6953"},"items":null}

  Apr 26 06:19:25.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-893" for this suite. @ 04/26/23 06:19:25.485
• [10.997 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/26/23 06:19:25.497
  Apr 26 06:19:25.498: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename disruption @ 04/26/23 06:19:25.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:25.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:25.582
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/26/23 06:19:25.584
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:19:25.592
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/26/23 06:19:27.609
  STEP: Waiting for all pods to be running @ 04/26/23 06:19:27.609
  Apr 26 06:19:27.613: INFO: pods: 0 < 3
  STEP: locating a running pod @ 04/26/23 06:19:29.619
  STEP: Updating the pdb to allow a pod to be evicted @ 04/26/23 06:19:29.628
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:19:29.638
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/26/23 06:19:31.644
  STEP: Waiting for all pods to be running @ 04/26/23 06:19:31.644
  STEP: Waiting for the pdb to observed all healthy pods @ 04/26/23 06:19:31.648
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/26/23 06:19:31.676
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:19:31.69
  STEP: Waiting for all pods to be running @ 04/26/23 06:19:33.697
  STEP: locating a running pod @ 04/26/23 06:19:33.701
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/26/23 06:19:33.709
  STEP: Waiting for the pdb to be deleted @ 04/26/23 06:19:33.719
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/26/23 06:19:33.721
  STEP: Waiting for all pods to be running @ 04/26/23 06:19:33.721
  Apr 26 06:19:33.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1051" for this suite. @ 04/26/23 06:19:33.751
• [8.263 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/26/23 06:19:33.762
  Apr 26 06:19:33.762: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/26/23 06:19:33.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:34.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:34.443
  Apr 26 06:19:34.445: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:19:35.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4846" for this suite. @ 04/26/23 06:19:35.014
• [1.261 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/26/23 06:19:35.024
  Apr 26 06:19:35.024: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:19:35.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:35.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:35.361
  STEP: Creating resourceQuota "e2e-rq-status-cvbpn" @ 04/26/23 06:19:35.368
  Apr 26 06:19:35.378: INFO: Resource quota "e2e-rq-status-cvbpn" reports spec: hard cpu limit of 500m
  Apr 26 06:19:35.378: INFO: Resource quota "e2e-rq-status-cvbpn" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-cvbpn" /status @ 04/26/23 06:19:35.378
  STEP: Confirm /status for "e2e-rq-status-cvbpn" resourceQuota via watch @ 04/26/23 06:19:35.388
  Apr 26 06:19:35.389: INFO: observed resourceQuota "e2e-rq-status-cvbpn" in namespace "resourcequota-5834" with hard status: v1.ResourceList(nil)
  Apr 26 06:19:35.389: INFO: Found resourceQuota "e2e-rq-status-cvbpn" in namespace "resourcequota-5834" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 26 06:19:35.389: INFO: ResourceQuota "e2e-rq-status-cvbpn" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/26/23 06:19:35.392
  Apr 26 06:19:35.400: INFO: Resource quota "e2e-rq-status-cvbpn" reports spec: hard cpu limit of 1
  Apr 26 06:19:35.400: INFO: Resource quota "e2e-rq-status-cvbpn" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-cvbpn" /status @ 04/26/23 06:19:35.4
  STEP: Confirm /status for "e2e-rq-status-cvbpn" resourceQuota via watch @ 04/26/23 06:19:35.407
  Apr 26 06:19:35.408: INFO: observed resourceQuota "e2e-rq-status-cvbpn" in namespace "resourcequota-5834" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 26 06:19:35.408: INFO: Found resourceQuota "e2e-rq-status-cvbpn" in namespace "resourcequota-5834" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 26 06:19:35.408: INFO: ResourceQuota "e2e-rq-status-cvbpn" /status was patched
  STEP: Get "e2e-rq-status-cvbpn" /status @ 04/26/23 06:19:35.408
  Apr 26 06:19:35.411: INFO: Resourcequota "e2e-rq-status-cvbpn" reports status: hard cpu of 1
  Apr 26 06:19:35.411: INFO: Resourcequota "e2e-rq-status-cvbpn" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-cvbpn" /status before checking Spec is unchanged @ 04/26/23 06:19:35.414
  Apr 26 06:19:35.421: INFO: Resourcequota "e2e-rq-status-cvbpn" reports status: hard cpu of 2
  Apr 26 06:19:35.421: INFO: Resourcequota "e2e-rq-status-cvbpn" reports status: hard memory of 2Gi
  Apr 26 06:19:35.422: INFO: Found resourceQuota "e2e-rq-status-cvbpn" in namespace "resourcequota-5834" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Apr 26 06:19:40.431: INFO: ResourceQuota "e2e-rq-status-cvbpn" Spec was unchanged and /status reset
  Apr 26 06:19:40.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5834" for this suite. @ 04/26/23 06:19:40.435
• [5.419 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/26/23 06:19:40.445
  Apr 26 06:19:40.445: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename dns @ 04/26/23 06:19:40.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:40.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:40.904
  STEP: Creating a test headless service @ 04/26/23 06:19:40.907
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7273.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7273.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/26/23 06:19:40.914
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7273.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7273.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/26/23 06:19:40.914
  STEP: creating a pod to probe DNS @ 04/26/23 06:19:40.914
  STEP: submitting the pod to kubernetes @ 04/26/23 06:19:40.914
  STEP: retrieving the pod @ 04/26/23 06:19:42.939
  STEP: looking for the results for each expected name from probers @ 04/26/23 06:19:42.942
  Apr 26 06:19:42.956: INFO: DNS probes using dns-7273/dns-test-84bf45fc-22bf-4707-bc5d-c57e936bad53 succeeded

  Apr 26 06:19:42.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 06:19:42.959
  STEP: deleting the test headless service @ 04/26/23 06:19:42.98
  STEP: Destroying namespace "dns-7273" for this suite. @ 04/26/23 06:19:43.01
• [2.573 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/26/23 06:19:43.019
  Apr 26 06:19:43.019: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 06:19:43.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:43.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:43.905
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:19:43.907
  STEP: Saw pod success @ 04/26/23 06:19:47.93
  Apr 26 06:19:47.933: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-8774bd48-1ac2-4ecc-8ebf-138452fb7276 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:19:47.939
  Apr 26 06:19:47.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1602" for this suite. @ 04/26/23 06:19:47.96
• [4.949 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/26/23 06:19:47.969
  Apr 26 06:19:47.969: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 06:19:47.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:48.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:48.298
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/26/23 06:19:48.3
  STEP: Saw pod success @ 04/26/23 06:19:52.322
  Apr 26 06:19:52.325: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-ab4e2f6c-77f2-4f53-95f6-3d33aa79c969 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:19:52.33
  Apr 26 06:19:52.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7957" for this suite. @ 04/26/23 06:19:52.356
• [4.395 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/26/23 06:19:52.366
  Apr 26 06:19:52.366: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 06:19:52.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:19:53.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:19:53.149
  STEP: Setting up server cert @ 04/26/23 06:19:53.222
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 06:19:53.721
  STEP: Deploying the webhook pod @ 04/26/23 06:19:53.751
  STEP: Wait for the deployment to be ready @ 04/26/23 06:19:53.768
  Apr 26 06:19:53.775: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/26/23 06:19:55.785
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:19:55.835
  Apr 26 06:19:56.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 26 06:19:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9436-crds.webhook.example.com via the AdmissionRegistration API @ 04/26/23 06:19:57.349
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/26/23 06:19:57.369
  Apr 26 06:19:59.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-57" for this suite. @ 04/26/23 06:20:00.022
  STEP: Destroying namespace "webhook-markers-5176" for this suite. @ 04/26/23 06:20:00.031
• [7.672 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/26/23 06:20:00.04
  Apr 26 06:20:00.040: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 06:20:00.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:00.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:00.297
  STEP: creating the pod @ 04/26/23 06:20:00.299
  Apr 26 06:20:00.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-45 create -f -'
  Apr 26 06:20:01.125: INFO: stderr: ""
  Apr 26 06:20:01.125: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/26/23 06:20:03.133
  Apr 26 06:20:03.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-45 label pods pause testing-label=testing-label-value'
  Apr 26 06:20:03.210: INFO: stderr: ""
  Apr 26 06:20:03.210: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/26/23 06:20:03.21
  Apr 26 06:20:03.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-45 get pod pause -L testing-label'
  Apr 26 06:20:03.276: INFO: stderr: ""
  Apr 26 06:20:03.276: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/26/23 06:20:03.276
  Apr 26 06:20:03.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-45 label pods pause testing-label-'
  Apr 26 06:20:03.351: INFO: stderr: ""
  Apr 26 06:20:03.351: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/26/23 06:20:03.351
  Apr 26 06:20:03.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-45 get pod pause -L testing-label'
  Apr 26 06:20:03.415: INFO: stderr: ""
  Apr 26 06:20:03.415: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 04/26/23 06:20:03.416
  Apr 26 06:20:03.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-45 delete --grace-period=0 --force -f -'
  Apr 26 06:20:03.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 06:20:03.495: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 26 06:20:03.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-45 get rc,svc -l name=pause --no-headers'
  Apr 26 06:20:03.568: INFO: stderr: "No resources found in kubectl-45 namespace.\n"
  Apr 26 06:20:03.568: INFO: stdout: ""
  Apr 26 06:20:03.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-45 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 26 06:20:03.633: INFO: stderr: ""
  Apr 26 06:20:03.633: INFO: stdout: ""
  Apr 26 06:20:03.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-45" for this suite. @ 04/26/23 06:20:03.638
• [3.607 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/26/23 06:20:03.647
  Apr 26 06:20:03.647: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 06:20:03.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:03.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:03.905
  STEP: creating an Endpoint @ 04/26/23 06:20:03.912
  STEP: waiting for available Endpoint @ 04/26/23 06:20:03.919
  STEP: listing all Endpoints @ 04/26/23 06:20:03.92
  STEP: updating the Endpoint @ 04/26/23 06:20:03.924
  STEP: fetching the Endpoint @ 04/26/23 06:20:03.931
  STEP: patching the Endpoint @ 04/26/23 06:20:03.933
  STEP: fetching the Endpoint @ 04/26/23 06:20:03.942
  STEP: deleting the Endpoint by Collection @ 04/26/23 06:20:03.944
  STEP: waiting for Endpoint deletion @ 04/26/23 06:20:03.953
  STEP: fetching the Endpoint @ 04/26/23 06:20:03.954
  Apr 26 06:20:03.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9912" for this suite. @ 04/26/23 06:20:03.96
• [0.320 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/26/23 06:20:03.968
  Apr 26 06:20:03.968: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replicaset @ 04/26/23 06:20:03.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:04.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:04.906
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/26/23 06:20:04.908
  STEP: When a replicaset with a matching selector is created @ 04/26/23 06:20:06.93
  STEP: Then the orphan pod is adopted @ 04/26/23 06:20:06.937
  STEP: When the matched label of one of its pods change @ 04/26/23 06:20:07.944
  Apr 26 06:20:07.947: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/26/23 06:20:07.959
  Apr 26 06:20:08.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5374" for this suite. @ 04/26/23 06:20:08.97
• [5.010 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/26/23 06:20:08.979
  Apr 26 06:20:08.979: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 06:20:08.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:09.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:09.313
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-165 @ 04/26/23 06:20:09.315
  STEP: changing the ExternalName service to type=NodePort @ 04/26/23 06:20:09.322
  STEP: creating replication controller externalname-service in namespace services-165 @ 04/26/23 06:20:09.383
  I0426 06:20:09.392081      22 runners.go:194] Created replication controller with name: externalname-service, namespace: services-165, replica count: 2
  I0426 06:20:12.443148      22 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 06:20:12.443: INFO: Creating new exec pod
  Apr 26 06:20:15.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-165 exec execpodkm6vh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 26 06:20:15.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 26 06:20:15.611: INFO: stdout: "externalname-service-n5qzs"
  Apr 26 06:20:15.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-165 exec execpodkm6vh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.144 80'
  Apr 26 06:20:15.743: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.144 80\nConnection to 10.152.183.144 80 port [tcp/http] succeeded!\n"
  Apr 26 06:20:15.743: INFO: stdout: "externalname-service-n5qzs"
  Apr 26 06:20:15.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-165 exec execpodkm6vh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.127 31688'
  Apr 26 06:20:15.876: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.3.127 31688\nConnection to 172.31.3.127 31688 port [tcp/*] succeeded!\n"
  Apr 26 06:20:15.876: INFO: stdout: "externalname-service-n5qzs"
  Apr 26 06:20:15.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-165 exec execpodkm6vh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.1.91 31688'
  Apr 26 06:20:16.003: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.1.91 31688\nConnection to 172.31.1.91 31688 port [tcp/*] succeeded!\n"
  Apr 26 06:20:16.003: INFO: stdout: "externalname-service-qftrj"
  Apr 26 06:20:16.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 06:20:16.019: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-165" for this suite. @ 04/26/23 06:20:16.147
• [7.177 seconds]
------------------------------
S
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/26/23 06:20:16.156
  Apr 26 06:20:16.156: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 06:20:16.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:16.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:16.73
  STEP: creating a secret @ 04/26/23 06:20:16.732
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/26/23 06:20:16.74
  STEP: patching the secret @ 04/26/23 06:20:16.743
  STEP: deleting the secret using a LabelSelector @ 04/26/23 06:20:16.753
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/26/23 06:20:16.763
  Apr 26 06:20:16.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5070" for this suite. @ 04/26/23 06:20:16.769
• [0.621 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/26/23 06:20:16.778
  Apr 26 06:20:16.778: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 06:20:16.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:17.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:17.421
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:20:17.423
  STEP: Saw pod success @ 04/26/23 06:20:21.446
  Apr 26 06:20:21.462: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-8225e9ed-7fc8-4fdd-827d-7251a6ed0f32 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:20:21.471
  Apr 26 06:20:21.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6418" for this suite. @ 04/26/23 06:20:21.509
• [4.770 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/26/23 06:20:21.55
  Apr 26 06:20:21.551: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename dns @ 04/26/23 06:20:21.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:21.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:21.654
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3478.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3478.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/26/23 06:20:21.657
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3478.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3478.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/26/23 06:20:21.657
  STEP: creating a pod to probe /etc/hosts @ 04/26/23 06:20:21.657
  STEP: submitting the pod to kubernetes @ 04/26/23 06:20:21.657
  STEP: retrieving the pod @ 04/26/23 06:20:23.686
  STEP: looking for the results for each expected name from probers @ 04/26/23 06:20:23.69
  Apr 26 06:20:23.703: INFO: DNS probes using dns-3478/dns-test-130c2e46-e271-4f6d-81d3-6440ad486a87 succeeded

  Apr 26 06:20:23.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 06:20:23.706
  STEP: Destroying namespace "dns-3478" for this suite. @ 04/26/23 06:20:23.729
• [2.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/26/23 06:20:23.751
  Apr 26 06:20:23.751: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 06:20:23.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:23.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:23.905
  STEP: Creating configMap configmap-2543/configmap-test-0e4e83c0-c34f-4102-9cb1-bd70f0ba4589 @ 04/26/23 06:20:23.907
  STEP: Creating a pod to test consume configMaps @ 04/26/23 06:20:23.95
  STEP: Saw pod success @ 04/26/23 06:20:27.973
  Apr 26 06:20:27.976: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-6a4f79a4-a937-4564-b0a3-2e0ecc483834 container env-test: <nil>
  STEP: delete the pod @ 04/26/23 06:20:27.982
  Apr 26 06:20:28.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2543" for this suite. @ 04/26/23 06:20:28.004
• [4.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/26/23 06:20:28.014
  Apr 26 06:20:28.014: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 06:20:28.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:28.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:28.677
  STEP: Create a pod @ 04/26/23 06:20:28.679
  STEP: patching /status @ 04/26/23 06:20:30.698
  Apr 26 06:20:30.708: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 26 06:20:30.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2437" for this suite. @ 04/26/23 06:20:30.712
• [2.706 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/26/23 06:20:30.721
  Apr 26 06:20:30.721: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename taint-single-pod @ 04/26/23 06:20:30.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:20:30.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:20:30.904
  Apr 26 06:20:30.906: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 26 06:21:30.916: INFO: Waiting for terminating namespaces to be deleted...
  Apr 26 06:21:30.920: INFO: Starting informer...
  STEP: Starting pod... @ 04/26/23 06:21:30.92
  Apr 26 06:21:31.341: INFO: Pod is running on ip-172-31-3-127. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/26/23 06:21:31.341
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/26/23 06:21:31.352
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/26/23 06:21:31.355
  Apr 26 06:21:31.356: INFO: Pod wasn't evicted. Proceeding
  Apr 26 06:21:31.356: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/26/23 06:21:31.371
  STEP: Waiting some time to make sure that toleration time passed. @ 04/26/23 06:21:31.378
  Apr 26 06:22:46.379: INFO: Pod wasn't evicted. Test successful
  Apr 26 06:22:46.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-1143" for this suite. @ 04/26/23 06:22:46.387
• [135.683 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/26/23 06:22:46.405
  Apr 26 06:22:46.405: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-preemption @ 04/26/23 06:22:46.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:22:47.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:22:47.344
  Apr 26 06:22:47.378: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 26 06:23:47.393: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/26/23 06:23:47.397
  Apr 26 06:23:47.397: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/26/23 06:23:47.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:23:48.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:23:48.905
  Apr 26 06:23:48.925: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Apr 26 06:23:48.928: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 26 06:23:48.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 06:23:49.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-7866" for this suite. @ 04/26/23 06:23:49.046
  STEP: Destroying namespace "sched-preemption-9579" for this suite. @ 04/26/23 06:23:49.054
• [62.657 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/26/23 06:23:49.064
  Apr 26 06:23:49.064: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 06:23:49.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:23:49.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:23:49.345
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/26/23 06:23:49.348
  STEP: Saw pod success @ 04/26/23 06:23:53.373
  Apr 26 06:23:53.375: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-8307dcf7-9f55-41eb-9239-99312a9c9a98 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:23:53.392
  Apr 26 06:23:53.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7041" for this suite. @ 04/26/23 06:23:53.413
• [4.358 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/26/23 06:23:53.422
  Apr 26 06:23:53.422: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename cronjob @ 04/26/23 06:23:53.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:23:53.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:23:53.905
  STEP: Creating a cronjob @ 04/26/23 06:23:53.907
  STEP: Ensuring more than one job is running at a time @ 04/26/23 06:23:53.914
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/26/23 06:25:01.918
  STEP: Removing cronjob @ 04/26/23 06:25:01.922
  Apr 26 06:25:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5684" for this suite. @ 04/26/23 06:25:01.938
• [68.527 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/26/23 06:25:01.949
  Apr 26 06:25:01.949: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 06:25:01.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:25:02.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:25:02.345
  STEP: Setting up server cert @ 04/26/23 06:25:03.071
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 06:25:04.143
  STEP: Deploying the webhook pod @ 04/26/23 06:25:04.155
  STEP: Wait for the deployment to be ready @ 04/26/23 06:25:04.169
  Apr 26 06:25:04.191: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/26/23 06:25:06.201
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:25:06.247
  Apr 26 06:25:07.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 26 06:25:07.251: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/26/23 06:25:07.763
  STEP: Creating a custom resource that should be denied by the webhook @ 04/26/23 06:25:07.782
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/26/23 06:25:09.794
  STEP: Updating the custom resource with disallowed data should be denied @ 04/26/23 06:25:09.803
  STEP: Deleting the custom resource should be denied @ 04/26/23 06:25:09.811
  STEP: Remove the offending key and value from the custom resource data @ 04/26/23 06:25:09.817
  STEP: Deleting the updated custom resource should be successful @ 04/26/23 06:25:09.828
  Apr 26 06:25:09.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-803" for this suite. @ 04/26/23 06:25:10.497
  STEP: Destroying namespace "webhook-markers-5079" for this suite. @ 04/26/23 06:25:10.507
• [8.566 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/26/23 06:25:10.516
  Apr 26 06:25:10.516: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/26/23 06:25:10.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:25:10.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:25:10.807
  STEP: create the container to handle the HTTPGet hook request. @ 04/26/23 06:25:10.813
  STEP: create the pod with lifecycle hook @ 04/26/23 06:25:12.835
  STEP: delete the pod with lifecycle hook @ 04/26/23 06:25:14.852
  STEP: check prestop hook @ 04/26/23 06:25:18.872
  Apr 26 06:25:18.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7843" for this suite. @ 04/26/23 06:25:18.882
• [8.375 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/26/23 06:25:18.892
  Apr 26 06:25:18.892: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 06:25:18.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:25:19.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:25:19.905
  STEP: Creating pod test-grpc-622ab7e5-d581-4dfc-a962-68c4b69f4fe3 in namespace container-probe-1145 @ 04/26/23 06:25:19.907
  Apr 26 06:25:21.925: INFO: Started pod test-grpc-622ab7e5-d581-4dfc-a962-68c4b69f4fe3 in namespace container-probe-1145
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/26/23 06:25:21.925
  Apr 26 06:25:21.928: INFO: Initial restart count of pod test-grpc-622ab7e5-d581-4dfc-a962-68c4b69f4fe3 is 0
  Apr 26 06:26:28.081: INFO: Restart count of pod container-probe-1145/test-grpc-622ab7e5-d581-4dfc-a962-68c4b69f4fe3 is now 1 (1m6.152872396s elapsed)
  Apr 26 06:26:28.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 06:26:28.085
  STEP: Destroying namespace "container-probe-1145" for this suite. @ 04/26/23 06:26:28.138
• [69.254 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/26/23 06:26:28.147
  Apr 26 06:26:28.147: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/26/23 06:26:28.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:26:28.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:26:28.408
  STEP: Creating 50 configmaps @ 04/26/23 06:26:28.41
  STEP: Creating RC which spawns configmap-volume pods @ 04/26/23 06:26:28.819
  Apr 26 06:26:28.833: INFO: Pod name wrapped-volume-race-4fc71e92-e650-4596-8a98-f50b42d11dcb: Found 0 pods out of 5
  Apr 26 06:26:33.842: INFO: Pod name wrapped-volume-race-4fc71e92-e650-4596-8a98-f50b42d11dcb: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/26/23 06:26:33.842
  STEP: Creating RC which spawns configmap-volume pods @ 04/26/23 06:26:33.862
  Apr 26 06:26:33.880: INFO: Pod name wrapped-volume-race-1ea35bd8-6804-4411-b6cb-08cc731dfd88: Found 0 pods out of 5
  Apr 26 06:26:38.893: INFO: Pod name wrapped-volume-race-1ea35bd8-6804-4411-b6cb-08cc731dfd88: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/26/23 06:26:38.893
  STEP: Creating RC which spawns configmap-volume pods @ 04/26/23 06:26:38.914
  Apr 26 06:26:38.934: INFO: Pod name wrapped-volume-race-602e395e-898f-4915-be80-3e6e80b1a7b3: Found 0 pods out of 5
  Apr 26 06:26:43.951: INFO: Pod name wrapped-volume-race-602e395e-898f-4915-be80-3e6e80b1a7b3: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/26/23 06:26:43.951
  Apr 26 06:26:43.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-602e395e-898f-4915-be80-3e6e80b1a7b3 in namespace emptydir-wrapper-5438, will wait for the garbage collector to delete the pods @ 04/26/23 06:26:43.972
  Apr 26 06:26:44.048: INFO: Deleting ReplicationController wrapped-volume-race-602e395e-898f-4915-be80-3e6e80b1a7b3 took: 21.870455ms
  Apr 26 06:26:44.848: INFO: Terminating ReplicationController wrapped-volume-race-602e395e-898f-4915-be80-3e6e80b1a7b3 pods took: 800.621669ms
  STEP: deleting ReplicationController wrapped-volume-race-1ea35bd8-6804-4411-b6cb-08cc731dfd88 in namespace emptydir-wrapper-5438, will wait for the garbage collector to delete the pods @ 04/26/23 06:26:46.449
  Apr 26 06:26:46.515: INFO: Deleting ReplicationController wrapped-volume-race-1ea35bd8-6804-4411-b6cb-08cc731dfd88 took: 11.643468ms
  Apr 26 06:26:47.516: INFO: Terminating ReplicationController wrapped-volume-race-1ea35bd8-6804-4411-b6cb-08cc731dfd88 pods took: 1.001076668s
  STEP: deleting ReplicationController wrapped-volume-race-4fc71e92-e650-4596-8a98-f50b42d11dcb in namespace emptydir-wrapper-5438, will wait for the garbage collector to delete the pods @ 04/26/23 06:26:49.018
  Apr 26 06:26:49.086: INFO: Deleting ReplicationController wrapped-volume-race-4fc71e92-e650-4596-8a98-f50b42d11dcb took: 12.775269ms
  Apr 26 06:26:49.986: INFO: Terminating ReplicationController wrapped-volume-race-4fc71e92-e650-4596-8a98-f50b42d11dcb pods took: 900.557838ms
  STEP: Cleaning up the configMaps @ 04/26/23 06:26:51.387
  STEP: Destroying namespace "emptydir-wrapper-5438" for this suite. @ 04/26/23 06:26:51.794
• [23.656 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/26/23 06:26:51.803
  Apr 26 06:26:51.804: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename cronjob @ 04/26/23 06:26:51.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:26:52.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:26:52.222
  STEP: Creating a cronjob @ 04/26/23 06:26:52.224
  STEP: creating @ 04/26/23 06:26:52.224
  STEP: getting @ 04/26/23 06:26:52.231
  STEP: listing @ 04/26/23 06:26:52.234
  STEP: watching @ 04/26/23 06:26:52.237
  Apr 26 06:26:52.237: INFO: starting watch
  STEP: cluster-wide listing @ 04/26/23 06:26:52.238
  STEP: cluster-wide watching @ 04/26/23 06:26:52.24
  Apr 26 06:26:52.240: INFO: starting watch
  STEP: patching @ 04/26/23 06:26:52.241
  STEP: updating @ 04/26/23 06:26:52.248
  Apr 26 06:26:52.258: INFO: waiting for watch events with expected annotations
  Apr 26 06:26:52.258: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/26/23 06:26:52.258
  STEP: updating /status @ 04/26/23 06:26:52.267
  STEP: get /status @ 04/26/23 06:26:52.273
  STEP: deleting @ 04/26/23 06:26:52.276
  STEP: deleting a collection @ 04/26/23 06:26:52.293
  Apr 26 06:26:52.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7116" for this suite. @ 04/26/23 06:26:52.31
• [0.515 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/26/23 06:26:52.32
  Apr 26 06:26:52.320: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 06:26:52.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:26:52.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:26:52.905
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/26/23 06:26:52.907
  STEP: Saw pod success @ 04/26/23 06:26:56.946
  Apr 26 06:26:56.949: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-efc2b0ad-b60b-4966-a395-4ebc8fc624c0 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:26:56.965
  Apr 26 06:26:56.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-467" for this suite. @ 04/26/23 06:26:56.988
• [4.676 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/26/23 06:26:56.997
  Apr 26 06:26:56.997: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename deployment @ 04/26/23 06:26:56.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:26:57.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:26:57.418
  STEP: creating a Deployment @ 04/26/23 06:26:57.423
  Apr 26 06:26:57.423: INFO: Creating simple deployment test-deployment-4l7rq
  Apr 26 06:26:57.440: INFO: new replicaset for deployment "test-deployment-4l7rq" is yet to be created
  STEP: Getting /status @ 04/26/23 06:26:59.452
  Apr 26 06:26:59.455: INFO: Deployment test-deployment-4l7rq has Conditions: [{Available True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4l7rq-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/26/23 06:26:59.455
  Apr 26 06:26:59.466: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 6, 26, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 6, 26, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 6, 26, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 6, 26, 57, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-4l7rq-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/26/23 06:26:59.466
  Apr 26 06:26:59.468: INFO: Observed &Deployment event: ADDED
  Apr 26 06:26:59.468: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4l7rq-5994cf9475"}
  Apr 26 06:26:59.468: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.468: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4l7rq-5994cf9475"}
  Apr 26 06:26:59.468: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 26 06:26:59.468: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.468: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 26 06:26:59.468: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4l7rq-5994cf9475" is progressing.}
  Apr 26 06:26:59.469: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.469: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 26 06:26:59.469: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4l7rq-5994cf9475" has successfully progressed.}
  Apr 26 06:26:59.469: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.469: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 26 06:26:59.469: INFO: Observed Deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4l7rq-5994cf9475" has successfully progressed.}
  Apr 26 06:26:59.469: INFO: Found Deployment test-deployment-4l7rq in namespace deployment-9903 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 26 06:26:59.469: INFO: Deployment test-deployment-4l7rq has an updated status
  STEP: patching the Statefulset Status @ 04/26/23 06:26:59.469
  Apr 26 06:26:59.469: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 26 06:26:59.477: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/26/23 06:26:59.477
  Apr 26 06:26:59.478: INFO: Observed &Deployment event: ADDED
  Apr 26 06:26:59.478: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4l7rq-5994cf9475"}
  Apr 26 06:26:59.478: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.478: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4l7rq-5994cf9475"}
  Apr 26 06:26:59.478: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 26 06:26:59.478: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.478: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 26 06:26:59.478: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:57 +0000 UTC 2023-04-26 06:26:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4l7rq-5994cf9475" is progressing.}
  Apr 26 06:26:59.478: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.478: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 26 06:26:59.478: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4l7rq-5994cf9475" has successfully progressed.}
  Apr 26 06:26:59.479: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.479: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 26 06:26:59.479: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-26 06:26:58 +0000 UTC 2023-04-26 06:26:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4l7rq-5994cf9475" has successfully progressed.}
  Apr 26 06:26:59.479: INFO: Observed deployment test-deployment-4l7rq in namespace deployment-9903 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 26 06:26:59.479: INFO: Observed &Deployment event: MODIFIED
  Apr 26 06:26:59.479: INFO: Found deployment test-deployment-4l7rq in namespace deployment-9903 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 26 06:26:59.479: INFO: Deployment test-deployment-4l7rq has a patched status
  Apr 26 06:26:59.482: INFO: Deployment "test-deployment-4l7rq":
  &Deployment{ObjectMeta:{test-deployment-4l7rq  deployment-9903  8d1708ef-d69f-4437-82be-18e40657c481 9410 1 2023-04-26 06:26:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-26 06:26:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-04-26 06:26:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049db5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 26 06:26:59.485: INFO: New ReplicaSet "test-deployment-4l7rq-5994cf9475" of Deployment "test-deployment-4l7rq":
  &ReplicaSet{ObjectMeta:{test-deployment-4l7rq-5994cf9475  deployment-9903  6121dae8-5be3-42db-93c9-d1e3849bc24e 9404 1 2023-04-26 06:26:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-4l7rq 8d1708ef-d69f-4437-82be-18e40657c481 0xc004a9b4c7 0xc004a9b4c8}] [] [{kubelite Update apps/v1 2023-04-26 06:26:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d1708ef-d69f-4437-82be-18e40657c481\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:26:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a9b588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 06:26:59.488: INFO: Pod "test-deployment-4l7rq-5994cf9475-rwj8g" is available:
  &Pod{ObjectMeta:{test-deployment-4l7rq-5994cf9475-rwj8g test-deployment-4l7rq-5994cf9475- deployment-9903  3cba492f-6ba5-4166-b1e6-202c652cd124 9403 0 2023-04-26 06:26:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:fdbad643dfc0430585f1f543e5dbd2881a0c0bb4755a39bb5b7a6acfe9534e5f cni.projectcalico.org/podIP:10.1.96.15/32 cni.projectcalico.org/podIPs:10.1.96.15/32] [{apps/v1 ReplicaSet test-deployment-4l7rq-5994cf9475 6121dae8-5be3-42db-93c9-d1e3849bc24e 0xc0049db967 0xc0049db968}] [] [{kubelite Update v1 2023-04-26 06:26:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6121dae8-5be3-42db-93c9-d1e3849bc24e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:26:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9tw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9tw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:26:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:26:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:26:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:26:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.15,StartTime:2023-04-26 06:26:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:26:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://775b155ba9eb015751ef22bf0d74fbe422996b6e3774e25564b731fc8a813e5d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.15,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:26:59.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9903" for this suite. @ 04/26/23 06:26:59.493
• [2.504 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/26/23 06:26:59.503
  Apr 26 06:26:59.503: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 06:26:59.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:00.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:00.333
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/26/23 06:27:00.335
  Apr 26 06:27:00.335: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/26/23 06:27:06.108
  Apr 26 06:27:06.109: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:27:07.520: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:27:13.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9925" for this suite. @ 04/26/23 06:27:13.65
• [14.156 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/26/23 06:27:13.659
  Apr 26 06:27:13.659: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 06:27:13.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:13.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:13.68
  STEP: Saw pod success @ 04/26/23 06:27:19.737
  Apr 26 06:27:19.740: INFO: Trying to get logs from node ip-172-31-3-127 pod client-envvars-95a803a2-58c5-4a8c-ad87-bd9b2010cacc container env3cont: <nil>
  STEP: delete the pod @ 04/26/23 06:27:19.753
  Apr 26 06:27:19.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2341" for this suite. @ 04/26/23 06:27:19.775
• [6.122 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/26/23 06:27:19.782
  Apr 26 06:27:19.782: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replication-controller @ 04/26/23 06:27:19.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:19.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:19.801
  STEP: creating a ReplicationController @ 04/26/23 06:27:19.807
  STEP: waiting for RC to be added @ 04/26/23 06:27:19.813
  STEP: waiting for available Replicas @ 04/26/23 06:27:19.813
  STEP: patching ReplicationController @ 04/26/23 06:27:22.339
  STEP: waiting for RC to be modified @ 04/26/23 06:27:22.349
  STEP: patching ReplicationController status @ 04/26/23 06:27:22.35
  STEP: waiting for RC to be modified @ 04/26/23 06:27:22.357
  STEP: waiting for available Replicas @ 04/26/23 06:27:22.358
  STEP: fetching ReplicationController status @ 04/26/23 06:27:22.365
  STEP: patching ReplicationController scale @ 04/26/23 06:27:22.367
  STEP: waiting for RC to be modified @ 04/26/23 06:27:22.375
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/26/23 06:27:22.375
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/26/23 06:27:24.264
  STEP: updating ReplicationController status @ 04/26/23 06:27:24.266
  STEP: waiting for RC to be modified @ 04/26/23 06:27:24.273
  STEP: listing all ReplicationControllers @ 04/26/23 06:27:24.273
  STEP: checking that ReplicationController has expected values @ 04/26/23 06:27:24.276
  STEP: deleting ReplicationControllers by collection @ 04/26/23 06:27:24.276
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/26/23 06:27:24.285
  Apr 26 06:27:24.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0426 06:27:24.332170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-6538" for this suite. @ 04/26/23 06:27:24.334
• [4.559 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/26/23 06:27:24.341
  Apr 26 06:27:24.341: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 06:27:24.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:24.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:24.36
  STEP: Creating the pod @ 04/26/23 06:27:24.363
  E0426 06:27:25.332977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:26.333177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:27:26.899: INFO: Successfully updated pod "annotationupdateb48c891d-6997-4568-bda1-e882748fee0e"
  E0426 06:27:27.333844      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:28.334624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:29.335507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:30.335764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:27:30.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8595" for this suite. @ 04/26/23 06:27:30.932
• [6.638 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/26/23 06:27:30.98
  Apr 26 06:27:30.980: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:27:30.98
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:31.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:31.037
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:27:31.04
  E0426 06:27:31.335952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:32.336199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:33.336303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:34.336581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:27:35.07
  Apr 26 06:27:35.072: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-dbe21700-b1ed-4749-a93f-c37f9d10fed6 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:27:35.079
  Apr 26 06:27:35.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4291" for this suite. @ 04/26/23 06:27:35.112
• [4.144 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/26/23 06:27:35.124
  Apr 26 06:27:35.124: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename runtimeclass @ 04/26/23 06:27:35.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:35.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:35.153
  E0426 06:27:35.337418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:36.337626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:27:37.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3647" for this suite. @ 04/26/23 06:27:37.196
• [2.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/26/23 06:27:37.207
  Apr 26 06:27:37.207: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 06:27:37.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:37.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:37.232
  STEP: creating the pod @ 04/26/23 06:27:37.234
  STEP: submitting the pod to kubernetes @ 04/26/23 06:27:37.234
  STEP: verifying QOS class is set on the pod @ 04/26/23 06:27:37.247
  Apr 26 06:27:37.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7308" for this suite. @ 04/26/23 06:27:37.252
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/26/23 06:27:37.279
  Apr 26 06:27:37.279: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename init-container @ 04/26/23 06:27:37.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:37.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:37.304
  STEP: creating the pod @ 04/26/23 06:27:37.307
  Apr 26 06:27:37.307: INFO: PodSpec: initContainers in spec.initContainers
  E0426 06:27:37.337639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:38.337999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:39.338175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:40.339177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:27:40.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3901" for this suite. @ 04/26/23 06:27:40.762
• [3.496 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/26/23 06:27:40.776
  Apr 26 06:27:40.776: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename certificates @ 04/26/23 06:27:40.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:40.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:40.822
  E0426 06:27:41.339413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 04/26/23 06:27:41.637
  STEP: getting /apis/certificates.k8s.io @ 04/26/23 06:27:41.642
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/26/23 06:27:41.643
  STEP: creating @ 04/26/23 06:27:41.644
  STEP: getting @ 04/26/23 06:27:41.683
  STEP: listing @ 04/26/23 06:27:41.685
  STEP: watching @ 04/26/23 06:27:41.688
  Apr 26 06:27:41.688: INFO: starting watch
  STEP: patching @ 04/26/23 06:27:41.689
  STEP: updating @ 04/26/23 06:27:41.7
  Apr 26 06:27:41.713: INFO: waiting for watch events with expected annotations
  Apr 26 06:27:41.713: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/26/23 06:27:41.713
  STEP: patching /approval @ 04/26/23 06:27:41.716
  STEP: updating /approval @ 04/26/23 06:27:41.728
  STEP: getting /status @ 04/26/23 06:27:41.756
  STEP: patching /status @ 04/26/23 06:27:41.766
  STEP: updating /status @ 04/26/23 06:27:41.787
  STEP: deleting @ 04/26/23 06:27:41.8
  STEP: deleting a collection @ 04/26/23 06:27:41.824
  Apr 26 06:27:41.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-110" for this suite. @ 04/26/23 06:27:41.855
• [1.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/26/23 06:27:41.868
  Apr 26 06:27:41.868: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename var-expansion @ 04/26/23 06:27:41.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:41.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:41.893
  STEP: Creating a pod to test env composition @ 04/26/23 06:27:41.896
  E0426 06:27:42.339898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:43.340694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:44.341433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:45.341667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:27:45.916
  Apr 26 06:27:45.918: INFO: Trying to get logs from node ip-172-31-3-127 pod var-expansion-2d598e37-a01a-45b5-aaae-f1ed654689bc container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 06:27:45.923
  Apr 26 06:27:45.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3927" for this suite. @ 04/26/23 06:27:45.942
• [4.081 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/26/23 06:27:45.95
  Apr 26 06:27:45.950: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 06:27:45.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:45.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:45.973
  STEP: Creating configMap with name configmap-test-volume-7f8a4621-2ca6-4241-a3d5-ed8d1cd94e28 @ 04/26/23 06:27:45.975
  STEP: Creating a pod to test consume configMaps @ 04/26/23 06:27:45.982
  E0426 06:27:46.342522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:47.343271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:48.343722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:49.343894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:27:50
  Apr 26 06:27:50.003: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-5f1e5f3c-3e90-4bbc-8ead-8f5373d48c1c container configmap-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:27:50.007
  Apr 26 06:27:50.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-520" for this suite. @ 04/26/23 06:27:50.03
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/26/23 06:27:50.038
  Apr 26 06:27:50.038: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename watch @ 04/26/23 06:27:50.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:50.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:50.057
  STEP: creating a new configmap @ 04/26/23 06:27:50.06
  STEP: modifying the configmap once @ 04/26/23 06:27:50.066
  STEP: modifying the configmap a second time @ 04/26/23 06:27:50.074
  STEP: deleting the configmap @ 04/26/23 06:27:50.082
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/26/23 06:27:50.088
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/26/23 06:27:50.089
  Apr 26 06:27:50.089: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2483  a03e5255-abe7-43fd-ba39-e751da4b7458 9932 0 2023-04-26 06:27:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-26 06:27:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 06:27:50.089: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2483  a03e5255-abe7-43fd-ba39-e751da4b7458 9933 0 2023-04-26 06:27:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-26 06:27:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 06:27:50.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2483" for this suite. @ 04/26/23 06:27:50.092
• [0.061 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/26/23 06:27:50.1
  Apr 26 06:27:50.100: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename containers @ 04/26/23 06:27:50.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:50.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:50.118
  STEP: Creating a pod to test override arguments @ 04/26/23 06:27:50.121
  E0426 06:27:50.344015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:51.344275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:52.345051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:53.345772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:27:54.14
  Apr 26 06:27:54.142: INFO: Trying to get logs from node ip-172-31-3-127 pod client-containers-106e9b17-da60-4f6a-998c-1f0dccaf5f66 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 06:27:54.147
  Apr 26 06:27:54.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5383" for this suite. @ 04/26/23 06:27:54.166
• [4.073 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/26/23 06:27:54.173
  Apr 26 06:27:54.173: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl-logs @ 04/26/23 06:27:54.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:27:54.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:27:54.191
  STEP: creating an pod @ 04/26/23 06:27:54.194
  Apr 26 06:27:54.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-logs-8723 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Apr 26 06:27:54.266: INFO: stderr: ""
  Apr 26 06:27:54.266: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/26/23 06:27:54.266
  Apr 26 06:27:54.266: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0426 06:27:54.346040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:55.346413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:27:56.272: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/26/23 06:27:56.272
  Apr 26 06:27:56.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-logs-8723 logs logs-generator logs-generator'
  Apr 26 06:27:56.346: INFO: stderr: ""
  Apr 26 06:27:56.346: INFO: stdout: "I0426 06:27:54.965958       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/7pq2 280\nI0426 06:27:55.166262       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/8vrc 405\nI0426 06:27:55.366661       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/rq6j 580\nI0426 06:27:55.566045       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/sz5b 555\nI0426 06:27:55.766486       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/2xr 285\nI0426 06:27:55.966971       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/6jvf 429\nI0426 06:27:56.166315       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/6p9 371\n"
  E0426 06:27:56.346473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: limiting log lines @ 04/26/23 06:27:56.346
  Apr 26 06:27:56.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-logs-8723 logs logs-generator logs-generator --tail=1'
  Apr 26 06:27:56.416: INFO: stderr: ""
  Apr 26 06:27:56.416: INFO: stdout: "I0426 06:27:56.366695       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/md8d 518\n"
  Apr 26 06:27:56.416: INFO: got output "I0426 06:27:56.366695       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/md8d 518\n"
  STEP: limiting log bytes @ 04/26/23 06:27:56.416
  Apr 26 06:27:56.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-logs-8723 logs logs-generator logs-generator --limit-bytes=1'
  Apr 26 06:27:56.488: INFO: stderr: ""
  Apr 26 06:27:56.488: INFO: stdout: "I"
  Apr 26 06:27:56.488: INFO: got output "I"
  STEP: exposing timestamps @ 04/26/23 06:27:56.488
  Apr 26 06:27:56.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-logs-8723 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 26 06:27:56.559: INFO: stderr: ""
  Apr 26 06:27:56.559: INFO: stdout: "2023-04-26T06:27:56.366817535Z I0426 06:27:56.366695       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/md8d 518\n"
  Apr 26 06:27:56.559: INFO: got output "2023-04-26T06:27:56.366817535Z I0426 06:27:56.366695       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/md8d 518\n"
  STEP: restricting to a time range @ 04/26/23 06:27:56.56
  E0426 06:27:57.347232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:27:58.347649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:27:59.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-logs-8723 logs logs-generator logs-generator --since=1s'
  Apr 26 06:27:59.133: INFO: stderr: ""
  Apr 26 06:27:59.133: INFO: stdout: "I0426 06:27:58.166156       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/k7t 200\nI0426 06:27:58.366527       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/knn 309\nI0426 06:27:58.567012       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/c4q 212\nI0426 06:27:58.766491       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/8268 599\nI0426 06:27:58.966932       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/sp6 377\n"
  Apr 26 06:27:59.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-logs-8723 logs logs-generator logs-generator --since=24h'
  Apr 26 06:27:59.208: INFO: stderr: ""
  Apr 26 06:27:59.208: INFO: stdout: "I0426 06:27:54.965958       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/7pq2 280\nI0426 06:27:55.166262       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/8vrc 405\nI0426 06:27:55.366661       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/rq6j 580\nI0426 06:27:55.566045       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/sz5b 555\nI0426 06:27:55.766486       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/2xr 285\nI0426 06:27:55.966971       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/6jvf 429\nI0426 06:27:56.166315       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/6p9 371\nI0426 06:27:56.366695       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/md8d 518\nI0426 06:27:56.567100       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/7vf 312\nI0426 06:27:56.766594       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/bq8 525\nI0426 06:27:56.966979       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/qsn 328\nI0426 06:27:57.166308       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/xpsj 560\nI0426 06:27:57.366797       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/6qc 347\nI0426 06:27:57.566127       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/tnqj 287\nI0426 06:27:57.766463       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/bms 593\nI0426 06:27:57.966914       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/zg84 596\nI0426 06:27:58.166156       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/k7t 200\nI0426 06:27:58.366527       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/knn 309\nI0426 06:27:58.567012       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/c4q 212\nI0426 06:27:58.766491       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/8268 599\nI0426 06:27:58.966932       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/sp6 377\nI0426 06:27:59.166260       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/8vqz 460\n"
  Apr 26 06:27:59.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-logs-8723 delete pod logs-generator'
  E0426 06:27:59.348164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:00.349218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:28:00.528: INFO: stderr: ""
  Apr 26 06:28:00.528: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 26 06:28:00.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-8723" for this suite. @ 04/26/23 06:28:00.531
• [6.368 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/26/23 06:28:00.541
  Apr 26 06:28:00.541: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:28:00.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:00.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:00.567
  STEP: Creating projection with secret that has name projected-secret-test-e3d199cc-2680-4e4c-bdfa-e119fcd437a3 @ 04/26/23 06:28:00.569
  STEP: Creating a pod to test consume secrets @ 04/26/23 06:28:00.576
  E0426 06:28:01.349819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:02.350135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:03.350214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:04.351262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:28:04.595
  Apr 26 06:28:04.598: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-secrets-dda11425-191b-4bdf-b0a8-2739c9cf3a90 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:28:04.605
  Apr 26 06:28:04.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3046" for this suite. @ 04/26/23 06:28:04.625
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/26/23 06:28:04.633
  Apr 26 06:28:04.633: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-pred @ 04/26/23 06:28:04.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:04.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:04.656
  Apr 26 06:28:04.659: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 26 06:28:04.665: INFO: Waiting for terminating namespaces to be deleted...
  Apr 26 06:28:04.668: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-91 before test
  Apr 26 06:28:04.676: INFO: calico-node-d6hf2 from kube-system started at 2023-04-26 06:03:27 +0000 UTC (1 container statuses recorded)
  Apr 26 06:28:04.676: INFO: 	Container calico-node ready: true, restart count 0
  Apr 26 06:28:04.676: INFO: sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-wbxsz from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:28:04.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:28:04.676: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 26 06:28:04.676: INFO: sonobuoy-e2e-job-5266529d9a654ea5 from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:28:04.676: INFO: 	Container e2e ready: true, restart count 0
  Apr 26 06:28:04.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:28:04.676: INFO: sonobuoy from sonobuoy started at 2023-04-26 06:05:24 +0000 UTC (1 container statuses recorded)
  Apr 26 06:28:04.676: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 26 06:28:04.676: INFO: coredns-7745f9f87f-hgdql from kube-system started at 2023-04-26 06:06:48 +0000 UTC (1 container statuses recorded)
  Apr 26 06:28:04.676: INFO: 	Container coredns ready: true, restart count 0
  Apr 26 06:28:04.676: INFO: calico-kube-controllers-6c99c8747f-plv8f from kube-system started at 2023-04-26 06:06:48 +0000 UTC (1 container statuses recorded)
  Apr 26 06:28:04.676: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Apr 26 06:28:04.676: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-3-127 before test
  Apr 26 06:28:04.683: INFO: calico-node-578zf from kube-system started at 2023-04-26 06:02:34 +0000 UTC (1 container statuses recorded)
  Apr 26 06:28:04.683: INFO: 	Container calico-node ready: true, restart count 0
  Apr 26 06:28:04.683: INFO: sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-ndzdn from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:28:04.683: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:28:04.683: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 26 06:28:04.683: INFO: pod-qos-class-6bee1d78-a440-45a3-9f4a-b1489e2010df from pods-7308 started at 2023-04-26 06:27:37 +0000 UTC (1 container statuses recorded)
  Apr 26 06:28:04.683: INFO: 	Container agnhost ready: false, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/26/23 06:28:04.683
  E0426 06:28:05.351999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:06.352278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/26/23 06:28:06.702
  STEP: Trying to apply a random label on the found node. @ 04/26/23 06:28:06.717
  STEP: verifying the node has the label kubernetes.io/e2e-e8fe7836-3641-49f8-a79d-9091d186db08 42 @ 04/26/23 06:28:06.73
  STEP: Trying to relaunch the pod, now with labels. @ 04/26/23 06:28:06.737
  E0426 06:28:07.353361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:08.353579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-e8fe7836-3641-49f8-a79d-9091d186db08 off the node ip-172-31-3-127 @ 04/26/23 06:28:08.753
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-e8fe7836-3641-49f8-a79d-9091d186db08 @ 04/26/23 06:28:08.766
  Apr 26 06:28:08.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-463" for this suite. @ 04/26/23 06:28:08.777
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/26/23 06:28:08.787
  Apr 26 06:28:08.787: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 06:28:08.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:08.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:08.806
  STEP: creating service multi-endpoint-test in namespace services-916 @ 04/26/23 06:28:08.809
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-916 to expose endpoints map[] @ 04/26/23 06:28:08.82
  Apr 26 06:28:08.824: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0426 06:28:09.354216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:28:09.830: INFO: successfully validated that service multi-endpoint-test in namespace services-916 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-916 @ 04/26/23 06:28:09.83
  E0426 06:28:10.355203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:11.355497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-916 to expose endpoints map[pod1:[100]] @ 04/26/23 06:28:11.849
  Apr 26 06:28:11.856: INFO: successfully validated that service multi-endpoint-test in namespace services-916 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-916 @ 04/26/23 06:28:11.856
  E0426 06:28:12.356171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:13.356371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-916 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/26/23 06:28:13.872
  Apr 26 06:28:13.882: INFO: successfully validated that service multi-endpoint-test in namespace services-916 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/26/23 06:28:13.882
  Apr 26 06:28:13.882: INFO: Creating new exec pod
  E0426 06:28:14.357470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:15.357816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:16.358014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:28:16.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-916 exec execpodpfncp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Apr 26 06:28:17.036: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 26 06:28:17.036: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:28:17.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-916 exec execpodpfncp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.59 80'
  Apr 26 06:28:17.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.59 80\nConnection to 10.152.183.59 80 port [tcp/http] succeeded!\n"
  Apr 26 06:28:17.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:28:17.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-916 exec execpodpfncp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Apr 26 06:28:17.293: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 26 06:28:17.293: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 06:28:17.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-916 exec execpodpfncp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.59 81'
  E0426 06:28:17.359114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:28:17.433: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.59 81\nConnection to 10.152.183.59 81 port [tcp/*] succeeded!\n"
  Apr 26 06:28:17.433: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-916 @ 04/26/23 06:28:17.433
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-916 to expose endpoints map[pod2:[101]] @ 04/26/23 06:28:17.45
  E0426 06:28:18.359625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:19.359977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:28:19.465: INFO: successfully validated that service multi-endpoint-test in namespace services-916 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-916 @ 04/26/23 06:28:19.465
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-916 to expose endpoints map[] @ 04/26/23 06:28:19.48
  E0426 06:28:20.360435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:21.361393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:28:21.489: INFO: successfully validated that service multi-endpoint-test in namespace services-916 exposes endpoints map[]
  Apr 26 06:28:21.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-916" for this suite. @ 04/26/23 06:28:21.519
• [12.739 seconds]
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/26/23 06:28:21.526
  Apr 26 06:28:21.526: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 06:28:21.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:21.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:21.546
  STEP: fetching services @ 04/26/23 06:28:21.548
  Apr 26 06:28:21.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6701" for this suite. @ 04/26/23 06:28:21.553
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/26/23 06:28:21.56
  Apr 26 06:28:21.560: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:28:21.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:21.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:21.576
  STEP: Counting existing ResourceQuota @ 04/26/23 06:28:21.578
  E0426 06:28:22.361559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:23.361765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:24.361943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:25.362916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:26.362991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/26/23 06:28:26.606
  STEP: Ensuring resource quota status is calculated @ 04/26/23 06:28:26.624
  E0426 06:28:27.363770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:28.364234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 04/26/23 06:28:28.627
  STEP: Ensuring resource quota status captures replicaset creation @ 04/26/23 06:28:28.666
  E0426 06:28:29.364413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:30.364591      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 04/26/23 06:28:30.704
  STEP: Ensuring resource quota status released usage @ 04/26/23 06:28:30.711
  E0426 06:28:31.365308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:32.365577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:28:32.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9932" for this suite. @ 04/26/23 06:28:32.732
• [11.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/26/23 06:28:32.742
  Apr 26 06:28:32.742: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename security-context @ 04/26/23 06:28:32.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:32.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:32.761
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/26/23 06:28:32.763
  E0426 06:28:33.366368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:34.367215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:35.367815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:36.368077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:28:36.794
  Apr 26 06:28:36.813: INFO: Trying to get logs from node ip-172-31-3-127 pod security-context-83563e10-7c91-4e4e-ad2c-80fb70c3b677 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:28:36.818
  Apr 26 06:28:36.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-8010" for this suite. @ 04/26/23 06:28:36.837
• [4.101 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/26/23 06:28:36.844
  Apr 26 06:28:36.844: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename runtimeclass @ 04/26/23 06:28:36.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:36.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:36.862
  E0426 06:28:37.368822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:38.369932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:28:38.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5884" for this suite. @ 04/26/23 06:28:38.892
• [2.055 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/26/23 06:28:38.899
  Apr 26 06:28:38.899: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 06:28:38.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:38.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:38.921
  STEP: Creating a pod to test downward api env vars @ 04/26/23 06:28:38.924
  E0426 06:28:39.370719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:40.370961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:41.371209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:42.371505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:28:42.942
  Apr 26 06:28:42.945: INFO: Trying to get logs from node ip-172-31-3-127 pod downward-api-a49573c4-cf66-47f1-9a51-3333450dd73b container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 06:28:42.95
  Apr 26 06:28:42.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4158" for this suite. @ 04/26/23 06:28:42.972
• [4.080 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/26/23 06:28:42.98
  Apr 26 06:28:42.980: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename namespaces @ 04/26/23 06:28:42.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:42.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:43
  STEP: Creating a test namespace @ 04/26/23 06:28:43.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:43.019
  STEP: Creating a pod in the namespace @ 04/26/23 06:28:43.021
  STEP: Waiting for the pod to have running status @ 04/26/23 06:28:43.03
  E0426 06:28:43.371948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:44.373059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 04/26/23 06:28:45.037
  STEP: Waiting for the namespace to be removed. @ 04/26/23 06:28:45.046
  E0426 06:28:45.373911      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:46.374052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:47.374635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:48.374810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:49.375260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:50.375437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:51.375741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:52.376384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:53.376996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:54.377271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:55.377349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/26/23 06:28:56.049
  STEP: Verifying there are no pods in the namespace @ 04/26/23 06:28:56.071
  Apr 26 06:28:56.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6852" for this suite. @ 04/26/23 06:28:56.076
  STEP: Destroying namespace "nsdeletetest-8268" for this suite. @ 04/26/23 06:28:56.084
  Apr 26 06:28:56.086: INFO: Namespace nsdeletetest-8268 was already deleted
  STEP: Destroying namespace "nsdeletetest-4309" for this suite. @ 04/26/23 06:28:56.086
• [13.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/26/23 06:28:56.095
  Apr 26 06:28:56.095: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:28:56.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:28:56.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:28:56.122
  STEP: Creating projection with secret that has name projected-secret-test-4a612a6c-d480-43f0-a6f3-f6837bf2b219 @ 04/26/23 06:28:56.125
  STEP: Creating a pod to test consume secrets @ 04/26/23 06:28:56.131
  E0426 06:28:56.378218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:57.378960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:58.379068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:28:59.379174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:29:00.149
  Apr 26 06:29:00.151: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-secrets-32fbd18b-cfe1-40d9-8186-9374eeecd57e container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:29:00.156
  Apr 26 06:29:00.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2327" for this suite. @ 04/26/23 06:29:00.178
• [4.089 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 04/26/23 06:29:00.184
  Apr 26 06:29:00.184: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename daemonsets @ 04/26/23 06:29:00.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:29:00.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:29:00.205
  STEP: Creating simple DaemonSet "daemon-set" @ 04/26/23 06:29:00.221
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/26/23 06:29:00.228
  Apr 26 06:29:00.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 06:29:00.233: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:29:00.380220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:01.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 06:29:01.241: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:29:01.380389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:02.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 06:29:02.240: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/26/23 06:29:02.242
  Apr 26 06:29:02.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 06:29:02.260: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:29:02.381329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:03.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 06:29:03.266: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:29:03.381906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:04.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 06:29:04.267: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:29:04.382372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:05.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 06:29:05.270: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:29:05.383395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:06.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 06:29:06.267: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:29:06.383464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:07.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 06:29:07.267: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/26/23 06:29:07.269
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4493, will wait for the garbage collector to delete the pods @ 04/26/23 06:29:07.269
  Apr 26 06:29:07.330: INFO: Deleting DaemonSet.extensions daemon-set took: 7.834314ms
  E0426 06:29:07.383803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:07.431: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.841291ms
  E0426 06:29:08.384412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:09.385361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:09.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 06:29:09.536: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 26 06:29:09.539: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10600"},"items":null}

  Apr 26 06:29:09.542: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10601"},"items":null}

  Apr 26 06:29:09.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4493" for this suite. @ 04/26/23 06:29:09.553
• [9.376 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/26/23 06:29:09.561
  Apr 26 06:29:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 06:29:09.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:29:09.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:29:09.582
  STEP: creating the pod @ 04/26/23 06:29:09.585
  STEP: submitting the pod to kubernetes @ 04/26/23 06:29:09.585
  E0426 06:29:10.386299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:11.386338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/26/23 06:29:11.606
  STEP: updating the pod @ 04/26/23 06:29:11.609
  Apr 26 06:29:12.121: INFO: Successfully updated pod "pod-update-ecb0396b-d8ea-47e5-8c11-107031416bcb"
  STEP: verifying the updated pod is in kubernetes @ 04/26/23 06:29:12.124
  Apr 26 06:29:12.126: INFO: Pod update OK
  Apr 26 06:29:12.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3877" for this suite. @ 04/26/23 06:29:12.13
• [2.576 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/26/23 06:29:12.137
  Apr 26 06:29:12.137: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:29:12.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:29:12.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:29:12.156
  STEP: Creating configMap with name projected-configmap-test-volume-97255a67-3026-41cd-bc22-2751ba81d79f @ 04/26/23 06:29:12.159
  STEP: Creating a pod to test consume configMaps @ 04/26/23 06:29:12.165
  E0426 06:29:12.386468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:13.387230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:14.387714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:15.387952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:29:16.184
  Apr 26 06:29:16.186: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-configmaps-5ddd151a-9e4f-4df0-9e99-8510145a48b2 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 06:29:16.191
  Apr 26 06:29:16.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-503" for this suite. @ 04/26/23 06:29:16.213
• [4.084 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/26/23 06:29:16.221
  Apr 26 06:29:16.221: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/26/23 06:29:16.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:29:16.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:29:16.244
  STEP: create the container to handle the HTTPGet hook request. @ 04/26/23 06:29:16.249
  E0426 06:29:16.389019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:17.389597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/26/23 06:29:18.267
  E0426 06:29:18.389745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:19.389979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/26/23 06:29:20.282
  STEP: delete the pod with lifecycle hook @ 04/26/23 06:29:20.296
  E0426 06:29:20.390546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:21.391273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:22.392137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:23.392579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:24.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3678" for this suite. @ 04/26/23 06:29:24.315
• [8.103 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/26/23 06:29:24.325
  Apr 26 06:29:24.325: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 06:29:24.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:29:24.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:29:24.351
  STEP: validating api versions @ 04/26/23 06:29:24.354
  Apr 26 06:29:24.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-233 api-versions'
  E0426 06:29:24.392827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:29:24.419: INFO: stderr: ""
  Apr 26 06:29:24.419: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Apr 26 06:29:24.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-233" for this suite. @ 04/26/23 06:29:24.422
• [0.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/26/23 06:29:24.433
  Apr 26 06:29:24.433: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename cronjob @ 04/26/23 06:29:24.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:29:24.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:29:24.454
  STEP: Creating a ForbidConcurrent cronjob @ 04/26/23 06:29:24.456
  STEP: Ensuring a job is scheduled @ 04/26/23 06:29:24.465
  E0426 06:29:25.393229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:26.393460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:27.393749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:28.394168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:29.395218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:30.395539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:31.396116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:32.396843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:33.397547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:34.397807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:35.398722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:36.399261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:37.399333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:38.400404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:39.400814      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:40.401047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:41.401321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:42.402384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:43.403214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:44.404175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:45.404271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:46.404534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:47.404594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:48.405050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:49.405999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:50.406147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:51.406983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:52.407432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:53.408432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:54.408724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:55.409303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:56.409552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:57.410274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:58.410420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:29:59.411304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:00.411515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/26/23 06:30:00.468
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/26/23 06:30:00.471
  STEP: Ensuring no more jobs are scheduled @ 04/26/23 06:30:00.474
  E0426 06:30:01.411895      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:02.411916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:03.412443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:04.412728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:05.412898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:06.413120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:07.413331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:08.413766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:09.413895      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:10.414199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:11.415034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:12.415506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:13.415605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:14.415811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:15.416199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:16.416371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:17.417368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:18.417606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:19.418156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:20.418684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:21.419447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:22.420360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:23.420675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:24.420892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:25.421344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:26.422384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:27.422959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:28.423730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:29.424644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:30.424935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:31.425081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:32.425806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:33.426205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:34.427307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:35.427980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:36.428134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:37.428671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:38.428863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:39.429683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:40.429950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:41.430189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:42.430633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:43.430850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:44.431222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:45.431595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:46.432096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:47.432754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:48.432964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:49.433104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:50.433438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:51.433906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:52.434860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:53.435139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:54.435603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:55.435769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:56.436052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:57.436406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:58.436591      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:30:59.437091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:00.437308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:01.437517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:02.438267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:03.439301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:04.439450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:05.439623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:06.439876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:07.440698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:08.440951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:09.441093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:10.441364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:11.441868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:12.442377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:13.443242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:14.443469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:15.443736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:16.444010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:17.444099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:18.444357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:19.444434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:20.444868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:21.445213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:22.445615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:23.445803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:24.446062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:25.446199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:26.447276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:27.447480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:28.447621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:29.448602      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:30.448834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:31.449428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:32.449665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:33.449846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:34.450797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:35.451757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:36.451983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:37.452413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:38.452656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:39.453645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:40.453989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:41.455001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:42.455554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:43.455678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:44.455909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:45.456264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:46.456511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:47.457094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:48.457332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:49.457931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:50.458165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:51.458425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:52.459091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:53.459877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:54.460138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:55.460496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:56.460772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:57.461383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:58.461659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:31:59.462113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:00.462159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:01.463243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:02.463630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:03.463926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:04.464137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:05.464430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:06.464672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:07.465533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:08.466264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:09.467213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:10.467531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:11.468036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:12.468676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:13.469194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:14.469592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:15.470434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:16.471166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:17.472055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:18.472285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:19.472628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:20.473070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:21.474054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:22.474495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:23.475389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:24.475986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:25.477092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:26.477416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:27.478282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:28.479078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:29.480232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:30.480264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:31.480533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:32.481488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:33.481756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:34.482232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:35.483243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:36.484202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:37.485283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:38.485916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:39.486133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:40.486413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:41.486675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:42.487555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:43.487715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:44.488045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:45.488303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:46.488941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:47.489995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:48.490327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:49.491250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:50.491486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:51.491783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:52.492043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:53.492336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:54.492966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:55.493244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:56.493516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:57.494130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:58.494126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:32:59.494381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:00.495333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:01.495771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:02.495885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:03.496134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:04.496535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:05.496753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:06.497391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:07.497801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:08.498420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:09.498642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:10.499394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:11.499764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:12.499718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:13.500115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:14.500975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:15.501192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:16.501526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:17.502164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:18.502635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:19.502905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:20.502984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:21.503258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:22.503498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:23.503722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:24.504472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:25.504704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:26.505778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:27.506261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:28.506639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:29.506877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:30.507561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:31.508055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:32.508502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:33.508940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:34.509310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:35.509931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:36.510644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:37.511569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:38.511929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:39.512397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:40.512474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:41.512647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:42.513240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:43.513679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:44.514672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:45.514870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:46.515414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:47.515829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:48.516912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:49.517159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:50.517546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:51.517760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:52.518321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:53.519245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:54.519626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:55.519844      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:56.520432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:57.521495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:58.522050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:33:59.522177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:00.523067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:01.523301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:02.523843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:03.524375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:04.524616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:05.524894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:06.525352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:07.526220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:08.527124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:09.527348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:10.527785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:11.528013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:12.528022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:13.528288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:14.529302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:15.529486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:16.530068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:17.530724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:18.531692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:19.531920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:20.532525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:21.532768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:22.533757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:23.534040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:24.535071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:25.535295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:26.535405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:27.535889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:28.536385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:29.536634      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:30.537186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:31.537453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:32.538517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:33.538663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:34.538773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:35.539526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:36.540625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:37.541378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:38.542159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:39.542232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:40.543236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:41.543553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:42.543658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:43.543856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:44.544299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:45.544533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:46.544942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:47.545432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:48.545768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:49.546012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:50.546351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:51.547247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:52.547974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:53.548220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:54.548632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:55.549168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:56.549287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:57.549754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:58.550762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:34:59.550915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/26/23 06:35:00.479
  Apr 26 06:35:00.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9370" for this suite. @ 04/26/23 06:35:00.489
• [336.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/26/23 06:35:00.499
  Apr 26 06:35:00.499: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 06:35:00.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:35:00.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:35:00.528
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/26/23 06:35:00.53
  E0426 06:35:00.551610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:01.551829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:02.552696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:03.552997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:35:04.55
  Apr 26 06:35:04.552: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-60a792b3-ba39-4937-850a-a942f391db1c container test-container: <nil>
  E0426 06:35:04.553046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/26/23 06:35:04.565
  Apr 26 06:35:04.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6244" for this suite. @ 04/26/23 06:35:04.585
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/26/23 06:35:04.592
  Apr 26 06:35:04.593: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 06:35:04.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:35:04.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:35:04.611
  Apr 26 06:35:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 06:35:05.554149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/26/23 06:35:06.045
  Apr 26 06:35:06.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-9186 --namespace=crd-publish-openapi-9186 create -f -'
  E0426 06:35:06.555270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:35:06.758: INFO: stderr: ""
  Apr 26 06:35:06.758: INFO: stdout: "e2e-test-crd-publish-openapi-1359-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 26 06:35:06.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-9186 --namespace=crd-publish-openapi-9186 delete e2e-test-crd-publish-openapi-1359-crds test-cr'
  Apr 26 06:35:06.868: INFO: stderr: ""
  Apr 26 06:35:06.868: INFO: stdout: "e2e-test-crd-publish-openapi-1359-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 26 06:35:06.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-9186 --namespace=crd-publish-openapi-9186 apply -f -'
  Apr 26 06:35:07.552: INFO: stderr: ""
  Apr 26 06:35:07.552: INFO: stdout: "e2e-test-crd-publish-openapi-1359-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 26 06:35:07.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-9186 --namespace=crd-publish-openapi-9186 delete e2e-test-crd-publish-openapi-1359-crds test-cr'
  E0426 06:35:07.555287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:35:07.628: INFO: stderr: ""
  Apr 26 06:35:07.628: INFO: stdout: "e2e-test-crd-publish-openapi-1359-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/26/23 06:35:07.628
  Apr 26 06:35:07.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-9186 explain e2e-test-crd-publish-openapi-1359-crds'
  Apr 26 06:35:07.896: INFO: stderr: ""
  Apr 26 06:35:07.896: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-1359-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0426 06:35:08.556087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:09.556845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:35:09.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9186" for this suite. @ 04/26/23 06:35:09.724
• [5.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/26/23 06:35:09.733
  Apr 26 06:35:09.733: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 06:35:09.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:35:09.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:35:09.905
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/26/23 06:35:09.908
  Apr 26 06:35:09.908: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 06:35:10.556967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:11.558017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:35:11.722: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 06:35:12.558547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:13.559168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:14.559915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:15.560083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:16.561007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:35:17.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7625" for this suite. @ 04/26/23 06:35:17.394
• [7.669 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/26/23 06:35:17.402
  Apr 26 06:35:17.402: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-pred @ 04/26/23 06:35:17.403
  E0426 06:35:17.561817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:18.562111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:35:18.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:35:18.904
  Apr 26 06:35:18.906: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 26 06:35:18.913: INFO: Waiting for terminating namespaces to be deleted...
  Apr 26 06:35:18.916: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-91 before test
  Apr 26 06:35:18.924: INFO: calico-node-d6hf2 from kube-system started at 2023-04-26 06:03:27 +0000 UTC (1 container statuses recorded)
  Apr 26 06:35:18.924: INFO: 	Container calico-node ready: true, restart count 0
  Apr 26 06:35:18.924: INFO: sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-wbxsz from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:35:18.924: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:35:18.924: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 26 06:35:18.924: INFO: sonobuoy-e2e-job-5266529d9a654ea5 from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:35:18.924: INFO: 	Container e2e ready: true, restart count 0
  Apr 26 06:35:18.924: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:35:18.924: INFO: sonobuoy from sonobuoy started at 2023-04-26 06:05:24 +0000 UTC (1 container statuses recorded)
  Apr 26 06:35:18.924: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 26 06:35:18.924: INFO: coredns-7745f9f87f-hgdql from kube-system started at 2023-04-26 06:06:48 +0000 UTC (1 container statuses recorded)
  Apr 26 06:35:18.924: INFO: 	Container coredns ready: true, restart count 0
  Apr 26 06:35:18.924: INFO: calico-kube-controllers-6c99c8747f-plv8f from kube-system started at 2023-04-26 06:06:48 +0000 UTC (1 container statuses recorded)
  Apr 26 06:35:18.924: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Apr 26 06:35:18.924: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-3-127 before test
  Apr 26 06:35:18.931: INFO: calico-node-578zf from kube-system started at 2023-04-26 06:02:34 +0000 UTC (1 container statuses recorded)
  Apr 26 06:35:18.931: INFO: 	Container calico-node ready: true, restart count 0
  Apr 26 06:35:18.931: INFO: sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-ndzdn from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:35:18.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:35:18.931: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/26/23 06:35:18.931
  E0426 06:35:19.562169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:20.562290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/26/23 06:35:20.953
  STEP: Trying to apply a random label on the found node. @ 04/26/23 06:35:21.02
  STEP: verifying the node has the label kubernetes.io/e2e-2679a7cd-a906-4cf3-98d8-96197fef709c 95 @ 04/26/23 06:35:21.031
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/26/23 06:35:21.034
  E0426 06:35:21.563268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:22.563595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:23.564200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:24.564479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.3.127 on the node which pod4 resides and expect not scheduled @ 04/26/23 06:35:25.067
  E0426 06:35:25.565470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:26.565619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:27.566700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:28.567346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:29.567386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:30.567606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:31.567832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:32.568567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:33.569564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:34.569948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:35.570821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:36.571285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:37.571356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:38.571572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:39.571985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:40.572281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:41.573066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:42.573780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:43.574128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:44.574415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:45.575123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:46.575345      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:47.576279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:48.576553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:49.577686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:50.578062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:51.578699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:52.579297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:53.580081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:54.580299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:55.581348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:56.581551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:57.582358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:58.582535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:35:59.583245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:00.583487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:01.583959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:02.584634      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:03.584869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:04.584999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:05.586129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:06.586412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:07.586533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:08.586770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:09.587806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:10.588053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:11.588906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:12.589586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:13.590318      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:14.591256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:15.592120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:16.592297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:17.592967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:18.593230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:19.594049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:20.594148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:21.595160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:22.595795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:23.596722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:24.596972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:25.597018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:26.597266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:27.598196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:28.598428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:29.599255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:30.599448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:31.599709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:32.600381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:33.601298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:34.601599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:35.602164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:36.602205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:37.602629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:38.603284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:39.604022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:40.604258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:41.604636      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:42.604897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:43.605598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:44.605846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:45.606900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:46.607147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:47.607577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:48.607829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:49.608507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:50.608765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:51.609123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:52.609797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:53.610688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:54.611303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:55.611954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:56.612244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:57.612973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:58.613239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:36:59.613890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:00.614133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:01.615003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:02.615702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:03.616640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:04.616890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:05.617846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:06.618625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:07.619625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:08.619897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:09.620712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:10.620967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:11.621898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:12.622646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:13.623754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:14.623939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:15.624576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:16.624837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:17.626031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:18.626286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:19.627165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:20.627409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:21.627664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:22.628270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:23.628927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:24.629076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:25.629299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:26.629438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:27.630449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:28.630708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:29.631263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:30.631454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:31.631559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:32.632275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:33.632945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:34.633235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:35.634019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:36.634135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:37.634796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:38.634933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:39.635812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:40.635954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:41.636543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:42.637315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:43.638019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:44.638291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:45.639280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:46.639369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:47.639413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:48.639555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:49.640181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:50.640307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:51.640449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:52.641235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:53.642124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:54.642156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:55.643097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:56.643310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:57.644034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:58.644303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:37:59.644906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:00.645415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:01.645495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:02.646254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:03.647062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:04.647478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:05.648127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:06.648218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:07.648430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:08.648671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:09.649708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:10.650061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:11.650448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:12.650450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:13.651227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:14.651403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:15.651762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:16.652060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:17.652737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:18.652999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:19.653866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:20.654136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:21.654184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:22.654752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:23.655609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:24.655880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:25.656968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:26.657217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:27.658248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:28.659281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:29.659919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:30.660145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:31.660955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:32.661583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:33.662016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:34.662180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:35.662599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:36.663272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:37.663854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:38.664055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:39.664969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:40.665210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:41.666067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:42.666688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:43.667301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:44.667541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:45.667675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:46.667921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:47.668385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:48.668632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:49.669620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:50.669828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:51.670692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:52.671370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:53.672332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:54.672580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:55.673539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:56.673742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:57.674091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:58.674154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:38:59.674196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:00.675257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:01.676094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:02.676258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:03.677213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:04.677561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:05.678389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:06.678610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:07.679281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:08.679769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:09.680515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:10.680743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:11.681829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:12.682407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:13.682799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:14.683455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:15.684380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:16.684533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:17.685190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:18.685494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:19.686378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:20.686649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:21.687650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:22.687832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:23.688561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:24.689677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:25.690518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:26.691234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:27.691353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:28.691522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:29.692234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:30.693225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:31.693498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:32.693643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:33.694377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:34.695197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:35.695719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:36.696781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:37.697504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:38.697720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:39.698709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:40.698861      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:41.699915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:42.700511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:43.700884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:44.701106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:45.701870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:46.702091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:47.702507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:48.703237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:49.703902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:50.704126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:51.704707      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:52.705316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:53.705948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:54.706178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:55.707223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:56.707459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:57.707626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:58.707872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:39:59.707934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:00.708356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:01.709300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:02.709871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:03.710095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:04.710321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:05.711187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:06.711420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:07.711725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:08.711966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:09.712095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:10.712667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:11.712898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:12.713397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:13.713603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:14.714169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:15.714399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:16.715245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:17.716098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:18.716758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:19.716888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:20.717181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:21.717669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:22.718671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:23.718803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:24.719726      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-2679a7cd-a906-4cf3-98d8-96197fef709c off the node ip-172-31-3-127 @ 04/26/23 06:40:25.082
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-2679a7cd-a906-4cf3-98d8-96197fef709c @ 04/26/23 06:40:25.099
  Apr 26 06:40:25.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6814" for this suite. @ 04/26/23 06:40:25.117
• [307.733 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/26/23 06:40:25.137
  Apr 26 06:40:25.137: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename gc @ 04/26/23 06:40:25.137
  E0426 06:40:25.720714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:40:25.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:40:25.843
  STEP: create the rc @ 04/26/23 06:40:25.846
  W0426 06:40:25.853828      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0426 06:40:26.720782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:27.721821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:28.721939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:29.722214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:30.722344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/26/23 06:40:30.858
  STEP: wait for all pods to be garbage collected @ 04/26/23 06:40:30.867
  E0426 06:40:31.723210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:32.723939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:33.724060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:34.724463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:35.724697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/26/23 06:40:35.876
  W0426 06:40:35.880691      22 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 26 06:40:35.880: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 26 06:40:35.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5725" for this suite. @ 04/26/23 06:40:35.884
• [10.756 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/26/23 06:40:35.893
  Apr 26 06:40:35.893: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 06:40:35.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:40:36.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:40:36.226
  E0426 06:40:36.725048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 04/26/23 06:40:36.832
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 06:40:37.227
  STEP: Deploying the webhook pod @ 04/26/23 06:40:37.24
  STEP: Wait for the deployment to be ready @ 04/26/23 06:40:37.257
  Apr 26 06:40:37.264: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 06:40:37.725181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:38.725400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 06:40:39.274
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:40:39.31
  E0426 06:40:39.725554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:40:40.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/26/23 06:40:40.313
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/26/23 06:40:40.331
  Apr 26 06:40:40.331: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:40:40.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2759" for this suite. @ 04/26/23 06:40:40.404
  STEP: Destroying namespace "webhook-markers-3194" for this suite. @ 04/26/23 06:40:40.413
• [4.529 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/26/23 06:40:40.424
  Apr 26 06:40:40.424: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename proxy @ 04/26/23 06:40:40.424
  E0426 06:40:40.726163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:40:41.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:40:41.259
  Apr 26 06:40:41.261: INFO: Creating pod...
  E0426 06:40:41.726963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:42.727650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:40:43.279: INFO: Creating service...
  Apr 26 06:40:43.296: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/pods/agnhost/proxy/some/path/with/DELETE
  Apr 26 06:40:43.299: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 26 06:40:43.299: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/pods/agnhost/proxy/some/path/with/GET
  Apr 26 06:40:43.303: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 26 06:40:43.303: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/pods/agnhost/proxy/some/path/with/HEAD
  Apr 26 06:40:43.306: INFO: http.Client request:HEAD | StatusCode:200
  Apr 26 06:40:43.306: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 26 06:40:43.308: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 26 06:40:43.309: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/pods/agnhost/proxy/some/path/with/PATCH
  Apr 26 06:40:43.311: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 26 06:40:43.311: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/pods/agnhost/proxy/some/path/with/POST
  Apr 26 06:40:43.314: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 26 06:40:43.314: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/pods/agnhost/proxy/some/path/with/PUT
  Apr 26 06:40:43.317: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 26 06:40:43.317: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/services/test-service/proxy/some/path/with/DELETE
  Apr 26 06:40:43.319: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.332: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.342: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.351: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.362: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.372: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.382: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.392: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.402: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.412: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.422: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.432: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.442: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.451: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.462: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.472: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.481: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.492: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.502: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.511: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.522: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.532: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.542: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.552: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.562: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.572: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.581: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.592: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.602: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.611: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.622: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.631: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.642: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.652: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.662: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.672: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.682: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.692: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.701: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.712: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.722: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  E0426 06:40:43.728663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:40:43.732: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.742: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.751: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.762: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.772: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.782: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.792: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.803: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
  Apr 26 06:40:43.814: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 26 06:40:43.814: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/services/test-service/proxy/some/path/with/GET
  Apr 26 06:40:43.819: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 26 06:40:43.819: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/services/test-service/proxy/some/path/with/HEAD
  Apr 26 06:40:43.824: INFO: http.Client request:HEAD | StatusCode:200
  Apr 26 06:40:43.824: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/services/test-service/proxy/some/path/with/OPTIONS
  Apr 26 06:40:43.828: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 26 06:40:43.828: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/services/test-service/proxy/some/path/with/PATCH
  Apr 26 06:40:43.832: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 26 06:40:43.832: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/services/test-service/proxy/some/path/with/POST
  Apr 26 06:40:43.836: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 26 06:40:43.837: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7842/services/test-service/proxy/some/path/with/PUT
  Apr 26 06:40:43.845: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 26 06:40:43.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7842" for this suite. @ 04/26/23 06:40:43.848
• [3.433 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/26/23 06:40:43.857
  Apr 26 06:40:43.857: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:40:43.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:40:44.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:40:44.494
  STEP: Counting existing ResourceQuota @ 04/26/23 06:40:44.497
  E0426 06:40:44.729450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:45.730192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:46.731240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:47.731357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:48.731435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/26/23 06:40:49.502
  STEP: Ensuring resource quota status is calculated @ 04/26/23 06:40:49.51
  E0426 06:40:49.732429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:50.732579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 04/26/23 06:40:51.514
  STEP: Creating a NodePort Service @ 04/26/23 06:40:51.535
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/26/23 06:40:51.563
  STEP: Ensuring resource quota status captures service creation @ 04/26/23 06:40:51.592
  E0426 06:40:51.733261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:52.733990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 04/26/23 06:40:53.596
  STEP: Ensuring resource quota status released usage @ 04/26/23 06:40:53.665
  E0426 06:40:53.734344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:54.735296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:40:55.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2722" for this suite. @ 04/26/23 06:40:55.673
• [11.825 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/26/23 06:40:55.684
  Apr 26 06:40:55.684: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:40:55.684
  E0426 06:40:55.735807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:40:56.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:40:56.365
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:40:56.368
  E0426 06:40:56.735931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:57.736834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:58.737658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:40:59.737857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:41:00.389
  Apr 26 06:41:00.393: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-eeabcd67-538f-4c24-b733-0f3df3643f11 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:41:00.409
  Apr 26 06:41:00.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-896" for this suite. @ 04/26/23 06:41:00.434
• [4.760 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/26/23 06:41:00.444
  Apr 26 06:41:00.444: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename deployment @ 04/26/23 06:41:00.445
  E0426 06:41:00.738617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:41:00.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:41:00.876
  Apr 26 06:41:00.878: INFO: Creating simple deployment test-new-deployment
  Apr 26 06:41:00.895: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
  E0426 06:41:01.738761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:02.739244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 04/26/23 06:41:02.907
  STEP: updating a scale subresource @ 04/26/23 06:41:02.91
  STEP: verifying the deployment Spec.Replicas was modified @ 04/26/23 06:41:02.921
  STEP: Patch a scale subresource @ 04/26/23 06:41:02.924
  Apr 26 06:41:02.941: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-4296  7ceae195-0dd4-4cea-8fb8-e7e4a95fb5cf 12343 3 2023-04-26 06:41:00 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-26 06:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004856da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 06:41:01 +0000 UTC,LastTransitionTime:2023-04-26 06:41:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-04-26 06:41:01 +0000 UTC,LastTransitionTime:2023-04-26 06:41:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 26 06:41:02.944: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-4296  4302dde5-bd13-42a0-ab80-b08f46f45e9d 12337 1 2023-04-26 06:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 7ceae195-0dd4-4cea-8fb8-e7e4a95fb5cf 0xc004eec4e7 0xc004eec4e8}] [] [{kubelite Update apps/v1 2023-04-26 06:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ceae195-0dd4-4cea-8fb8-e7e4a95fb5cf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:41:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eec6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 06:41:02.947: INFO: Pod "test-new-deployment-67bd4bf6dc-fvbqj" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-fvbqj test-new-deployment-67bd4bf6dc- deployment-4296  8bae463e-1e9c-4c84-a6cf-252258c2f45f 12336 0 2023-04-26 06:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:45ca1d4668bb720e64a117ed6586446cc69d811377f832c2bfd4a87076ef08ea cni.projectcalico.org/podIP:10.1.96.45/32 cni.projectcalico.org/podIPs:10.1.96.45/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 4302dde5-bd13-42a0-ab80-b08f46f45e9d 0xc004eecb37 0xc004eecb38}] [] [{kubelite Update v1 2023-04-26 06:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4302dde5-bd13-42a0-ab80-b08f46f45e9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:41:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jxww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jxww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:41:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:41:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:41:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:41:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.45,StartTime:2023-04-26 06:41:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:41:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e790c743c9719b609d8c80e491c04289d76ce4352e0914d9390b339405fc13a0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.45,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:41:02.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4296" for this suite. @ 04/26/23 06:41:02.951
• [2.522 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/26/23 06:41:02.968
  Apr 26 06:41:02.968: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 06:41:02.969
  E0426 06:41:03.740050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:41:03.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:41:03.904
  STEP: Creating pod test-grpc-e4d1b820-534e-4f66-bc16-a6b774d057c7 in namespace container-probe-3828 @ 04/26/23 06:41:03.907
  E0426 06:41:04.740293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:05.740528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:06.740901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:07.741261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:41:07.932: INFO: Started pod test-grpc-e4d1b820-534e-4f66-bc16-a6b774d057c7 in namespace container-probe-3828
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/26/23 06:41:07.932
  Apr 26 06:41:07.934: INFO: Initial restart count of pod test-grpc-e4d1b820-534e-4f66-bc16-a6b774d057c7 is 0
  E0426 06:41:08.741379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:09.741693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:10.741821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:11.742166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:12.742984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:13.743262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:14.743323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:15.743550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:16.743744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:17.743912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:18.744706      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:19.745609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:20.746138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:21.746172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:22.747197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:23.748081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:24.748540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:25.748736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:26.749350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:27.750452      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:28.750596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:29.751253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:30.751460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:31.751757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:32.752277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:33.752868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:34.753239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:35.754304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:36.755098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:37.756154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:38.756874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:39.757114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:40.757833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:41.758705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:42.758952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:43.758976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:44.759161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:45.759702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:46.759848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:47.760191      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:48.760331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:49.760633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:50.761728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:51.761955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:52.762764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:53.763231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:54.763839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:55.764102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:56.764708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:57.765076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:58.765763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:41:59.766012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:00.767029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:01.767235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:02.767426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:03.767640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:04.768587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:05.768803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:06.769432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:07.769849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:08.770152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:09.770184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:10.770329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:11.771245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:12.772089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:13.772346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:14.772960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:15.773176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:16.773402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:17.773759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:18.773892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:19.774159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:20.775248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:21.775582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:22.776045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:23.776313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:24.776876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:25.777094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:26.778073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:27.778296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:28.779202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:29.780249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:30.780431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:31.780713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:32.781561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:33.781810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:34.782046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:35.782199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:36.782649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:37.782750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:38.783290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:39.783569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:40.784255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:41.784493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:42.784666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:43.784926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:44.785056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:45.785325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:46.786202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:47.786798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:48.787565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:49.787672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:50.787729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:51.787994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:52.788452      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:53.788760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:54.789205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:55.789663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:56.790083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:57.790608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:58.791307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:42:59.791579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:00.792661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:01.792922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:02.793367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:03.793625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:04.793921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:05.794147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:06.795246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:07.795674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:08.796767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:09.796880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:10.797755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:11.798741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:12.799008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:13.799271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:14.799741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:15.800006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:16.801151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:17.801526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:18.802092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:19.802205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:20.803049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:21.803320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:22.803432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:23.803705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:24.804175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:25.804407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:26.804724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:27.805127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:28.805492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:29.805746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:30.806493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:31.807264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:32.807414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:33.807647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:34.808472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:35.809556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:36.810449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:37.810810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:38.811256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:39.811424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:40.812467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:41.812729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:42.812788      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:43.812986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:44.813225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:45.813513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:46.814412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:47.815489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:48.815614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:49.815873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:50.816702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:51.816903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:52.817098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:53.817321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:54.817488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:55.817641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:56.817777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:57.818920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:58.819333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:43:59.819512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:00.820363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:01.820634      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:02.821514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:03.821810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:04.822000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:05.822135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:06.822909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:07.823309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:08.824277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:09.824459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:10.824527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:11.824694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:12.825159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:13.825469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:14.826353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:15.827436      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:16.828457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:17.829528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:18.830597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:19.830838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:20.831278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:21.831558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:22.832598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:23.832866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:24.833350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:25.834133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:26.835109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:27.835202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:28.836158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:29.836393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:30.836511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:31.836780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:32.837623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:33.837789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:34.838120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:35.838158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:36.838778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:37.838966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:38.840066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:39.840330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:40.841299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:41.841542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:42.842584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:43.843308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:44.843763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:45.844011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:46.844740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:47.844801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:48.844943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:49.845322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:50.845356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:51.845593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:52.846278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:53.846551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:54.846985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:55.847222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:56.847975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:57.848208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:58.848955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:44:59.849200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:00.850253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:01.851249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:02.851925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:03.852194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:04.852864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:05.853011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:06.853878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:07.854148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:45:08.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 06:45:08.513
  STEP: Destroying namespace "container-probe-3828" for this suite. @ 04/26/23 06:45:08.53
• [245.571 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/26/23 06:45:08.54
  Apr 26 06:45:08.540: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/26/23 06:45:08.541
  E0426 06:45:08.854343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:08.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:08.905
  STEP: creating a target pod @ 04/26/23 06:45:08.907
  E0426 06:45:09.855251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:10.855529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 04/26/23 06:45:10.929
  E0426 06:45:11.856638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:12.857391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:13.857581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:14.857851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 04/26/23 06:45:14.957
  Apr 26 06:45:14.957: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9055 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:45:14.957: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:45:14.957: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:45:14.958: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-9055/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Apr 26 06:45:15.046: INFO: Exec stderr: ""
  Apr 26 06:45:15.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-9055" for this suite. @ 04/26/23 06:45:15.076
• [6.546 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/26/23 06:45:15.086
  Apr 26 06:45:15.086: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 06:45:15.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:15.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:15.487
  STEP: creating Agnhost RC @ 04/26/23 06:45:15.489
  Apr 26 06:45:15.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2369 create -f -'
  E0426 06:45:15.858208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:45:16.258: INFO: stderr: ""
  Apr 26 06:45:16.259: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/26/23 06:45:16.259
  E0426 06:45:16.858945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:45:17.262: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 06:45:17.262: INFO: Found 0 / 1
  E0426 06:45:17.859401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:45:18.262: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 06:45:18.262: INFO: Found 1 / 1
  Apr 26 06:45:18.262: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/26/23 06:45:18.262
  Apr 26 06:45:18.265: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 06:45:18.265: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 26 06:45:18.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2369 patch pod agnhost-primary-76bw8 -p {"metadata":{"annotations":{"x":"y"}}}'
  Apr 26 06:45:18.340: INFO: stderr: ""
  Apr 26 06:45:18.340: INFO: stdout: "pod/agnhost-primary-76bw8 patched\n"
  STEP: checking annotations @ 04/26/23 06:45:18.34
  Apr 26 06:45:18.344: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 06:45:18.344: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 26 06:45:18.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2369" for this suite. @ 04/26/23 06:45:18.348
• [3.291 seconds]
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/26/23 06:45:18.377
  Apr 26 06:45:18.377: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 06:45:18.379
  E0426 06:45:18.859810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:18.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:18.904
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:45:18.906
  E0426 06:45:19.860514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:20.860888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:21.861451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:22.862285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:45:22.931
  Apr 26 06:45:22.934: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-43e22e27-ec2c-4ac5-b20d-d5abd5177909 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:45:22.941
  Apr 26 06:45:22.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6373" for this suite. @ 04/26/23 06:45:22.975
• [4.607 seconds]
------------------------------
SS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/26/23 06:45:22.985
  Apr 26 06:45:22.985: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename podtemplate @ 04/26/23 06:45:22.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:23.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:23.397
  Apr 26 06:45:23.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3411" for this suite. @ 04/26/23 06:45:23.441
• [0.465 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/26/23 06:45:23.451
  Apr 26 06:45:23.451: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename endpointslice @ 04/26/23 06:45:23.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:23.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:23.633
  STEP: getting /apis @ 04/26/23 06:45:23.635
  STEP: getting /apis/discovery.k8s.io @ 04/26/23 06:45:23.639
  STEP: getting /apis/discovery.k8s.iov1 @ 04/26/23 06:45:23.64
  STEP: creating @ 04/26/23 06:45:23.641
  STEP: getting @ 04/26/23 06:45:23.66
  STEP: listing @ 04/26/23 06:45:23.662
  STEP: watching @ 04/26/23 06:45:23.665
  Apr 26 06:45:23.665: INFO: starting watch
  STEP: cluster-wide listing @ 04/26/23 06:45:23.666
  STEP: cluster-wide watching @ 04/26/23 06:45:23.668
  Apr 26 06:45:23.668: INFO: starting watch
  STEP: patching @ 04/26/23 06:45:23.669
  STEP: updating @ 04/26/23 06:45:23.676
  Apr 26 06:45:23.684: INFO: waiting for watch events with expected annotations
  Apr 26 06:45:23.684: INFO: saw patched and updated annotations
  STEP: deleting @ 04/26/23 06:45:23.684
  STEP: deleting a collection @ 04/26/23 06:45:23.698
  Apr 26 06:45:23.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2379" for this suite. @ 04/26/23 06:45:23.72
• [0.278 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/26/23 06:45:23.729
  Apr 26 06:45:23.729: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename podtemplate @ 04/26/23 06:45:23.73
  E0426 06:45:23.862854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:23.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:23.905
  STEP: Create set of pod templates @ 04/26/23 06:45:23.907
  Apr 26 06:45:23.915: INFO: created test-podtemplate-1
  Apr 26 06:45:23.921: INFO: created test-podtemplate-2
  Apr 26 06:45:23.927: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/26/23 06:45:23.927
  STEP: delete collection of pod templates @ 04/26/23 06:45:23.93
  Apr 26 06:45:23.930: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/26/23 06:45:23.953
  Apr 26 06:45:23.953: INFO: requesting list of pod templates to confirm quantity
  Apr 26 06:45:23.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6907" for this suite. @ 04/26/23 06:45:23.96
• [0.238 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/26/23 06:45:23.968
  Apr 26 06:45:23.968: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename deployment @ 04/26/23 06:45:23.969
  E0426 06:45:24.863039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:24.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:24.919
  Apr 26 06:45:24.921: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 26 06:45:24.933: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0426 06:45:25.863330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:26.863733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:27.864503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:28.864993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:29.865354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:45:29.938: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/26/23 06:45:29.938
  Apr 26 06:45:29.938: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 26 06:45:29.946: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 26 06:45:29.951: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0426 06:45:30.866435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:31.866648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:45:31.957: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 26 06:45:31.960: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 26 06:45:31.968: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2200  8b7baec9-59d4-4cde-b5ee-84c416472945 13023 1 2023-04-26 06:45:29 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-26 06:45:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:45:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002780678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 06:45:30 +0000 UTC,LastTransitionTime:2023-04-26 06:45:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-04-26 06:45:31 +0000 UTC,LastTransitionTime:2023-04-26 06:45:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 26 06:45:31.971: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-2200  0033447b-e241-47ff-9f96-29865eb5c0b7 13013 1 2023-04-26 06:45:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 8b7baec9-59d4-4cde-b5ee-84c416472945 0xc00494ee07 0xc00494ee08}] [] [{kubelite Update apps/v1 2023-04-26 06:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b7baec9-59d4-4cde-b5ee-84c416472945\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:45:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00494eed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 06:45:31.971: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 26 06:45:31.971: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2200  fd7fe9e5-38aa-48b1-abac-65d38033df67 13022 2 2023-04-26 06:45:24 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 8b7baec9-59d4-4cde-b5ee-84c416472945 0xc00494ef37 0xc00494ef38}] [] [{e2e.test Update apps/v1 2023-04-26 06:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:45:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b7baec9-59d4-4cde-b5ee-84c416472945\"}":{}}},"f:spec":{"f:replicas":{}}} } {kubelite Update apps/v1 2023-04-26 06:45:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00494f0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 06:45:31.974: INFO: Pod "test-rolling-update-deployment-656d657cd8-jx8sg" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-jx8sg test-rolling-update-deployment-656d657cd8- deployment-2200  ab2e6bab-f095-4a3a-83e7-b558b844d214 13012 0 2023-04-26 06:45:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:8feb9f4193f8e49db976de39d05b10208253f8dde48f9a513a4c6de66a356d50 cni.projectcalico.org/podIP:10.1.96.55/32 cni.projectcalico.org/podIPs:10.1.96.55/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 0033447b-e241-47ff-9f96-29865eb5c0b7 0xc004d020c7 0xc004d020c8}] [] [{calico Update v1 2023-04-26 06:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0033447b-e241-47ff-9f96-29865eb5c0b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:45:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mm9nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mm9nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:45:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:45:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:45:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.55,StartTime:2023-04-26 06:45:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:45:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://9bd7f13fb5f497098e676ef936cc0037a66c1ca82f713d0f13202fa5323b3e84,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.55,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:45:31.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2200" for this suite. @ 04/26/23 06:45:31.977
• [8.018 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/26/23 06:45:31.987
  Apr 26 06:45:31.987: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename svcaccounts @ 04/26/23 06:45:31.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:32.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:32.417
  STEP: Creating a pod to test service account token:  @ 04/26/23 06:45:32.419
  E0426 06:45:32.867298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:33.867623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:34.868696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:35.869375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:45:36.443
  Apr 26 06:45:36.446: INFO: Trying to get logs from node ip-172-31-3-127 pod test-pod-e5ddef92-b1c9-4f82-b5be-a6499b123986 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 06:45:36.452
  Apr 26 06:45:36.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2606" for this suite. @ 04/26/23 06:45:36.473
• [4.494 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/26/23 06:45:36.481
  Apr 26 06:45:36.481: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename field-validation @ 04/26/23 06:45:36.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:36.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:36.831
  STEP: apply creating a deployment @ 04/26/23 06:45:36.833
  Apr 26 06:45:36.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8792" for this suite. @ 04/26/23 06:45:36.847
• [0.374 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/26/23 06:45:36.856
  Apr 26 06:45:36.856: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubelet-test @ 04/26/23 06:45:36.857
  E0426 06:45:36.869664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:37.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:37.419
  STEP: Waiting for pod completion @ 04/26/23 06:45:37.433
  E0426 06:45:37.870232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:38.871274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:39.871284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:40.871532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:45:41.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3888" for this suite. @ 04/26/23 06:45:41.453
• [4.605 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/26/23 06:45:41.462
  Apr 26 06:45:41.462: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:45:41.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:41.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:41.64
  STEP: Counting existing ResourceQuota @ 04/26/23 06:45:41.642
  E0426 06:45:41.871721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:42.872853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:43.873586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:44.874594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:45.875228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/26/23 06:45:46.648
  STEP: Ensuring resource quota status is calculated @ 04/26/23 06:45:46.658
  E0426 06:45:46.875909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:47.876992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 04/26/23 06:45:48.662
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/26/23 06:45:48.684
  E0426 06:45:48.877961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:49.878178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/26/23 06:45:50.688
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/26/23 06:45:50.69
  STEP: Ensuring a pod cannot update its resource requirements @ 04/26/23 06:45:50.692
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/26/23 06:45:50.696
  E0426 06:45:50.878577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:51.878842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/26/23 06:45:52.7
  STEP: Ensuring resource quota status released the pod usage @ 04/26/23 06:45:52.717
  E0426 06:45:52.879496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:53.879904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:45:54.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9140" for this suite. @ 04/26/23 06:45:54.726
• [13.272 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/26/23 06:45:54.735
  Apr 26 06:45:54.735: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 06:45:54.735
  E0426 06:45:54.880986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:55.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:55.274
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/26/23 06:45:55.276
  E0426 06:45:55.882014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:56.882140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:57.883112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:45:58.883373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:45:59.3
  Apr 26 06:45:59.303: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-8a8e4557-67e2-4f2c-bcaa-8a627076a138 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:45:59.309
  Apr 26 06:45:59.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-373" for this suite. @ 04/26/23 06:45:59.332
• [4.605 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/26/23 06:45:59.343
  Apr 26 06:45:59.343: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename ingressclass @ 04/26/23 06:45:59.344
  E0426 06:45:59.883639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:45:59.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:45:59.905
  STEP: getting /apis @ 04/26/23 06:45:59.907
  STEP: getting /apis/networking.k8s.io @ 04/26/23 06:45:59.91
  STEP: getting /apis/networking.k8s.iov1 @ 04/26/23 06:45:59.911
  STEP: creating @ 04/26/23 06:45:59.912
  STEP: getting @ 04/26/23 06:45:59.933
  STEP: listing @ 04/26/23 06:45:59.935
  STEP: watching @ 04/26/23 06:45:59.938
  Apr 26 06:45:59.938: INFO: starting watch
  STEP: patching @ 04/26/23 06:45:59.939
  STEP: updating @ 04/26/23 06:45:59.946
  Apr 26 06:45:59.952: INFO: waiting for watch events with expected annotations
  Apr 26 06:45:59.952: INFO: saw patched and updated annotations
  STEP: deleting @ 04/26/23 06:45:59.952
  STEP: deleting a collection @ 04/26/23 06:45:59.965
  Apr 26 06:45:59.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-9850" for this suite. @ 04/26/23 06:45:59.988
• [0.653 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/26/23 06:45:59.996
  Apr 26 06:45:59.996: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename job @ 04/26/23 06:45:59.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:46:00.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:46:00.308
  STEP: Creating a job @ 04/26/23 06:46:00.311
  STEP: Ensuring job reaches completions @ 04/26/23 06:46:00.333
  E0426 06:46:00.883791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:01.883914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:02.884139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:03.885055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:04.885800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:05.886067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:06.887082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:07.887456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:08.888135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:09.889194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:46:10.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4451" for this suite. @ 04/26/23 06:46:10.341
• [10.353 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/26/23 06:46:10.35
  Apr 26 06:46:10.350: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 06:46:10.351
  E0426 06:46:10.890054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:46:10.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:46:10.904
  STEP: Creating secret with name secret-test-98e34884-cb40-4bbf-bc3e-90201783d0c9 @ 04/26/23 06:46:10.906
  STEP: Creating a pod to test consume secrets @ 04/26/23 06:46:10.915
  E0426 06:46:11.890214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:12.890411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:13.891282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:14.891509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:46:14.937
  Apr 26 06:46:14.939: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-secrets-2ae51403-9f0a-4aae-8c67-5b683b332e0d container secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:46:14.946
  Apr 26 06:46:14.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8706" for this suite. @ 04/26/23 06:46:14.969
• [4.627 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/26/23 06:46:14.981
  Apr 26 06:46:14.981: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 06:46:14.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:46:15.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:46:15.312
  STEP: Creating configMap with name configmap-test-volume-09cc5d21-88a5-4f04-8a8f-2ccb1eaf9bd4 @ 04/26/23 06:46:15.314
  STEP: Creating a pod to test consume configMaps @ 04/26/23 06:46:15.321
  E0426 06:46:15.892594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:16.892859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:17.893947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:18.894155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:46:19.343
  Apr 26 06:46:19.346: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-f6e491de-9512-4a69-a72b-07e7963bf3a7 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 06:46:19.352
  Apr 26 06:46:19.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8415" for this suite. @ 04/26/23 06:46:19.376
• [4.403 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/26/23 06:46:19.385
  Apr 26 06:46:19.385: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename podtemplate @ 04/26/23 06:46:19.386
  E0426 06:46:19.895179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:46:19.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:46:19.904
  STEP: Create a pod template @ 04/26/23 06:46:19.906
  STEP: Replace a pod template @ 04/26/23 06:46:19.913
  Apr 26 06:46:19.923: INFO: Found updated podtemplate annotation: "true"

  Apr 26 06:46:19.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6303" for this suite. @ 04/26/23 06:46:19.927
• [0.550 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/26/23 06:46:19.935
  Apr 26 06:46:19.935: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename var-expansion @ 04/26/23 06:46:19.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:46:20.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:46:20.713
  STEP: creating the pod with failed condition @ 04/26/23 06:46:20.715
  E0426 06:46:20.895284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:21.895510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:22.895973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:23.896178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:24.896663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:25.896947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:26.897526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:27.898633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:28.899737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:29.899989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:30.901105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:31.901245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:32.901739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:33.902082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:34.902122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:35.903203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:36.904340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:37.904776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:38.905555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:39.905748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:40.906587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:41.907259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:42.907534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:43.907783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:44.908091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:45.908234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:46.908824      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:47.909183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:48.909931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:49.910224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:50.910340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:51.911276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:52.912131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:53.912385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:54.913012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:55.913415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:56.914270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:57.914696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:58.915725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:46:59.915964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:00.916737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:01.917020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:02.917421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:03.917807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:04.917889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:05.918120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:06.918926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:07.919183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:08.920026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:09.920254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:10.921388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:11.921537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:12.921590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:13.921848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:14.922894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:15.923239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:16.923303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:17.923826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:18.924477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:19.924693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:20.924922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:21.925217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:22.925976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:23.926124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:24.926621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:25.926795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:26.927221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:27.928226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:28.929065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:29.929242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:30.930118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:31.931238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:32.932094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:33.932329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:34.932808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:35.933937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:36.934572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:37.934994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:38.935864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:39.936183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:40.937251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:41.937444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:42.937512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:43.937747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:44.938091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:45.938170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:46.938608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:47.939258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:48.939577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:49.940572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:50.940808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:51.941486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:52.942230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:53.943226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:54.944230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:55.944889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:56.945133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:57.945239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:58.945586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:47:59.946247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:00.947145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:01.947518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:02.948020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:03.948135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:04.948271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:05.948447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:06.948728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:07.949132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:08.949412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:09.950195      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:10.951271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:11.951860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:12.952255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:13.953075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:14.953569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:15.953694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:16.953848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:17.954973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:18.955128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:19.956019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 04/26/23 06:48:20.727
  E0426 06:48:20.956095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:48:21.242: INFO: Successfully updated pod "var-expansion-281e1899-912a-4cbc-85fa-7ab0348eb76a"
  STEP: waiting for pod running @ 04/26/23 06:48:21.242
  E0426 06:48:21.956462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:22.956744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 04/26/23 06:48:23.25
  Apr 26 06:48:23.250: INFO: Deleting pod "var-expansion-281e1899-912a-4cbc-85fa-7ab0348eb76a" in namespace "var-expansion-2552"
  Apr 26 06:48:23.259: INFO: Wait up to 5m0s for pod "var-expansion-281e1899-912a-4cbc-85fa-7ab0348eb76a" to be fully deleted
  E0426 06:48:23.957685      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:24.957955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:25.958724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:26.959227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:27.960168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:28.960408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:29.960939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:30.961092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:31.961954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:32.962162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:33.963065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:34.963191      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:35.963477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:36.963774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:37.964160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:38.964301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:39.965388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:40.965770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:41.965895      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:42.966813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:43.967563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:44.967811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:45.967859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:46.968156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:47.968327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:48.968603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:49.969701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:50.969828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:51.970181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:52.971276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:53.972188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:54.972449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:48:55.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2552" for this suite. @ 04/26/23 06:48:55.334
• [155.409 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/26/23 06:48:55.345
  Apr 26 06:48:55.345: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 06:48:55.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:48:55.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:48:55.637
  STEP: creating pod @ 04/26/23 06:48:55.639
  E0426 06:48:55.973534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:56.973765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:48:57.662: INFO: Pod pod-hostip-aaf2b05d-6071-4668-9c8c-5e381d55091c has hostIP: 172.31.3.127
  Apr 26 06:48:57.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3779" for this suite. @ 04/26/23 06:48:57.666
• [2.329 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/26/23 06:48:57.675
  Apr 26 06:48:57.675: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename subpath @ 04/26/23 06:48:57.676
  E0426 06:48:57.974468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:48:58.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:48:58.905
  STEP: Setting up data @ 04/26/23 06:48:58.907
  STEP: Creating pod pod-subpath-test-downwardapi-55zq @ 04/26/23 06:48:58.921
  STEP: Creating a pod to test atomic-volume-subpath @ 04/26/23 06:48:58.921
  E0426 06:48:58.975483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:48:59.976043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:00.977007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:01.977217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:02.977765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:03.977891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:04.978002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:05.978251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:06.978990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:07.979239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:08.980103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:09.980258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:10.980301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:11.980549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:12.980810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:13.981338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:14.982119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:15.983240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:16.983279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:17.983390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:18.984181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:19.984629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:20.985229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:21.985504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:22.986076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:49:22.994
  Apr 26 06:49:22.997: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-subpath-test-downwardapi-55zq container test-container-subpath-downwardapi-55zq: <nil>
  STEP: delete the pod @ 04/26/23 06:49:23.015
  STEP: Deleting pod pod-subpath-test-downwardapi-55zq @ 04/26/23 06:49:23.036
  Apr 26 06:49:23.036: INFO: Deleting pod "pod-subpath-test-downwardapi-55zq" in namespace "subpath-4165"
  Apr 26 06:49:23.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4165" for this suite. @ 04/26/23 06:49:23.042
• [25.375 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/26/23 06:49:23.051
  Apr 26 06:49:23.051: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 06:49:23.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:49:23.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:49:23.905
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/26/23 06:49:23.907
  E0426 06:49:23.986611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:24.987352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:25.988296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:26.988521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:49:27.93
  Apr 26 06:49:27.932: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-81483882-835c-4f16-b0d8-9d11b852062c container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:49:27.938
  Apr 26 06:49:27.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2531" for this suite. @ 04/26/23 06:49:27.961
• [4.919 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/26/23 06:49:27.97
  Apr 26 06:49:27.970: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:49:27.971
  E0426 06:49:27.989450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:49:28.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:49:28.905
  E0426 06:49:28.990276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:29.990931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:30.991888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:31.992221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:32.992903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:33.993219      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:34.993858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:35.993951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:36.994408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:37.995128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:38.995576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:39.996446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:40.996868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:41.997836      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:42.998474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:43.998612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:44.998600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/26/23 06:49:45.91
  E0426 06:49:45.999447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:47.000053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:48.000745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:49.001116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:50.001462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/26/23 06:49:50.914
  STEP: Ensuring resource quota status is calculated @ 04/26/23 06:49:50.923
  E0426 06:49:51.001569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:52.001816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 04/26/23 06:49:52.927
  STEP: Ensuring resource quota status captures configMap creation @ 04/26/23 06:49:52.944
  E0426 06:49:53.002123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:54.002249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 04/26/23 06:49:54.949
  STEP: Ensuring resource quota status released usage @ 04/26/23 06:49:54.986
  E0426 06:49:55.003060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:56.003313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:49:56.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8319" for this suite. @ 04/26/23 06:49:56.993
  E0426 06:49:57.003806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
• [29.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/26/23 06:49:57.006
  Apr 26 06:49:57.006: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename containers @ 04/26/23 06:49:57.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:49:57.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:49:57.418
  E0426 06:49:58.003928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:49:59.004234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:49:59.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2684" for this suite. @ 04/26/23 06:49:59.448
• [2.449 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/26/23 06:49:59.456
  Apr 26 06:49:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename dns @ 04/26/23 06:49:59.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:49:59.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:49:59.904
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/26/23 06:49:59.907
  Apr 26 06:49:59.919: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7110  cda62b8c-21ec-4dac-9bbd-a5b27ee0b369 14030 0 2023-04-26 06:49:59 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-26 06:49:59 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xtxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xtxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0426 06:50:00.004825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:01.005096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/26/23 06:50:01.926
  Apr 26 06:50:01.926: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7110 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:50:01.926: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:50:01.927: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:50:01.927: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7110/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0426 06:50:02.006154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS server is configured on pod... @ 04/26/23 06:50:02.012
  Apr 26 06:50:02.012: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7110 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:50:02.012: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:50:02.013: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:50:02.013: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7110/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 26 06:50:02.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 06:50:02.112: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-7110" for this suite. @ 04/26/23 06:50:02.155
• [2.708 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/26/23 06:50:02.165
  Apr 26 06:50:02.165: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 06:50:02.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:02.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:02.418
  STEP: creating secret secrets-3515/secret-test-ec797301-c205-474b-8edb-9a6ecc06fd9c @ 04/26/23 06:50:02.42
  STEP: Creating a pod to test consume secrets @ 04/26/23 06:50:02.427
  E0426 06:50:03.007237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:04.007445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:05.007808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:06.008108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:50:06.451
  Apr 26 06:50:06.454: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-45cbfbd9-9b91-4919-80f4-40d126511cd4 container env-test: <nil>
  STEP: delete the pod @ 04/26/23 06:50:06.461
  Apr 26 06:50:06.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3515" for this suite. @ 04/26/23 06:50:06.483
• [4.326 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/26/23 06:50:06.492
  Apr 26 06:50:06.492: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 06:50:06.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:06.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:06.725
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/26/23 06:50:06.727
  Apr 26 06:50:06.727: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 06:50:07.008278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:08.009356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:08.128: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 06:50:09.010447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:10.011549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:11.012424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:12.013438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:13.013890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:14.014144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:14.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1137" for this suite. @ 04/26/23 06:50:14.214
• [7.733 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/26/23 06:50:14.225
  Apr 26 06:50:14.225: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 06:50:14.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:14.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:14.502
  STEP: Creating configMap configmap-6776/configmap-test-b99105f9-c0ae-4c08-810a-c84e395b6964 @ 04/26/23 06:50:14.504
  STEP: Creating a pod to test consume configMaps @ 04/26/23 06:50:14.512
  E0426 06:50:15.014346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:16.014589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:17.014730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:18.015579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:50:18.534
  Apr 26 06:50:18.537: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-bb4bd893-6de2-4204-ae16-e5ba02257cf9 container env-test: <nil>
  STEP: delete the pod @ 04/26/23 06:50:18.544
  Apr 26 06:50:18.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6776" for this suite. @ 04/26/23 06:50:18.566
• [4.349 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/26/23 06:50:18.575
  Apr 26 06:50:18.575: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 06:50:18.576
  E0426 06:50:19.016085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:19.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:19.906
  Apr 26 06:50:19.908: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: creating the pod @ 04/26/23 06:50:19.908
  STEP: submitting the pod to kubernetes @ 04/26/23 06:50:19.908
  E0426 06:50:20.016119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:21.016277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:21.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3590" for this suite. @ 04/26/23 06:50:21.944
• [3.377 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/26/23 06:50:21.953
  Apr 26 06:50:21.953: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 06:50:21.953
  E0426 06:50:22.016647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:22.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:22.417
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:50:22.419
  E0426 06:50:23.017819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:24.017914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:25.017947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:26.018153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:50:26.442
  Apr 26 06:50:26.445: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-f5ece44b-5326-472e-8dce-1bbb1288bcd3 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:50:26.452
  Apr 26 06:50:26.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-447" for this suite. @ 04/26/23 06:50:26.474
• [4.530 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/26/23 06:50:26.482
  Apr 26 06:50:26.482: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 06:50:26.483
  E0426 06:50:27.018937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:27.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:27.419
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/26/23 06:50:27.421
  Apr 26 06:50:27.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-9814 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Apr 26 06:50:27.495: INFO: stderr: ""
  Apr 26 06:50:27.495: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/26/23 06:50:27.495
  Apr 26 06:50:27.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-9814 delete pods e2e-test-httpd-pod'
  E0426 06:50:28.019472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:29.019629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:30.019830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:30.871: INFO: stderr: ""
  Apr 26 06:50:30.871: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 26 06:50:30.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9814" for this suite. @ 04/26/23 06:50:30.874
• [4.402 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/26/23 06:50:30.885
  Apr 26 06:50:30.885: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename svcaccounts @ 04/26/23 06:50:30.886
  E0426 06:50:31.020342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:31.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:31.53
  Apr 26 06:50:31.535: INFO: Got root ca configmap in namespace "svcaccounts-7235"
  Apr 26 06:50:31.544: INFO: Deleted root ca configmap in namespace "svcaccounts-7235"
  E0426 06:50:32.020373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 04/26/23 06:50:32.045
  Apr 26 06:50:32.048: INFO: Recreated root ca configmap in namespace "svcaccounts-7235"
  Apr 26 06:50:32.057: INFO: Updated root ca configmap in namespace "svcaccounts-7235"
  STEP: waiting for the root ca configmap reconciled @ 04/26/23 06:50:32.557
  Apr 26 06:50:32.562: INFO: Reconciled root ca configmap in namespace "svcaccounts-7235"
  Apr 26 06:50:32.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7235" for this suite. @ 04/26/23 06:50:32.565
• [1.690 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/26/23 06:50:32.575
  Apr 26 06:50:32.575: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename svcaccounts @ 04/26/23 06:50:32.576
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:32.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:32.905
  Apr 26 06:50:32.934: INFO: created pod pod-service-account-defaultsa
  Apr 26 06:50:32.934: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Apr 26 06:50:32.942: INFO: created pod pod-service-account-mountsa
  Apr 26 06:50:32.943: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 26 06:50:32.955: INFO: created pod pod-service-account-nomountsa
  Apr 26 06:50:32.955: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 26 06:50:33.007: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 26 06:50:33.007: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Apr 26 06:50:33.015: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 26 06:50:33.015: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  E0426 06:50:33.020818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:33.024: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 26 06:50:33.024: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 26 06:50:33.032: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 26 06:50:33.032: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 26 06:50:33.040: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 26 06:50:33.040: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Apr 26 06:50:33.058: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 26 06:50:33.058: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 26 06:50:33.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1226" for this suite. @ 04/26/23 06:50:33.117
• [0.641 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/26/23 06:50:33.216
  Apr 26 06:50:33.216: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 06:50:33.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:33.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:33.282
  STEP: set up a multi version CRD @ 04/26/23 06:50:33.284
  Apr 26 06:50:33.285: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 06:50:34.021098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:35.021285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:36.021774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 04/26/23 06:50:36.914
  STEP: check the new version name is served @ 04/26/23 06:50:36.929
  E0426 06:50:37.022568      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:38.023289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 04/26/23 06:50:38.324
  E0426 06:50:39.023974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/26/23 06:50:39.076
  E0426 06:50:40.024996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:41.025883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:41.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7268" for this suite. @ 04/26/23 06:50:41.905
• [8.696 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/26/23 06:50:41.913
  Apr 26 06:50:41.913: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename runtimeclass @ 04/26/23 06:50:41.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:41.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:41.933
  STEP: getting /apis @ 04/26/23 06:50:41.935
  STEP: getting /apis/node.k8s.io @ 04/26/23 06:50:41.939
  STEP: getting /apis/node.k8s.io/v1 @ 04/26/23 06:50:41.94
  STEP: creating @ 04/26/23 06:50:41.941
  STEP: watching @ 04/26/23 06:50:41.959
  Apr 26 06:50:41.959: INFO: starting watch
  STEP: getting @ 04/26/23 06:50:41.967
  STEP: listing @ 04/26/23 06:50:41.969
  STEP: patching @ 04/26/23 06:50:41.971
  STEP: updating @ 04/26/23 06:50:41.979
  Apr 26 06:50:41.986: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/26/23 06:50:41.986
  STEP: deleting a collection @ 04/26/23 06:50:41.997
  Apr 26 06:50:42.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6232" for this suite. @ 04/26/23 06:50:42.016
• [0.112 seconds]
------------------------------
SSSSSSSS  E0426 06:50:42.026046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/26/23 06:50:42.026
  Apr 26 06:50:42.026: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 06:50:42.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:42.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:42.05
  STEP: Creating the pod @ 04/26/23 06:50:42.053
  E0426 06:50:43.026201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:44.027280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:44.596: INFO: Successfully updated pod "labelsupdatef910a71e-2d71-4ff8-9ac1-fb1087aa12c9"
  E0426 06:50:45.027514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:46.027853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:47.028211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:48.028377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:48.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7176" for this suite. @ 04/26/23 06:50:48.617
• [6.598 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/26/23 06:50:48.626
  Apr 26 06:50:48.626: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename proxy @ 04/26/23 06:50:48.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:48.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:48.697
  Apr 26 06:50:48.699: INFO: Creating pod...
  E0426 06:50:49.028565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:50.028790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:50.715: INFO: Creating service...
  Apr 26 06:50:50.726: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/pods/agnhost/proxy?method=DELETE
  Apr 26 06:50:50.730: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 26 06:50:50.730: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/pods/agnhost/proxy?method=OPTIONS
  Apr 26 06:50:50.736: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 26 06:50:50.736: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/pods/agnhost/proxy?method=PATCH
  Apr 26 06:50:50.739: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 26 06:50:50.739: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/pods/agnhost/proxy?method=POST
  Apr 26 06:50:50.741: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 26 06:50:50.742: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/pods/agnhost/proxy?method=PUT
  Apr 26 06:50:50.744: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 26 06:50:50.744: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 26 06:50:50.747: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 26 06:50:50.747: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 26 06:50:50.751: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 26 06:50:50.751: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 26 06:50:50.754: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 26 06:50:50.754: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/services/e2e-proxy-test-service/proxy?method=POST
  Apr 26 06:50:50.758: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 26 06:50:50.758: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 26 06:50:50.761: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 26 06:50:50.761: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/pods/agnhost/proxy?method=GET
  Apr 26 06:50:50.763: INFO: http.Client request:GET StatusCode:301
  Apr 26 06:50:50.763: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/services/e2e-proxy-test-service/proxy?method=GET
  Apr 26 06:50:50.766: INFO: http.Client request:GET StatusCode:301
  Apr 26 06:50:50.766: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/pods/agnhost/proxy?method=HEAD
  Apr 26 06:50:50.768: INFO: http.Client request:HEAD StatusCode:301
  Apr 26 06:50:50.768: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-29/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 26 06:50:50.771: INFO: http.Client request:HEAD StatusCode:301
  Apr 26 06:50:50.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-29" for this suite. @ 04/26/23 06:50:50.774
• [2.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/26/23 06:50:50.784
  Apr 26 06:50:50.784: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename limitrange @ 04/26/23 06:50:50.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:50.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:50.804
  STEP: Creating a LimitRange @ 04/26/23 06:50:50.825
  STEP: Setting up watch @ 04/26/23 06:50:50.825
  STEP: Submitting a LimitRange @ 04/26/23 06:50:50.949
  STEP: Verifying LimitRange creation was observed @ 04/26/23 06:50:50.956
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/26/23 06:50:50.957
  Apr 26 06:50:50.959: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 26 06:50:50.959: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/26/23 06:50:50.959
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/26/23 06:50:50.966
  Apr 26 06:50:50.969: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 26 06:50:50.969: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/26/23 06:50:50.969
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/26/23 06:50:50.985
  Apr 26 06:50:50.988: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 26 06:50:50.988: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/26/23 06:50:50.988
  STEP: Failing to create a Pod with more than max resources @ 04/26/23 06:50:50.99
  STEP: Updating a LimitRange @ 04/26/23 06:50:50.992
  STEP: Verifying LimitRange updating is effective @ 04/26/23 06:50:51
  E0426 06:50:51.029031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:52.029220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 04/26/23 06:50:53.004
  STEP: Failing to create a Pod with more than max resources @ 04/26/23 06:50:53.011
  STEP: Deleting a LimitRange @ 04/26/23 06:50:53.013
  STEP: Verifying the LimitRange was deleted @ 04/26/23 06:50:53.022
  E0426 06:50:53.029225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:54.029699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:55.029811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:56.030154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:50:57.031267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:58.026: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/26/23 06:50:58.026
  E0426 06:50:58.031776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:50:58.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-1882" for this suite. @ 04/26/23 06:50:58.04
• [7.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/26/23 06:50:58.051
  Apr 26 06:50:58.051: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename subpath @ 04/26/23 06:50:58.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:50:58.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:50:58.073
  STEP: Setting up data @ 04/26/23 06:50:58.075
  STEP: Creating pod pod-subpath-test-secret-7jpj @ 04/26/23 06:50:58.087
  STEP: Creating a pod to test atomic-volume-subpath @ 04/26/23 06:50:58.087
  E0426 06:50:59.031951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:00.032208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:01.033257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:02.033382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:03.033570      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:04.033830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:05.034143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:06.034411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:07.034423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:08.034640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:09.035297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:10.035565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:11.035650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:12.036137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:13.036535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:14.036715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:15.037449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:16.037907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:17.038378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:18.038826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:19.039209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:20.039517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:21.039571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:22.039873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:51:22.147
  Apr 26 06:51:22.150: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-subpath-test-secret-7jpj container test-container-subpath-secret-7jpj: <nil>
  STEP: delete the pod @ 04/26/23 06:51:22.155
  STEP: Deleting pod pod-subpath-test-secret-7jpj @ 04/26/23 06:51:22.171
  Apr 26 06:51:22.171: INFO: Deleting pod "pod-subpath-test-secret-7jpj" in namespace "subpath-5557"
  Apr 26 06:51:22.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5557" for this suite. @ 04/26/23 06:51:22.176
• [24.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/26/23 06:51:22.186
  Apr 26 06:51:22.187: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:51:22.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:51:22.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:51:22.206
  STEP: Creating the pod @ 04/26/23 06:51:22.209
  E0426 06:51:23.040406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:24.040575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:51:24.744: INFO: Successfully updated pod "annotationupdate8cac3302-5b6a-4e67-acc6-bb660c721d6c"
  E0426 06:51:25.041229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:26.041459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:27.041694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:28.042056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:51:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3360" for this suite. @ 04/26/23 06:51:28.768
• [6.589 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/26/23 06:51:28.775
  Apr 26 06:51:28.775: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 06:51:28.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:51:28.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:51:28.795
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/26/23 06:51:28.798
  E0426 06:51:29.042638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:30.042805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:31.043662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:32.044009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:51:32.818
  Apr 26 06:51:32.820: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-07b24e3f-de90-4001-a63d-99bb06086e4f container test-container: <nil>
  STEP: delete the pod @ 04/26/23 06:51:32.825
  Apr 26 06:51:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2804" for this suite. @ 04/26/23 06:51:32.845
• [4.076 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/26/23 06:51:32.851
  Apr 26 06:51:32.851: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename disruption @ 04/26/23 06:51:32.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:51:32.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:51:32.871
  STEP: Creating a kubernetes client @ 04/26/23 06:51:32.873
  Apr 26 06:51:32.873: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename disruption-2 @ 04/26/23 06:51:32.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:51:32.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:51:32.891
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:51:32.899
  E0426 06:51:33.044086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:34.044525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:51:34.911
  E0426 06:51:35.045222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:36.045352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/26/23 06:51:36.923
  E0426 06:51:37.046092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:38.046179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 04/26/23 06:51:38.93
  STEP: listing a collection of PDBs in namespace disruption-2020 @ 04/26/23 06:51:38.932
  STEP: deleting a collection of PDBs @ 04/26/23 06:51:38.935
  STEP: Waiting for the PDB collection to be deleted @ 04/26/23 06:51:38.948
  Apr 26 06:51:38.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 06:51:38.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5064" for this suite. @ 04/26/23 06:51:38.959
  STEP: Destroying namespace "disruption-2020" for this suite. @ 04/26/23 06:51:38.97
• [6.127 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/26/23 06:51:38.979
  Apr 26 06:51:38.979: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:51:38.98
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:51:38.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:51:39.001
  STEP: Creating the pod @ 04/26/23 06:51:39.004
  E0426 06:51:39.047101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:40.047298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:41.047764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:51:41.544: INFO: Successfully updated pod "labelsupdatef29441f0-7f33-4dad-9c95-6a0277110f03"
  E0426 06:51:42.048549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:43.049107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:44.050156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:45.050293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:51:45.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8928" for this suite. @ 04/26/23 06:51:45.565
• [6.596 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/26/23 06:51:45.578
  Apr 26 06:51:45.578: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-preemption @ 04/26/23 06:51:45.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:51:45.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:51:45.601
  Apr 26 06:51:45.621: INFO: Waiting up to 1m0s for all nodes to be ready
  E0426 06:51:46.051236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:47.051531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:48.051854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:49.052136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:50.053265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:51.053433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:52.054075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:53.054313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:54.055034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:55.055304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:56.055738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:57.056005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:58.056454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:51:59.056676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:00.056902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:01.057063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:02.057389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:03.057879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:04.058596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:05.058731      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:06.058894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:07.059142      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:08.059785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:09.060024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:10.060909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:11.061191      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:12.061483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:13.062160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:14.063156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:15.063394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:16.063564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:17.063875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:18.064567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:19.064813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:20.065052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:21.065306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:22.065532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:23.066465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:24.067223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:25.067523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:26.067942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:27.068219      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:28.068891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:29.069096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:30.069444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:31.069684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:32.070384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:33.070944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:34.071205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:35.071406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:36.071503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:37.071738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:38.071903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:39.072031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:40.072121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:41.072406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:42.072827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:43.073310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:44.073526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:45.073718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:52:45.638: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/26/23 06:52:45.641
  Apr 26 06:52:45.641: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/26/23 06:52:45.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:52:45.662
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:52:45.664
  STEP: Finding an available node @ 04/26/23 06:52:45.667
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/26/23 06:52:45.667
  E0426 06:52:46.073914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:47.074158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/26/23 06:52:47.684
  Apr 26 06:52:47.699: INFO: found a healthy node: ip-172-31-3-127
  E0426 06:52:48.075323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:49.075480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:50.076493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:51.076640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:52.076817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:53.077818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:52:53.785: INFO: pods created so far: [1 1 1]
  Apr 26 06:52:53.785: INFO: length of pods created so far: 3
  E0426 06:52:54.078890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:55.079151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:52:55.796: INFO: pods created so far: [2 2 1]
  E0426 06:52:56.079955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:57.081104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:58.081674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:52:59.081902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:00.082096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:01.082182      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:02.083266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:02.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 06:53:02.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-5930" for this suite. @ 04/26/23 06:53:02.904
  STEP: Destroying namespace "sched-preemption-2765" for this suite. @ 04/26/23 06:53:02.912
• [77.343 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/26/23 06:53:02.923
  Apr 26 06:53:02.923: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename deployment @ 04/26/23 06:53:02.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:53:02.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:53:02.942
  Apr 26 06:53:02.945: INFO: Creating deployment "webserver-deployment"
  Apr 26 06:53:02.952: INFO: Waiting for observed generation 1
  E0426 06:53:03.083646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:04.083772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:04.957: INFO: Waiting for all required pods to come up
  Apr 26 06:53:04.962: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/26/23 06:53:04.962
  E0426 06:53:05.084433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:06.085290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:06.971: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 26 06:53:06.976: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Apr 26 06:53:06.987: INFO: Updating deployment webserver-deployment
  Apr 26 06:53:06.987: INFO: Waiting for observed generation 2
  E0426 06:53:07.085985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:08.086257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:09.006: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 26 06:53:09.011: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 26 06:53:09.015: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 26 06:53:09.031: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 26 06:53:09.031: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 26 06:53:09.036: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 26 06:53:09.045: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 26 06:53:09.045: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 26 06:53:09.059: INFO: Updating deployment webserver-deployment
  Apr 26 06:53:09.059: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Apr 26 06:53:09.070: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Apr 26 06:53:09.079: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0426 06:53:09.086835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:10.087559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:11.088026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:11.092: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-2337  55e6170c-b63e-419c-9c98-e0eda703aab2 15610 3 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d0f5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-26 06:53:09 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-04-26 06:53:09 +0000 UTC,LastTransitionTime:2023-04-26 06:53:02 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Apr 26 06:53:11.096: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-2337  8e90c19e-4482-48b9-8bbe-44cc5499c530 15603 3 2023-04-26 06:53:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 55e6170c-b63e-419c-9c98-e0eda703aab2 0xc002d27147 0xc002d27148}] [] [{kubelite Update apps/v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55e6170c-b63e-419c-9c98-e0eda703aab2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d271f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 06:53:11.100: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 26 06:53:11.102: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-2337  b99d1558-44b5-4ef9-9880-4fb11dd86953 15600 3 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 55e6170c-b63e-419c-9c98-e0eda703aab2 0xc002d27047 0xc002d27048}] [] [{kubelite Update apps/v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55e6170c-b63e-419c-9c98-e0eda703aab2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d270e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 06:53:11.114: INFO: Pod "webserver-deployment-67bd4bf6dc-vntnc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vntnc webserver-deployment-67bd4bf6dc- deployment-2337  af175d4b-7456-4e7e-9597-1d1198af28e9 15311 0 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:96965a23d715cf5246fbdb165cc3ba538c31cae742172885c8e747dd01f68c53 cni.projectcalico.org/podIP:10.1.96.30/32 cni.projectcalico.org/podIPs:10.1.96.30/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc003d0fa07 0xc003d0fa08}] [] [{kubelite Update v1 2023-04-26 06:53:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5lsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5lsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.30,StartTime:2023-04-26 06:53:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:53:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://abb6b94687293028d2154f75741d1def22033b0b1573fb1b6b8ed0d5d75067d5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.30,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.115: INFO: Pod "webserver-deployment-67bd4bf6dc-tvfrb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tvfrb webserver-deployment-67bd4bf6dc- deployment-2337  c36809fc-3034-4bb2-8774-2fa0a6033259 15313 0 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:068607f9bd100139d837600f6e1834cf6b2baedf59b19d3d625ce6241013252e cni.projectcalico.org/podIP:10.1.96.33/32 cni.projectcalico.org/podIPs:10.1.96.33/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc003d0fc50 0xc003d0fc51}] [] [{kubelite Update v1 2023-04-26 06:53:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lc2tr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lc2tr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.33,StartTime:2023-04-26 06:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://437ef33b5a4aea9886162b469a8b9b47130b9234cc4c62442f5a17fa5dc6c4b8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.33,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.116: INFO: Pod "webserver-deployment-67bd4bf6dc-7nshk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7nshk webserver-deployment-67bd4bf6dc- deployment-2337  03c920e3-4670-4fa7-bb27-538f85b1f1b5 15318 0 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:4045b233f0a251369e9f9ab1fd425d143d7dd7ec9771fc452b5ad985f04c3654 cni.projectcalico.org/podIP:10.1.96.32/32 cni.projectcalico.org/podIPs:10.1.96.32/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc003d0feb0 0xc003d0feb1}] [] [{kubelite Update v1 2023-04-26 06:53:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t4qqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t4qqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.32,StartTime:2023-04-26 06:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f53d20589fdf3eb7ad78348d023b4ac2f3360eb9b698abe30d383c504d3e8fe3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.32,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.117: INFO: Pod "webserver-deployment-67bd4bf6dc-4tthc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4tthc webserver-deployment-67bd4bf6dc- deployment-2337  59bc2ff1-2b97-4def-9c0e-181287ec6bad 15321 0 2023-04-26 06:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:5cf382fde2b8d537b0a84967004e4b093d45df997bdd09eb7fc22efd83eb920f cni.projectcalico.org/podIP:10.1.96.34/32 cni.projectcalico.org/podIPs:10.1.96.34/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9e530 0xc004b9e531}] [] [{calico Update v1 2023-04-26 06:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nmfn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nmfn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.34,StartTime:2023-04-26 06:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://490542c7032664398fc13db329ca0529dd1d65a452d3994f3d862538c48316a8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.34,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.118: INFO: Pod "webserver-deployment-67bd4bf6dc-4jvpd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4jvpd webserver-deployment-67bd4bf6dc- deployment-2337  34a226be-a661-4b16-95ea-8d22e57b74e6 15325 0 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:22469bca36d203c58c403c69a3a7824136522b29d4a8fac3a12b63424f3c9e0a cni.projectcalico.org/podIP:10.1.96.31/32 cni.projectcalico.org/podIPs:10.1.96.31/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9e750 0xc004b9e751}] [] [{kubelite Update v1 2023-04-26 06:53:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5hpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5hpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.31,StartTime:2023-04-26 06:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:53:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8ad514e3a6978793947fe1b4bbb2867494cb748269dbbfe3b34d18629396bc9b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.31,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.119: INFO: Pod "webserver-deployment-67bd4bf6dc-xscxf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xscxf webserver-deployment-67bd4bf6dc- deployment-2337  f211b61f-85a8-416e-9147-2f422bf52ccb 15341 0 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:b08b6616c49a4a9d89ea37eaaff5f15e30b00f7aed94ead14624170a82784995 cni.projectcalico.org/podIP:10.1.93.228/32 cni.projectcalico.org/podIPs:10.1.93.228/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9e980 0xc004b9e981}] [] [{kubelite Update v1 2023-04-26 06:53:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.93.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fq5zr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fq5zr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:10.1.93.228,StartTime:2023-04-26 06:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://02eb486346a8d37b22ae263b76bc35258b2c408ff35717df68bfdff567ebf581,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.93.228,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.120: INFO: Pod "webserver-deployment-67bd4bf6dc-5nsjf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5nsjf webserver-deployment-67bd4bf6dc- deployment-2337  4d3697eb-8f50-4a90-b0dd-3709334b27d8 15344 0 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:016db2878d38b2168c3c4357234f94978e409b7880283bd16a17ef329442f7c5 cni.projectcalico.org/podIP:10.1.93.226/32 cni.projectcalico.org/podIPs:10.1.93.226/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9ebb0 0xc004b9ebb1}] [] [{kubelite Update v1 2023-04-26 06:53:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.93.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pr5fd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pr5fd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:10.1.93.226,StartTime:2023-04-26 06:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff82a794e5857898a5d6edd76e1c0c5f432c902460918b79274994f07f32450b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.93.226,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.121: INFO: Pod "webserver-deployment-67bd4bf6dc-rvmdf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rvmdf webserver-deployment-67bd4bf6dc- deployment-2337  e11f5fd7-65a5-4e2f-bdf1-de59f0d72c34 15345 0 2023-04-26 06:53:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:bd419daca6c004855328796e06346642e961ad271905abd3e8ebd1f427e3279b cni.projectcalico.org/podIP:10.1.93.230/32 cni.projectcalico.org/podIPs:10.1.93.230/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9ede0 0xc004b9ede1}] [] [{kubelite Update v1 2023-04-26 06:53:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.93.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cb87l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cb87l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:10.1.93.230,StartTime:2023-04-26 06:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 06:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0da76653ab0b1fc6c5e689c788e02985c7b952ab984309f80ae93a7a3c7ab4ce,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.93.230,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.121: INFO: Pod "webserver-deployment-67bd4bf6dc-vvdnt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vvdnt webserver-deployment-67bd4bf6dc- deployment-2337  007f4990-ba93-464b-816a-10f69f664be2 15622 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9eff0 0xc004b9eff1}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6wcm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6wcm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.124: INFO: Pod "webserver-deployment-7b75d79cf5-nbr2m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nbr2m webserver-deployment-7b75d79cf5- deployment-2337  d20218da-b92e-4376-bd7a-5c51b73f6b17 15626 0 2023-04-26 06:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:8e9d5c4593d1d02786107bdc596bf052d442fe60f56173a5ba5d86dd162eec69 cni.projectcalico.org/podIP:10.1.96.35/32 cni.projectcalico.org/podIPs:10.1.96.35/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc004b9f217 0xc004b9f218}] [] [{calico Update v1 2023-04-26 06:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjqgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjqgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.35,StartTime:2023-04-26 06:53:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:50c9f9b030f45e1f845641b71597d90144129ab3520525a6a7b6598aea278504: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.124: INFO: Pod "webserver-deployment-7b75d79cf5-2hp8q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2hp8q webserver-deployment-7b75d79cf5- deployment-2337  d2888029-5f09-4ca0-846c-c3d7da71bb60 15631 0 2023-04-26 06:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:5bf2c12328165b7d298911e57c906f237ff70ce1cf83e1c231418b83a25b9c97 cni.projectcalico.org/podIP:10.1.96.38/32 cni.projectcalico.org/podIPs:10.1.96.38/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc004b9f4c0 0xc004b9f4c1}] [] [{calico Update v1 2023-04-26 06:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmwg9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmwg9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.38,StartTime:2023-04-26 06:53:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:50c9f9b030f45e1f845641b71597d90144129ab3520525a6a7b6598aea278504: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.38,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.125: INFO: Pod "webserver-deployment-7b75d79cf5-vkf7b" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vkf7b webserver-deployment-7b75d79cf5- deployment-2337  9813fb47-dfe1-4d79-b95c-2db94f8dc5b8 15640 0 2023-04-26 06:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:93257f3296fc55263539990f7dc5a22a4c35fbbdaea9cedae8df62479dfc742d cni.projectcalico.org/podIP:10.1.96.36/32 cni.projectcalico.org/podIPs:10.1.96.36/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc004b9f720 0xc004b9f721}] [] [{calico Update v1 2023-04-26 06:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hqj9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hqj9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.36,StartTime:2023-04-26 06:53:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.36,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.125: INFO: Pod "webserver-deployment-67bd4bf6dc-x68nm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x68nm webserver-deployment-67bd4bf6dc- deployment-2337  872fdf7d-565b-4e1e-802e-0799ebd29a89 15645 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:b6878417e15caa9ad1696d6eb7081d1e64c1c1e0e5c5c61a5ecf31aa16279d3d cni.projectcalico.org/podIP:10.1.96.37/32 cni.projectcalico.org/podIPs:10.1.96.37/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9f9a0 0xc004b9f9a1}] [] [{calico Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ph7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ph7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.127: INFO: Pod "webserver-deployment-7b75d79cf5-jvq5n" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jvq5n webserver-deployment-7b75d79cf5- deployment-2337  da0fbb6a-3b18-4a51-afb2-f5e28f3b41fc 15646 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:a46f57a4c4a896cb0c7a03e61a11611db0b2985982d89d3c903540778a55aa9b cni.projectcalico.org/podIP:10.1.93.233/32 cni.projectcalico.org/podIPs:10.1.93.233/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc004b9fbb7 0xc004b9fbb8}] [] [{calico Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgtj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgtj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.127: INFO: Pod "webserver-deployment-67bd4bf6dc-cpgg4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cpgg4 webserver-deployment-67bd4bf6dc- deployment-2337  bf411a2c-bf5e-4649-897f-370a4f5be40f 15652 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9fdc0 0xc004b9fdc1}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w58nj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w58nj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.128: INFO: Pod "webserver-deployment-67bd4bf6dc-dxt9v" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dxt9v webserver-deployment-67bd4bf6dc- deployment-2337  efe5e3c5-48cd-444c-b5be-d683b4e16ed1 15653 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:4e0a5e08fbc2e7a7148048f712cde15d68248d340bfcc160a0931696197cba4f cni.projectcalico.org/podIP:10.1.93.234/32 cni.projectcalico.org/podIPs:10.1.93.234/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc004b9ffb7 0xc004b9ffb8}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sd5t8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sd5t8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.128: INFO: Pod "webserver-deployment-7b75d79cf5-wdbpb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wdbpb webserver-deployment-7b75d79cf5- deployment-2337  da352bee-fba3-4d64-8ac0-1300d875eb35 15660 0 2023-04-26 06:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:5060bef56dc8e4a9fe7fb566d32f44ae0dfb9610a5243b0cfaa14b129fec20e8 cni.projectcalico.org/podIP:10.1.93.232/32 cni.projectcalico.org/podIPs:10.1.93.232/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc00468be17 0xc00468be18}] [] [{kubelite Update v1 2023-04-26 06:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.93.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvmbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvmbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:10.1.93.232,StartTime:2023-04-26 06:53:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.93.232,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.129: INFO: Pod "webserver-deployment-7b75d79cf5-57nq4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-57nq4 webserver-deployment-7b75d79cf5- deployment-2337  554264ec-3c4c-43e8-97cd-beacc500c7a0 15662 0 2023-04-26 06:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:6b8c1979f7df640b8442c56a4de0de7b83c242009cbe180950f93c27bcbc9492 cni.projectcalico.org/podIP:10.1.93.231/32 cni.projectcalico.org/podIPs:10.1.93.231/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc003382080 0xc003382081}] [] [{kubelite Update v1 2023-04-26 06:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 06:53:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.93.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2l5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2l5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:10.1.93.231,StartTime:2023-04-26 06:53:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.93.231,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.131: INFO: Pod "webserver-deployment-67bd4bf6dc-6cdpv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6cdpv webserver-deployment-67bd4bf6dc- deployment-2337  3ef33b9e-21a2-4c3e-a47a-25e6a9096daf 15663 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:bdfbf45ce0ba45287696da5f5b90127c44814bc8df86209f45b524b543be893a cni.projectcalico.org/podIP:10.1.96.40/32 cni.projectcalico.org/podIPs:10.1.96.40/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc0033822e0 0xc0033822e1}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kkpnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kkpnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.131: INFO: Pod "webserver-deployment-67bd4bf6dc-z9mzd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z9mzd webserver-deployment-67bd4bf6dc- deployment-2337  bbbd3c2c-2534-40a0-b3be-322a4d6a49cf 15669 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:5e2417fc6b29ce61c726126c5c79698d444410c9ef9010e449939017a50dc793 cni.projectcalico.org/podIP:10.1.93.235/32 cni.projectcalico.org/podIPs:10.1.93.235/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc0033824f7 0xc0033824f8}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79pgc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79pgc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.132: INFO: Pod "webserver-deployment-7b75d79cf5-gk97b" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gk97b webserver-deployment-7b75d79cf5- deployment-2337  11769d4a-f7a9-4134-ad10-91fd26cd8af1 15679 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:929bf4fc47d80ac55a8909bb8539688cc3c8e826f438ac2d53e14d3bc587db0c cni.projectcalico.org/podIP:10.1.93.236/32 cni.projectcalico.org/podIPs:10.1.93.236/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc003382707 0xc003382708}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-426l4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-426l4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.132: INFO: Pod "webserver-deployment-7b75d79cf5-flnsq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-flnsq webserver-deployment-7b75d79cf5- deployment-2337  75f462f3-3c75-4e23-b518-d046326dbd1c 15683 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:d4a546b59137e3308a179b227a333a0ae834e8dc0ce2c7814bb3345001b10eae cni.projectcalico.org/podIP:10.1.96.41/32 cni.projectcalico.org/podIPs:10.1.96.41/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc003382940 0xc003382941}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ppjst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ppjst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.133: INFO: Pod "webserver-deployment-7b75d79cf5-549bb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-549bb webserver-deployment-7b75d79cf5- deployment-2337  92ce7347-00a4-445e-9599-30d084049871 15695 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:b721194117101af69f76cd9a2602ece70332e5ce75271a43dcf6d45b8ab15fc2 cni.projectcalico.org/podIP:10.1.96.42/32 cni.projectcalico.org/podIPs:10.1.96.42/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc003382b77 0xc003382b78}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5qqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5qqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.133: INFO: Pod "webserver-deployment-7b75d79cf5-2jkrp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2jkrp webserver-deployment-7b75d79cf5- deployment-2337  908241c9-9a4f-4ec8-96c3-a2e03315de06 15696 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:8e1ccf2aa42f723af12bc6d5d3ef59d4de67186d923d75cbe1592e3abc823375 cni.projectcalico.org/podIP:10.1.93.237/32 cni.projectcalico.org/podIPs:10.1.93.237/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc003382da7 0xc003382da8}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-86477,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-86477,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.133: INFO: Pod "webserver-deployment-67bd4bf6dc-24cd5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-24cd5 webserver-deployment-67bd4bf6dc- deployment-2337  06894725-9d88-400a-af84-48e37d4fb3ba 15703 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:2c8297a464a9dc0b14d059c93d3eb3ea8dee05219473be67331fc28f3f3a4ca2 cni.projectcalico.org/podIP:10.1.93.238/32 cni.projectcalico.org/podIPs:10.1.93.238/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc003382fd0 0xc003382fd1}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfbvj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfbvj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.134: INFO: Pod "webserver-deployment-67bd4bf6dc-xs4qw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xs4qw webserver-deployment-67bd4bf6dc- deployment-2337  0ea5c419-1940-49c3-bec7-27c1ce537e59 15704 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:e7d66fec54911c02382dd818cf0d798235279c43d52cd06d6eb3c293f66f93c5 cni.projectcalico.org/podIP:10.1.96.43/32 cni.projectcalico.org/podIPs:10.1.96.43/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc0033831e7 0xc0033831e8}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l9v9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l9v9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.134: INFO: Pod "webserver-deployment-67bd4bf6dc-82vzs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-82vzs webserver-deployment-67bd4bf6dc- deployment-2337  694aa791-3ab3-4605-bc13-7841f0e81fbd 15711 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:fed3d3a232d3c16936fac99b0d76df532905109a9b5bf2c6bf3194c43b487cc5 cni.projectcalico.org/podIP:10.1.93.239/32 cni.projectcalico.org/podIPs:10.1.93.239/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc0033833f7 0xc0033833f8}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66k76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66k76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.134: INFO: Pod "webserver-deployment-7b75d79cf5-5w2rp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5w2rp webserver-deployment-7b75d79cf5- deployment-2337  e8a046a6-6942-4126-a2ff-db72f03ea1a0 15715 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:db85e8cb847ebb1717dcb0fa7066b4b9719ba42cef9bb5d2b248af20c895d3a8 cni.projectcalico.org/podIP:10.1.96.39/32 cni.projectcalico.org/podIPs:10.1.96.39/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc003383617 0xc003383618}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrvkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrvkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.134: INFO: Pod "webserver-deployment-7b75d79cf5-6ckgn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6ckgn webserver-deployment-7b75d79cf5- deployment-2337  b665f502-3fc0-4c4b-a384-82b841b05a4c 15722 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:6a99298663029f6ed0380d492c1480a6358c08ce55e187a2388757d470aac378 cni.projectcalico.org/podIP:10.1.93.240/32 cni.projectcalico.org/podIPs:10.1.93.240/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc003383847 0xc003383848}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lsm9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lsm9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.135: INFO: Pod "webserver-deployment-7b75d79cf5-m4fdf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-m4fdf webserver-deployment-7b75d79cf5- deployment-2337  2f5ca648-b49d-4093-8788-f1d8ccb379e7 15725 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:f929997ea4bf58f0517f78457ad02fb27ce0c0d3e6817c335ec0a51117010b11 cni.projectcalico.org/podIP:10.1.96.44/32 cni.projectcalico.org/podIPs:10.1.96.44/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8e90c19e-4482-48b9-8bbe-44cc5499c530 0xc003383aa0 0xc003383aa1}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e90c19e-4482-48b9-8bbe-44cc5499c530\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qrq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qrq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.135: INFO: Pod "webserver-deployment-67bd4bf6dc-wdxc8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wdxc8 webserver-deployment-67bd4bf6dc- deployment-2337  d7ad8261-85b7-479f-9b56-53d1253f9c86 15732 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:aabcd5b9765bc2ec20538d037e8ff1a3d80b7cf39cb219bd423a038ef7699d09 cni.projectcalico.org/podIP:10.1.93.241/32 cni.projectcalico.org/podIPs:10.1.93.241/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc003383ce7 0xc003383ce8}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pgxp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pgxp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.136: INFO: Pod "webserver-deployment-67bd4bf6dc-95cjv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-95cjv webserver-deployment-67bd4bf6dc- deployment-2337  acb11d09-7aee-4ae2-bf48-d248388abf05 15743 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:e3aedb9b3e14f77323d8f33feaa952b47eaeeafc63ba647537ba21a18a8db925 cni.projectcalico.org/podIP:10.1.93.242/32 cni.projectcalico.org/podIPs:10.1.93.242/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc003383f07 0xc003383f08}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pppp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pppp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-91,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.91,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.136: INFO: Pod "webserver-deployment-67bd4bf6dc-2gvr7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2gvr7 webserver-deployment-67bd4bf6dc- deployment-2337  974be763-596a-4992-8a4b-34e99753fa6e 15745 0 2023-04-26 06:53:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:516d69b37d0fe89bdd78fcab838063ada3a2b079d5c958a6b4f730bcc137c152 cni.projectcalico.org/podIP:10.1.96.46/32 cni.projectcalico.org/podIPs:10.1.96.46/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b99d1558-44b5-4ef9-9880-4fb11dd86953 0xc00434e2a7 0xc00434e2a8}] [] [{kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b99d1558-44b5-4ef9-9880-4fb11dd86953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-04-26 06:53:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-26 06:53:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwxgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwxgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 06:53:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:,StartTime:2023-04-26 06:53:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 06:53:11.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2337" for this suite. @ 04/26/23 06:53:11.151
• [8.254 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 04/26/23 06:53:11.179
  Apr 26 06:53:11.179: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename statefulset @ 04/26/23 06:53:11.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:53:11.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:53:11.214
  STEP: Creating service test in namespace statefulset-8507 @ 04/26/23 06:53:11.219
  STEP: Looking for a node to schedule stateful set and pod @ 04/26/23 06:53:11.235
  STEP: Creating pod with conflicting port in namespace statefulset-8507 @ 04/26/23 06:53:11.252
  STEP: Waiting until pod test-pod will start running in namespace statefulset-8507 @ 04/26/23 06:53:11.318
  E0426 06:53:12.088187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:13.088313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-8507 @ 04/26/23 06:53:13.332
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8507 @ 04/26/23 06:53:13.339
  Apr 26 06:53:13.354: INFO: Observed stateful pod in namespace: statefulset-8507, name: ss-0, uid: 9d8b821a-c3c6-436a-8098-b390407af583, status phase: Pending. Waiting for statefulset controller to delete.
  E0426 06:53:14.088426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:14.252: INFO: Observed stateful pod in namespace: statefulset-8507, name: ss-0, uid: 9d8b821a-c3c6-436a-8098-b390407af583, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 26 06:53:14.265: INFO: Observed stateful pod in namespace: statefulset-8507, name: ss-0, uid: 9d8b821a-c3c6-436a-8098-b390407af583, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 26 06:53:14.271: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8507
  STEP: Removing pod with conflicting port in namespace statefulset-8507 @ 04/26/23 06:53:14.271
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8507 and will be in running state @ 04/26/23 06:53:14.295
  E0426 06:53:15.088925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:16.089077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:16.301: INFO: Deleting all statefulset in ns statefulset-8507
  Apr 26 06:53:16.303: INFO: Scaling statefulset ss to 0
  E0426 06:53:17.089078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:18.089161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:19.089332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:20.089489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:21.090376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:22.091277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:23.092000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:24.092341      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:25.093003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:26.093227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:26.321: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 06:53:26.324: INFO: Deleting statefulset ss
  Apr 26 06:53:26.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8507" for this suite. @ 04/26/23 06:53:26.341
• [15.172 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/26/23 06:53:26.351
  Apr 26 06:53:26.351: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename namespaces @ 04/26/23 06:53:26.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:53:26.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:53:26.373
  STEP: Creating a test namespace @ 04/26/23 06:53:26.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:53:26.395
  STEP: Creating a service in the namespace @ 04/26/23 06:53:26.397
  STEP: Deleting the namespace @ 04/26/23 06:53:26.408
  STEP: Waiting for the namespace to be removed. @ 04/26/23 06:53:26.422
  E0426 06:53:27.093840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:28.094665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:29.095153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:30.095402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:31.095932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:32.096828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/26/23 06:53:32.425
  STEP: Verifying there is no service in the namespace @ 04/26/23 06:53:32.444
  Apr 26 06:53:32.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7946" for this suite. @ 04/26/23 06:53:32.449
  STEP: Destroying namespace "nsdeletetest-6006" for this suite. @ 04/26/23 06:53:32.456
  Apr 26 06:53:32.458: INFO: Namespace nsdeletetest-6006 was already deleted
  STEP: Destroying namespace "nsdeletetest-2692" for this suite. @ 04/26/23 06:53:32.458
• [6.114 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/26/23 06:53:32.466
  Apr 26 06:53:32.466: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename namespaces @ 04/26/23 06:53:32.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:53:32.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:53:32.485
  STEP: Read namespace status @ 04/26/23 06:53:32.488
  Apr 26 06:53:32.490: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/26/23 06:53:32.49
  Apr 26 06:53:32.497: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/26/23 06:53:32.497
  Apr 26 06:53:32.506: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 26 06:53:32.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9771" for this suite. @ 04/26/23 06:53:32.509
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/26/23 06:53:32.517
  Apr 26 06:53:32.517: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-watch @ 04/26/23 06:53:32.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:53:32.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:53:32.54
  Apr 26 06:53:32.543: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 06:53:33.097176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:34.097828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 04/26/23 06:53:35.077
  Apr 26 06:53:35.085: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T06:53:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T06:53:35Z]] name:name1 resourceVersion:16379 uid:d86cb1c6-5ff1-4892-98c4-85fe6a3d1489] num:map[num1:9223372036854775807 num2:1000000]]}
  E0426 06:53:35.098266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:36.098381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:37.098740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:38.099139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:39.099384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:40.100478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:41.101234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:42.101654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:43.102145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:44.102546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 04/26/23 06:53:45.085
  Apr 26 06:53:45.095: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T06:53:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T06:53:45Z]] name:name2 resourceVersion:16408 uid:978c11ab-1fa1-4c7d-9a8f-9bad589aee45] num:map[num1:9223372036854775807 num2:1000000]]}
  E0426 06:53:45.103306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:46.103525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:47.103768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:48.104133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:49.104275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:50.104522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:51.104682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:52.104915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:53.105104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:54.105228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 04/26/23 06:53:55.095
  E0426 06:53:55.105674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:53:55.108: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T06:53:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T06:53:55Z]] name:name1 resourceVersion:16425 uid:d86cb1c6-5ff1-4892-98c4-85fe6a3d1489] num:map[num1:9223372036854775807 num2:1000000]]}
  E0426 06:53:56.105825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:57.106070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:58.106139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:53:59.106551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:00.106736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:01.107275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:02.107439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:03.108500      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:04.108630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 04/26/23 06:54:05.108
  E0426 06:54:05.108825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:54:05.118: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T06:53:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T06:54:05Z]] name:name2 resourceVersion:16442 uid:978c11ab-1fa1-4c7d-9a8f-9bad589aee45] num:map[num1:9223372036854775807 num2:1000000]]}
  E0426 06:54:06.109797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:07.110072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:08.110157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:09.111203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:10.111445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:11.111855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:12.111992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:13.113017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:14.113220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:15.113457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 04/26/23 06:54:15.119
  Apr 26 06:54:15.129: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T06:53:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T06:53:55Z]] name:name1 resourceVersion:16459 uid:d86cb1c6-5ff1-4892-98c4-85fe6a3d1489] num:map[num1:9223372036854775807 num2:1000000]]}
  E0426 06:54:16.113593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:17.113832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:18.114316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:19.114566      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:20.115267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:21.115498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:22.115751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:23.116689      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:24.117116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:25.117290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 04/26/23 06:54:25.129
  Apr 26 06:54:25.140: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-26T06:53:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-26T06:54:05Z]] name:name2 resourceVersion:16476 uid:978c11ab-1fa1-4c7d-9a8f-9bad589aee45] num:map[num1:9223372036854775807 num2:1000000]]}
  E0426 06:54:26.118141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:27.119257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:28.119659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:29.119943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:30.120430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:31.120624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:32.120997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:33.121828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:34.122081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:35.122159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:54:35.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-4806" for this suite. @ 04/26/23 06:54:35.656
• [63.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/26/23 06:54:35.665
  Apr 26 06:54:35.665: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename cronjob @ 04/26/23 06:54:35.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:54:35.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:54:35.683
  STEP: Creating a ReplaceConcurrent cronjob @ 04/26/23 06:54:35.712
  STEP: Ensuring a job is scheduled @ 04/26/23 06:54:35.718
  E0426 06:54:36.122905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:37.123610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:38.124718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:39.124952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:40.125128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:41.125347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:42.125611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:43.126471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:44.126576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:45.126866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:46.127571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:47.127696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:48.127873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:49.128075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:50.129184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:51.129436      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:52.129877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:53.130643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:54.130785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:55.131207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:56.131547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:57.131842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:58.132077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:54:59.132289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:00.132843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:01.133346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/26/23 06:55:01.722
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/26/23 06:55:01.725
  STEP: Ensuring the job is replaced with a new one @ 04/26/23 06:55:01.727
  E0426 06:55:02.133954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:03.134903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:04.135363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:05.135614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:06.136285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:07.136406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:08.136593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:09.136811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:10.137853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:11.138396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:12.139402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:13.140069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:14.141042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:15.141621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:16.142133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:17.143220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:18.144126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:19.144388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:20.144413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:21.144649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:22.146455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:23.147274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:24.148033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:25.148298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:26.149181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:27.150104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:28.150730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:29.150973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:30.151392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:31.151679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:32.152756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:33.153322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:34.154177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:35.155290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:36.156402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:37.156573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:38.157504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:39.157812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:40.158159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:41.159246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:42.159331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:43.159949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:44.159961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:45.160255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:46.160724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:47.161573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:48.162355      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:49.162836      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:50.163205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:51.163475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:52.164152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:53.164809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:54.165808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:55.165957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:56.166141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:57.166418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:58.167241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:55:59.167397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:00.167576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:01.167891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/26/23 06:56:01.731
  Apr 26 06:56:01.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8433" for this suite. @ 04/26/23 06:56:01.748
• [86.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/26/23 06:56:01.759
  Apr 26 06:56:01.759: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replicaset @ 04/26/23 06:56:01.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:01.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:01.786
  Apr 26 06:56:01.801: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0426 06:56:02.168957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:03.169545      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:04.170152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:05.170438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:06.170645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:56:06.805: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/26/23 06:56:06.805
  STEP: Scaling up "test-rs" replicaset  @ 04/26/23 06:56:06.805
  Apr 26 06:56:06.815: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/26/23 06:56:06.815
  W0426 06:56:06.826418      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 26 06:56:06.827: INFO: observed ReplicaSet test-rs in namespace replicaset-50 with ReadyReplicas 1, AvailableReplicas 1
  Apr 26 06:56:06.849: INFO: observed ReplicaSet test-rs in namespace replicaset-50 with ReadyReplicas 1, AvailableReplicas 1
  Apr 26 06:56:06.874: INFO: observed ReplicaSet test-rs in namespace replicaset-50 with ReadyReplicas 1, AvailableReplicas 1
  Apr 26 06:56:06.888: INFO: observed ReplicaSet test-rs in namespace replicaset-50 with ReadyReplicas 1, AvailableReplicas 1
  E0426 06:56:07.170857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:56:07.644: INFO: observed ReplicaSet test-rs in namespace replicaset-50 with ReadyReplicas 2, AvailableReplicas 2
  Apr 26 06:56:08.065: INFO: observed Replicaset test-rs in namespace replicaset-50 with ReadyReplicas 3 found true
  Apr 26 06:56:08.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-50" for this suite. @ 04/26/23 06:56:08.069
• [6.316 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/26/23 06:56:08.076
  Apr 26 06:56:08.076: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 06:56:08.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:08.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:08.094
  STEP: Creating secret with name secret-test-0952bdcc-13df-4d2c-a5e4-40e1fa7d64e2 @ 04/26/23 06:56:08.097
  STEP: Creating a pod to test consume secrets @ 04/26/23 06:56:08.102
  E0426 06:56:08.170965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:09.173045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:10.173876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:11.174133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:56:12.121
  Apr 26 06:56:12.124: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-secrets-c2e63cb2-903b-413a-898c-270a9bd7bbdf container secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:56:12.137
  Apr 26 06:56:12.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2694" for this suite. @ 04/26/23 06:56:12.162
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/26/23 06:56:12.169
  Apr 26 06:56:12.169: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:56:12.17
  E0426 06:56:12.174650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:12.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:12.193
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 06:56:12.195
  E0426 06:56:13.175333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:14.175580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:15.175661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:16.175912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:56:16.217
  Apr 26 06:56:16.219: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-2c63c045-1f03-4079-b333-5e55141e7faf container client-container: <nil>
  STEP: delete the pod @ 04/26/23 06:56:16.224
  Apr 26 06:56:16.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6355" for this suite. @ 04/26/23 06:56:16.245
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/26/23 06:56:16.256
  Apr 26 06:56:16.256: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename field-validation @ 04/26/23 06:56:16.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:16.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:16.277
  STEP: apply creating a deployment @ 04/26/23 06:56:16.28
  Apr 26 06:56:16.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1528" for this suite. @ 04/26/23 06:56:16.292
• [0.043 seconds]
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/26/23 06:56:16.3
  Apr 26 06:56:16.300: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename gc @ 04/26/23 06:56:16.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:16.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:16.32
  STEP: create the deployment @ 04/26/23 06:56:16.322
  W0426 06:56:16.329996      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/26/23 06:56:16.33
  STEP: delete the deployment @ 04/26/23 06:56:16.838
  STEP: wait for all rs to be garbage collected @ 04/26/23 06:56:16.855
  STEP: expected 0 rs, got 1 rs @ 04/26/23 06:56:16.865
  STEP: expected 0 pods, got 2 pods @ 04/26/23 06:56:16.874
  E0426 06:56:17.175914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/26/23 06:56:17.385
  W0426 06:56:17.389972      22 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 26 06:56:17.390: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 26 06:56:17.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2556" for this suite. @ 04/26/23 06:56:17.393
• [1.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/26/23 06:56:17.4
  Apr 26 06:56:17.400: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-pred @ 04/26/23 06:56:17.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:17.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:17.419
  Apr 26 06:56:17.422: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 26 06:56:17.455: INFO: Waiting for terminating namespaces to be deleted...
  Apr 26 06:56:17.486: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-91 before test
  Apr 26 06:56:17.502: INFO: calico-node-d6hf2 from kube-system started at 2023-04-26 06:03:27 +0000 UTC (1 container statuses recorded)
  Apr 26 06:56:17.502: INFO: 	Container calico-node ready: true, restart count 0
  Apr 26 06:56:17.502: INFO: sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-wbxsz from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:56:17.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:56:17.502: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 26 06:56:17.502: INFO: sonobuoy-e2e-job-5266529d9a654ea5 from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:56:17.502: INFO: 	Container e2e ready: true, restart count 0
  Apr 26 06:56:17.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:56:17.502: INFO: sonobuoy from sonobuoy started at 2023-04-26 06:05:24 +0000 UTC (1 container statuses recorded)
  Apr 26 06:56:17.502: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 26 06:56:17.502: INFO: coredns-7745f9f87f-hgdql from kube-system started at 2023-04-26 06:06:48 +0000 UTC (1 container statuses recorded)
  Apr 26 06:56:17.502: INFO: 	Container coredns ready: true, restart count 0
  Apr 26 06:56:17.502: INFO: calico-kube-controllers-6c99c8747f-plv8f from kube-system started at 2023-04-26 06:06:48 +0000 UTC (1 container statuses recorded)
  Apr 26 06:56:17.502: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Apr 26 06:56:17.502: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-3-127 before test
  Apr 26 06:56:17.511: INFO: calico-node-578zf from kube-system started at 2023-04-26 06:02:34 +0000 UTC (1 container statuses recorded)
  Apr 26 06:56:17.511: INFO: 	Container calico-node ready: true, restart count 0
  Apr 26 06:56:17.511: INFO: sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-ndzdn from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 06:56:17.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 06:56:17.511: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 26 06:56:17.511: INFO: replace-28041535-pgc57 from cronjob-8433 started at 2023-04-26 06:55:00 +0000 UTC (1 container statuses recorded)
  Apr 26 06:56:17.511: INFO: 	Container c ready: true, restart count 0
  Apr 26 06:56:17.511: INFO: replace-28041536-nwg7s from cronjob-8433 started at 2023-04-26 06:56:00 +0000 UTC (1 container statuses recorded)
  Apr 26 06:56:17.511: INFO: 	Container c ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/26/23 06:56:17.511
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175969b8a335d1cd], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] @ 04/26/23 06:56:17.538
  E0426 06:56:18.176442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:56:18.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-97" for this suite. @ 04/26/23 06:56:18.539
• [1.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/26/23 06:56:18.55
  Apr 26 06:56:18.550: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:56:18.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:18.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:18.569
  STEP: Discovering how many secrets are in namespace by default @ 04/26/23 06:56:18.571
  E0426 06:56:19.176541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:20.176680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:21.177270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:22.177784      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:23.177942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/26/23 06:56:23.575
  E0426 06:56:24.178888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:25.179614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:26.180295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:27.180413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:28.181442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/26/23 06:56:28.579
  STEP: Ensuring resource quota status is calculated @ 04/26/23 06:56:28.588
  E0426 06:56:29.182390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:30.182657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 04/26/23 06:56:30.592
  STEP: Ensuring resource quota status captures secret creation @ 04/26/23 06:56:30.619
  E0426 06:56:31.182860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:32.183771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 04/26/23 06:56:32.622
  STEP: Ensuring resource quota status released usage @ 04/26/23 06:56:32.629
  E0426 06:56:33.184303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:34.184484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:56:34.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4856" for this suite. @ 04/26/23 06:56:34.636
• [16.094 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/26/23 06:56:34.645
  Apr 26 06:56:34.645: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:56:34.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:34.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:34.666
  STEP: Creating a ResourceQuota @ 04/26/23 06:56:34.668
  STEP: Getting a ResourceQuota @ 04/26/23 06:56:34.675
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/26/23 06:56:34.677
  STEP: Patching the ResourceQuota @ 04/26/23 06:56:34.68
  STEP: Deleting a Collection of ResourceQuotas @ 04/26/23 06:56:34.688
  STEP: Verifying the deleted ResourceQuota @ 04/26/23 06:56:34.699
  Apr 26 06:56:34.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9821" for this suite. @ 04/26/23 06:56:34.704
• [0.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/26/23 06:56:34.713
  Apr 26 06:56:34.713: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-runtime @ 04/26/23 06:56:34.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:34.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:34.736
  STEP: create the container @ 04/26/23 06:56:34.739
  W0426 06:56:34.748134      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/26/23 06:56:34.748
  E0426 06:56:35.185351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:36.185721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:37.185795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/26/23 06:56:37.763
  STEP: the container should be terminated @ 04/26/23 06:56:37.765
  STEP: the termination message should be set @ 04/26/23 06:56:37.766
  Apr 26 06:56:37.766: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/26/23 06:56:37.766
  Apr 26 06:56:37.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2944" for this suite. @ 04/26/23 06:56:37.79
• [3.085 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/26/23 06:56:37.798
  Apr 26 06:56:37.798: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-preemption @ 04/26/23 06:56:37.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:56:37.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:56:37.817
  Apr 26 06:56:37.840: INFO: Waiting up to 1m0s for all nodes to be ready
  E0426 06:56:38.186859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:39.187366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:40.187612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:41.187776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:42.188204      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:43.188684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:44.189701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:45.190220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:46.190590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:47.190740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:48.190841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:49.191967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:50.192309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:51.192548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:52.192824      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:53.193754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:54.194107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:55.194130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:56.195245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:57.196356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:58.197456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:56:59.197660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:00.198070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:01.198517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:02.199471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:03.199661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:04.200081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:05.201141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:06.201520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:07.201700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:08.201875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:09.202124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:10.202789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:11.202955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:12.203236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:13.203721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:14.204612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:15.204863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:16.205115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:17.205294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:18.205857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:19.206075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:20.207091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:21.207553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:22.208376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:23.209150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:24.210123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:25.210168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:26.211148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:27.211380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:28.212067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:29.212213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:30.212396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:31.212616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:32.213461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:33.214112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:34.214163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:35.215275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:36.215597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:37.215840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:57:37.857: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/26/23 06:57:37.86
  Apr 26 06:57:37.910: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 26 06:57:37.917: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 26 06:57:37.938: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 26 06:57:37.947: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/26/23 06:57:37.947
  E0426 06:57:38.216438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:39.216690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/26/23 06:57:39.962
  E0426 06:57:40.217426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:41.217664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:42.218747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:43.219315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:44.220172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:45.220378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:57:45.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-40" for this suite. @ 04/26/23 06:57:46.027
• [68.236 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/26/23 06:57:46.035
  Apr 26 06:57:46.035: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename hostport @ 04/26/23 06:57:46.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:57:46.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:57:46.057
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/26/23 06:57:46.062
  E0426 06:57:46.221507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:47.222509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.1.91 on the node which pod1 resides and expect scheduled @ 04/26/23 06:57:48.077
  E0426 06:57:48.222861      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:49.223270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:50.224064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:51.224310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.1.91 but use UDP protocol on the node which pod2 resides @ 04/26/23 06:57:52.095
  E0426 06:57:52.224750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:53.225079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:54.225783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:55.226143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/26/23 06:57:56.122
  Apr 26 06:57:56.122: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.1.91 http://127.0.0.1:54323/hostname] Namespace:hostport-437 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:57:56.122: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:57:56.123: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:57:56.123: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-437/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.1.91+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.91, port: 54323 @ 04/26/23 06:57:56.206
  Apr 26 06:57:56.206: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.1.91:54323/hostname] Namespace:hostport-437 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:57:56.206: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:57:56.206: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:57:56.206: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-437/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.1.91%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0426 06:57:56.226712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.1.91, port: 54323 UDP @ 04/26/23 06:57:56.29
  Apr 26 06:57:56.290: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.1.91 54323] Namespace:hostport-437 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 06:57:56.290: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 06:57:56.290: INFO: ExecWithOptions: Clientset creation
  Apr 26 06:57:56.290: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-437/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.1.91+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0426 06:57:57.227645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:58.228150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:57:59.228350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:00.228499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:01.228580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:01.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-437" for this suite. @ 04/26/23 06:58:01.364
• [15.336 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/26/23 06:58:01.375
  Apr 26 06:58:01.375: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 06:58:01.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:01.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:01.43
  STEP: Creating a ResourceQuota @ 04/26/23 06:58:01.432
  STEP: Getting a ResourceQuota @ 04/26/23 06:58:01.438
  STEP: Updating a ResourceQuota @ 04/26/23 06:58:01.441
  STEP: Verifying a ResourceQuota was modified @ 04/26/23 06:58:01.451
  STEP: Deleting a ResourceQuota @ 04/26/23 06:58:01.453
  STEP: Verifying the deleted ResourceQuota @ 04/26/23 06:58:01.462
  Apr 26 06:58:01.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4228" for this suite. @ 04/26/23 06:58:01.467
• [0.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/26/23 06:58:01.477
  Apr 26 06:58:01.477: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 06:58:01.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:01.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:01.499
  STEP: creating all guestbook components @ 04/26/23 06:58:01.502
  Apr 26 06:58:01.502: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 26 06:58:01.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 create -f -'
  E0426 06:58:02.229453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:02.287: INFO: stderr: ""
  Apr 26 06:58:02.287: INFO: stdout: "service/agnhost-replica created\n"
  Apr 26 06:58:02.287: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 26 06:58:02.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 create -f -'
  Apr 26 06:58:02.698: INFO: stderr: ""
  Apr 26 06:58:02.698: INFO: stdout: "service/agnhost-primary created\n"
  Apr 26 06:58:02.698: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 26 06:58:02.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 create -f -'
  Apr 26 06:58:03.000: INFO: stderr: ""
  Apr 26 06:58:03.000: INFO: stdout: "service/frontend created\n"
  Apr 26 06:58:03.000: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 26 06:58:03.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 create -f -'
  E0426 06:58:03.230286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:03.261: INFO: stderr: ""
  Apr 26 06:58:03.261: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 26 06:58:03.261: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 26 06:58:03.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 create -f -'
  Apr 26 06:58:03.510: INFO: stderr: ""
  Apr 26 06:58:03.510: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 26 06:58:03.510: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 26 06:58:03.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 create -f -'
  Apr 26 06:58:03.753: INFO: stderr: ""
  Apr 26 06:58:03.753: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/26/23 06:58:03.753
  Apr 26 06:58:03.753: INFO: Waiting for all frontend pods to be Running.
  E0426 06:58:04.231053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:05.232116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:06.232135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:07.234683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:08.234880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:08.804: INFO: Waiting for frontend to serve content.
  Apr 26 06:58:08.812: INFO: Trying to add a new entry to the guestbook.
  Apr 26 06:58:08.819: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/26/23 06:58:08.826
  Apr 26 06:58:08.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 delete --grace-period=0 --force -f -'
  Apr 26 06:58:08.912: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 06:58:08.912: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/26/23 06:58:08.912
  Apr 26 06:58:08.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 delete --grace-period=0 --force -f -'
  Apr 26 06:58:08.995: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 06:58:08.995: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/26/23 06:58:08.995
  Apr 26 06:58:08.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 delete --grace-period=0 --force -f -'
  Apr 26 06:58:09.079: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 06:58:09.079: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/26/23 06:58:09.079
  Apr 26 06:58:09.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 delete --grace-period=0 --force -f -'
  Apr 26 06:58:09.151: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 06:58:09.151: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/26/23 06:58:09.151
  Apr 26 06:58:09.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 delete --grace-period=0 --force -f -'
  Apr 26 06:58:09.228: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 06:58:09.228: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/26/23 06:58:09.228
  Apr 26 06:58:09.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2263 delete --grace-period=0 --force -f -'
  E0426 06:58:09.235506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:09.316: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 06:58:09.316: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 26 06:58:09.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2263" for this suite. @ 04/26/23 06:58:09.324
• [7.867 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/26/23 06:58:09.345
  Apr 26 06:58:09.345: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 06:58:09.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:09.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:09.381
  STEP: Setting up server cert @ 04/26/23 06:58:09.42
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 06:58:09.932
  STEP: Deploying the webhook pod @ 04/26/23 06:58:09.944
  STEP: Wait for the deployment to be ready @ 04/26/23 06:58:09.964
  Apr 26 06:58:09.974: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 06:58:10.236577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:11.236809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 06:58:11.983
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:58:11.996
  E0426 06:58:12.237356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:12.997: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/26/23 06:58:13.07
  STEP: Creating a configMap that should be mutated @ 04/26/23 06:58:13.081
  STEP: Deleting the collection of validation webhooks @ 04/26/23 06:58:13.109
  STEP: Creating a configMap that should not be mutated @ 04/26/23 06:58:13.174
  Apr 26 06:58:13.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0426 06:58:13.237879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-986" for this suite. @ 04/26/23 06:58:13.25
  STEP: Destroying namespace "webhook-markers-5280" for this suite. @ 04/26/23 06:58:13.262
• [3.934 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/26/23 06:58:13.28
  Apr 26 06:58:13.280: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 06:58:13.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:13.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:13.303
  STEP: Creating secret with name secret-test-f8a66518-3c4b-4de4-8d63-5e361a6ea8b9 @ 04/26/23 06:58:13.305
  STEP: Creating a pod to test consume secrets @ 04/26/23 06:58:13.311
  E0426 06:58:14.238291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:15.238620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:16.239088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:17.239584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:58:17.33
  Apr 26 06:58:17.332: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-secrets-f59a40eb-2056-45c4-b5b7-e3554e1c7870 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:58:17.345
  Apr 26 06:58:17.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-933" for this suite. @ 04/26/23 06:58:17.366
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/26/23 06:58:17.374
  Apr 26 06:58:17.374: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename dns @ 04/26/23 06:58:17.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:17.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:17.394
  STEP: Creating a test headless service @ 04/26/23 06:58:17.436
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 223.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.223_udp@PTR;check="$$(dig +tcp +noall +answer +search 223.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.223_tcp@PTR;sleep 1; done
   @ 04/26/23 06:58:17.468
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5324.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5324.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5324.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5324.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 223.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.223_udp@PTR;check="$$(dig +tcp +noall +answer +search 223.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.223_tcp@PTR;sleep 1; done
   @ 04/26/23 06:58:17.468
  STEP: creating a pod to probe DNS @ 04/26/23 06:58:17.468
  STEP: submitting the pod to kubernetes @ 04/26/23 06:58:17.468
  E0426 06:58:18.239746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:19.240388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/26/23 06:58:19.495
  STEP: looking for the results for each expected name from probers @ 04/26/23 06:58:19.497
  Apr 26 06:58:19.501: INFO: Unable to read wheezy_udp@dns-test-service.dns-5324.svc.cluster.local from pod dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e: the server could not find the requested resource (get pods dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e)
  Apr 26 06:58:19.503: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5324.svc.cluster.local from pod dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e: the server could not find the requested resource (get pods dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e)
  Apr 26 06:58:19.506: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local from pod dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e: the server could not find the requested resource (get pods dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e)
  Apr 26 06:58:19.508: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local from pod dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e: the server could not find the requested resource (get pods dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e)
  Apr 26 06:58:19.521: INFO: Unable to read jessie_udp@dns-test-service.dns-5324.svc.cluster.local from pod dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e: the server could not find the requested resource (get pods dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e)
  Apr 26 06:58:19.524: INFO: Unable to read jessie_tcp@dns-test-service.dns-5324.svc.cluster.local from pod dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e: the server could not find the requested resource (get pods dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e)
  Apr 26 06:58:19.526: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local from pod dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e: the server could not find the requested resource (get pods dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e)
  Apr 26 06:58:19.529: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local from pod dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e: the server could not find the requested resource (get pods dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e)
  Apr 26 06:58:19.540: INFO: Lookups using dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e failed for: [wheezy_udp@dns-test-service.dns-5324.svc.cluster.local wheezy_tcp@dns-test-service.dns-5324.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local jessie_udp@dns-test-service.dns-5324.svc.cluster.local jessie_tcp@dns-test-service.dns-5324.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5324.svc.cluster.local]

  E0426 06:58:20.241117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:21.241305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:22.241485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:23.241633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:24.241776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:24.582: INFO: DNS probes using dns-5324/dns-test-45272b57-1c63-42d0-903f-4ae622a31b1e succeeded

  Apr 26 06:58:24.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 06:58:24.585
  STEP: deleting the test service @ 04/26/23 06:58:24.607
  STEP: deleting the test headless service @ 04/26/23 06:58:24.683
  STEP: Destroying namespace "dns-5324" for this suite. @ 04/26/23 06:58:24.699
• [7.335 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 04/26/23 06:58:24.709
  Apr 26 06:58:24.709: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename statefulset @ 04/26/23 06:58:24.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:24.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:24.731
  STEP: Creating service test in namespace statefulset-1700 @ 04/26/23 06:58:24.733
  Apr 26 06:58:24.758: INFO: Found 0 stateful pods, waiting for 1
  E0426 06:58:25.242166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:26.242189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:27.243235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:28.243518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:29.243764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:30.243994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:31.244227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:32.244445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:33.245071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:34.245311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:34.762: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/26/23 06:58:34.767
  W0426 06:58:34.780340      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 26 06:58:34.785: INFO: Found 1 stateful pods, waiting for 2
  E0426 06:58:35.245810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:36.245983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:37.246163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:38.246267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:39.246615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:40.246803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:41.247253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:42.247466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:43.247746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:44.247966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:44.790: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 06:58:44.790: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/26/23 06:58:44.795
  STEP: Delete all of the StatefulSets @ 04/26/23 06:58:44.798
  STEP: Verify that StatefulSets have been deleted @ 04/26/23 06:58:44.806
  Apr 26 06:58:44.810: INFO: Deleting all statefulset in ns statefulset-1700
  Apr 26 06:58:44.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1700" for this suite. @ 04/26/23 06:58:44.845
• [20.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/26/23 06:58:44.861
  Apr 26 06:58:44.861: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename svcaccounts @ 04/26/23 06:58:44.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:44.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:44.884
  E0426 06:58:45.248352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:46.248667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/26/23 06:58:46.918
  Apr 26 06:58:46.918: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9126 pod-service-account-79a1c0bf-a4ce-4b83-8bb2-ff8e5860f9ff -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 04/26/23 06:58:47.058
  Apr 26 06:58:47.058: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9126 pod-service-account-79a1c0bf-a4ce-4b83-8bb2-ff8e5860f9ff -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 04/26/23 06:58:47.207
  Apr 26 06:58:47.207: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9126 pod-service-account-79a1c0bf-a4ce-4b83-8bb2-ff8e5860f9ff -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  E0426 06:58:47.249506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:47.354: INFO: Got root ca configmap in namespace "svcaccounts-9126"
  Apr 26 06:58:47.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9126" for this suite. @ 04/26/23 06:58:47.36
• [2.506 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/26/23 06:58:47.369
  Apr 26 06:58:47.369: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubelet-test @ 04/26/23 06:58:47.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:47.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:47.388
  E0426 06:58:48.250379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:49.251284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:50.251535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:51.251700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:51.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2206" for this suite. @ 04/26/23 06:58:51.435
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/26/23 06:58:51.443
  Apr 26 06:58:51.443: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-webhook @ 04/26/23 06:58:51.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:51.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:51.466
  STEP: Setting up server cert @ 04/26/23 06:58:51.469
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/26/23 06:58:52.173
  STEP: Deploying the custom resource conversion webhook pod @ 04/26/23 06:58:52.192
  STEP: Wait for the deployment to be ready @ 04/26/23 06:58:52.242
  Apr 26 06:58:52.247: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0426 06:58:52.252751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:53.253340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:54.253660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 06:58:54.257
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:58:54.27
  E0426 06:58:55.254174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:55.270: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 26 06:58:55.274: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 06:58:56.254383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:58:57.255272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/26/23 06:58:57.818
  STEP: v2 custom resource should be converted @ 04/26/23 06:58:57.825
  Apr 26 06:58:57.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0426 06:58:58.255405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-1955" for this suite. @ 04/26/23 06:58:58.393
• [6.963 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/26/23 06:58:58.407
  Apr 26 06:58:58.408: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/26/23 06:58:58.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:58.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:58.44
  STEP: creating @ 04/26/23 06:58:58.445
  STEP: getting @ 04/26/23 06:58:58.469
  STEP: listing @ 04/26/23 06:58:58.473
  STEP: deleting @ 04/26/23 06:58:58.476
  Apr 26 06:58:58.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9448" for this suite. @ 04/26/23 06:58:58.498
• [0.099 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 04/26/23 06:58:58.507
  Apr 26 06:58:58.507: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename daemonsets @ 04/26/23 06:58:58.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:58:58.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:58:58.529
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/26/23 06:58:58.546
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/26/23 06:58:58.554
  Apr 26 06:58:58.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 06:58:58.562: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:58:59.256066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:58:59.568: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 06:58:59.568: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 06:59:00.256085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:59:00.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 06:59:00.574: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/26/23 06:59:00.576
  Apr 26 06:59:00.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 06:59:00.596: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/26/23 06:59:00.596
  E0426 06:59:01.256286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting DaemonSet "daemon-set" @ 04/26/23 06:59:01.604
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3063, will wait for the garbage collector to delete the pods @ 04/26/23 06:59:01.604
  Apr 26 06:59:01.668: INFO: Deleting DaemonSet.extensions daemon-set took: 10.711537ms
  Apr 26 06:59:01.769: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.455929ms
  E0426 06:59:02.257327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:03.258318      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:04.259224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:59:04.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 06:59:04.573: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 26 06:59:04.576: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18350"},"items":null}

  Apr 26 06:59:04.578: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18350"},"items":null}

  Apr 26 06:59:04.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3063" for this suite. @ 04/26/23 06:59:04.589
• [6.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/26/23 06:59:04.598
  Apr 26 06:59:04.598: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 06:59:04.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:59:04.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:59:04.617
  STEP: Setting up server cert @ 04/26/23 06:59:04.641
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 06:59:05.112
  STEP: Deploying the webhook pod @ 04/26/23 06:59:05.12
  STEP: Wait for the deployment to be ready @ 04/26/23 06:59:05.133
  Apr 26 06:59:05.139: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 06:59:05.259655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:06.259857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 06:59:07.148
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:59:07.158
  E0426 06:59:07.260216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:59:08.159: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/26/23 06:59:08.162
  STEP: create a configmap that should be updated by the webhook @ 04/26/23 06:59:08.177
  Apr 26 06:59:08.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3220" for this suite. @ 04/26/23 06:59:08.249
  STEP: Destroying namespace "webhook-markers-7224" for this suite. @ 04/26/23 06:59:08.258
  E0426 06:59:08.260848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
• [3.667 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/26/23 06:59:08.265
  Apr 26 06:59:08.265: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 06:59:08.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:59:08.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:59:08.284
  STEP: Setting up server cert @ 04/26/23 06:59:08.307
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 06:59:09.055
  STEP: Deploying the webhook pod @ 04/26/23 06:59:09.063
  STEP: Wait for the deployment to be ready @ 04/26/23 06:59:09.077
  Apr 26 06:59:09.082: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 06:59:09.261850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:10.262104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 06:59:11.091
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 06:59:11.102
  E0426 06:59:11.262131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:59:12.103: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/26/23 06:59:12.106
  Apr 26 06:59:12.127: INFO: Waiting for webhook configuration to be ready...
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/26/23 06:59:12.236
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/26/23 06:59:12.243
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/26/23 06:59:12.253
  E0426 06:59:12.263217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/26/23 06:59:12.265
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/26/23 06:59:12.273
  Apr 26 06:59:12.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7111" for this suite. @ 04/26/23 06:59:12.333
  STEP: Destroying namespace "webhook-markers-4100" for this suite. @ 04/26/23 06:59:12.343
• [4.092 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/26/23 06:59:12.357
  Apr 26 06:59:12.357: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 06:59:12.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:59:12.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:59:12.382
  STEP: creating a Pod with a static label @ 04/26/23 06:59:12.393
  STEP: watching for Pod to be ready @ 04/26/23 06:59:12.401
  Apr 26 06:59:12.402: INFO: observed Pod pod-test in namespace pods-4828 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 26 06:59:12.408: INFO: observed Pod pod-test in namespace pods-4828 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC  }]
  Apr 26 06:59:12.419: INFO: observed Pod pod-test in namespace pods-4828 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC  }]
  Apr 26 06:59:13.007: INFO: observed Pod pod-test in namespace pods-4828 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC  }]
  E0426 06:59:13.263985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:59:13.523: INFO: Found Pod pod-test in namespace pods-4828 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 06:59:12 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/26/23 06:59:13.526
  STEP: getting the Pod and ensuring that it's patched @ 04/26/23 06:59:13.54
  STEP: replacing the Pod's status Ready condition to False @ 04/26/23 06:59:13.542
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/26/23 06:59:13.561
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/26/23 06:59:13.561
  STEP: watching for the Pod to be deleted @ 04/26/23 06:59:13.577
  Apr 26 06:59:13.581: INFO: observed event type MODIFIED
  E0426 06:59:14.264122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:15.264581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:59:15.526: INFO: observed event type MODIFIED
  Apr 26 06:59:15.803: INFO: observed event type MODIFIED
  Apr 26 06:59:15.885: INFO: observed event type MODIFIED
  E0426 06:59:16.265725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 06:59:16.529: INFO: observed event type MODIFIED
  Apr 26 06:59:16.549: INFO: observed event type MODIFIED
  Apr 26 06:59:16.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4828" for this suite. @ 04/26/23 06:59:16.559
• [4.209 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/26/23 06:59:16.567
  Apr 26 06:59:16.567: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 06:59:16.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:59:16.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:59:16.586
  STEP: Creating projection with secret that has name projected-secret-test-ba8824b2-18a5-409d-970f-cc2f092668e9 @ 04/26/23 06:59:16.589
  STEP: Creating a pod to test consume secrets @ 04/26/23 06:59:16.595
  E0426 06:59:17.266160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:18.266163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:19.267239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:20.267368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 06:59:20.613
  Apr 26 06:59:20.616: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-secrets-5fc2e574-4725-430e-bf08-bd5efd85d88d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 06:59:20.621
  Apr 26 06:59:20.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9321" for this suite. @ 04/26/23 06:59:20.643
• [4.084 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/26/23 06:59:20.652
  Apr 26 06:59:20.652: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename cronjob @ 04/26/23 06:59:20.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 06:59:20.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 06:59:20.673
  STEP: Creating a suspended cronjob @ 04/26/23 06:59:20.677
  STEP: Ensuring no jobs are scheduled @ 04/26/23 06:59:20.684
  E0426 06:59:21.267855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:22.268957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:23.269876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:24.270164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:25.271237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:26.271475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:27.271925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:28.272298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:29.272543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:30.272783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:31.273221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:32.273468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:33.274039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:34.274181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:35.274779      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:36.275339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:37.275680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:38.276089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:39.276872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:40.277122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:41.277319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:42.277486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:43.277754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:44.278012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:45.278186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:46.279246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:47.279406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:48.279878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:49.280094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:50.280314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:51.280734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:52.280935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:53.281929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:54.282154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:55.282636      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:56.283260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:57.283484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:58.283870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 06:59:59.283928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:00.284195      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:01.284334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:02.285400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:03.286311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:04.286559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:05.287333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:06.287589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:07.287924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:08.288157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:09.289277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:10.289526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:11.290029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:12.290194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:13.290670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:14.290718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:15.291260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:16.291532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:17.291776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:18.292145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:19.292338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:20.292612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:21.292783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:22.293077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:23.293198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:24.294164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:25.295227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:26.295490      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:27.295869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:28.296258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:29.297056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:30.297311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:31.298020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:32.298150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:33.299216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:34.299502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:35.299937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:36.300940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:37.301135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:38.301314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:39.301660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:40.301877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:41.302264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:42.302403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:43.303220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:44.303398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:45.304231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:46.304453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:47.304834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:48.305293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:49.305651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:50.305915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:51.306115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:52.306426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:53.306747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:54.306965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:55.307273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:56.307486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:57.307608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:58.308005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:00:59.308124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:00.308368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:01.309379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:02.309528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:03.310101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:04.310164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:05.311211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:06.312184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:07.313245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:08.314090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:09.314108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:10.314346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:11.315227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:12.316230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:13.317218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:14.317318      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:15.317921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:16.318163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:17.319282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:18.320015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:19.320441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:20.320690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:21.321013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:22.321264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:23.321955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:24.322251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:25.323286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:26.324086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:27.324543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:28.324921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:29.325965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:30.326135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:31.326184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:32.327199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:33.328188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:34.328393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:35.328970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:36.329096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:37.329576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:38.330141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:39.330538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:40.330800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:41.330854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:42.331291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:43.331895      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:44.332097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:45.332126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:46.332294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:47.332746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:48.333231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:49.333614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:50.333791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:51.334131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:52.334319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:53.335025      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:54.335324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:55.335675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:56.335897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:57.336155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:58.336529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:01:59.336685      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:00.336939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:01.337260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:02.337438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:03.338264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:04.338600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:05.339655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:06.339908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:07.340030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:08.340468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:09.340754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:10.340973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:11.341963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:12.342184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:13.342663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:14.342798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:15.343081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:16.343313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:17.344244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:18.344690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:19.345581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:20.345792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:21.346826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:22.347357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:23.347547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:24.347693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:25.348078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:26.348297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:27.348463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:28.348622      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:29.349538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:30.349787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:31.350672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:32.350870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:33.351555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:34.351804      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:35.352040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:36.352301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:37.352442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:38.352892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:39.353059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:40.353871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:41.354242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:42.355244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:43.356216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:44.356471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:45.357083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:46.357561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:47.358054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:48.358509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:49.358628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:50.359230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:51.359738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:52.360658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:53.361054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:54.361647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:55.361876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:56.362142      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:57.363257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:58.363676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:02:59.364192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:00.364422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:01.364802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:02.365024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:03.365909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:04.366184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:05.367304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:06.367552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:07.367982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:08.368352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:09.368832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:10.369012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:11.369586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:12.369829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:13.370631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:14.371262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:15.372011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:16.372154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:17.372201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:18.372630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:19.372782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:20.373059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:21.373724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:22.374139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:23.374235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:24.375269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:25.376362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:26.376605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:27.377043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:28.377390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:29.378316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:30.379259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:31.379867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:32.380115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:33.380729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:34.380936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:35.381311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:36.381581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:37.382127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:38.382285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:39.383283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:40.383511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:41.384581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:42.384841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:43.385900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:44.386132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:45.386855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:46.387281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:47.387847      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:48.388823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:49.389066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:50.389240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:51.389509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:52.389778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:53.389829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:54.390070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:55.390166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:56.391258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:57.391743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:58.392230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:03:59.392676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:00.392965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:01.393071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:02.393359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:03.393746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:04.394051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:05.394149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:06.395278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:07.395398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:08.395873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:09.396651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:10.396871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:11.396903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:12.397430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:13.397878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:14.397918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:15.398710      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:16.398931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:17.399293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:18.399833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:19.399910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:20.400175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/26/23 07:04:20.689
  STEP: Removing cronjob @ 04/26/23 07:04:20.692
  Apr 26 07:04:20.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7792" for this suite. @ 04/26/23 07:04:20.703
• [300.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/26/23 07:04:20.711
  Apr 26 07:04:20.711: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:04:20.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:04:20.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:04:20.74
  STEP: creating service nodeport-test with type=NodePort in namespace services-2494 @ 04/26/23 07:04:20.743
  STEP: creating replication controller nodeport-test in namespace services-2494 @ 04/26/23 07:04:20.761
  I0426 07:04:20.776554      22 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-2494, replica count: 2
  E0426 07:04:21.401265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:22.402048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:23.402166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0426 07:04:23.826772      22 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 07:04:23.826: INFO: Creating new exec pod
  E0426 07:04:24.403260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:25.403497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:26.403559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:04:26.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2494 exec execpodmsnwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 26 07:04:26.986: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 26 07:04:26.986: INFO: stdout: "nodeport-test-dmxtd"
  Apr 26 07:04:26.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2494 exec execpodmsnwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.251 80'
  Apr 26 07:04:27.124: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.251 80\nConnection to 10.152.183.251 80 port [tcp/http] succeeded!\n"
  Apr 26 07:04:27.124: INFO: stdout: "nodeport-test-dmxtd"
  Apr 26 07:04:27.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2494 exec execpodmsnwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.1.91 32168'
  Apr 26 07:04:27.250: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.1.91 32168\nConnection to 172.31.1.91 32168 port [tcp/*] succeeded!\n"
  Apr 26 07:04:27.250: INFO: stdout: ""
  E0426 07:04:27.404532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:04:28.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2494 exec execpodmsnwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.1.91 32168'
  Apr 26 07:04:28.388: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.1.91 32168\nConnection to 172.31.1.91 32168 port [tcp/*] succeeded!\n"
  Apr 26 07:04:28.388: INFO: stdout: "nodeport-test-kqk9v"
  Apr 26 07:04:28.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2494 exec execpodmsnwx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.127 32168'
  E0426 07:04:28.405548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:04:28.513: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.3.127 32168\nConnection to 172.31.3.127 32168 port [tcp/*] succeeded!\n"
  Apr 26 07:04:28.514: INFO: stdout: "nodeport-test-kqk9v"
  Apr 26 07:04:28.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2494" for this suite. @ 04/26/23 07:04:28.517
• [7.814 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/26/23 07:04:28.525
  Apr 26 07:04:28.525: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:04:28.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:04:28.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:04:28.545
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/26/23 07:04:28.548
  Apr 26 07:04:28.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2208 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 26 07:04:28.619: INFO: stderr: ""
  Apr 26 07:04:28.619: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/26/23 07:04:28.619
  E0426 07:04:29.405718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:30.406808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:31.407877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:32.408442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:33.408971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/26/23 07:04:33.67
  Apr 26 07:04:33.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2208 get pod e2e-test-httpd-pod -o json'
  Apr 26 07:04:33.737: INFO: stderr: ""
  Apr 26 07:04:33.737: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"13dbe5a041ac967464244bd654d3598047e26be7498fc1d89b64427b084df710\",\n            \"cni.projectcalico.org/podIP\": \"10.1.96.13/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.1.96.13/32\"\n        },\n        \"creationTimestamp\": \"2023-04-26T07:04:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2208\",\n        \"resourceVersion\": \"19176\",\n        \"uid\": \"f46fe904-f5ea-4f18-9ec6-5edcdd85e68b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-22qkk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-3-127\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-22qkk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T07:04:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T07:04:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T07:04:30Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-26T07:04:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://9c999f4d13e6746f289926e7e48efecf127d6dd9d70ad627549475f6bfba7f57\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-26T07:04:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.3.127\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.1.96.13\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.1.96.13\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-26T07:04:28Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/26/23 07:04:33.737
  Apr 26 07:04:33.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2208 replace -f -'
  Apr 26 07:04:34.117: INFO: stderr: ""
  Apr 26 07:04:34.117: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/26/23 07:04:34.117
  Apr 26 07:04:34.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2208 delete pods e2e-test-httpd-pod'
  E0426 07:04:34.410072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:35.410190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:04:36.371: INFO: stderr: ""
  Apr 26 07:04:36.371: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 26 07:04:36.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2208" for this suite. @ 04/26/23 07:04:36.375
• [7.859 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/26/23 07:04:36.386
  Apr 26 07:04:36.386: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 07:04:36.386
  E0426 07:04:36.410783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:04:36.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:04:36.417
  STEP: Creating configMap with name configmap-test-volume-map-e62c4c7a-e597-456b-a045-5466af88b26d @ 04/26/23 07:04:36.419
  STEP: Creating a pod to test consume configMaps @ 04/26/23 07:04:36.428
  E0426 07:04:37.411704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:38.412201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:39.412708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:40.412980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:04:40.449
  Apr 26 07:04:40.452: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-2690f460-77cb-4597-b443-78ab420594d7 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:04:40.465
  Apr 26 07:04:40.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8116" for this suite. @ 04/26/23 07:04:40.484
• [4.104 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/26/23 07:04:40.491
  Apr 26 07:04:40.491: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replicaset @ 04/26/23 07:04:40.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:04:40.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:04:40.509
  STEP: Create a Replicaset @ 04/26/23 07:04:40.514
  STEP: Verify that the required pods have come up. @ 04/26/23 07:04:40.521
  Apr 26 07:04:40.523: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0426 07:04:41.414063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:42.414885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:43.415318      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:44.415578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:45.415833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:04:45.526: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/26/23 07:04:45.527
  STEP: Getting /status @ 04/26/23 07:04:45.527
  Apr 26 07:04:45.529: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/26/23 07:04:45.529
  Apr 26 07:04:45.539: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/26/23 07:04:45.539
  Apr 26 07:04:45.540: INFO: Observed &ReplicaSet event: ADDED
  Apr 26 07:04:45.540: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 26 07:04:45.540: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 26 07:04:45.541: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 26 07:04:45.541: INFO: Found replicaset test-rs in namespace replicaset-9510 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 26 07:04:45.541: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/26/23 07:04:45.541
  Apr 26 07:04:45.541: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 26 07:04:45.548: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/26/23 07:04:45.548
  Apr 26 07:04:45.549: INFO: Observed &ReplicaSet event: ADDED
  Apr 26 07:04:45.549: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 26 07:04:45.549: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 26 07:04:45.550: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 26 07:04:45.550: INFO: Observed replicaset test-rs in namespace replicaset-9510 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 26 07:04:45.550: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 26 07:04:45.550: INFO: Found replicaset test-rs in namespace replicaset-9510 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 26 07:04:45.550: INFO: Replicaset test-rs has a patched status
  Apr 26 07:04:45.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9510" for this suite. @ 04/26/23 07:04:45.553
• [5.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/26/23 07:04:45.561
  Apr 26 07:04:45.561: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 07:04:45.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:04:45.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:04:45.579
  STEP: Counting existing ResourceQuota @ 04/26/23 07:04:45.581
  E0426 07:04:46.416690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:47.417420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:48.417690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:49.417963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:50.418119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/26/23 07:04:50.585
  STEP: Ensuring resource quota status is calculated @ 04/26/23 07:04:50.593
  E0426 07:04:51.418769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:52.419445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:04:52.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5203" for this suite. @ 04/26/23 07:04:52.601
• [7.047 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/26/23 07:04:52.608
  Apr 26 07:04:52.608: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:04:52.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:04:52.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:04:52.633
  STEP: Setting up server cert @ 04/26/23 07:04:52.66
  E0426 07:04:53.419813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:04:53.458
  STEP: Deploying the webhook pod @ 04/26/23 07:04:53.468
  STEP: Wait for the deployment to be ready @ 04/26/23 07:04:53.481
  Apr 26 07:04:53.486: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:04:54.420094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:55.420251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:04:55.495
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:04:55.511
  E0426 07:04:56.420677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:04:56.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/26/23 07:04:56.515
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/26/23 07:04:56.532
  STEP: Creating a configMap that should not be mutated @ 04/26/23 07:04:56.539
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/26/23 07:04:56.553
  STEP: Creating a configMap that should be mutated @ 04/26/23 07:04:56.561
  Apr 26 07:04:56.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9065" for this suite. @ 04/26/23 07:04:56.634
  STEP: Destroying namespace "webhook-markers-7519" for this suite. @ 04/26/23 07:04:56.644
• [4.048 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/26/23 07:04:56.657
  Apr 26 07:04:56.657: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename field-validation @ 04/26/23 07:04:56.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:04:56.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:04:56.681
  Apr 26 07:04:56.683: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:04:57.421383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:04:58.421672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0426 07:04:59.210655      22 warnings.go:70] unknown field "alpha"
  W0426 07:04:59.210684      22 warnings.go:70] unknown field "beta"
  W0426 07:04:59.210691      22 warnings.go:70] unknown field "delta"
  W0426 07:04:59.210697      22 warnings.go:70] unknown field "epsilon"
  W0426 07:04:59.210703      22 warnings.go:70] unknown field "gamma"
  Apr 26 07:04:59.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3310" for this suite. @ 04/26/23 07:04:59.24
• [2.590 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/26/23 07:04:59.247
  Apr 26 07:04:59.247: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename gc @ 04/26/23 07:04:59.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:04:59.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:04:59.27
  STEP: create the rc @ 04/26/23 07:04:59.276
  W0426 07:04:59.281795      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0426 07:04:59.422153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:00.422262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:01.423037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:02.423711      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:03.425436      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:04.426569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/26/23 07:05:05.286
  STEP: wait for the rc to be deleted @ 04/26/23 07:05:05.338
  E0426 07:05:05.427079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:05:06.399: INFO: 83 pods remaining
  Apr 26 07:05:06.399: INFO: 80 pods has nil DeletionTimestamp
  Apr 26 07:05:06.399: INFO: 
  E0426 07:05:06.428634      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:05:07.424: INFO: 73 pods remaining
  Apr 26 07:05:07.424: INFO: 71 pods has nil DeletionTimestamp
  Apr 26 07:05:07.424: INFO: 
  E0426 07:05:07.430099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:05:08.382: INFO: 58 pods remaining
  Apr 26 07:05:08.392: INFO: 58 pods has nil DeletionTimestamp
  Apr 26 07:05:08.392: INFO: 
  E0426 07:05:08.430251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:09.430312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:05:09.467: INFO: 40 pods remaining
  Apr 26 07:05:09.478: INFO: 40 pods has nil DeletionTimestamp
  Apr 26 07:05:09.478: INFO: 
  Apr 26 07:05:10.425: INFO: 33 pods remaining
  Apr 26 07:05:10.425: INFO: 32 pods has nil DeletionTimestamp
  Apr 26 07:05:10.425: INFO: 
  E0426 07:05:10.431035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:05:11.373: INFO: 17 pods remaining
  Apr 26 07:05:11.373: INFO: 17 pods has nil DeletionTimestamp
  Apr 26 07:05:11.373: INFO: 
  E0426 07:05:11.432229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:05:12.374: INFO: 0 pods remaining
  Apr 26 07:05:12.374: INFO: 0 pods has nil DeletionTimestamp
  Apr 26 07:05:12.374: INFO: 
  E0426 07:05:12.432663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/26/23 07:05:13.361
  W0426 07:05:13.367324      22 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 26 07:05:13.367: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 26 07:05:13.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3695" for this suite. @ 04/26/23 07:05:13.374
• [14.141 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/26/23 07:05:13.389
  Apr 26 07:05:13.389: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename init-container @ 04/26/23 07:05:13.39
  E0426 07:05:13.433653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:05:13.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:05:13.448
  STEP: creating the pod @ 04/26/23 07:05:13.456
  Apr 26 07:05:13.456: INFO: PodSpec: initContainers in spec.initContainers
  E0426 07:05:14.434398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:15.434485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:16.438143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:17.438658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:18.438846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:19.438908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:20.439377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:21.439547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:22.440268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:23.440527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:24.440616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:25.440907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:26.441213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:27.442133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:28.442326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:29.443250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:30.443450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:31.443737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:32.444469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:33.444655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:34.444853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:35.445099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:36.445289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:37.446325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:38.447333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:39.447448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:40.447571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:41.447679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:42.448082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:43.448215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:44.448584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:45.448917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:46.449072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:47.449595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:48.449737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:49.450097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:50.450173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:51.450291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:52.450670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:53.451225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:54.451461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:55.451575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:56.451742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:57.452793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:05:58.453045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:05:59.085: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-512becb4-fcd8-46ff-a963-31f3f5256530", GenerateName:"", Namespace:"init-container-1715", SelfLink:"", UID:"7cb9969c-d4aa-4633-a7ce-cc04ccafeffd", ResourceVersion:"21544", Generation:0, CreationTimestamp:time.Date(2023, time.April, 26, 7, 5, 13, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"456312065"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"4c4f3438290c2ed485870d6e240daaef6b2cbe5753bb4c1f2aaf1b0a15c61700", "cni.projectcalico.org/podIP":"10.1.96.8/32", "cni.projectcalico.org/podIPs":"10.1.96.8/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 26, 7, 5, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005e5cf78), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 26, 7, 5, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005e5cfc0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelite", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 26, 7, 5, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005e5d008), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-hfsg6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0043aae00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hfsg6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hfsg6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hfsg6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0050a7b18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-3-127", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0002d22a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050a7ba0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0050a7bc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0050a7bc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0050a7bcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00098e570), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 7, 5, 13, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 7, 5, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 7, 5, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 26, 7, 5, 13, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.3.127", PodIP:"10.1.96.8", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.1.96.8"}}, StartTime:time.Date(2023, time.April, 26, 7, 5, 13, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002d2460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002d24d0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://f15ee70a1affd97673b5741ad157c2a541bcd4101b02b3be1dd047d49e4c844e", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0043aae80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0043aae60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0050a7c4f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 26 07:05:59.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1715" for this suite. @ 04/26/23 07:05:59.09
• [45.708 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/26/23 07:05:59.098
  Apr 26 07:05:59.098: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename disruption @ 04/26/23 07:05:59.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:05:59.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:05:59.12
  STEP: Waiting for the pdb to be processed @ 04/26/23 07:05:59.129
  E0426 07:05:59.453117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:00.454136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/26/23 07:06:01.202
  Apr 26 07:06:01.204: INFO: running pods: 0 < 3
  E0426 07:06:01.455214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:02.455664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:06:03.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5415" for this suite. @ 04/26/23 07:06:03.214
• [4.138 seconds]
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/26/23 07:06:03.236
  Apr 26 07:06:03.236: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename security-context-test @ 04/26/23 07:06:03.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:06:03.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:06:03.255
  E0426 07:06:03.456522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:04.457132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:05.457771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:06.458020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:06:07.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3372" for this suite. @ 04/26/23 07:06:07.281
• [4.065 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/26/23 07:06:07.302
  Apr 26 07:06:07.302: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename var-expansion @ 04/26/23 07:06:07.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:06:07.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:06:07.322
  STEP: Creating a pod to test substitution in volume subpath @ 04/26/23 07:06:07.324
  E0426 07:06:07.458264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:08.458361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:09.459196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:10.459358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:06:11.343
  Apr 26 07:06:11.345: INFO: Trying to get logs from node ip-172-31-3-127 pod var-expansion-009e4e19-cd8e-4f8b-99af-7c346c990e9c container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 07:06:11.367
  Apr 26 07:06:11.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3972" for this suite. @ 04/26/23 07:06:11.387
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 04/26/23 07:06:11.397
  Apr 26 07:06:11.397: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename field-validation @ 04/26/23 07:06:11.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:06:11.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:06:11.42
  Apr 26 07:06:11.422: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:06:11.459586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:12.460067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:13.460261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:06:13.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3907" for this suite. @ 04/26/23 07:06:13.982
• [2.593 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/26/23 07:06:13.991
  Apr 26 07:06:13.991: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename gc @ 04/26/23 07:06:13.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:06:14.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:06:14.011
  STEP: create the rc @ 04/26/23 07:06:14.017
  W0426 07:06:14.024603      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0426 07:06:14.461360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:15.462049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:16.463113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:17.464785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:18.465511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:19.466305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/26/23 07:06:20.057
  STEP: wait for the rc to be deleted @ 04/26/23 07:06:20.142
  E0426 07:06:20.466658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:21.466629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:22.467097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:23.467209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:24.468220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/26/23 07:06:25.155
  E0426 07:06:25.469162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:26.469796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:27.470040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:28.470158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:29.471250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:30.471522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:31.471737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:32.472392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:33.472637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:34.472901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:35.473104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:36.473353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:37.474358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:38.474599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:39.475257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:40.475497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:41.475740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:42.476315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:43.476558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:44.476601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:45.476937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:46.477167      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:47.478229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:48.478324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:49.478455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:50.479259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:51.479862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:52.480463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:53.480809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:06:54.481078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/26/23 07:06:55.176
  W0426 07:06:55.186633      22 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 26 07:06:55.186: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 26 07:06:55.186: INFO: Deleting pod "simpletest.rc-dtrlt" in namespace "gc-2496"
  Apr 26 07:06:55.223: INFO: Deleting pod "simpletest.rc-qbxhc" in namespace "gc-2496"
  Apr 26 07:06:55.239: INFO: Deleting pod "simpletest.rc-vn9xw" in namespace "gc-2496"
  Apr 26 07:06:55.255: INFO: Deleting pod "simpletest.rc-xt2pk" in namespace "gc-2496"
  Apr 26 07:06:55.272: INFO: Deleting pod "simpletest.rc-6qdd5" in namespace "gc-2496"
  Apr 26 07:06:55.293: INFO: Deleting pod "simpletest.rc-567b2" in namespace "gc-2496"
  Apr 26 07:06:55.309: INFO: Deleting pod "simpletest.rc-rzp9k" in namespace "gc-2496"
  Apr 26 07:06:55.334: INFO: Deleting pod "simpletest.rc-zhrjr" in namespace "gc-2496"
  Apr 26 07:06:55.444: INFO: Deleting pod "simpletest.rc-9s8qn" in namespace "gc-2496"
  E0426 07:06:55.481757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:06:55.539: INFO: Deleting pod "simpletest.rc-524ld" in namespace "gc-2496"
  Apr 26 07:06:55.558: INFO: Deleting pod "simpletest.rc-9k7ss" in namespace "gc-2496"
  Apr 26 07:06:55.584: INFO: Deleting pod "simpletest.rc-fgr64" in namespace "gc-2496"
  Apr 26 07:06:55.626: INFO: Deleting pod "simpletest.rc-bg5m2" in namespace "gc-2496"
  Apr 26 07:06:55.670: INFO: Deleting pod "simpletest.rc-gc7md" in namespace "gc-2496"
  Apr 26 07:06:55.700: INFO: Deleting pod "simpletest.rc-vgrll" in namespace "gc-2496"
  Apr 26 07:06:55.768: INFO: Deleting pod "simpletest.rc-nzgn4" in namespace "gc-2496"
  Apr 26 07:06:55.790: INFO: Deleting pod "simpletest.rc-cxdgm" in namespace "gc-2496"
  Apr 26 07:06:55.842: INFO: Deleting pod "simpletest.rc-ss6k9" in namespace "gc-2496"
  Apr 26 07:06:55.874: INFO: Deleting pod "simpletest.rc-svhfm" in namespace "gc-2496"
  Apr 26 07:06:55.914: INFO: Deleting pod "simpletest.rc-2v5f6" in namespace "gc-2496"
  Apr 26 07:06:55.947: INFO: Deleting pod "simpletest.rc-lmtmk" in namespace "gc-2496"
  Apr 26 07:06:56.042: INFO: Deleting pod "simpletest.rc-pgbtf" in namespace "gc-2496"
  Apr 26 07:06:56.087: INFO: Deleting pod "simpletest.rc-grljb" in namespace "gc-2496"
  Apr 26 07:06:56.164: INFO: Deleting pod "simpletest.rc-xtmn5" in namespace "gc-2496"
  Apr 26 07:06:56.211: INFO: Deleting pod "simpletest.rc-rtjph" in namespace "gc-2496"
  Apr 26 07:06:56.310: INFO: Deleting pod "simpletest.rc-44n6g" in namespace "gc-2496"
  Apr 26 07:06:56.371: INFO: Deleting pod "simpletest.rc-9vprm" in namespace "gc-2496"
  Apr 26 07:06:56.409: INFO: Deleting pod "simpletest.rc-9vk66" in namespace "gc-2496"
  Apr 26 07:06:56.440: INFO: Deleting pod "simpletest.rc-x6m6l" in namespace "gc-2496"
  E0426 07:06:56.482127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:06:56.575: INFO: Deleting pod "simpletest.rc-s8zcb" in namespace "gc-2496"
  Apr 26 07:06:56.623: INFO: Deleting pod "simpletest.rc-nbndl" in namespace "gc-2496"
  Apr 26 07:06:56.667: INFO: Deleting pod "simpletest.rc-ldgxz" in namespace "gc-2496"
  Apr 26 07:06:56.719: INFO: Deleting pod "simpletest.rc-gvcf6" in namespace "gc-2496"
  Apr 26 07:06:56.794: INFO: Deleting pod "simpletest.rc-4kz2g" in namespace "gc-2496"
  Apr 26 07:06:56.826: INFO: Deleting pod "simpletest.rc-xrsvh" in namespace "gc-2496"
  Apr 26 07:06:56.867: INFO: Deleting pod "simpletest.rc-v7pz9" in namespace "gc-2496"
  Apr 26 07:06:56.948: INFO: Deleting pod "simpletest.rc-shcmk" in namespace "gc-2496"
  Apr 26 07:06:57.061: INFO: Deleting pod "simpletest.rc-b856p" in namespace "gc-2496"
  Apr 26 07:06:57.133: INFO: Deleting pod "simpletest.rc-4drxj" in namespace "gc-2496"
  Apr 26 07:06:57.195: INFO: Deleting pod "simpletest.rc-gh2mg" in namespace "gc-2496"
  Apr 26 07:06:57.268: INFO: Deleting pod "simpletest.rc-pb7ds" in namespace "gc-2496"
  Apr 26 07:06:57.366: INFO: Deleting pod "simpletest.rc-tsxpb" in namespace "gc-2496"
  Apr 26 07:06:57.449: INFO: Deleting pod "simpletest.rc-7btpb" in namespace "gc-2496"
  E0426 07:06:57.482717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:06:57.711: INFO: Deleting pod "simpletest.rc-8xmnd" in namespace "gc-2496"
  Apr 26 07:06:57.840: INFO: Deleting pod "simpletest.rc-khwlf" in namespace "gc-2496"
  Apr 26 07:06:57.904: INFO: Deleting pod "simpletest.rc-dnzh7" in namespace "gc-2496"
  Apr 26 07:06:57.982: INFO: Deleting pod "simpletest.rc-mc695" in namespace "gc-2496"
  Apr 26 07:06:58.045: INFO: Deleting pod "simpletest.rc-b22hk" in namespace "gc-2496"
  Apr 26 07:06:58.170: INFO: Deleting pod "simpletest.rc-gxgrl" in namespace "gc-2496"
  Apr 26 07:06:58.197: INFO: Deleting pod "simpletest.rc-wpqtv" in namespace "gc-2496"
  Apr 26 07:06:58.283: INFO: Deleting pod "simpletest.rc-gz5gr" in namespace "gc-2496"
  Apr 26 07:06:58.313: INFO: Deleting pod "simpletest.rc-cd8zl" in namespace "gc-2496"
  Apr 26 07:06:58.382: INFO: Deleting pod "simpletest.rc-m92fs" in namespace "gc-2496"
  Apr 26 07:06:58.466: INFO: Deleting pod "simpletest.rc-kmm2n" in namespace "gc-2496"
  E0426 07:06:58.484322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:06:58.498: INFO: Deleting pod "simpletest.rc-nmfkr" in namespace "gc-2496"
  Apr 26 07:06:58.527: INFO: Deleting pod "simpletest.rc-85gzv" in namespace "gc-2496"
  Apr 26 07:06:58.591: INFO: Deleting pod "simpletest.rc-9g587" in namespace "gc-2496"
  Apr 26 07:06:58.668: INFO: Deleting pod "simpletest.rc-x295t" in namespace "gc-2496"
  Apr 26 07:06:58.704: INFO: Deleting pod "simpletest.rc-j254p" in namespace "gc-2496"
  Apr 26 07:06:58.737: INFO: Deleting pod "simpletest.rc-cckfr" in namespace "gc-2496"
  Apr 26 07:06:58.875: INFO: Deleting pod "simpletest.rc-92l9w" in namespace "gc-2496"
  Apr 26 07:06:58.905: INFO: Deleting pod "simpletest.rc-ws7hk" in namespace "gc-2496"
  Apr 26 07:06:58.976: INFO: Deleting pod "simpletest.rc-4ssvt" in namespace "gc-2496"
  Apr 26 07:06:59.017: INFO: Deleting pod "simpletest.rc-dd7ct" in namespace "gc-2496"
  Apr 26 07:06:59.086: INFO: Deleting pod "simpletest.rc-k6l4l" in namespace "gc-2496"
  Apr 26 07:06:59.151: INFO: Deleting pod "simpletest.rc-74x9d" in namespace "gc-2496"
  Apr 26 07:06:59.205: INFO: Deleting pod "simpletest.rc-tkl5z" in namespace "gc-2496"
  Apr 26 07:06:59.247: INFO: Deleting pod "simpletest.rc-mwh75" in namespace "gc-2496"
  Apr 26 07:06:59.341: INFO: Deleting pod "simpletest.rc-mclfq" in namespace "gc-2496"
  Apr 26 07:06:59.415: INFO: Deleting pod "simpletest.rc-lzx57" in namespace "gc-2496"
  Apr 26 07:06:59.447: INFO: Deleting pod "simpletest.rc-fzclc" in namespace "gc-2496"
  Apr 26 07:06:59.477: INFO: Deleting pod "simpletest.rc-5hgjp" in namespace "gc-2496"
  E0426 07:06:59.484962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:06:59.598: INFO: Deleting pod "simpletest.rc-9jpc7" in namespace "gc-2496"
  Apr 26 07:06:59.637: INFO: Deleting pod "simpletest.rc-p2qpw" in namespace "gc-2496"
  Apr 26 07:06:59.825: INFO: Deleting pod "simpletest.rc-62klb" in namespace "gc-2496"
  Apr 26 07:06:59.886: INFO: Deleting pod "simpletest.rc-2cwsb" in namespace "gc-2496"
  Apr 26 07:06:59.938: INFO: Deleting pod "simpletest.rc-r4dbn" in namespace "gc-2496"
  Apr 26 07:06:59.977: INFO: Deleting pod "simpletest.rc-z6wx8" in namespace "gc-2496"
  Apr 26 07:07:00.012: INFO: Deleting pod "simpletest.rc-qsq5q" in namespace "gc-2496"
  Apr 26 07:07:00.082: INFO: Deleting pod "simpletest.rc-bqj56" in namespace "gc-2496"
  Apr 26 07:07:00.169: INFO: Deleting pod "simpletest.rc-gns7f" in namespace "gc-2496"
  Apr 26 07:07:00.256: INFO: Deleting pod "simpletest.rc-sngf6" in namespace "gc-2496"
  Apr 26 07:07:00.326: INFO: Deleting pod "simpletest.rc-7wslm" in namespace "gc-2496"
  Apr 26 07:07:00.428: INFO: Deleting pod "simpletest.rc-zfnpd" in namespace "gc-2496"
  E0426 07:07:00.486134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:00.501: INFO: Deleting pod "simpletest.rc-dfbj8" in namespace "gc-2496"
  Apr 26 07:07:00.582: INFO: Deleting pod "simpletest.rc-nzwjg" in namespace "gc-2496"
  Apr 26 07:07:00.635: INFO: Deleting pod "simpletest.rc-mc4hw" in namespace "gc-2496"
  Apr 26 07:07:00.674: INFO: Deleting pod "simpletest.rc-xxxdb" in namespace "gc-2496"
  Apr 26 07:07:00.794: INFO: Deleting pod "simpletest.rc-zxn7c" in namespace "gc-2496"
  Apr 26 07:07:00.822: INFO: Deleting pod "simpletest.rc-44bfn" in namespace "gc-2496"
  Apr 26 07:07:00.905: INFO: Deleting pod "simpletest.rc-7szsj" in namespace "gc-2496"
  Apr 26 07:07:00.948: INFO: Deleting pod "simpletest.rc-zzkts" in namespace "gc-2496"
  Apr 26 07:07:01.072: INFO: Deleting pod "simpletest.rc-dc42f" in namespace "gc-2496"
  Apr 26 07:07:01.119: INFO: Deleting pod "simpletest.rc-gv4jx" in namespace "gc-2496"
  Apr 26 07:07:01.191: INFO: Deleting pod "simpletest.rc-wznx4" in namespace "gc-2496"
  Apr 26 07:07:01.273: INFO: Deleting pod "simpletest.rc-fjx6t" in namespace "gc-2496"
  Apr 26 07:07:01.354: INFO: Deleting pod "simpletest.rc-sl8vg" in namespace "gc-2496"
  Apr 26 07:07:01.413: INFO: Deleting pod "simpletest.rc-r9vxh" in namespace "gc-2496"
  Apr 26 07:07:01.461: INFO: Deleting pod "simpletest.rc-xdc7m" in namespace "gc-2496"
  E0426 07:07:01.487159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:01.514: INFO: Deleting pod "simpletest.rc-8knh9" in namespace "gc-2496"
  Apr 26 07:07:01.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2496" for this suite. @ 04/26/23 07:07:01.575
• [47.607 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/26/23 07:07:01.6
  Apr 26 07:07:01.600: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pod-network-test @ 04/26/23 07:07:01.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:07:01.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:07:01.648
  STEP: Performing setup for networking test in namespace pod-network-test-4215 @ 04/26/23 07:07:01.654
  STEP: creating a selector @ 04/26/23 07:07:01.654
  STEP: Creating the service pods in kubernetes @ 04/26/23 07:07:01.654
  Apr 26 07:07:01.654: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0426 07:07:02.488051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:03.488339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:04.489207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:05.489935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:06.491040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:07.492157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:08.492266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:09.493012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:10.493098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:11.493351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:12.494292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:13.495222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:14.495581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:15.495753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:16.495834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:17.496736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:18.497450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:19.497727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:20.498122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:21.499260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:22.500242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:23.500456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/26/23 07:07:23.855
  E0426 07:07:24.500683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:25.500952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:25.881: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Apr 26 07:07:25.881: INFO: Going to poll 10.1.93.248 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Apr 26 07:07:25.883: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.93.248 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4215 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:07:25.883: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:07:25.884: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:07:25.884: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4215/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.1.93.248+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0426 07:07:26.501723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:26.939: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 26 07:07:26.939: INFO: Going to poll 10.1.96.3 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Apr 26 07:07:26.942: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.96.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4215 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:07:26.942: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:07:26.943: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:07:26.943: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4215/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.1.96.3+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0426 07:07:27.502261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:28.011: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 26 07:07:28.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4215" for this suite. @ 04/26/23 07:07:28.019
• [26.429 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/26/23 07:07:28.03
  Apr 26 07:07:28.030: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:07:28.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:07:28.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:07:28.094
  STEP: Setting up server cert @ 04/26/23 07:07:28.155
  E0426 07:07:28.502817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:07:28.561
  STEP: Deploying the webhook pod @ 04/26/23 07:07:28.573
  STEP: Wait for the deployment to be ready @ 04/26/23 07:07:28.65
  Apr 26 07:07:28.655: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:07:29.503937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:30.504229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:07:30.665
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:07:30.688
  E0426 07:07:31.505086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:31.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 26 07:07:31.691: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4228-crds.webhook.example.com via the AdmissionRegistration API @ 04/26/23 07:07:32.217
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/26/23 07:07:32.246
  E0426 07:07:32.506199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:33.507275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:34.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0426 07:07:34.508182      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6090" for this suite. @ 04/26/23 07:07:34.973
  STEP: Destroying namespace "webhook-markers-3489" for this suite. @ 04/26/23 07:07:34.983
• [6.966 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/26/23 07:07:34.996
  Apr 26 07:07:34.996: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:07:34.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:07:35.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:07:35.052
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8775 @ 04/26/23 07:07:35.085
  STEP: changing the ExternalName service to type=ClusterIP @ 04/26/23 07:07:35.1
  STEP: creating replication controller externalname-service in namespace services-8775 @ 04/26/23 07:07:35.158
  I0426 07:07:35.189712      22 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8775, replica count: 2
  E0426 07:07:35.509116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:36.510250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:37.510902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0426 07:07:38.240833      22 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 07:07:38.240: INFO: Creating new exec pod
  E0426 07:07:38.511260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:39.511413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:40.512367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:41.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8775 exec execpodbpmb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 26 07:07:41.407: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 26 07:07:41.407: INFO: stdout: ""
  E0426 07:07:41.513267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:42.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8775 exec execpodbpmb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0426 07:07:42.514122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:42.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 26 07:07:42.567: INFO: stdout: "externalname-service-g7v6g"
  Apr 26 07:07:42.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8775 exec execpodbpmb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.30 80'
  Apr 26 07:07:42.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
  Apr 26 07:07:42.695: INFO: stdout: ""
  E0426 07:07:43.514794      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:43.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8775 exec execpodbpmb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.30 80'
  Apr 26 07:07:43.831: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
  Apr 26 07:07:43.831: INFO: stdout: ""
  E0426 07:07:44.515435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:44.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8775 exec execpodbpmb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.30 80'
  Apr 26 07:07:44.822: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
  Apr 26 07:07:44.823: INFO: stdout: ""
  E0426 07:07:45.515809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:45.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8775 exec execpodbpmb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.30 80'
  Apr 26 07:07:45.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
  Apr 26 07:07:45.843: INFO: stdout: ""
  E0426 07:07:46.516041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:46.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8775 exec execpodbpmb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.30 80'
  Apr 26 07:07:46.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
  Apr 26 07:07:46.843: INFO: stdout: ""
  E0426 07:07:47.516489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:47.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8775 exec execpodbpmb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.30 80'
  Apr 26 07:07:47.837: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
  Apr 26 07:07:47.837: INFO: stdout: "externalname-service-zppsw"
  Apr 26 07:07:47.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 07:07:47.840: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-8775" for this suite. @ 04/26/23 07:07:47.866
• [12.879 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/26/23 07:07:47.876
  Apr 26 07:07:47.876: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:07:47.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:07:47.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:07:47.896
  Apr 26 07:07:47.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-3765 version'
  Apr 26 07:07:47.972: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 26 07:07:47.972: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.0\", GitCommit:\"1b4df30b3cdfeaba6024e81e559a6cd09a089d65\", GitTreeState:\"clean\", BuildDate:\"2023-04-11T17:10:18Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.0\", GitCommit:\"1b4df30b3cdfeaba6024e81e559a6cd09a089d65\", GitTreeState:\"clean\", BuildDate:\"2023-04-13T07:41:44Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 26 07:07:47.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3765" for this suite. @ 04/26/23 07:07:47.975
• [0.107 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/26/23 07:07:47.984
  Apr 26 07:07:47.984: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 07:07:47.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:07:48.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:07:48.003
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 07:07:48.006
  E0426 07:07:48.517629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:49.518006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:50.518785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:51.519568      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:07:52.024
  Apr 26 07:07:52.027: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-e5d70b3c-5edb-452e-902a-5a50400d7d28 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 07:07:52.04
  Apr 26 07:07:52.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7525" for this suite. @ 04/26/23 07:07:52.063
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/26/23 07:07:52.072
  Apr 26 07:07:52.072: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:07:52.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:07:52.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:07:52.094
  STEP: create deployment with httpd image @ 04/26/23 07:07:52.096
  Apr 26 07:07:52.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6318 create -f -'
  E0426 07:07:52.520097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:52.978: INFO: stderr: ""
  Apr 26 07:07:52.978: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/26/23 07:07:52.979
  Apr 26 07:07:52.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6318 diff -f -'
  E0426 07:07:53.520498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:07:53.869: INFO: rc: 1
  Apr 26 07:07:53.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6318 delete -f -'
  Apr 26 07:07:53.948: INFO: stderr: ""
  Apr 26 07:07:53.949: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 26 07:07:53.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6318" for this suite. @ 04/26/23 07:07:53.965
• [1.927 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/26/23 07:07:54
  Apr 26 07:07:54.000: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename svcaccounts @ 04/26/23 07:07:54.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:07:54.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:07:54.086
  Apr 26 07:07:54.111: INFO: created pod
  E0426 07:07:54.521531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:55.521783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:56.522878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:57.523262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:07:58.123
  E0426 07:07:58.523525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:07:59.523789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:00.524013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:01.524228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:02.524662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:03.524932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:04.525198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:05.525457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:06.525746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:07.526131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:08.526373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:09.526576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:10.526848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:11.527125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:12.527329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:13.528043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:14.527859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:15.527990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:16.528242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:17.528495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:18.529434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:19.529643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:20.529869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:21.530023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:22.530694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:23.530830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:24.531238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:25.531394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:26.531597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:27.532064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:08:28.123: INFO: polling logs
  Apr 26 07:08:28.130: INFO: Pod logs: 
  I0426 07:07:54.930033       1 log.go:198] OK: Got token
  I0426 07:07:54.930071       1 log.go:198] validating with in-cluster discovery
  I0426 07:07:54.930367       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0426 07:07:54.930396       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7969:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682493474, NotBefore:1682492874, IssuedAt:1682492874, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7969", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"781143c0-4880-4af7-8ed4-a43813839b8c"}}}
  I0426 07:07:54.938354       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0426 07:07:54.945344       1 log.go:198] OK: Validated signature on JWT
  I0426 07:07:54.945483       1 log.go:198] OK: Got valid claims from token!
  I0426 07:07:54.945518       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7969:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682493474, NotBefore:1682492874, IssuedAt:1682492874, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7969", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"781143c0-4880-4af7-8ed4-a43813839b8c"}}}

  Apr 26 07:08:28.130: INFO: completed pod
  Apr 26 07:08:28.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7969" for this suite. @ 04/26/23 07:08:28.142
• [34.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/26/23 07:08:28.149
  Apr 26 07:08:28.149: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/26/23 07:08:28.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:28.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:28.173
  STEP: creating @ 04/26/23 07:08:28.176
  STEP: getting @ 04/26/23 07:08:28.188
  STEP: listing in namespace @ 04/26/23 07:08:28.191
  STEP: patching @ 04/26/23 07:08:28.194
  STEP: deleting @ 04/26/23 07:08:28.203
  Apr 26 07:08:28.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-178" for this suite. @ 04/26/23 07:08:28.215
• [0.072 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/26/23 07:08:28.222
  Apr 26 07:08:28.222: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename security-context-test @ 04/26/23 07:08:28.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:28.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:28.245
  E0426 07:08:28.532577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:29.532907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:30.533657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:31.533964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:08:32.270: INFO: Got logs for pod "busybox-privileged-false-b0f02889-eb3a-40b2-b341-8e64b11d7f58": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 26 07:08:32.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-631" for this suite. @ 04/26/23 07:08:32.273
• [4.059 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/26/23 07:08:32.281
  Apr 26 07:08:32.282: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/26/23 07:08:32.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:32.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:32.303
  STEP: Setting up the test @ 04/26/23 07:08:32.306
  STEP: Creating hostNetwork=false pod @ 04/26/23 07:08:32.306
  E0426 07:08:32.534999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:33.535282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 04/26/23 07:08:34.324
  E0426 07:08:34.536174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:35.536326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 04/26/23 07:08:36.339
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/26/23 07:08:36.339
  Apr 26 07:08:36.339: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.339: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.340: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.340: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 26 07:08:36.406: INFO: Exec stderr: ""
  Apr 26 07:08:36.406: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.406: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.406: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.406: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 26 07:08:36.480: INFO: Exec stderr: ""
  Apr 26 07:08:36.480: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.480: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.481: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.481: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0426 07:08:36.536519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:08:36.545: INFO: Exec stderr: ""
  Apr 26 07:08:36.545: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.545: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.545: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.545: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 26 07:08:36.608: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/26/23 07:08:36.608
  Apr 26 07:08:36.608: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.608: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.609: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.609: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 26 07:08:36.672: INFO: Exec stderr: ""
  Apr 26 07:08:36.672: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.672: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.673: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.673: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 26 07:08:36.715: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/26/23 07:08:36.715
  Apr 26 07:08:36.715: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.715: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.715: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.715: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 26 07:08:36.776: INFO: Exec stderr: ""
  Apr 26 07:08:36.776: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.776: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.777: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.777: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 26 07:08:36.848: INFO: Exec stderr: ""
  Apr 26 07:08:36.848: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.848: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.849: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.849: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 26 07:08:36.924: INFO: Exec stderr: ""
  Apr 26 07:08:36.924: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-168 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:08:36.924: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:08:36.924: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:08:36.924: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-168/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 26 07:08:36.979: INFO: Exec stderr: ""
  Apr 26 07:08:36.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-168" for this suite. @ 04/26/23 07:08:36.983
• [4.712 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/26/23 07:08:36.996
  Apr 26 07:08:36.996: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename namespaces @ 04/26/23 07:08:36.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:37.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:37.015
  STEP: Creating namespace "e2e-ns-98hf2" @ 04/26/23 07:08:37.017
  Apr 26 07:08:37.035: INFO: Namespace "e2e-ns-98hf2-1536" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-98hf2-1536" @ 04/26/23 07:08:37.035
  Apr 26 07:08:37.043: INFO: Namespace "e2e-ns-98hf2-1536" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-98hf2-1536" @ 04/26/23 07:08:37.043
  Apr 26 07:08:37.051: INFO: Namespace "e2e-ns-98hf2-1536" has []v1.FinalizerName{"kubernetes"}
  Apr 26 07:08:37.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4075" for this suite. @ 04/26/23 07:08:37.054
  STEP: Destroying namespace "e2e-ns-98hf2-1536" for this suite. @ 04/26/23 07:08:37.061
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/26/23 07:08:37.07
  Apr 26 07:08:37.071: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 07:08:37.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:37.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:37.088
  Apr 26 07:08:37.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-282" for this suite. @ 04/26/23 07:08:37.13
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/26/23 07:08:37.138
  Apr 26 07:08:37.138: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:08:37.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:37.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:37.155
  STEP: creating a collection of services @ 04/26/23 07:08:37.158
  Apr 26 07:08:37.158: INFO: Creating e2e-svc-a-ct9w9
  Apr 26 07:08:37.170: INFO: Creating e2e-svc-b-bbj5q
  Apr 26 07:08:37.182: INFO: Creating e2e-svc-c-gdzkz
  STEP: deleting service collection @ 04/26/23 07:08:37.196
  Apr 26 07:08:37.228: INFO: Collection of services has been deleted
  Apr 26 07:08:37.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8337" for this suite. @ 04/26/23 07:08:37.231
• [0.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/26/23 07:08:37.24
  Apr 26 07:08:37.240: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename tables @ 04/26/23 07:08:37.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:37.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:37.258
  Apr 26 07:08:37.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-6300" for this suite. @ 04/26/23 07:08:37.265
• [0.033 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/26/23 07:08:37.273
  Apr 26 07:08:37.273: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename var-expansion @ 04/26/23 07:08:37.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:37.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:37.291
  E0426 07:08:37.537387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:38.537610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:08:39.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 07:08:39.320: INFO: Deleting pod "var-expansion-272ce83d-9246-4db4-ae62-a51686632c5a" in namespace "var-expansion-4759"
  Apr 26 07:08:39.328: INFO: Wait up to 5m0s for pod "var-expansion-272ce83d-9246-4db4-ae62-a51686632c5a" to be fully deleted
  E0426 07:08:39.538329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:40.539368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-4759" for this suite. @ 04/26/23 07:08:41.335
• [4.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/26/23 07:08:41.347
  Apr 26 07:08:41.347: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename containers @ 04/26/23 07:08:41.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:41.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:41.368
  STEP: Creating a pod to test override command @ 04/26/23 07:08:41.37
  E0426 07:08:41.539664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:42.540104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:43.541003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:44.541236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:08:45.389
  Apr 26 07:08:45.391: INFO: Trying to get logs from node ip-172-31-1-91 pod client-containers-4ca4f8d5-646d-47e9-841e-3ef1f20f39a1 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:08:45.412
  Apr 26 07:08:45.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5106" for this suite. @ 04/26/23 07:08:45.432
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/26/23 07:08:45.439
  Apr 26 07:08:45.439: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:08:45.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:45.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:45.472
  STEP: Creating configMap with name configmap-projected-all-test-volume-7382a3d7-718e-420f-938b-3628da49fc7b @ 04/26/23 07:08:45.475
  STEP: Creating secret with name secret-projected-all-test-volume-578eb813-ba8c-4e96-94c2-8a12f18d81c7 @ 04/26/23 07:08:45.481
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/26/23 07:08:45.487
  E0426 07:08:45.541934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:46.542126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:47.543232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:48.544246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:08:49.507
  Apr 26 07:08:49.509: INFO: Trying to get logs from node ip-172-31-3-127 pod projected-volume-4f171ca2-b0a4-4516-994a-cf4ec7eb6ed5 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 07:08:49.514
  Apr 26 07:08:49.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4139" for this suite. @ 04/26/23 07:08:49.532
• [4.099 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/26/23 07:08:49.539
  Apr 26 07:08:49.539: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sysctl @ 04/26/23 07:08:49.54
  E0426 07:08:49.545206      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:49.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:49.557
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/26/23 07:08:49.56
  Apr 26 07:08:49.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3231" for this suite. @ 04/26/23 07:08:49.567
• [0.035 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/26/23 07:08:49.575
  Apr 26 07:08:49.575: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename svcaccounts @ 04/26/23 07:08:49.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:49.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:49.592
  STEP: creating a ServiceAccount @ 04/26/23 07:08:49.595
  STEP: watching for the ServiceAccount to be added @ 04/26/23 07:08:49.604
  STEP: patching the ServiceAccount @ 04/26/23 07:08:49.606
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/26/23 07:08:49.613
  STEP: deleting the ServiceAccount @ 04/26/23 07:08:49.617
  Apr 26 07:08:49.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5274" for this suite. @ 04/26/23 07:08:49.665
• [0.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/26/23 07:08:49.678
  Apr 26 07:08:49.678: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename runtimeclass @ 04/26/23 07:08:49.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:49.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:49.697
  STEP: Deleting RuntimeClass runtimeclass-5809-delete-me @ 04/26/23 07:08:49.705
  STEP: Waiting for the RuntimeClass to disappear @ 04/26/23 07:08:49.711
  Apr 26 07:08:49.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5809" for this suite. @ 04/26/23 07:08:49.722
• [0.051 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 04/26/23 07:08:49.729
  Apr 26 07:08:49.729: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename statefulset @ 04/26/23 07:08:49.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:08:49.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:08:49.75
  STEP: Creating service test in namespace statefulset-6767 @ 04/26/23 07:08:49.753
  STEP: Creating statefulset ss in namespace statefulset-6767 @ 04/26/23 07:08:49.773
  Apr 26 07:08:49.793: INFO: Found 0 stateful pods, waiting for 1
  E0426 07:08:50.545305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:51.545643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:52.546271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:53.546398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:54.547237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:55.547336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:56.547669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:57.548614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:58.548837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:08:59.549812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:08:59.798: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/26/23 07:08:59.803
  STEP: Getting /status @ 04/26/23 07:08:59.811
  Apr 26 07:08:59.814: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/26/23 07:08:59.814
  Apr 26 07:08:59.826: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/26/23 07:08:59.826
  Apr 26 07:08:59.827: INFO: Observed &StatefulSet event: ADDED
  Apr 26 07:08:59.827: INFO: Found Statefulset ss in namespace statefulset-6767 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 26 07:08:59.827: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/26/23 07:08:59.827
  Apr 26 07:08:59.828: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 26 07:08:59.837: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/26/23 07:08:59.837
  Apr 26 07:08:59.838: INFO: Observed &StatefulSet event: ADDED
  Apr 26 07:08:59.838: INFO: Deleting all statefulset in ns statefulset-6767
  Apr 26 07:08:59.841: INFO: Scaling statefulset ss to 0
  E0426 07:09:00.550258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:01.550364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:02.551248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:03.551462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:04.551693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:05.551883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:06.552921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:07.553059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:08.553277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:09.553408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:09:09.858: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 07:09:09.860: INFO: Deleting statefulset ss
  Apr 26 07:09:09.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6767" for this suite. @ 04/26/23 07:09:09.878
• [20.159 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/26/23 07:09:09.888
  Apr 26 07:09:09.888: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename watch @ 04/26/23 07:09:09.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:09.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:09.908
  STEP: creating a watch on configmaps @ 04/26/23 07:09:09.911
  STEP: creating a new configmap @ 04/26/23 07:09:09.912
  STEP: modifying the configmap once @ 04/26/23 07:09:09.922
  STEP: closing the watch once it receives two notifications @ 04/26/23 07:09:09.931
  Apr 26 07:09:09.931: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2275  a1d3ad87-f7f7-49ca-9b8b-96ae24502acc 25167 0 2023-04-26 07:09:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 07:09:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:09:09.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2275  a1d3ad87-f7f7-49ca-9b8b-96ae24502acc 25168 0 2023-04-26 07:09:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 07:09:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/26/23 07:09:09.931
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/26/23 07:09:09.939
  STEP: deleting the configmap @ 04/26/23 07:09:09.94
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/26/23 07:09:09.948
  Apr 26 07:09:09.948: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2275  a1d3ad87-f7f7-49ca-9b8b-96ae24502acc 25169 0 2023-04-26 07:09:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 07:09:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:09:09.949: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2275  a1d3ad87-f7f7-49ca-9b8b-96ae24502acc 25170 0 2023-04-26 07:09:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-26 07:09:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:09:09.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2275" for this suite. @ 04/26/23 07:09:09.952
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/26/23 07:09:09.96
  Apr 26 07:09:09.960: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:09:09.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:09.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:09.985
  STEP: Setting up server cert @ 04/26/23 07:09:10.015
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:09:10.342
  STEP: Deploying the webhook pod @ 04/26/23 07:09:10.351
  STEP: Wait for the deployment to be ready @ 04/26/23 07:09:10.365
  Apr 26 07:09:10.381: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:09:10.553584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:11.554307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:09:12.39
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:09:12.403
  E0426 07:09:12.554644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:09:13.403: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/26/23 07:09:13.481
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/26/23 07:09:13.522
  E0426 07:09:13.555461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the collection of validation webhooks @ 04/26/23 07:09:13.558
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/26/23 07:09:13.631
  Apr 26 07:09:13.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3360" for this suite. @ 04/26/23 07:09:13.704
  STEP: Destroying namespace "webhook-markers-8567" for this suite. @ 04/26/23 07:09:13.716
• [3.766 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/26/23 07:09:13.728
  Apr 26 07:09:13.728: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename subpath @ 04/26/23 07:09:13.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:13.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:13.757
  STEP: Setting up data @ 04/26/23 07:09:13.76
  STEP: Creating pod pod-subpath-test-projected-xqhq @ 04/26/23 07:09:13.774
  STEP: Creating a pod to test atomic-volume-subpath @ 04/26/23 07:09:13.774
  E0426 07:09:14.555720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:15.555851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:16.555913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:17.556657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:18.557360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:19.557602      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:20.558149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:21.558323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:22.559395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:23.559652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:24.560603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:25.560819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:26.560974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:27.561741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:28.562123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:29.562297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:30.562665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:31.563263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:32.564207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:33.564373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:34.564441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:35.564661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:36.565312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:37.566279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:09:37.832
  Apr 26 07:09:37.835: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-subpath-test-projected-xqhq container test-container-subpath-projected-xqhq: <nil>
  STEP: delete the pod @ 04/26/23 07:09:37.84
  STEP: Deleting pod pod-subpath-test-projected-xqhq @ 04/26/23 07:09:37.873
  Apr 26 07:09:37.873: INFO: Deleting pod "pod-subpath-test-projected-xqhq" in namespace "subpath-372"
  Apr 26 07:09:37.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-372" for this suite. @ 04/26/23 07:09:37.878
• [24.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/26/23 07:09:37.887
  Apr 26 07:09:37.887: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 07:09:37.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:37.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:37.912
  STEP: Creating secret with name secret-test-dc28d288-f3b6-4fe3-bfd7-2fef1f1248a8 @ 04/26/23 07:09:37.915
  STEP: Creating a pod to test consume secrets @ 04/26/23 07:09:37.951
  E0426 07:09:38.567229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:39.568061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:40.568332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:41.568746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:09:41.977
  Apr 26 07:09:41.979: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-secrets-3709a3e4-ea47-458f-834f-ac92a9dfd143 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 07:09:41.984
  Apr 26 07:09:42.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6753" for this suite. @ 04/26/23 07:09:42.005
• [4.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/26/23 07:09:42.016
  Apr 26 07:09:42.016: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename gc @ 04/26/23 07:09:42.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:42.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:42.036
  Apr 26 07:09:42.138: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"266016cb-9e48-4898-9923-2688205f29ec", Controller:(*bool)(0xc005036976), BlockOwnerDeletion:(*bool)(0xc005036977)}}
  Apr 26 07:09:42.150: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"84384634-7f52-4bb8-b5a5-e651dd4e1980", Controller:(*bool)(0xc004a9b57a), BlockOwnerDeletion:(*bool)(0xc004a9b57b)}}
  Apr 26 07:09:42.160: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"63e72349-3927-4ace-adf8-0a7193e22e8c", Controller:(*bool)(0xc0047b21be), BlockOwnerDeletion:(*bool)(0xc0047b21bf)}}
  E0426 07:09:42.569219      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:43.569471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:44.569721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:45.570810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:46.571883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:09:47.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3774" for this suite. @ 04/26/23 07:09:47.174
• [5.165 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/26/23 07:09:47.181
  Apr 26 07:09:47.181: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 07:09:47.182
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:47.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:47.202
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 07:09:47.204
  E0426 07:09:47.572387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:48.572601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:49.573613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:50.573797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:09:51.228
  Apr 26 07:09:51.231: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-ab6ca711-8dee-4dee-b790-2cd6cc7a6f8b container client-container: <nil>
  STEP: delete the pod @ 04/26/23 07:09:51.235
  Apr 26 07:09:51.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2351" for this suite. @ 04/26/23 07:09:51.256
• [4.082 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/26/23 07:09:51.264
  Apr 26 07:09:51.264: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename security-context-test @ 04/26/23 07:09:51.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:51.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:51.284
  E0426 07:09:51.574679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:52.575682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:53.575777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:54.576056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:09:55.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8048" for this suite. @ 04/26/23 07:09:55.314
• [4.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/26/23 07:09:55.324
  Apr 26 07:09:55.324: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename var-expansion @ 04/26/23 07:09:55.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:55.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:55.344
  STEP: Creating a pod to test substitution in container's command @ 04/26/23 07:09:55.346
  E0426 07:09:55.576993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:56.577325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:57.577484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:09:58.577520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:09:59.37
  Apr 26 07:09:59.372: INFO: Trying to get logs from node ip-172-31-3-127 pod var-expansion-cd969411-a67c-4185-9798-3852d3270616 container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 07:09:59.377
  Apr 26 07:09:59.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4967" for this suite. @ 04/26/23 07:09:59.402
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/26/23 07:09:59.41
  Apr 26 07:09:59.411: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/26/23 07:09:59.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:09:59.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:09:59.431
  Apr 26 07:09:59.433: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:09:59.577656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:10:00.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9604" for this suite. @ 04/26/23 07:10:00.464
• [1.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/26/23 07:10:00.474
  Apr 26 07:10:00.474: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 07:10:00.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:10:00.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:10:00.496
  STEP: Creating pod liveness-4dbfcffe-cec1-40b3-ac86-59733c341ee1 in namespace container-probe-3063 @ 04/26/23 07:10:00.498
  E0426 07:10:00.578481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:01.579574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:10:02.517: INFO: Started pod liveness-4dbfcffe-cec1-40b3-ac86-59733c341ee1 in namespace container-probe-3063
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/26/23 07:10:02.517
  Apr 26 07:10:02.519: INFO: Initial restart count of pod liveness-4dbfcffe-cec1-40b3-ac86-59733c341ee1 is 0
  E0426 07:10:02.580542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:03.580787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:04.581471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:05.581721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:06.582110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:07.582585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:08.583635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:09.583926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:10.584310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:11.584560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:12.585544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:13.586283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:14.586556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:15.587278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:16.587290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:17.587782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:18.588607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:19.588755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:20.588923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:21.589089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:22.589887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:23.590610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:24.591425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:25.591654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:26.591757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:27.592702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:28.593433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:29.593619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:30.594165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:31.594418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:32.594507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:33.595273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:34.595518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:35.595602      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:36.595845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:37.596592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:38.596777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:39.597368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:40.597635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:41.598157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:42.598762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:43.599649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:44.599902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:45.600923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:46.601210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:47.601698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:48.601941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:49.602194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:50.602654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:51.603241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:52.604331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:53.604493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:54.604749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:55.605116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:56.605266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:57.605498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:58.605677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:10:59.606057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:00.606338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:01.606823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:02.607404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:03.607991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:04.608210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:05.609342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:06.609599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:07.609858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:08.610121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:09.610243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:10.611335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:11.611792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:12.612366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:13.612567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:14.612833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:15.612934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:16.613153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:17.613832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:18.614128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:19.614147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:20.615246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:21.615621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:22.616333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:23.616495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:24.616763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:25.617097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:26.617365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:27.617856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:28.618460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:29.618955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:30.619172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:31.619553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:32.620333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:33.620707      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:34.621446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:35.621640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:36.621861      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:37.622566      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:38.622870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:39.623101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:40.623378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:41.623508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:42.623961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:43.624334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:44.624555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:45.625535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:46.625773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:47.626540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:48.627628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:49.628606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:50.629572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:51.630169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:52.630896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:53.631682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:54.631873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:55.632073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:56.632323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:57.633211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:58.633468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:11:59.634150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:00.634233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:01.635279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:02.636035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:03.636356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:04.636682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:05.637687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:06.637935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:07.638374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:08.639112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:09.639666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:10.639938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:11.640175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:12.640928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:13.641397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:14.641571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:15.642080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:16.642154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:17.643196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:18.643433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:19.644125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:20.644361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:21.644838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:22.645386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:23.645716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:24.646178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:25.646272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:26.646416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:27.646583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:28.646836      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:29.647081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:30.647249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:31.647875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:32.648365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:33.648575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:34.648888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:35.648921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:36.649159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:37.649724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:38.650261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:39.651250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:40.651487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:41.651872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:42.652865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:43.653176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:44.653402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:45.654177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:46.655277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:47.655770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:48.656412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:49.656543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:50.656660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:51.656866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:52.657578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:53.658437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:54.659340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:55.659827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:56.660063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:57.660614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:58.661104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:12:59.661304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:00.661564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:01.662150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:02.662811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:03.663683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:04.663903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:05.664333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:06.664785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:07.665386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:08.665634      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:09.666593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:10.667279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:11.667632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:12.667720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:13.667767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:14.667887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:15.668817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:16.668960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:17.669534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:18.669693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:19.670144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:20.671283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:21.671849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:22.672497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:23.672776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:24.673008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:25.673995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:26.674162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:27.675018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:28.675468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:29.676169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:30.676295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:31.676550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:32.677338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:33.678124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:34.678488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:35.679361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:36.679612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:37.680227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:38.680394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:39.681479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:40.681795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:41.682543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:42.683247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:43.683653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:44.683877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:45.684566      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:46.684824      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:47.685406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:48.685639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:49.686144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:50.687283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:51.687750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:52.688649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:53.689143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:54.689406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:55.689815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:56.689942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:57.690033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:58.690183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:13:59.690831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:00.691161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:01.691756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:02.692444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:14:03.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 07:14:03.063
  STEP: Destroying namespace "container-probe-3063" for this suite. @ 04/26/23 07:14:03.08
• [242.654 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/26/23 07:14:03.129
  Apr 26 07:14:03.129: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 07:14:03.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:14:03.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:14:03.149
  STEP: Creating pod test-webserver-dfb57935-d2fa-46a1-876d-e6522837fc0a in namespace container-probe-6152 @ 04/26/23 07:14:03.151
  E0426 07:14:03.692574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:04.692829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:14:05.168: INFO: Started pod test-webserver-dfb57935-d2fa-46a1-876d-e6522837fc0a in namespace container-probe-6152
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/26/23 07:14:05.168
  Apr 26 07:14:05.170: INFO: Initial restart count of pod test-webserver-dfb57935-d2fa-46a1-876d-e6522837fc0a is 0
  E0426 07:14:05.692903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:06.693178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:07.694009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:08.694137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:09.694899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:10.695247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:11.695574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:12.696370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:13.696493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:14.696862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:15.696978      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:16.697232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:17.697771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:18.697926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:19.698129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:20.698163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:21.698698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:22.699226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:23.699748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:24.699977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:25.700670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:26.700917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:27.701723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:28.701966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:29.702744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:30.703019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:31.703244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:32.703887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:33.704153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:34.704409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:35.705130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:36.705348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:37.706278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:38.706427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:39.707545      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:40.707766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:41.707949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:42.708500      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:43.709648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:44.709888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:45.710131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:46.710374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:47.710660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:48.711227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:49.711279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:50.711494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:51.711957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:52.712109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:53.712483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:54.712701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:55.713056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:56.713220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:57.713582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:58.713773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:14:59.714479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:00.714511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:01.715407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:02.715579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:03.716232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:04.716390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:05.716586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:06.716818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:07.717027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:08.717420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:09.717853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:10.718118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:11.718149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:12.718855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:13.719806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:14.719970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:15.720864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:16.721113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:17.721310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:18.721663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:19.722104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:20.722125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:21.722162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:22.722905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:23.723013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:24.723311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:25.723445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:26.723639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:27.724417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:28.725022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:29.725168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:30.725428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:31.725790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:32.726499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:33.727059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:34.727199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:35.727341      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:36.727671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:37.728231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:38.728549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:39.729396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:40.729562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:41.729716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:42.729795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:43.730383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:44.731251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:45.732183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:46.732412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:47.733316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:48.733562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:49.734450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:50.735221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:51.735575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:52.736509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:53.736881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:54.737097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:55.737461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:56.737631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:57.738629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:58.739292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:15:59.739882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:00.740117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:01.740243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:02.740971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:03.741787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:04.742078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:05.743176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:06.743374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:07.743705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:08.743959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:09.744131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:10.744398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:11.744770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:12.745533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:13.745841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:14.745865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:15.746652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:16.746896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:17.747315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:18.747894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:19.748102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:20.748346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:21.749399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:22.750123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:23.750625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:24.751226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:25.751440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:26.751792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:27.751852      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:28.752187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:29.753090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:30.753332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:31.753866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:32.754497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:33.754739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:34.754987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:35.755133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:36.755291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:37.755463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:38.755820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:39.756927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:40.757202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:41.757767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:42.758275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:43.758601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:44.758812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:45.759869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:46.760111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:47.760344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:48.760587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:49.760704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:50.760931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:51.761941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:52.762734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:53.763717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:54.763896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:55.764410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:56.764683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:57.764834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:58.765054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:16:59.765642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:00.765878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:01.766845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:02.767491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:03.768555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:04.768818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:05.769317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:06.769550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:07.770158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:08.771216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:09.771977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:10.772054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:11.773048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:12.773692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:13.774142      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:14.775250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:15.776246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:16.776487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:17.777350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:18.777571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:19.778173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:20.778461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:21.779255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:22.779919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:23.780440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:24.780692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:25.780879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:26.781153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:27.782130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:28.783256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:29.784339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:30.784595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:31.785389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:32.786112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:33.786627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:34.787282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:35.788362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:36.788621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:37.788703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:38.789417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:39.789798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:40.789949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:41.790385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:42.790984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:43.791187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:44.791413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:45.792533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:46.792816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:47.793058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:48.793299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:49.794063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:50.794185      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:51.794859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:52.795559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:53.796296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:54.796556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:55.796720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:56.796968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:57.797026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:58.797263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:17:59.798111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:00.798367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:01.799399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:02.799535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:03.800384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:04.800639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:05.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 07:18:05.675
  STEP: Destroying namespace "container-probe-6152" for this suite. @ 04/26/23 07:18:05.691
• [242.573 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/26/23 07:18:05.703
  Apr 26 07:18:05.703: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replication-controller @ 04/26/23 07:18:05.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:05.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:05.724
  STEP: Given a ReplicationController is created @ 04/26/23 07:18:05.727
  STEP: When the matched label of one of its pods change @ 04/26/23 07:18:05.737
  Apr 26 07:18:05.741: INFO: Pod name pod-release: Found 0 pods out of 1
  E0426 07:18:05.801388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:06.801839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:07.802908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:08.803225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:09.803493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:10.744: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/26/23 07:18:10.755
  E0426 07:18:10.803717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:11.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4684" for this suite. @ 04/26/23 07:18:11.765
• [6.070 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/26/23 07:18:11.773
  Apr 26 07:18:11.773: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/26/23 07:18:11.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:11.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:11.792
  STEP: getting /apis @ 04/26/23 07:18:11.795
  STEP: getting /apis/storage.k8s.io @ 04/26/23 07:18:11.799
  STEP: getting /apis/storage.k8s.io/v1 @ 04/26/23 07:18:11.8
  STEP: creating @ 04/26/23 07:18:11.801
  E0426 07:18:11.804193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: watching @ 04/26/23 07:18:11.817
  Apr 26 07:18:11.817: INFO: starting watch
  STEP: getting @ 04/26/23 07:18:11.824
  STEP: listing in namespace @ 04/26/23 07:18:11.826
  STEP: listing across namespaces @ 04/26/23 07:18:11.829
  STEP: patching @ 04/26/23 07:18:11.831
  STEP: updating @ 04/26/23 07:18:11.837
  Apr 26 07:18:11.844: INFO: waiting for watch events with expected annotations in namespace
  Apr 26 07:18:11.844: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/26/23 07:18:11.844
  STEP: deleting a collection @ 04/26/23 07:18:11.856
  Apr 26 07:18:11.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-8051" for this suite. @ 04/26/23 07:18:11.878
• [0.114 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/26/23 07:18:11.888
  Apr 26 07:18:11.888: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 07:18:11.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:11.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:11.908
  Apr 26 07:18:11.938: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:18:12.805050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/26/23 07:18:13.348
  Apr 26 07:18:13.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-8642 --namespace=crd-publish-openapi-8642 create -f -'
  E0426 07:18:13.805489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:14.075: INFO: stderr: ""
  Apr 26 07:18:14.075: INFO: stdout: "e2e-test-crd-publish-openapi-8684-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 26 07:18:14.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-8642 --namespace=crd-publish-openapi-8642 delete e2e-test-crd-publish-openapi-8684-crds test-cr'
  Apr 26 07:18:14.148: INFO: stderr: ""
  Apr 26 07:18:14.149: INFO: stdout: "e2e-test-crd-publish-openapi-8684-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 26 07:18:14.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-8642 --namespace=crd-publish-openapi-8642 apply -f -'
  Apr 26 07:18:14.382: INFO: stderr: ""
  Apr 26 07:18:14.382: INFO: stdout: "e2e-test-crd-publish-openapi-8684-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 26 07:18:14.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-8642 --namespace=crd-publish-openapi-8642 delete e2e-test-crd-publish-openapi-8684-crds test-cr'
  Apr 26 07:18:14.494: INFO: stderr: ""
  Apr 26 07:18:14.494: INFO: stdout: "e2e-test-crd-publish-openapi-8684-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/26/23 07:18:14.494
  Apr 26 07:18:14.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-8642 explain e2e-test-crd-publish-openapi-8684-crds'
  Apr 26 07:18:14.788: INFO: stderr: ""
  Apr 26 07:18:14.788: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-8684-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0426 07:18:14.806034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:15.806123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:16.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8642" for this suite. @ 04/26/23 07:18:16.211
• [4.330 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/26/23 07:18:16.219
  Apr 26 07:18:16.219: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename var-expansion @ 04/26/23 07:18:16.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:16.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:16.241
  STEP: Creating a pod to test substitution in container's args @ 04/26/23 07:18:16.244
  E0426 07:18:16.806214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:17.807321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:18.807831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:19.807990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:18:20.262
  Apr 26 07:18:20.265: INFO: Trying to get logs from node ip-172-31-3-127 pod var-expansion-dcf65d5a-2642-414e-8920-ed5b38b912fe container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 07:18:20.277
  Apr 26 07:18:20.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8238" for this suite. @ 04/26/23 07:18:20.299
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/26/23 07:18:20.309
  Apr 26 07:18:20.309: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:18:20.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:20.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:20.334
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-96faf881-424c-40b0-9fd2-8a26a8b79790 @ 04/26/23 07:18:20.34
  STEP: Creating the pod @ 04/26/23 07:18:20.347
  E0426 07:18:20.808160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:21.808376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-96faf881-424c-40b0-9fd2-8a26a8b79790 @ 04/26/23 07:18:22.37
  STEP: waiting to observe update in volume @ 04/26/23 07:18:22.379
  E0426 07:18:22.808410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:23.808716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:24.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-256" for this suite. @ 04/26/23 07:18:24.393
• [4.093 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/26/23 07:18:24.402
  Apr 26 07:18:24.402: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename proxy @ 04/26/23 07:18:24.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:24.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:24.42
  STEP: starting an echo server on multiple ports @ 04/26/23 07:18:24.435
  STEP: creating replication controller proxy-service-jjgvz in namespace proxy-8499 @ 04/26/23 07:18:24.435
  I0426 07:18:24.442039      22 runners.go:194] Created replication controller with name: proxy-service-jjgvz, namespace: proxy-8499, replica count: 1
  E0426 07:18:24.809556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0426 07:18:25.493801      22 runners.go:194] proxy-service-jjgvz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0426 07:18:25.810442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0426 07:18:26.494050      22 runners.go:194] proxy-service-jjgvz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0426 07:18:26.810563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0426 07:18:27.495033      22 runners.go:194] proxy-service-jjgvz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 07:18:27.498: INFO: setup took 3.074928509s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/26/23 07:18:27.498
  Apr 26 07:18:27.517: INFO: (0) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 18.208688ms)
  Apr 26 07:18:27.517: INFO: (0) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 18.527042ms)
  Apr 26 07:18:27.517: INFO: (0) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 18.695831ms)
  Apr 26 07:18:27.517: INFO: (0) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 19.073164ms)
  Apr 26 07:18:27.517: INFO: (0) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 19.518962ms)
  Apr 26 07:18:27.525: INFO: (0) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 26.535011ms)
  Apr 26 07:18:27.527: INFO: (0) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 28.390094ms)
  Apr 26 07:18:27.527: INFO: (0) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 28.942803ms)
  Apr 26 07:18:27.527: INFO: (0) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 29.170703ms)
  Apr 26 07:18:27.530: INFO: (0) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 31.35518ms)
  Apr 26 07:18:27.530: INFO: (0) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 31.310137ms)
  Apr 26 07:18:27.530: INFO: (0) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 32.101045ms)
  Apr 26 07:18:27.530: INFO: (0) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 32.059623ms)
  Apr 26 07:18:27.530: INFO: (0) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 32.359992ms)
  Apr 26 07:18:27.549: INFO: (0) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 50.51279ms)
  Apr 26 07:18:27.549: INFO: (0) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 51.357523ms)
  Apr 26 07:18:27.558: INFO: (1) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 7.920786ms)
  Apr 26 07:18:27.558: INFO: (1) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 8.307991ms)
  Apr 26 07:18:27.558: INFO: (1) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 8.619339ms)
  Apr 26 07:18:27.559: INFO: (1) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 9.397829ms)
  Apr 26 07:18:27.560: INFO: (1) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 10.340326ms)
  Apr 26 07:18:27.560: INFO: (1) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 10.5508ms)
  Apr 26 07:18:27.561: INFO: (1) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 11.382764ms)
  Apr 26 07:18:27.569: INFO: (1) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 19.186798ms)
  Apr 26 07:18:27.569: INFO: (1) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 19.180168ms)
  Apr 26 07:18:27.571: INFO: (1) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 21.097217ms)
  Apr 26 07:18:27.574: INFO: (1) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 24.459364ms)
  Apr 26 07:18:27.574: INFO: (1) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 24.632414ms)
  Apr 26 07:18:27.574: INFO: (1) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 24.627439ms)
  Apr 26 07:18:27.574: INFO: (1) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 24.608179ms)
  Apr 26 07:18:27.574: INFO: (1) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 24.714019ms)
  Apr 26 07:18:27.583: INFO: (1) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 33.736549ms)
  Apr 26 07:18:27.591: INFO: (2) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 7.990269ms)
  Apr 26 07:18:27.592: INFO: (2) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 8.304763ms)
  Apr 26 07:18:27.592: INFO: (2) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 8.588823ms)
  Apr 26 07:18:27.593: INFO: (2) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 9.845582ms)
  Apr 26 07:18:27.593: INFO: (2) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 10.008466ms)
  Apr 26 07:18:27.594: INFO: (2) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 10.513703ms)
  Apr 26 07:18:27.595: INFO: (2) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 11.906328ms)
  Apr 26 07:18:27.597: INFO: (2) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 13.319914ms)
  Apr 26 07:18:27.597: INFO: (2) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 13.117051ms)
  Apr 26 07:18:27.597: INFO: (2) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 13.118261ms)
  Apr 26 07:18:27.597: INFO: (2) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 13.667534ms)
  Apr 26 07:18:27.598: INFO: (2) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 14.872967ms)
  Apr 26 07:18:27.598: INFO: (2) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 14.769671ms)
  Apr 26 07:18:27.598: INFO: (2) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 14.878512ms)
  Apr 26 07:18:27.599: INFO: (2) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 14.96299ms)
  Apr 26 07:18:27.600: INFO: (2) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 16.233075ms)
  Apr 26 07:18:27.608: INFO: (3) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 8.29988ms)
  Apr 26 07:18:27.611: INFO: (3) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 11.365974ms)
  Apr 26 07:18:27.615: INFO: (3) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 14.950156ms)
  Apr 26 07:18:27.616: INFO: (3) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 16.475472ms)
  Apr 26 07:18:27.618: INFO: (3) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 17.905478ms)
  Apr 26 07:18:27.618: INFO: (3) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 17.90283ms)
  Apr 26 07:18:27.618: INFO: (3) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 18.104299ms)
  Apr 26 07:18:27.618: INFO: (3) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 18.335267ms)
  Apr 26 07:18:27.620: INFO: (3) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 19.768154ms)
  Apr 26 07:18:27.623: INFO: (3) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 22.774398ms)
  Apr 26 07:18:27.623: INFO: (3) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 23.128806ms)
  Apr 26 07:18:27.623: INFO: (3) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 23.177459ms)
  Apr 26 07:18:27.623: INFO: (3) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 23.441285ms)
  Apr 26 07:18:27.627: INFO: (3) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 27.051129ms)
  Apr 26 07:18:27.627: INFO: (3) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 26.954958ms)
  Apr 26 07:18:27.630: INFO: (3) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 30.517294ms)
  Apr 26 07:18:27.641: INFO: (4) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 10.072085ms)
  Apr 26 07:18:27.642: INFO: (4) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 11.688344ms)
  Apr 26 07:18:27.643: INFO: (4) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 12.677345ms)
  Apr 26 07:18:27.647: INFO: (4) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 16.071247ms)
  Apr 26 07:18:27.647: INFO: (4) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 16.411927ms)
  Apr 26 07:18:27.647: INFO: (4) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 16.645051ms)
  Apr 26 07:18:27.651: INFO: (4) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 20.300691ms)
  Apr 26 07:18:27.651: INFO: (4) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 20.238258ms)
  Apr 26 07:18:27.651: INFO: (4) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 20.364063ms)
  Apr 26 07:18:27.657: INFO: (4) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 26.118494ms)
  Apr 26 07:18:27.658: INFO: (4) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 27.909285ms)
  Apr 26 07:18:27.659: INFO: (4) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 28.191394ms)
  Apr 26 07:18:27.659: INFO: (4) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 28.203493ms)
  Apr 26 07:18:27.659: INFO: (4) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 28.282826ms)
  Apr 26 07:18:27.659: INFO: (4) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 28.490237ms)
  Apr 26 07:18:27.659: INFO: (4) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 28.759001ms)
  Apr 26 07:18:27.666: INFO: (5) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 6.748242ms)
  Apr 26 07:18:27.667: INFO: (5) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 6.894001ms)
  Apr 26 07:18:27.667: INFO: (5) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 7.295667ms)
  Apr 26 07:18:27.668: INFO: (5) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 7.277758ms)
  Apr 26 07:18:27.668: INFO: (5) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 8.541955ms)
  Apr 26 07:18:27.669: INFO: (5) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 8.105096ms)
  Apr 26 07:18:27.669: INFO: (5) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 9.42277ms)
  Apr 26 07:18:27.670: INFO: (5) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 9.846604ms)
  Apr 26 07:18:27.670: INFO: (5) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 9.738971ms)
  Apr 26 07:18:27.670: INFO: (5) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 10.055863ms)
  Apr 26 07:18:27.670: INFO: (5) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 11.0991ms)
  Apr 26 07:18:27.670: INFO: (5) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 9.664221ms)
  Apr 26 07:18:27.671: INFO: (5) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 11.150637ms)
  Apr 26 07:18:27.671: INFO: (5) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 10.917766ms)
  Apr 26 07:18:27.675: INFO: (5) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 14.672905ms)
  Apr 26 07:18:27.675: INFO: (5) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 14.444512ms)
  Apr 26 07:18:27.680: INFO: (6) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 5.24019ms)
  Apr 26 07:18:27.683: INFO: (6) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 7.347161ms)
  Apr 26 07:18:27.683: INFO: (6) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 7.644558ms)
  Apr 26 07:18:27.683: INFO: (6) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 7.689601ms)
  Apr 26 07:18:27.684: INFO: (6) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 8.854232ms)
  Apr 26 07:18:27.684: INFO: (6) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 9.24505ms)
  Apr 26 07:18:27.685: INFO: (6) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 9.995008ms)
  Apr 26 07:18:27.686: INFO: (6) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 10.42484ms)
  Apr 26 07:18:27.686: INFO: (6) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 10.397103ms)
  Apr 26 07:18:27.686: INFO: (6) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 10.855256ms)
  Apr 26 07:18:27.686: INFO: (6) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 11.185797ms)
  Apr 26 07:18:27.687: INFO: (6) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 11.424554ms)
  Apr 26 07:18:27.688: INFO: (6) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 12.478093ms)
  Apr 26 07:18:27.688: INFO: (6) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 12.521744ms)
  Apr 26 07:18:27.688: INFO: (6) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 12.867675ms)
  Apr 26 07:18:27.689: INFO: (6) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 13.228556ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 9.18197ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 9.101728ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 9.083321ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 8.980022ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 9.243791ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 8.896027ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 9.316334ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 9.52273ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 9.485307ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 9.334424ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 9.557307ms)
  Apr 26 07:18:27.698: INFO: (7) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 9.422251ms)
  Apr 26 07:18:27.699: INFO: (7) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 9.69669ms)
  Apr 26 07:18:27.699: INFO: (7) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 10.466395ms)
  Apr 26 07:18:27.699: INFO: (7) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 10.258004ms)
  Apr 26 07:18:27.699: INFO: (7) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 10.671426ms)
  Apr 26 07:18:27.705: INFO: (8) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 5.161587ms)
  Apr 26 07:18:27.705: INFO: (8) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 5.288338ms)
  Apr 26 07:18:27.705: INFO: (8) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 5.380024ms)
  Apr 26 07:18:27.707: INFO: (8) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 7.316955ms)
  Apr 26 07:18:27.707: INFO: (8) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 7.580751ms)
  Apr 26 07:18:27.708: INFO: (8) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 8.489395ms)
  Apr 26 07:18:27.708: INFO: (8) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 8.53484ms)
  Apr 26 07:18:27.708: INFO: (8) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 8.723775ms)
  Apr 26 07:18:27.709: INFO: (8) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 9.33282ms)
  Apr 26 07:18:27.709: INFO: (8) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 9.477321ms)
  Apr 26 07:18:27.710: INFO: (8) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 10.204187ms)
  Apr 26 07:18:27.710: INFO: (8) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 10.171972ms)
  Apr 26 07:18:27.710: INFO: (8) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 10.339004ms)
  Apr 26 07:18:27.710: INFO: (8) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 10.517511ms)
  Apr 26 07:18:27.710: INFO: (8) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 10.245704ms)
  Apr 26 07:18:27.710: INFO: (8) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 10.356505ms)
  Apr 26 07:18:27.713: INFO: (9) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 3.143482ms)
  Apr 26 07:18:27.716: INFO: (9) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 5.509273ms)
  Apr 26 07:18:27.716: INFO: (9) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 5.707119ms)
  Apr 26 07:18:27.717: INFO: (9) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 6.628669ms)
  Apr 26 07:18:27.717: INFO: (9) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 6.591865ms)
  Apr 26 07:18:27.717: INFO: (9) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 7.259329ms)
  Apr 26 07:18:27.718: INFO: (9) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 7.371598ms)
  Apr 26 07:18:27.718: INFO: (9) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 7.342938ms)
  Apr 26 07:18:27.722: INFO: (9) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 12.011805ms)
  Apr 26 07:18:27.722: INFO: (9) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 11.892361ms)
  Apr 26 07:18:27.722: INFO: (9) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 12.034434ms)
  Apr 26 07:18:27.723: INFO: (9) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 13.240556ms)
  Apr 26 07:18:27.723: INFO: (9) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 13.358437ms)
  Apr 26 07:18:27.724: INFO: (9) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 13.334722ms)
  Apr 26 07:18:27.724: INFO: (9) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 13.791039ms)
  Apr 26 07:18:27.724: INFO: (9) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 13.804246ms)
  Apr 26 07:18:27.730: INFO: (10) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 5.643557ms)
  Apr 26 07:18:27.730: INFO: (10) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 5.678004ms)
  Apr 26 07:18:27.732: INFO: (10) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 7.853307ms)
  Apr 26 07:18:27.736: INFO: (10) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 11.059194ms)
  Apr 26 07:18:27.736: INFO: (10) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 11.239384ms)
  Apr 26 07:18:27.736: INFO: (10) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 11.379848ms)
  Apr 26 07:18:27.736: INFO: (10) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 12.107641ms)
  Apr 26 07:18:27.737: INFO: (10) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 12.418449ms)
  Apr 26 07:18:27.737: INFO: (10) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 12.642902ms)
  Apr 26 07:18:27.741: INFO: (10) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 16.919426ms)
  Apr 26 07:18:27.742: INFO: (10) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 16.951051ms)
  Apr 26 07:18:27.742: INFO: (10) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 17.37744ms)
  Apr 26 07:18:27.743: INFO: (10) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 18.196386ms)
  Apr 26 07:18:27.743: INFO: (10) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 18.118817ms)
  Apr 26 07:18:27.743: INFO: (10) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 18.598583ms)
  Apr 26 07:18:27.743: INFO: (10) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 18.700914ms)
  Apr 26 07:18:27.753: INFO: (11) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 9.967888ms)
  Apr 26 07:18:27.753: INFO: (11) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 9.834365ms)
  Apr 26 07:18:27.753: INFO: (11) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 10.000798ms)
  Apr 26 07:18:27.755: INFO: (11) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 11.470894ms)
  Apr 26 07:18:27.755: INFO: (11) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 11.390368ms)
  Apr 26 07:18:27.756: INFO: (11) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 12.447202ms)
  Apr 26 07:18:27.758: INFO: (11) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 14.661317ms)
  Apr 26 07:18:27.760: INFO: (11) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 16.627149ms)
  Apr 26 07:18:27.762: INFO: (11) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 18.271163ms)
  Apr 26 07:18:27.762: INFO: (11) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 18.990827ms)
  Apr 26 07:18:27.762: INFO: (11) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 18.858534ms)
  Apr 26 07:18:27.764: INFO: (11) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 20.387976ms)
  Apr 26 07:18:27.764: INFO: (11) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 20.510212ms)
  Apr 26 07:18:27.764: INFO: (11) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 20.814869ms)
  Apr 26 07:18:27.765: INFO: (11) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 21.672526ms)
  Apr 26 07:18:27.765: INFO: (11) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 21.702631ms)
  Apr 26 07:18:27.771: INFO: (12) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 6.0909ms)
  Apr 26 07:18:27.772: INFO: (12) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 6.577535ms)
  Apr 26 07:18:27.773: INFO: (12) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 7.536652ms)
  Apr 26 07:18:27.774: INFO: (12) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 8.727052ms)
  Apr 26 07:18:27.775: INFO: (12) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 9.671036ms)
  Apr 26 07:18:27.775: INFO: (12) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 9.901204ms)
  Apr 26 07:18:27.776: INFO: (12) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 10.829217ms)
  Apr 26 07:18:27.777: INFO: (12) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 11.263552ms)
  Apr 26 07:18:27.778: INFO: (12) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 12.150309ms)
  Apr 26 07:18:27.778: INFO: (12) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 13.0885ms)
  Apr 26 07:18:27.779: INFO: (12) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 13.56299ms)
  Apr 26 07:18:27.779: INFO: (12) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 13.439814ms)
  Apr 26 07:18:27.779: INFO: (12) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 13.634035ms)
  Apr 26 07:18:27.780: INFO: (12) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 14.508412ms)
  Apr 26 07:18:27.781: INFO: (12) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 15.239814ms)
  Apr 26 07:18:27.781: INFO: (12) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 15.137379ms)
  Apr 26 07:18:27.786: INFO: (13) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 4.732398ms)
  Apr 26 07:18:27.788: INFO: (13) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 7.095575ms)
  Apr 26 07:18:27.789: INFO: (13) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 8.028847ms)
  Apr 26 07:18:27.789: INFO: (13) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 8.136753ms)
  Apr 26 07:18:27.790: INFO: (13) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 8.632155ms)
  Apr 26 07:18:27.790: INFO: (13) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 8.202974ms)
  Apr 26 07:18:27.791: INFO: (13) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 9.639748ms)
  Apr 26 07:18:27.792: INFO: (13) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 10.515003ms)
  Apr 26 07:18:27.792: INFO: (13) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 10.712671ms)
  Apr 26 07:18:27.793: INFO: (13) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 11.863921ms)
  Apr 26 07:18:27.793: INFO: (13) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 12.292507ms)
  Apr 26 07:18:27.794: INFO: (13) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 13.239067ms)
  Apr 26 07:18:27.795: INFO: (13) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 13.989721ms)
  Apr 26 07:18:27.795: INFO: (13) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 14.421656ms)
  Apr 26 07:18:27.796: INFO: (13) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 15.030244ms)
  Apr 26 07:18:27.797: INFO: (13) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 16.049969ms)
  Apr 26 07:18:27.805: INFO: (14) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 7.685877ms)
  Apr 26 07:18:27.805: INFO: (14) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 7.598633ms)
  Apr 26 07:18:27.806: INFO: (14) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 8.752595ms)
  Apr 26 07:18:27.807: INFO: (14) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 9.388534ms)
  Apr 26 07:18:27.807: INFO: (14) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 10.006297ms)
  Apr 26 07:18:27.809: INFO: (14) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 10.946431ms)
  Apr 26 07:18:27.810: INFO: (14) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 12.09886ms)
  E0426 07:18:27.811051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:27.811: INFO: (14) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 13.538778ms)
  Apr 26 07:18:27.811: INFO: (14) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 13.603612ms)
  Apr 26 07:18:27.812: INFO: (14) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 14.21066ms)
  Apr 26 07:18:27.812: INFO: (14) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 14.022252ms)
  Apr 26 07:18:27.812: INFO: (14) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 14.337364ms)
  Apr 26 07:18:27.813: INFO: (14) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 14.836082ms)
  Apr 26 07:18:27.813: INFO: (14) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 15.051317ms)
  Apr 26 07:18:27.815: INFO: (14) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 17.323049ms)
  Apr 26 07:18:27.815: INFO: (14) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 17.07851ms)
  Apr 26 07:18:27.821: INFO: (15) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 6.605137ms)
  Apr 26 07:18:27.822: INFO: (15) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 7.198552ms)
  Apr 26 07:18:27.823: INFO: (15) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 8.187309ms)
  Apr 26 07:18:27.823: INFO: (15) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 8.288471ms)
  Apr 26 07:18:27.823: INFO: (15) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 8.221354ms)
  Apr 26 07:18:27.824: INFO: (15) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 8.870498ms)
  Apr 26 07:18:27.825: INFO: (15) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 9.370793ms)
  Apr 26 07:18:27.825: INFO: (15) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 9.754074ms)
  Apr 26 07:18:27.826: INFO: (15) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 10.674192ms)
  Apr 26 07:18:27.826: INFO: (15) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 10.913455ms)
  Apr 26 07:18:27.827: INFO: (15) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 11.481669ms)
  Apr 26 07:18:27.827: INFO: (15) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 12.011617ms)
  Apr 26 07:18:27.827: INFO: (15) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 11.955097ms)
  Apr 26 07:18:27.827: INFO: (15) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 11.915255ms)
  Apr 26 07:18:27.827: INFO: (15) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 12.360148ms)
  Apr 26 07:18:27.828: INFO: (15) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 12.524364ms)
  Apr 26 07:18:27.834: INFO: (16) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 6.379334ms)
  Apr 26 07:18:27.834: INFO: (16) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 6.335278ms)
  Apr 26 07:18:27.834: INFO: (16) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 6.563259ms)
  Apr 26 07:18:27.835: INFO: (16) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 6.961071ms)
  Apr 26 07:18:27.839: INFO: (16) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 10.923164ms)
  Apr 26 07:18:27.839: INFO: (16) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 10.915686ms)
  Apr 26 07:18:27.839: INFO: (16) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 10.810771ms)
  Apr 26 07:18:27.839: INFO: (16) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 11.050177ms)
  Apr 26 07:18:27.839: INFO: (16) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 11.11694ms)
  Apr 26 07:18:27.839: INFO: (16) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 11.304777ms)
  Apr 26 07:18:27.840: INFO: (16) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 11.721682ms)
  Apr 26 07:18:27.840: INFO: (16) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 11.916211ms)
  Apr 26 07:18:27.842: INFO: (16) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 13.388123ms)
  Apr 26 07:18:27.843: INFO: (16) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 14.499202ms)
  Apr 26 07:18:27.843: INFO: (16) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 15.193924ms)
  Apr 26 07:18:27.843: INFO: (16) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 15.288574ms)
  Apr 26 07:18:27.849: INFO: (17) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 5.05332ms)
  Apr 26 07:18:27.849: INFO: (17) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 5.589222ms)
  Apr 26 07:18:27.853: INFO: (17) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 9.280432ms)
  Apr 26 07:18:27.854: INFO: (17) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 10.689832ms)
  Apr 26 07:18:27.854: INFO: (17) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 10.738671ms)
  Apr 26 07:18:27.856: INFO: (17) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 11.935674ms)
  Apr 26 07:18:27.856: INFO: (17) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 12.043211ms)
  Apr 26 07:18:27.856: INFO: (17) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 12.336511ms)
  Apr 26 07:18:27.856: INFO: (17) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 12.407781ms)
  Apr 26 07:18:27.856: INFO: (17) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 12.373862ms)
  Apr 26 07:18:27.856: INFO: (17) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 12.679982ms)
  Apr 26 07:18:27.856: INFO: (17) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 12.566851ms)
  Apr 26 07:18:27.856: INFO: (17) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 12.800907ms)
  Apr 26 07:18:27.858: INFO: (17) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 14.049928ms)
  Apr 26 07:18:27.858: INFO: (17) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 14.089659ms)
  Apr 26 07:18:27.858: INFO: (17) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 14.360497ms)
  Apr 26 07:18:27.861: INFO: (18) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 3.411284ms)
  Apr 26 07:18:27.864: INFO: (18) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 5.582625ms)
  Apr 26 07:18:27.865: INFO: (18) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 6.702758ms)
  Apr 26 07:18:27.865: INFO: (18) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 7.24007ms)
  Apr 26 07:18:27.867: INFO: (18) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 9.017151ms)
  Apr 26 07:18:27.867: INFO: (18) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 9.221436ms)
  Apr 26 07:18:27.869: INFO: (18) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 10.702643ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 12.460373ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 12.432526ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 12.604337ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 12.474971ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 12.466715ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 12.433949ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 12.45755ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 12.461315ms)
  Apr 26 07:18:27.871: INFO: (18) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 12.661327ms)
  Apr 26 07:18:27.878: INFO: (19) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:462/proxy/: tls qux (200; 6.816585ms)
  Apr 26 07:18:27.878: INFO: (19) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname1/proxy/: tls baz (200; 7.748259ms)
  Apr 26 07:18:27.879: INFO: (19) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:460/proxy/: tls baz (200; 7.717127ms)
  Apr 26 07:18:27.879: INFO: (19) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname2/proxy/: bar (200; 8.271411ms)
  Apr 26 07:18:27.880: INFO: (19) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 9.303928ms)
  Apr 26 07:18:27.880: INFO: (19) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww/proxy/rewriteme">test</a> (200; 9.396678ms)
  Apr 26 07:18:27.881: INFO: (19) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">... (200; 9.722458ms)
  Apr 26 07:18:27.881: INFO: (19) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:1080/proxy/rewriteme">test<... (200; 10.547533ms)
  Apr 26 07:18:27.881: INFO: (19) /api/v1/namespaces/proxy-8499/pods/proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 10.552746ms)
  Apr 26 07:18:27.882: INFO: (19) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname2/proxy/: bar (200; 10.909864ms)
  Apr 26 07:18:27.882: INFO: (19) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:162/proxy/: bar (200; 10.913534ms)
  Apr 26 07:18:27.882: INFO: (19) /api/v1/namespaces/proxy-8499/services/http:proxy-service-jjgvz:portname1/proxy/: foo (200; 11.386025ms)
  Apr 26 07:18:27.883: INFO: (19) /api/v1/namespaces/proxy-8499/services/https:proxy-service-jjgvz:tlsportname2/proxy/: tls qux (200; 11.771045ms)
  Apr 26 07:18:27.883: INFO: (19) /api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/: <a href="/api/v1/namespaces/proxy-8499/pods/https:proxy-service-jjgvz-pn8ww:443/proxy/tlsrewritem... (200; 11.951349ms)
  Apr 26 07:18:27.883: INFO: (19) /api/v1/namespaces/proxy-8499/services/proxy-service-jjgvz:portname1/proxy/: foo (200; 11.813138ms)
  Apr 26 07:18:27.883: INFO: (19) /api/v1/namespaces/proxy-8499/pods/http:proxy-service-jjgvz-pn8ww:160/proxy/: foo (200; 12.259397ms)
  Apr 26 07:18:27.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-jjgvz in namespace proxy-8499, will wait for the garbage collector to delete the pods @ 04/26/23 07:18:27.886
  Apr 26 07:18:27.948: INFO: Deleting ReplicationController proxy-service-jjgvz took: 8.743176ms
  Apr 26 07:18:28.049: INFO: Terminating ReplicationController proxy-service-jjgvz pods took: 100.963377ms
  E0426 07:18:28.811174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:29.812141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-8499" for this suite. @ 04/26/23 07:18:29.95
• [5.555 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/26/23 07:18:29.958
  Apr 26 07:18:29.958: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename resourcequota @ 04/26/23 07:18:29.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:29.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:29.982
  STEP: Creating a ResourceQuota with terminating scope @ 04/26/23 07:18:29.984
  STEP: Ensuring ResourceQuota status is calculated @ 04/26/23 07:18:29.991
  E0426 07:18:30.812504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:31.812758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 04/26/23 07:18:31.994
  STEP: Ensuring ResourceQuota status is calculated @ 04/26/23 07:18:32
  E0426 07:18:32.813610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:33.813896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 04/26/23 07:18:34.004
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/26/23 07:18:34.02
  E0426 07:18:34.814395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:35.814649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/26/23 07:18:36.024
  E0426 07:18:36.814898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:37.815239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/26/23 07:18:38.028
  STEP: Ensuring resource quota status released the pod usage @ 04/26/23 07:18:38.05
  E0426 07:18:38.815362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:39.815507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 04/26/23 07:18:40.054
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/26/23 07:18:40.068
  E0426 07:18:40.816297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:41.816599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/26/23 07:18:42.072
  E0426 07:18:42.817369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:43.817636      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/26/23 07:18:44.076
  STEP: Ensuring resource quota status released the pod usage @ 04/26/23 07:18:44.09
  E0426 07:18:44.818644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:45.819011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:46.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9664" for this suite. @ 04/26/23 07:18:46.098
• [16.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/26/23 07:18:46.107
  Apr 26 07:18:46.107: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:18:46.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:46.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:46.132
  STEP: Setting up server cert @ 04/26/23 07:18:46.167
  E0426 07:18:46.819838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:18:46.841
  STEP: Deploying the webhook pod @ 04/26/23 07:18:46.851
  STEP: Wait for the deployment to be ready @ 04/26/23 07:18:46.864
  Apr 26 07:18:46.869: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:18:47.820652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:48.820910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:18:48.878
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:18:48.89
  E0426 07:18:49.821377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:49.890: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/26/23 07:18:49.893
  STEP: create a pod that should be updated by the webhook @ 04/26/23 07:18:49.908
  Apr 26 07:18:49.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6948" for this suite. @ 04/26/23 07:18:49.986
  STEP: Destroying namespace "webhook-markers-5582" for this suite. @ 04/26/23 07:18:49.994
• [3.898 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/26/23 07:18:50.005
  Apr 26 07:18:50.005: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename var-expansion @ 04/26/23 07:18:50.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:50.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:50.029
  E0426 07:18:50.821443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:51.821790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:18:52.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 07:18:52.050: INFO: Deleting pod "var-expansion-123d3f5c-bb76-4c77-a388-b904e26f723a" in namespace "var-expansion-4228"
  Apr 26 07:18:52.058: INFO: Wait up to 5m0s for pod "var-expansion-123d3f5c-bb76-4c77-a388-b904e26f723a" to be fully deleted
  E0426 07:18:52.822141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:53.822307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-4228" for this suite. @ 04/26/23 07:18:54.066
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/26/23 07:18:54.075
  Apr 26 07:18:54.075: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-preemption @ 04/26/23 07:18:54.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:18:54.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:18:54.093
  Apr 26 07:18:54.114: INFO: Waiting up to 1m0s for all nodes to be ready
  E0426 07:18:54.822518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:55.823256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:56.823629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:57.823938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:58.824007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:18:59.824220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:00.824862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:01.825150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:02.825848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:03.826148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:04.826565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:05.826813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:06.827113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:07.827554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:08.827791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:09.827999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:10.828723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:11.828830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:12.829300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:13.829526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:14.830253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:15.831289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:16.831680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:17.832750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:18.832973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:19.833112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:20.833227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:21.833496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:22.833624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:23.833907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:24.833992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:25.834257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:26.835351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:27.835774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:28.836913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:29.837150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:30.837465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:31.837633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:32.838059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:33.838131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:34.838313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:35.838435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:36.838748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:37.838874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:38.839084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:39.839325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:40.839466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:41.839763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:42.840581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:43.840857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:44.840940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:45.841191      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:46.841560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:47.841946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:48.842202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:49.842466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:50.843254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:51.843423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:52.844458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:53.844765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:19:54.137: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/26/23 07:19:54.14
  Apr 26 07:19:54.158: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 26 07:19:54.170: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 26 07:19:54.190: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 26 07:19:54.205: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/26/23 07:19:54.205
  E0426 07:19:54.845466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:55.846246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/26/23 07:19:56.219
  E0426 07:19:56.847283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:57.848136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:58.848295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:19:59.848398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:00.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9545" for this suite. @ 04/26/23 07:20:00.316
• [66.250 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/26/23 07:20:00.325
  Apr 26 07:20:00.325: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename watch @ 04/26/23 07:20:00.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:00.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:00.344
  STEP: creating a watch on configmaps with label A @ 04/26/23 07:20:00.346
  STEP: creating a watch on configmaps with label B @ 04/26/23 07:20:00.347
  STEP: creating a watch on configmaps with label A or B @ 04/26/23 07:20:00.348
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/26/23 07:20:00.35
  Apr 26 07:20:00.356: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9770  ee4e2503-b064-498e-8252-bce44415a3ff 27046 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:20:00.356: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9770  ee4e2503-b064-498e-8252-bce44415a3ff 27046 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/26/23 07:20:00.356
  Apr 26 07:20:00.365: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9770  ee4e2503-b064-498e-8252-bce44415a3ff 27047 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:20:00.365: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9770  ee4e2503-b064-498e-8252-bce44415a3ff 27047 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/26/23 07:20:00.365
  Apr 26 07:20:00.373: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9770  ee4e2503-b064-498e-8252-bce44415a3ff 27048 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:20:00.373: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9770  ee4e2503-b064-498e-8252-bce44415a3ff 27048 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/26/23 07:20:00.373
  Apr 26 07:20:00.379: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9770  ee4e2503-b064-498e-8252-bce44415a3ff 27049 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:20:00.379: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9770  ee4e2503-b064-498e-8252-bce44415a3ff 27049 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/26/23 07:20:00.38
  Apr 26 07:20:00.385: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9770  875b0285-41a5-4a5e-add9-19480facc9e3 27050 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:20:00.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9770  875b0285-41a5-4a5e-add9-19480facc9e3 27050 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0426 07:20:00.848481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:01.849482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:02.850385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:03.850526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:04.851241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:05.851304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:06.851551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:07.851898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:08.852165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:09.852425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/26/23 07:20:10.387
  Apr 26 07:20:10.395: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9770  875b0285-41a5-4a5e-add9-19480facc9e3 27122 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:20:10.395: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9770  875b0285-41a5-4a5e-add9-19480facc9e3 27122 0 2023-04-26 07:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-26 07:20:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0426 07:20:10.853389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:11.853578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:12.854491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:13.854766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:14.855232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:15.855489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:16.855963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:17.856355      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:18.856630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:19.856862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:20.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9770" for this suite. @ 04/26/23 07:20:20.401
• [20.082 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/26/23 07:20:20.408
  Apr 26 07:20:20.408: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replicaset @ 04/26/23 07:20:20.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:20.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:20.461
  Apr 26 07:20:20.464: INFO: Creating ReplicaSet my-hostname-basic-940c7b3a-1f68-489e-8e19-2c1c8f5d190b
  Apr 26 07:20:20.473: INFO: Pod name my-hostname-basic-940c7b3a-1f68-489e-8e19-2c1c8f5d190b: Found 0 pods out of 1
  E0426 07:20:20.857100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:21.857426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:22.858291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:23.858812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:24.858977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:25.477: INFO: Pod name my-hostname-basic-940c7b3a-1f68-489e-8e19-2c1c8f5d190b: Found 1 pods out of 1
  Apr 26 07:20:25.477: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-940c7b3a-1f68-489e-8e19-2c1c8f5d190b" is running
  Apr 26 07:20:25.479: INFO: Pod "my-hostname-basic-940c7b3a-1f68-489e-8e19-2c1c8f5d190b-fhvgm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 07:20:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 07:20:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 07:20:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 07:20:20 +0000 UTC Reason: Message:}])
  Apr 26 07:20:25.479: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/26/23 07:20:25.479
  Apr 26 07:20:25.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8092" for this suite. @ 04/26/23 07:20:25.49
• [5.090 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/26/23 07:20:25.498
  Apr 26 07:20:25.498: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 07:20:25.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:25.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:25.521
  STEP: Creating secret with name secret-test-map-86110497-bddb-4af9-9201-2abd7a859720 @ 04/26/23 07:20:25.524
  STEP: Creating a pod to test consume secrets @ 04/26/23 07:20:25.532
  E0426 07:20:25.859738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:26.859949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:27.860269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:28.860416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:20:29.556
  Apr 26 07:20:29.559: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-secrets-c31f34a6-d770-4137-8f4d-c0b2bed49e0a container secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 07:20:29.572
  Apr 26 07:20:29.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6039" for this suite. @ 04/26/23 07:20:29.593
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/26/23 07:20:29.604
  Apr 26 07:20:29.604: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 07:20:29.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:29.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:29.628
  Apr 26 07:20:29.631: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:20:29.860525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:30.861132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/26/23 07:20:31.064
  Apr 26 07:20:31.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 create -f -'
  E0426 07:20:31.861552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:31.865: INFO: stderr: ""
  Apr 26 07:20:31.865: INFO: stdout: "e2e-test-crd-publish-openapi-7934-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 26 07:20:31.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 delete e2e-test-crd-publish-openapi-7934-crds test-foo'
  Apr 26 07:20:31.947: INFO: stderr: ""
  Apr 26 07:20:31.947: INFO: stdout: "e2e-test-crd-publish-openapi-7934-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 26 07:20:31.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 apply -f -'
  Apr 26 07:20:32.629: INFO: stderr: ""
  Apr 26 07:20:32.629: INFO: stdout: "e2e-test-crd-publish-openapi-7934-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 26 07:20:32.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 delete e2e-test-crd-publish-openapi-7934-crds test-foo'
  Apr 26 07:20:32.703: INFO: stderr: ""
  Apr 26 07:20:32.703: INFO: stdout: "e2e-test-crd-publish-openapi-7934-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/26/23 07:20:32.703
  Apr 26 07:20:32.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 create -f -'
  E0426 07:20:32.862368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:32.924: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/26/23 07:20:32.924
  Apr 26 07:20:32.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 create -f -'
  Apr 26 07:20:33.153: INFO: rc: 1
  Apr 26 07:20:33.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 apply -f -'
  Apr 26 07:20:33.377: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/26/23 07:20:33.378
  Apr 26 07:20:33.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 create -f -'
  Apr 26 07:20:33.603: INFO: rc: 1
  Apr 26 07:20:33.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 --namespace=crd-publish-openapi-6739 apply -f -'
  E0426 07:20:33.862506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:33.889: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/26/23 07:20:33.889
  Apr 26 07:20:33.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 explain e2e-test-crd-publish-openapi-7934-crds'
  Apr 26 07:20:34.171: INFO: stderr: ""
  Apr 26 07:20:34.171: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7934-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/26/23 07:20:34.171
  Apr 26 07:20:34.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 explain e2e-test-crd-publish-openapi-7934-crds.metadata'
  Apr 26 07:20:34.480: INFO: stderr: ""
  Apr 26 07:20:34.480: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7934-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 26 07:20:34.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 explain e2e-test-crd-publish-openapi-7934-crds.spec'
  Apr 26 07:20:34.787: INFO: stderr: ""
  Apr 26 07:20:34.787: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7934-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 26 07:20:34.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 explain e2e-test-crd-publish-openapi-7934-crds.spec.bars'
  E0426 07:20:34.862583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:35.076: INFO: stderr: ""
  Apr 26 07:20:35.076: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7934-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/26/23 07:20:35.076
  Apr 26 07:20:35.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=crd-publish-openapi-6739 explain e2e-test-crd-publish-openapi-7934-crds.spec.bars2'
  Apr 26 07:20:35.350: INFO: rc: 1
  E0426 07:20:35.862702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:36.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6739" for this suite. @ 04/26/23 07:20:36.754
• [7.156 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/26/23 07:20:36.761
  Apr 26 07:20:36.761: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 07:20:36.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:36.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:36.783
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 07:20:36.786
  E0426 07:20:36.862993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:37.863946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:38.865019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:39.865212      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:20:40.806
  Apr 26 07:20:40.808: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-d57dde8b-67ec-4538-9603-49ee05593d5f container client-container: <nil>
  STEP: delete the pod @ 04/26/23 07:20:40.813
  Apr 26 07:20:40.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6155" for this suite. @ 04/26/23 07:20:40.832
• [4.079 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/26/23 07:20:40.84
  Apr 26 07:20:40.840: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 07:20:40.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:40.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:40.86
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 07:20:40.862
  E0426 07:20:40.866003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:41.867015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:42.867400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:43.867618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:44.867888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:20:44.883
  Apr 26 07:20:44.886: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-08368103-a2c4-44c9-ab39-113fac76b335 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 07:20:44.89
  Apr 26 07:20:44.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7451" for this suite. @ 04/26/23 07:20:44.909
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/26/23 07:20:44.918
  Apr 26 07:20:44.918: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename disruption @ 04/26/23 07:20:44.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:44.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:44.939
  STEP: creating the pdb @ 04/26/23 07:20:44.942
  STEP: Waiting for the pdb to be processed @ 04/26/23 07:20:44.948
  E0426 07:20:45.868262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:46.868337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 04/26/23 07:20:46.953
  STEP: Waiting for the pdb to be processed @ 04/26/23 07:20:46.963
  E0426 07:20:47.869167      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:48.869238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 04/26/23 07:20:48.969
  STEP: Waiting for the pdb to be processed @ 04/26/23 07:20:48.979
  E0426 07:20:49.869316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:50.869790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 04/26/23 07:20:50.994
  Apr 26 07:20:50.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8168" for this suite. @ 04/26/23 07:20:50.999
• [6.088 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/26/23 07:20:51.007
  Apr 26 07:20:51.007: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 07:20:51.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:51.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:51.029
  STEP: creating a ConfigMap @ 04/26/23 07:20:51.032
  STEP: fetching the ConfigMap @ 04/26/23 07:20:51.038
  STEP: patching the ConfigMap @ 04/26/23 07:20:51.04
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/26/23 07:20:51.047
  STEP: deleting the ConfigMap by collection with a label selector @ 04/26/23 07:20:51.051
  STEP: listing all ConfigMaps in test namespace @ 04/26/23 07:20:51.059
  Apr 26 07:20:51.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5058" for this suite. @ 04/26/23 07:20:51.064
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/26/23 07:20:51.073
  Apr 26 07:20:51.073: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:20:51.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:51.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:51.095
  STEP: creating a replication controller @ 04/26/23 07:20:51.097
  Apr 26 07:20:51.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 create -f -'
  Apr 26 07:20:51.854: INFO: stderr: ""
  Apr 26 07:20:51.854: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/26/23 07:20:51.854
  Apr 26 07:20:51.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0426 07:20:51.870013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:51.921: INFO: stderr: ""
  Apr 26 07:20:51.921: INFO: stdout: ""
  STEP: Replicas for name=update-demo: expected=2 actual=0 @ 04/26/23 07:20:51.921
  E0426 07:20:52.870078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:53.870175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:54.870313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:55.871228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:56.871547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:20:56.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 26 07:20:56.991: INFO: stderr: ""
  Apr 26 07:20:56.991: INFO: stdout: "update-demo-nautilus-ds55z update-demo-nautilus-67lrr "
  Apr 26 07:20:56.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 get pods update-demo-nautilus-ds55z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 26 07:20:57.056: INFO: stderr: ""
  Apr 26 07:20:57.056: INFO: stdout: "true"
  Apr 26 07:20:57.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 get pods update-demo-nautilus-ds55z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 26 07:20:57.119: INFO: stderr: ""
  Apr 26 07:20:57.119: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 26 07:20:57.120: INFO: validating pod update-demo-nautilus-ds55z
  Apr 26 07:20:57.123: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 26 07:20:57.123: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 26 07:20:57.123: INFO: update-demo-nautilus-ds55z is verified up and running
  Apr 26 07:20:57.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 get pods update-demo-nautilus-67lrr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 26 07:20:57.189: INFO: stderr: ""
  Apr 26 07:20:57.189: INFO: stdout: "true"
  Apr 26 07:20:57.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 get pods update-demo-nautilus-67lrr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 26 07:20:57.254: INFO: stderr: ""
  Apr 26 07:20:57.254: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 26 07:20:57.254: INFO: validating pod update-demo-nautilus-67lrr
  Apr 26 07:20:57.258: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 26 07:20:57.258: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 26 07:20:57.258: INFO: update-demo-nautilus-67lrr is verified up and running
  STEP: using delete to clean up resources @ 04/26/23 07:20:57.258
  Apr 26 07:20:57.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 delete --grace-period=0 --force -f -'
  Apr 26 07:20:57.326: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 07:20:57.326: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 26 07:20:57.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 get rc,svc -l name=update-demo --no-headers'
  Apr 26 07:20:57.402: INFO: stderr: "No resources found in kubectl-6287 namespace.\n"
  Apr 26 07:20:57.402: INFO: stdout: ""
  Apr 26 07:20:57.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-6287 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 26 07:20:57.470: INFO: stderr: ""
  Apr 26 07:20:57.470: INFO: stdout: ""
  Apr 26 07:20:57.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6287" for this suite. @ 04/26/23 07:20:57.474
• [6.446 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/26/23 07:20:57.52
  Apr 26 07:20:57.520: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:20:57.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:20:57.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:20:57.599
  STEP: Creating configMap with name projected-configmap-test-volume-9fd23312-37c4-4911-8946-3253104121e3 @ 04/26/23 07:20:57.618
  STEP: Creating a pod to test consume configMaps @ 04/26/23 07:20:57.643
  E0426 07:20:57.872512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:58.872821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:20:59.873039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:00.873352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:21:01.688
  Apr 26 07:21:01.690: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-configmaps-bdc25cd3-423b-448a-9f67-14910254b1be container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:21:01.695
  Apr 26 07:21:01.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3789" for this suite. @ 04/26/23 07:21:01.719
• [4.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 04/26/23 07:21:01.728
  Apr 26 07:21:01.728: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename statefulset @ 04/26/23 07:21:01.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:21:01.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:21:01.754
  STEP: Creating service test in namespace statefulset-4966 @ 04/26/23 07:21:01.757
  STEP: Creating stateful set ss in namespace statefulset-4966 @ 04/26/23 07:21:01.763
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4966 @ 04/26/23 07:21:01.771
  Apr 26 07:21:01.774: INFO: Found 0 stateful pods, waiting for 1
  E0426 07:21:01.874152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:02.874868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:03.875293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:04.875535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:05.875748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:06.876028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:07.876249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:08.876714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:09.876994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:10.877233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:11.779: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/26/23 07:21:11.779
  Apr 26 07:21:11.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-4966 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0426 07:21:11.877608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:11.916: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 07:21:11.916: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 07:21:11.916: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 26 07:21:11.919: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0426 07:21:12.878379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:13.879298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:14.879537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:15.879679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:16.880199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:17.880681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:18.880937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:19.881100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:20.881430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:21.881694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:21.923: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 26 07:21:21.923: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 07:21:21.938: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Apr 26 07:21:21.938: INFO: ss-0  ip-172-31-3-127  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:01 +0000 UTC  }]
  Apr 26 07:21:21.938: INFO: 
  Apr 26 07:21:21.938: INFO: StatefulSet ss has not reached scale 3, at 1
  E0426 07:21:22.881822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:22.942: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996603738s
  E0426 07:21:23.882170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:23.947: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992394814s
  E0426 07:21:24.882189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:24.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987281836s
  E0426 07:21:25.883274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:25.956: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983310711s
  E0426 07:21:26.883466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:26.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978992438s
  E0426 07:21:27.883565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:27.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974870245s
  E0426 07:21:28.884154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:28.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97144261s
  E0426 07:21:29.885136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:29.972: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967424017s
  E0426 07:21:30.885299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:30.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.136563ms
  E0426 07:21:31.885423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4966 @ 04/26/23 07:21:31.977
  Apr 26 07:21:31.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-4966 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 07:21:32.116: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 26 07:21:32.116: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 07:21:32.116: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 26 07:21:32.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-4966 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 07:21:32.242: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 26 07:21:32.242: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 07:21:32.242: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 26 07:21:32.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-4966 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 07:21:32.400: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 26 07:21:32.400: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 07:21:32.400: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 26 07:21:32.403: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:21:32.403: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:21:32.403: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/26/23 07:21:32.403
  Apr 26 07:21:32.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-4966 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 07:21:32.536: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 07:21:32.536: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 07:21:32.536: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 26 07:21:32.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-4966 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 07:21:32.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 07:21:32.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 07:21:32.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 26 07:21:32.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-4966 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 07:21:32.810: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 07:21:32.810: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 07:21:32.810: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 26 07:21:32.811: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 07:21:32.814: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0426 07:21:32.886411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:33.886576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:34.887313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:35.887980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:36.888219      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:37.888648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:38.888831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:39.889010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:40.889151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:41.889189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:42.822: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 26 07:21:42.822: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 26 07:21:42.822: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 26 07:21:42.834: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Apr 26 07:21:42.834: INFO: ss-1  ip-172-31-1-91   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:21 +0000 UTC  }]
  Apr 26 07:21:42.834: INFO: ss-0  ip-172-31-3-127  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:01 +0000 UTC  }]
  Apr 26 07:21:42.834: INFO: ss-2  ip-172-31-3-127  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:21 +0000 UTC  }]
  Apr 26 07:21:42.834: INFO: 
  Apr 26 07:21:42.834: INFO: StatefulSet ss has not reached scale 0, at 3
  E0426 07:21:42.889789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:43.838: INFO: POD   NODE             PHASE      GRACE  CONDITIONS
  Apr 26 07:21:43.838: INFO: ss-0  ip-172-31-3-127  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:01 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:33 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:33 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-26 07:21:01 +0000 UTC  }]
  Apr 26 07:21:43.838: INFO: 
  Apr 26 07:21:43.838: INFO: StatefulSet ss has not reached scale 0, at 1
  E0426 07:21:43.890939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:44.843: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992272925s
  E0426 07:21:44.891851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:45.847: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987791081s
  E0426 07:21:45.891958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:46.852: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983652507s
  E0426 07:21:46.893041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:47.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.978274837s
  E0426 07:21:47.893693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:48.874: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973925181s
  E0426 07:21:48.894793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:49.879: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.95648997s
  E0426 07:21:49.895487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:50.883: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.952303101s
  E0426 07:21:50.896499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:51.887: INFO: Verifying statefulset ss doesn't scale past 0 for another 948.101351ms
  E0426 07:21:51.897354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4966 @ 04/26/23 07:21:52.888
  Apr 26 07:21:52.892: INFO: Scaling statefulset ss to 0
  E0426 07:21:52.897385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:21:52.901: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 07:21:52.904: INFO: Deleting all statefulset in ns statefulset-4966
  Apr 26 07:21:52.906: INFO: Scaling statefulset ss to 0
  Apr 26 07:21:52.914: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 07:21:52.917: INFO: Deleting statefulset ss
  Apr 26 07:21:52.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4966" for this suite. @ 04/26/23 07:21:52.934
• [51.214 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/26/23 07:21:52.943
  Apr 26 07:21:52.943: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sysctl @ 04/26/23 07:21:52.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:21:52.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:21:52.963
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/26/23 07:21:52.966
  STEP: Watching for error events or started pod @ 04/26/23 07:21:52.974
  E0426 07:21:53.897821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:54.898007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 04/26/23 07:21:54.978
  E0426 07:21:55.898146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:56.899252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 04/26/23 07:21:56.987
  STEP: Getting logs from the pod @ 04/26/23 07:21:56.987
  STEP: Checking that the sysctl is actually updated @ 04/26/23 07:21:56.992
  Apr 26 07:21:56.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-5467" for this suite. @ 04/26/23 07:21:56.995
• [4.059 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/26/23 07:21:57.003
  Apr 26 07:21:57.003: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/26/23 07:21:57.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:21:57.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:21:57.022
  Apr 26 07:21:57.024: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:21:57.899628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:58.899944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:21:59.900033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:22:00.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2346" for this suite. @ 04/26/23 07:22:00.129
• [3.135 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/26/23 07:22:00.138
  Apr 26 07:22:00.138: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:22:00.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:22:00.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:22:00.155
  STEP: Setting up server cert @ 04/26/23 07:22:00.179
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:22:00.89
  E0426 07:22:00.901000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook pod @ 04/26/23 07:22:00.903
  STEP: Wait for the deployment to be ready @ 04/26/23 07:22:00.923
  Apr 26 07:22:00.929: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:22:01.901520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:02.902302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:22:02.938
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:22:02.952
  E0426 07:22:03.902838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:22:03.953: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/26/23 07:22:03.956
  STEP: create a pod @ 04/26/23 07:22:03.972
  E0426 07:22:04.903666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:05.903880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/26/23 07:22:05.987
  Apr 26 07:22:05.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=webhook-5591 attach --namespace=webhook-5591 to-be-attached-pod -i -c=container1'
  Apr 26 07:22:06.064: INFO: rc: 1
  Apr 26 07:22:06.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5591" for this suite. @ 04/26/23 07:22:06.126
  STEP: Destroying namespace "webhook-markers-9787" for this suite. @ 04/26/23 07:22:06.138
• [6.021 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/26/23 07:22:06.16
  Apr 26 07:22:06.161: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pod-network-test @ 04/26/23 07:22:06.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:22:06.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:22:06.2
  STEP: Performing setup for networking test in namespace pod-network-test-7804 @ 04/26/23 07:22:06.203
  STEP: creating a selector @ 04/26/23 07:22:06.203
  STEP: Creating the service pods in kubernetes @ 04/26/23 07:22:06.203
  Apr 26 07:22:06.203: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0426 07:22:06.903982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:07.904988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:08.905490      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:09.905741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:10.906148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:11.906231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:12.906993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:13.907088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:14.907856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:15.908082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:16.909014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:17.909379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:18.910039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:19.910197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:20.911251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:21.911518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:22.911874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:23.912104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:24.912117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:25.912568      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:26.913068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:27.913427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/26/23 07:22:28.343
  E0426 07:22:28.913450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:29.913714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:22:30.360: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Apr 26 07:22:30.360: INFO: Breadth first check of 10.1.93.238 on host 172.31.1.91...
  Apr 26 07:22:30.362: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.96.46:9080/dial?request=hostname&protocol=udp&host=10.1.93.238&port=8081&tries=1'] Namespace:pod-network-test-7804 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:22:30.362: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:22:30.362: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:22:30.362: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7804/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.96.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.1.93.238%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 26 07:22:30.435: INFO: Waiting for responses: map[]
  Apr 26 07:22:30.435: INFO: reached 10.1.93.238 after 0/1 tries
  Apr 26 07:22:30.435: INFO: Breadth first check of 10.1.96.50 on host 172.31.3.127...
  Apr 26 07:22:30.439: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.96.46:9080/dial?request=hostname&protocol=udp&host=10.1.96.50&port=8081&tries=1'] Namespace:pod-network-test-7804 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:22:30.439: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:22:30.439: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:22:30.439: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7804/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.96.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.1.96.50%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 26 07:22:30.511: INFO: Waiting for responses: map[]
  Apr 26 07:22:30.511: INFO: reached 10.1.96.50 after 0/1 tries
  Apr 26 07:22:30.511: INFO: Going to retry 0 out of 2 pods....
  Apr 26 07:22:30.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7804" for this suite. @ 04/26/23 07:22:30.514
• [24.367 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 04/26/23 07:22:30.528
  Apr 26 07:22:30.528: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename daemonsets @ 04/26/23 07:22:30.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:22:30.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:22:30.551
  STEP: Creating simple DaemonSet "daemon-set" @ 04/26/23 07:22:30.574
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/26/23 07:22:30.588
  Apr 26 07:22:30.594: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:22:30.594: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 07:22:30.913748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:22:31.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:22:31.610: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 07:22:31.913797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:22:32.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 07:22:32.601: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 04/26/23 07:22:32.603
  Apr 26 07:22:32.606: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/26/23 07:22:32.606
  Apr 26 07:22:32.616: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/26/23 07:22:32.616
  Apr 26 07:22:32.618: INFO: Observed &DaemonSet event: ADDED
  Apr 26 07:22:32.618: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.618: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.618: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.619: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.619: INFO: Found daemon set daemon-set in namespace daemonsets-8554 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 26 07:22:32.619: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/26/23 07:22:32.619
  STEP: watching for the daemon set status to be patched @ 04/26/23 07:22:32.627
  Apr 26 07:22:32.628: INFO: Observed &DaemonSet event: ADDED
  Apr 26 07:22:32.628: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.629: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.629: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.629: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.629: INFO: Observed daemon set daemon-set in namespace daemonsets-8554 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 26 07:22:32.629: INFO: Observed &DaemonSet event: MODIFIED
  Apr 26 07:22:32.629: INFO: Found daemon set daemon-set in namespace daemonsets-8554 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 26 07:22:32.629: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/26/23 07:22:32.631
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8554, will wait for the garbage collector to delete the pods @ 04/26/23 07:22:32.631
  Apr 26 07:22:32.695: INFO: Deleting DaemonSet.extensions daemon-set took: 10.514762ms
  Apr 26 07:22:32.796: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.795254ms
  E0426 07:22:32.914322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:33.914652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:34.914665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:22:35.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:22:35.399: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 26 07:22:35.402: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28105"},"items":null}

  Apr 26 07:22:35.405: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28105"},"items":null}

  Apr 26 07:22:35.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8554" for this suite. @ 04/26/23 07:22:35.419
• [4.899 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/26/23 07:22:35.427
  Apr 26 07:22:35.427: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename sched-pred @ 04/26/23 07:22:35.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:22:35.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:22:35.445
  Apr 26 07:22:35.448: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 26 07:22:35.453: INFO: Waiting for terminating namespaces to be deleted...
  Apr 26 07:22:35.456: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-91 before test
  Apr 26 07:22:35.464: INFO: calico-node-d6hf2 from kube-system started at 2023-04-26 06:03:27 +0000 UTC (1 container statuses recorded)
  Apr 26 07:22:35.464: INFO: 	Container calico-node ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-wbxsz from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 07:22:35.464: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: sonobuoy-e2e-job-5266529d9a654ea5 from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 07:22:35.464: INFO: 	Container e2e ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: sonobuoy from sonobuoy started at 2023-04-26 06:05:24 +0000 UTC (1 container statuses recorded)
  Apr 26 07:22:35.464: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: coredns-7745f9f87f-hgdql from kube-system started at 2023-04-26 06:06:48 +0000 UTC (1 container statuses recorded)
  Apr 26 07:22:35.464: INFO: 	Container coredns ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: calico-kube-controllers-6c99c8747f-plv8f from kube-system started at 2023-04-26 06:06:48 +0000 UTC (1 container statuses recorded)
  Apr 26 07:22:35.464: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: netserver-0 from pod-network-test-7804 started at 2023-04-26 07:22:06 +0000 UTC (1 container statuses recorded)
  Apr 26 07:22:35.464: INFO: 	Container webserver ready: true, restart count 0
  Apr 26 07:22:35.464: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-3-127 before test
  Apr 26 07:22:35.471: INFO: calico-node-578zf from kube-system started at 2023-04-26 06:02:34 +0000 UTC (1 container statuses recorded)
  Apr 26 07:22:35.471: INFO: 	Container calico-node ready: true, restart count 0
  Apr 26 07:22:35.471: INFO: sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-ndzdn from sonobuoy started at 2023-04-26 06:05:27 +0000 UTC (2 container statuses recorded)
  Apr 26 07:22:35.471: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 26 07:22:35.471: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 26 07:22:35.471: INFO: netserver-1 from pod-network-test-7804 started at 2023-04-26 07:22:06 +0000 UTC (1 container statuses recorded)
  Apr 26 07:22:35.471: INFO: 	Container webserver ready: true, restart count 0
  Apr 26 07:22:35.471: INFO: test-container-pod from pod-network-test-7804 started at 2023-04-26 07:22:28 +0000 UTC (1 container statuses recorded)
  Apr 26 07:22:35.471: INFO: 	Container webserver ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-1-91 @ 04/26/23 07:22:35.544
  STEP: verifying the node has the label node ip-172-31-3-127 @ 04/26/23 07:22:35.56
  Apr 26 07:22:35.575: INFO: Pod calico-node-578zf requesting resource cpu=250m on Node ip-172-31-3-127
  Apr 26 07:22:35.575: INFO: Pod calico-node-d6hf2 requesting resource cpu=250m on Node ip-172-31-1-91
  Apr 26 07:22:35.575: INFO: Pod sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-wbxsz requesting resource cpu=0m on Node ip-172-31-1-91
  Apr 26 07:22:35.575: INFO: Pod sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-ndzdn requesting resource cpu=0m on Node ip-172-31-3-127
  Apr 26 07:22:35.575: INFO: Pod sonobuoy-e2e-job-5266529d9a654ea5 requesting resource cpu=0m on Node ip-172-31-1-91
  Apr 26 07:22:35.575: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-1-91
  Apr 26 07:22:35.575: INFO: Pod coredns-7745f9f87f-hgdql requesting resource cpu=100m on Node ip-172-31-1-91
  Apr 26 07:22:35.575: INFO: Pod calico-kube-controllers-6c99c8747f-plv8f requesting resource cpu=0m on Node ip-172-31-1-91
  Apr 26 07:22:35.575: INFO: Pod test-container-pod requesting resource cpu=0m on Node ip-172-31-3-127
  Apr 26 07:22:35.575: INFO: Pod netserver-1 requesting resource cpu=0m on Node ip-172-31-3-127
  Apr 26 07:22:35.575: INFO: Pod netserver-0 requesting resource cpu=0m on Node ip-172-31-1-91
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/26/23 07:22:35.575
  Apr 26 07:22:35.575: INFO: Creating a pod which consumes cpu=2555m on Node ip-172-31-1-91
  Apr 26 07:22:35.596: INFO: Creating a pod which consumes cpu=2625m on Node ip-172-31-3-127
  E0426 07:22:35.915683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:36.916173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/26/23 07:22:37.627
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ee588074-24fd-4658-a24a-df66c6d19722.17596b280fcedc7b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2688/filler-pod-ee588074-24fd-4658-a24a-df66c6d19722 to ip-172-31-1-91] @ 04/26/23 07:22:37.63
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0c969067-38fa-422b-81b7-b7f190c5abf5.17596b2810d8146b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2688/filler-pod-0c969067-38fa-422b-81b7-b7f190c5abf5 to ip-172-31-3-127] @ 04/26/23 07:22:37.63
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0c969067-38fa-422b-81b7-b7f190c5abf5.17596b2843001965], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/26/23 07:22:37.63
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0c969067-38fa-422b-81b7-b7f190c5abf5.17596b2844c93dda], Reason = [Created], Message = [Created container filler-pod-0c969067-38fa-422b-81b7-b7f190c5abf5] @ 04/26/23 07:22:37.63
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ee588074-24fd-4658-a24a-df66c6d19722.17596b28482adc6f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/26/23 07:22:37.63
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ee588074-24fd-4658-a24a-df66c6d19722.17596b284944f9cd], Reason = [Created], Message = [Created container filler-pod-ee588074-24fd-4658-a24a-df66c6d19722] @ 04/26/23 07:22:37.63
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0c969067-38fa-422b-81b7-b7f190c5abf5.17596b2849614a10], Reason = [Started], Message = [Started container filler-pod-0c969067-38fa-422b-81b7-b7f190c5abf5] @ 04/26/23 07:22:37.63
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ee588074-24fd-4658-a24a-df66c6d19722.17596b284e39a335], Reason = [Started], Message = [Started container filler-pod-ee588074-24fd-4658-a24a-df66c6d19722] @ 04/26/23 07:22:37.63
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17596b2888ea3089], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] @ 04/26/23 07:22:37.646
  E0426 07:22:37.916679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-1-91 @ 04/26/23 07:22:38.643
  STEP: verifying the node doesn't have the label node @ 04/26/23 07:22:38.655
  STEP: removing the label node off the node ip-172-31-3-127 @ 04/26/23 07:22:38.657
  STEP: verifying the node doesn't have the label node @ 04/26/23 07:22:38.668
  Apr 26 07:22:38.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2688" for this suite. @ 04/26/23 07:22:38.677
• [3.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/26/23 07:22:38.686
  Apr 26 07:22:38.686: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pod-network-test @ 04/26/23 07:22:38.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:22:38.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:22:38.706
  STEP: Performing setup for networking test in namespace pod-network-test-4395 @ 04/26/23 07:22:38.708
  STEP: creating a selector @ 04/26/23 07:22:38.708
  STEP: Creating the service pods in kubernetes @ 04/26/23 07:22:38.708
  Apr 26 07:22:38.708: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0426 07:22:38.917245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:39.917562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:40.918434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:41.919300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:42.919813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:43.921438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:44.922450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:45.923314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:46.924358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:47.924417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:48.925438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:49.925660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/26/23 07:22:50.761
  E0426 07:22:50.926657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:51.927281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:22:52.826: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Apr 26 07:22:52.826: INFO: Going to poll 10.1.93.204 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Apr 26 07:22:52.828: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.93.204:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4395 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:22:52.828: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:22:52.829: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:22:52.829: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4395/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.1.93.204%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 26 07:22:52.889: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 26 07:22:52.889: INFO: Going to poll 10.1.96.11 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Apr 26 07:22:52.892: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.96.11:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4395 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:22:52.892: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:22:52.893: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:22:52.893: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4395/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.1.96.11%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0426 07:22:52.928221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:22:52.972: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 26 07:22:52.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4395" for this suite. @ 04/26/23 07:22:52.976
• [14.298 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/26/23 07:22:52.984
  Apr 26 07:22:52.984: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:22:52.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:22:53.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:22:53.005
  STEP: Creating projection with secret that has name projected-secret-test-map-84592ed2-e3a9-41e5-bb54-65ad251030f8 @ 04/26/23 07:22:53.008
  STEP: Creating a pod to test consume secrets @ 04/26/23 07:22:53.015
  E0426 07:22:53.928615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:54.928872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:55.929658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:56.929890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:22:57.034
  Apr 26 07:22:57.036: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-secrets-43dab207-9744-4762-a782-99b8fb450f63 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 07:22:57.044
  Apr 26 07:22:57.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1181" for this suite. @ 04/26/23 07:22:57.063
• [4.085 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/26/23 07:22:57.07
  Apr 26 07:22:57.070: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replicaset @ 04/26/23 07:22:57.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:22:57.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:22:57.094
  STEP: Create a ReplicaSet @ 04/26/23 07:22:57.096
  STEP: Verify that the required pods have come up @ 04/26/23 07:22:57.103
  Apr 26 07:22:57.106: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0426 07:22:57.930554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:58.931151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:22:59.931528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:00.931642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:01.931860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:23:02.110: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/26/23 07:23:02.11
  Apr 26 07:23:02.113: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/26/23 07:23:02.113
  STEP: DeleteCollection of the ReplicaSets @ 04/26/23 07:23:02.116
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/26/23 07:23:02.126
  Apr 26 07:23:02.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9141" for this suite. @ 04/26/23 07:23:02.133
• [5.073 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/26/23 07:23:02.143
  Apr 26 07:23:02.143: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 07:23:02.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:23:02.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:23:02.199
  STEP: creating the pod @ 04/26/23 07:23:02.202
  STEP: setting up watch @ 04/26/23 07:23:02.202
  STEP: submitting the pod to kubernetes @ 04/26/23 07:23:02.306
  STEP: verifying the pod is in kubernetes @ 04/26/23 07:23:02.329
  STEP: verifying pod creation was observed @ 04/26/23 07:23:02.331
  E0426 07:23:02.931930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:03.932790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 04/26/23 07:23:04.343
  STEP: verifying pod deletion was observed @ 04/26/23 07:23:04.351
  E0426 07:23:04.933255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:05.933775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:23:06.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1463" for this suite. @ 04/26/23 07:23:06.53
• [4.394 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/26/23 07:23:06.538
  Apr 26 07:23:06.538: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:23:06.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:23:06.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:23:06.558
  STEP: Creating secret with name projected-secret-test-49dec827-cc4d-483d-9c0d-f5cb5a233cde @ 04/26/23 07:23:06.561
  STEP: Creating a pod to test consume secrets @ 04/26/23 07:23:06.566
  E0426 07:23:06.934897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:07.935918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:08.936817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:09.936960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:23:10.585
  Apr 26 07:23:10.588: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-secrets-cd39366f-4e16-4a7b-afeb-28498555deda container secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 07:23:10.592
  Apr 26 07:23:10.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2658" for this suite. @ 04/26/23 07:23:10.612
• [4.081 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/26/23 07:23:10.62
  Apr 26 07:23:10.620: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-runtime @ 04/26/23 07:23:10.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:23:10.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:23:10.662
  STEP: create the container @ 04/26/23 07:23:10.665
  W0426 07:23:10.674166      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/26/23 07:23:10.674
  E0426 07:23:10.937959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:11.938968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:12.939977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/26/23 07:23:13.688
  STEP: the container should be terminated @ 04/26/23 07:23:13.69
  STEP: the termination message should be set @ 04/26/23 07:23:13.69
  Apr 26 07:23:13.690: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/26/23 07:23:13.69
  Apr 26 07:23:13.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-624" for this suite. @ 04/26/23 07:23:13.711
• [3.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/26/23 07:23:13.72
  Apr 26 07:23:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename namespaces @ 04/26/23 07:23:13.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:23:13.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:23:13.739
  STEP: Updating Namespace "namespaces-955" @ 04/26/23 07:23:13.742
  Apr 26 07:23:13.750: INFO: Namespace "namespaces-955" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"f65d482f-32e7-4e21-bfa7-73a160598777", "kubernetes.io/metadata.name":"namespaces-955", "namespaces-955":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 26 07:23:13.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-955" for this suite. @ 04/26/23 07:23:13.754
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/26/23 07:23:13.761
  Apr 26 07:23:13.761: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:23:13.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:23:13.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:23:13.783
  STEP: creating a Service @ 04/26/23 07:23:13.787
  STEP: watching for the Service to be added @ 04/26/23 07:23:13.817
  Apr 26 07:23:13.850: INFO: Found Service test-service-pc6vk in namespace services-6796 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 26 07:23:13.851: INFO: Service test-service-pc6vk created
  STEP: Getting /status @ 04/26/23 07:23:13.851
  Apr 26 07:23:13.853: INFO: Service test-service-pc6vk has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/26/23 07:23:13.853
  STEP: watching for the Service to be patched @ 04/26/23 07:23:13.871
  Apr 26 07:23:13.872: INFO: observed Service test-service-pc6vk in namespace services-6796 with annotations: map[] & LoadBalancer: {[]}
  Apr 26 07:23:13.872: INFO: Found Service test-service-pc6vk in namespace services-6796 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 26 07:23:13.872: INFO: Service test-service-pc6vk has service status patched
  STEP: updating the ServiceStatus @ 04/26/23 07:23:13.872
  Apr 26 07:23:13.894: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/26/23 07:23:13.894
  Apr 26 07:23:13.895: INFO: Observed Service test-service-pc6vk in namespace services-6796 with annotations: map[] & Conditions: {[]}
  Apr 26 07:23:13.895: INFO: Observed event: &Service{ObjectMeta:{test-service-pc6vk  services-6796  0d890b56-c1a1-4e80-9955-d689a2b7c56c 28598 0 2023-04-26 07:23:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-26 07:23:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-26 07:23:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.168,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.168],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 26 07:23:13.895: INFO: Found Service test-service-pc6vk in namespace services-6796 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 26 07:23:13.895: INFO: Service test-service-pc6vk has service status updated
  STEP: patching the service @ 04/26/23 07:23:13.895
  STEP: watching for the Service to be patched @ 04/26/23 07:23:13.91
  Apr 26 07:23:13.912: INFO: observed Service test-service-pc6vk in namespace services-6796 with labels: map[test-service-static:true]
  Apr 26 07:23:13.912: INFO: observed Service test-service-pc6vk in namespace services-6796 with labels: map[test-service-static:true]
  Apr 26 07:23:13.912: INFO: observed Service test-service-pc6vk in namespace services-6796 with labels: map[test-service-static:true]
  Apr 26 07:23:13.912: INFO: Found Service test-service-pc6vk in namespace services-6796 with labels: map[test-service:patched test-service-static:true]
  Apr 26 07:23:13.912: INFO: Service test-service-pc6vk patched
  STEP: deleting the service @ 04/26/23 07:23:13.912
  E0426 07:23:13.940792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: watching for the Service to be deleted @ 04/26/23 07:23:13.943
  Apr 26 07:23:13.945: INFO: Observed event: ADDED
  Apr 26 07:23:13.945: INFO: Observed event: MODIFIED
  Apr 26 07:23:13.945: INFO: Observed event: MODIFIED
  Apr 26 07:23:13.945: INFO: Observed event: MODIFIED
  Apr 26 07:23:13.945: INFO: Found Service test-service-pc6vk in namespace services-6796 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 26 07:23:13.945: INFO: Service test-service-pc6vk deleted
  Apr 26 07:23:13.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6796" for this suite. @ 04/26/23 07:23:13.949
• [0.196 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 04/26/23 07:23:13.958
  Apr 26 07:23:13.958: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename statefulset @ 04/26/23 07:23:13.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:23:13.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:23:13.98
  STEP: Creating service test in namespace statefulset-2415 @ 04/26/23 07:23:13.983
  STEP: Creating a new StatefulSet @ 04/26/23 07:23:13.989
  Apr 26 07:23:14.002: INFO: Found 0 stateful pods, waiting for 3
  E0426 07:23:14.941011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:15.941171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:16.941482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:17.942505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:18.943352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:19.943632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:20.943805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:21.944039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:22.944298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:23.944540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:23:24.007: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:23:24.007: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:23:24.007: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/26/23 07:23:24.018
  Apr 26 07:23:24.039: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/26/23 07:23:24.039
  E0426 07:23:24.944800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:25.945060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:26.945334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:27.945797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:28.946095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:29.946143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:30.946262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:31.947254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:32.947797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:33.948033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/26/23 07:23:34.052
  STEP: Performing a canary update @ 04/26/23 07:23:34.052
  Apr 26 07:23:34.073: INFO: Updating stateful set ss2
  Apr 26 07:23:34.079: INFO: Waiting for Pod statefulset-2415/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0426 07:23:34.949181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:35.949405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:36.949645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:37.950693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:38.950896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:39.951905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:40.952795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:41.953044      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:42.953619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:43.953832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/26/23 07:23:44.086
  Apr 26 07:23:44.296: INFO: Found 2 stateful pods, waiting for 3
  E0426 07:23:44.954799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:45.955243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:46.955407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:47.956486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:48.956667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:49.956926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:50.957075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:51.957548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:52.958557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:53.959621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:23:54.301: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:23:54.301: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:23:54.301: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/26/23 07:23:54.307
  Apr 26 07:23:54.368: INFO: Updating stateful set ss2
  Apr 26 07:23:54.374: INFO: Waiting for Pod statefulset-2415/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0426 07:23:54.959741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:55.960046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:56.960310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:57.960772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:58.961175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:23:59.961393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:00.962213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:01.963257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:02.963525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:03.963659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:04.403: INFO: Updating stateful set ss2
  Apr 26 07:24:04.408: INFO: Waiting for StatefulSet statefulset-2415/ss2 to complete update
  Apr 26 07:24:04.408: INFO: Waiting for Pod statefulset-2415/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0426 07:24:04.964160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:05.964296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:06.964583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:07.965723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:08.965968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:09.966224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:10.967274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:11.967429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:12.967606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:13.967827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:14.416: INFO: Deleting all statefulset in ns statefulset-2415
  Apr 26 07:24:14.419: INFO: Scaling statefulset ss2 to 0
  E0426 07:24:14.968005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:15.968247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:16.968542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:17.968869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:18.969208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:19.969953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:20.970189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:21.971255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:22.971774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:23.971987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:24.490: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 07:24:24.492: INFO: Deleting statefulset ss2
  Apr 26 07:24:24.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2415" for this suite. @ 04/26/23 07:24:24.511
• [70.606 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/26/23 07:24:24.564
  Apr 26 07:24:24.564: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename containers @ 04/26/23 07:24:24.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:24:24.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:24:24.586
  STEP: Creating a pod to test override all @ 04/26/23 07:24:24.588
  E0426 07:24:24.972295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:25.972751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:26.973314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:27.973789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:24:28.608
  Apr 26 07:24:28.611: INFO: Trying to get logs from node ip-172-31-3-127 pod client-containers-abebc23a-6b26-478c-94f3-714d8a2a23cf container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:24:28.615
  Apr 26 07:24:28.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-731" for this suite. @ 04/26/23 07:24:28.64
• [4.084 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/26/23 07:24:28.649
  Apr 26 07:24:28.649: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/26/23 07:24:28.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:24:28.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:24:28.672
  Apr 26 07:24:28.675: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:24:28.974255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:29.974411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:30.974498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:31.974630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:32.974785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:33.975745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:34.976818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9370" for this suite. @ 04/26/23 07:24:35.059
• [6.417 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/26/23 07:24:35.066
  Apr 26 07:24:35.067: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:24:35.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:24:35.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:24:35.091
  STEP: creating service endpoint-test2 in namespace services-2668 @ 04/26/23 07:24:35.094
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2668 to expose endpoints map[] @ 04/26/23 07:24:35.107
  Apr 26 07:24:35.109: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0426 07:24:35.976873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:36.117: INFO: successfully validated that service endpoint-test2 in namespace services-2668 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2668 @ 04/26/23 07:24:36.117
  E0426 07:24:36.977477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:37.978330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2668 to expose endpoints map[pod1:[80]] @ 04/26/23 07:24:38.134
  Apr 26 07:24:38.142: INFO: successfully validated that service endpoint-test2 in namespace services-2668 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/26/23 07:24:38.142
  Apr 26 07:24:38.142: INFO: Creating new exec pod
  E0426 07:24:38.978875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:39.979100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:40.979413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:41.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2668 exec execpodkb5qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 26 07:24:41.295: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 26 07:24:41.295: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:24:41.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2668 exec execpodkb5qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.146 80'
  Apr 26 07:24:41.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.146 80\nConnection to 10.152.183.146 80 port [tcp/http] succeeded!\n"
  Apr 26 07:24:41.423: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-2668 @ 04/26/23 07:24:41.423
  E0426 07:24:41.980029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:42.980979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2668 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/26/23 07:24:43.443
  Apr 26 07:24:43.452: INFO: successfully validated that service endpoint-test2 in namespace services-2668 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/26/23 07:24:43.452
  E0426 07:24:43.981205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:44.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2668 exec execpodkb5qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 26 07:24:44.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 26 07:24:44.586: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:24:44.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2668 exec execpodkb5qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.146 80'
  Apr 26 07:24:44.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.146 80\nConnection to 10.152.183.146 80 port [tcp/http] succeeded!\n"
  Apr 26 07:24:44.719: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2668 @ 04/26/23 07:24:44.719
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2668 to expose endpoints map[pod2:[80]] @ 04/26/23 07:24:44.783
  E0426 07:24:44.981721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:45.799: INFO: successfully validated that service endpoint-test2 in namespace services-2668 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/26/23 07:24:45.799
  E0426 07:24:45.982274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:46.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2668 exec execpodkb5qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 26 07:24:46.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 26 07:24:46.925: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:24:46.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-2668 exec execpodkb5qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.146 80'
  E0426 07:24:46.982853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:47.051: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.146 80\nConnection to 10.152.183.146 80 port [tcp/http] succeeded!\n"
  Apr 26 07:24:47.051: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-2668 @ 04/26/23 07:24:47.051
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2668 to expose endpoints map[] @ 04/26/23 07:24:47.069
  E0426 07:24:47.983262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:48.983474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:49.078: INFO: successfully validated that service endpoint-test2 in namespace services-2668 exposes endpoints map[]
  Apr 26 07:24:49.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2668" for this suite. @ 04/26/23 07:24:49.106
• [14.047 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/26/23 07:24:49.113
  Apr 26 07:24:49.113: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-webhook @ 04/26/23 07:24:49.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:24:49.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:24:49.136
  STEP: Setting up server cert @ 04/26/23 07:24:49.139
  E0426 07:24:49.984260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/26/23 07:24:50.004
  STEP: Deploying the custom resource conversion webhook pod @ 04/26/23 07:24:50.016
  STEP: Wait for the deployment to be ready @ 04/26/23 07:24:50.033
  Apr 26 07:24:50.038: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0426 07:24:50.985110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:51.985385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:24:52.047
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:24:52.062
  E0426 07:24:52.985721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:53.062: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 26 07:24:53.065: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:24:53.986503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:54.986821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/26/23 07:24:55.612
  STEP: Create a v2 custom resource @ 04/26/23 07:24:55.632
  STEP: List CRs in v1 @ 04/26/23 07:24:55.642
  STEP: List CRs in v2 @ 04/26/23 07:24:55.654
  Apr 26 07:24:55.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0426 07:24:55.987489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-5455" for this suite. @ 04/26/23 07:24:56.234
• [7.131 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/26/23 07:24:56.245
  Apr 26 07:24:56.245: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubelet-test @ 04/26/23 07:24:56.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:24:56.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:24:56.286
  E0426 07:24:56.987594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:24:57.987964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:58.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-597" for this suite. @ 04/26/23 07:24:58.341
• [2.145 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/26/23 07:24:58.391
  Apr 26 07:24:58.391: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename limitrange @ 04/26/23 07:24:58.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:24:58.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:24:58.418
  STEP: Creating LimitRange "e2e-limitrange-kbjl7" in namespace "limitrange-2840" @ 04/26/23 07:24:58.421
  STEP: Creating another limitRange in another namespace @ 04/26/23 07:24:58.429
  Apr 26 07:24:58.451: INFO: Namespace "e2e-limitrange-kbjl7-7902" created
  Apr 26 07:24:58.451: INFO: Creating LimitRange "e2e-limitrange-kbjl7" in namespace "e2e-limitrange-kbjl7-7902"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-kbjl7" @ 04/26/23 07:24:58.459
  Apr 26 07:24:58.462: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-kbjl7" in "limitrange-2840" namespace @ 04/26/23 07:24:58.462
  Apr 26 07:24:58.472: INFO: LimitRange "e2e-limitrange-kbjl7" has been patched
  STEP: Delete LimitRange "e2e-limitrange-kbjl7" by Collection with labelSelector: "e2e-limitrange-kbjl7=patched" @ 04/26/23 07:24:58.472
  STEP: Confirm that the limitRange "e2e-limitrange-kbjl7" has been deleted @ 04/26/23 07:24:58.482
  Apr 26 07:24:58.482: INFO: Requesting list of LimitRange to confirm quantity
  Apr 26 07:24:58.484: INFO: Found 0 LimitRange with label "e2e-limitrange-kbjl7=patched"
  Apr 26 07:24:58.485: INFO: LimitRange "e2e-limitrange-kbjl7" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-kbjl7" @ 04/26/23 07:24:58.485
  Apr 26 07:24:58.487: INFO: Found 1 limitRange
  Apr 26 07:24:58.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-2840" for this suite. @ 04/26/23 07:24:58.49
  STEP: Destroying namespace "e2e-limitrange-kbjl7-7902" for this suite. @ 04/26/23 07:24:58.497
• [0.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/26/23 07:24:58.506
  Apr 26 07:24:58.506: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:24:58.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:24:58.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:24:58.527
  Apr 26 07:24:58.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-7391 create -f -'
  E0426 07:24:58.988671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:24:59.346: INFO: stderr: ""
  Apr 26 07:24:59.346: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 26 07:24:59.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-7391 create -f -'
  Apr 26 07:24:59.757: INFO: stderr: ""
  Apr 26 07:24:59.757: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/26/23 07:24:59.757
  E0426 07:24:59.988789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:00.760: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 07:25:00.760: INFO: Found 0 / 1
  E0426 07:25:00.989112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:01.761: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 07:25:01.761: INFO: Found 1 / 1
  Apr 26 07:25:01.761: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 26 07:25:01.763: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 07:25:01.763: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 26 07:25:01.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-7391 describe pod agnhost-primary-tvw5v'
  Apr 26 07:25:01.835: INFO: stderr: ""
  Apr 26 07:25:01.835: INFO: stdout: "Name:             agnhost-primary-tvw5v\nNamespace:        kubectl-7391\nPriority:         0\nService Account:  default\nNode:             ip-172-31-3-127/172.31.3.127\nStart Time:       Wed, 26 Apr 2023 07:24:59 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 42b0f0384a8e0c772f999fc9630f6ac8ca89e6e4fce9e318204f1ecadff454f4\n                  cni.projectcalico.org/podIP: 10.1.96.34/32\n                  cni.projectcalico.org/podIPs: 10.1.96.34/32\nStatus:           Running\nIP:               10.1.96.34\nIPs:\n  IP:           10.1.96.34\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://837164b26e1a5c923774e4dbca21bb590ab762f7dded88173ba12d4706c44e0c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 26 Apr 2023 07:25:00 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-htps5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-htps5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7391/agnhost-primary-tvw5v to ip-172-31-3-127\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Apr 26 07:25:01.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-7391 describe rc agnhost-primary'
  Apr 26 07:25:01.912: INFO: stderr: ""
  Apr 26 07:25:01.912: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7391\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-tvw5v\n"
  Apr 26 07:25:01.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-7391 describe service agnhost-primary'
  Apr 26 07:25:01.984: INFO: stderr: ""
  Apr 26 07:25:01.984: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7391\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.240\nIPs:               10.152.183.240\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.1.96.34:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 26 07:25:01.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-7391 describe node ip-172-31-3-127'
  E0426 07:25:01.989995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:02.132: INFO: stderr: ""
  Apr 26 07:25:02.132: INFO: stdout: "Name:               ip-172-31-3-127\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-3-127\n                    kubernetes.io/os=linux\n                    microk8s.io/cluster=true\n                    node.kubernetes.io/microk8s-controlplane=microk8s-controlplane\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.3.127/20\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.1.96.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 26 Apr 2023 05:57:40 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-3-127\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 26 Apr 2023 07:24:57 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 26 Apr 2023 06:02:39 +0000   Wed, 26 Apr 2023 06:02:39 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 26 Apr 2023 07:23:44 +0000   Wed, 26 Apr 2023 05:57:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 26 Apr 2023 07:23:44 +0000   Wed, 26 Apr 2023 05:57:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 26 Apr 2023 07:23:44 +0000   Wed, 26 Apr 2023 05:57:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 26 Apr 2023 07:23:44 +0000   Wed, 26 Apr 2023 05:57:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.3.127\n  Hostname:    ip-172-31-3-127\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    81106868Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16382964Ki\n  pods:                 110\nAllocatable:\n  cpu:                  4\n  ephemeral-storage:    80058292Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16280564Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 e576dd6957b54ab891f512f95abe9fc8\n  System UUID:                ec2ba748-a884-ce5a-78e8-179a27dbd99e\n  Boot ID:                    394d3bf1-eedb-4a14-b523-f6af97784e48\n  Kernel Version:             5.15.0-1031-aws\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.27.0\n  Kube-Proxy Version:         v1.27.0\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-578zf                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         82m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-50cfa967ca6b4768-ndzdn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kubelet-test-597            busybox-readonly-fs52290505-ade9-461c-8d83-8ba32ac12e81    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6s\n  kubectl-7391                agnhost-primary-tvw5v                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests   Limits\n  --------             --------   ------\n  cpu                  250m (6%)  0 (0%)\n  memory               0 (0%)     0 (0%)\n  ephemeral-storage    0 (0%)     0 (0%)\n  hugepages-1Gi        0 (0%)     0 (0%)\n  hugepages-2Mi        0 (0%)     0 (0%)\n  example.com/fakecpu  0          0\nEvents:                <none>\n"
  Apr 26 07:25:02.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-7391 describe namespace kubectl-7391'
  Apr 26 07:25:02.210: INFO: stderr: ""
  Apr 26 07:25:02.210: INFO: stdout: "Name:         kubectl-7391\nLabels:       e2e-framework=kubectl\n              e2e-run=f65d482f-32e7-4e21-bfa7-73a160598777\n              kubernetes.io/metadata.name=kubectl-7391\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 26 07:25:02.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7391" for this suite. @ 04/26/23 07:25:02.213
• [3.716 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/26/23 07:25:02.222
  Apr 26 07:25:02.222: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 07:25:02.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:25:02.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:25:02.255
  STEP: Creating a pod to test downward api env vars @ 04/26/23 07:25:02.258
  E0426 07:25:02.990509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:03.991259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:04.991496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:05.991712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:25:06.313
  Apr 26 07:25:06.315: INFO: Trying to get logs from node ip-172-31-3-127 pod downward-api-0464e1e3-9682-48ff-92aa-949c071bad90 container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 07:25:06.321
  Apr 26 07:25:06.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9996" for this suite. @ 04/26/23 07:25:06.34
• [4.126 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/26/23 07:25:06.349
  Apr 26 07:25:06.349: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:25:06.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:25:06.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:25:06.37
  STEP: Creating projection with secret that has name projected-secret-test-map-30664afc-dbe7-4731-a4a8-d171fd40984c @ 04/26/23 07:25:06.373
  STEP: Creating a pod to test consume secrets @ 04/26/23 07:25:06.379
  E0426 07:25:06.992502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:07.993107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:08.993729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:09.994127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:25:10.403
  Apr 26 07:25:10.449: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-secrets-ddeb7ad0-1bc4-40b4-b0bb-11153ad43d7b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 07:25:10.499
  Apr 26 07:25:10.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6691" for this suite. @ 04/26/23 07:25:10.526
• [4.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/26/23 07:25:10.534
  Apr 26 07:25:10.534: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 07:25:10.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:25:10.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:25:10.554
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/26/23 07:25:10.557
  E0426 07:25:10.995218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:11.995458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:12.995570      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:13.995872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:25:14.598
  Apr 26 07:25:14.601: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-89fef82c-1a61-409d-971d-a83bc421523b container test-container: <nil>
  STEP: delete the pod @ 04/26/23 07:25:14.649
  Apr 26 07:25:14.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5522" for this suite. @ 04/26/23 07:25:14.756
• [4.231 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/26/23 07:25:14.766
  Apr 26 07:25:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:25:14.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:25:14.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:25:14.792
  STEP: creating a replication controller @ 04/26/23 07:25:14.794
  Apr 26 07:25:14.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 create -f -'
  E0426 07:25:14.996570      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:15.101: INFO: stderr: ""
  Apr 26 07:25:15.101: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/26/23 07:25:15.101
  Apr 26 07:25:15.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 26 07:25:15.168: INFO: stderr: ""
  Apr 26 07:25:15.168: INFO: stdout: ""
  STEP: Replicas for name=update-demo: expected=2 actual=0 @ 04/26/23 07:25:15.168
  E0426 07:25:15.997153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:16.997387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:17.997621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:18.997869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:19.998123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:20.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 26 07:25:20.235: INFO: stderr: ""
  Apr 26 07:25:20.235: INFO: stdout: "update-demo-nautilus-89x82 update-demo-nautilus-gh6xz "
  Apr 26 07:25:20.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-89x82 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 26 07:25:20.301: INFO: stderr: ""
  Apr 26 07:25:20.301: INFO: stdout: "true"
  Apr 26 07:25:20.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-89x82 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 26 07:25:20.366: INFO: stderr: ""
  Apr 26 07:25:20.366: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 26 07:25:20.366: INFO: validating pod update-demo-nautilus-89x82
  Apr 26 07:25:20.369: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 26 07:25:20.369: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 26 07:25:20.369: INFO: update-demo-nautilus-89x82 is verified up and running
  Apr 26 07:25:20.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-gh6xz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 26 07:25:20.433: INFO: stderr: ""
  Apr 26 07:25:20.433: INFO: stdout: "true"
  Apr 26 07:25:20.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-gh6xz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 26 07:25:20.496: INFO: stderr: ""
  Apr 26 07:25:20.496: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 26 07:25:20.496: INFO: validating pod update-demo-nautilus-gh6xz
  Apr 26 07:25:20.500: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 26 07:25:20.500: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 26 07:25:20.500: INFO: update-demo-nautilus-gh6xz is verified up and running
  STEP: scaling down the replication controller @ 04/26/23 07:25:20.5
  Apr 26 07:25:20.501: INFO: scanned /root for discovery docs: <nil>
  Apr 26 07:25:20.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0426 07:25:20.998308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:21.585: INFO: stderr: ""
  Apr 26 07:25:21.585: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/26/23 07:25:21.585
  Apr 26 07:25:21.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 26 07:25:21.653: INFO: stderr: ""
  Apr 26 07:25:21.653: INFO: stdout: "update-demo-nautilus-89x82 "
  Apr 26 07:25:21.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-89x82 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 26 07:25:21.718: INFO: stderr: ""
  Apr 26 07:25:21.718: INFO: stdout: "true"
  Apr 26 07:25:21.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-89x82 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 26 07:25:21.783: INFO: stderr: ""
  Apr 26 07:25:21.783: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 26 07:25:21.783: INFO: validating pod update-demo-nautilus-89x82
  Apr 26 07:25:21.786: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 26 07:25:21.786: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 26 07:25:21.786: INFO: update-demo-nautilus-89x82 is verified up and running
  STEP: scaling up the replication controller @ 04/26/23 07:25:21.786
  Apr 26 07:25:21.788: INFO: scanned /root for discovery docs: <nil>
  Apr 26 07:25:21.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0426 07:25:21.998810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:22.870: INFO: stderr: ""
  Apr 26 07:25:22.870: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/26/23 07:25:22.87
  Apr 26 07:25:22.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 26 07:25:22.967: INFO: stderr: ""
  Apr 26 07:25:22.967: INFO: stdout: "update-demo-nautilus-89x82 update-demo-nautilus-hzxq4 "
  Apr 26 07:25:22.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-89x82 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0426 07:25:22.999473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:23.087: INFO: stderr: ""
  Apr 26 07:25:23.087: INFO: stdout: "true"
  Apr 26 07:25:23.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-89x82 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 26 07:25:23.169: INFO: stderr: ""
  Apr 26 07:25:23.169: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 26 07:25:23.169: INFO: validating pod update-demo-nautilus-89x82
  Apr 26 07:25:23.172: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 26 07:25:23.172: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 26 07:25:23.172: INFO: update-demo-nautilus-89x82 is verified up and running
  Apr 26 07:25:23.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-hzxq4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 26 07:25:23.237: INFO: stderr: ""
  Apr 26 07:25:23.237: INFO: stdout: ""
  Apr 26 07:25:23.237: INFO: update-demo-nautilus-hzxq4 is created but not running
  E0426 07:25:24.000521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:25.000771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:26.001265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:27.001567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:28.001902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:28.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 26 07:25:28.305: INFO: stderr: ""
  Apr 26 07:25:28.305: INFO: stdout: "update-demo-nautilus-89x82 update-demo-nautilus-hzxq4 "
  Apr 26 07:25:28.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-89x82 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 26 07:25:28.369: INFO: stderr: ""
  Apr 26 07:25:28.369: INFO: stdout: "true"
  Apr 26 07:25:28.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-89x82 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 26 07:25:28.433: INFO: stderr: ""
  Apr 26 07:25:28.433: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 26 07:25:28.433: INFO: validating pod update-demo-nautilus-89x82
  Apr 26 07:25:28.436: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 26 07:25:28.436: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 26 07:25:28.436: INFO: update-demo-nautilus-89x82 is verified up and running
  Apr 26 07:25:28.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-hzxq4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 26 07:25:28.501: INFO: stderr: ""
  Apr 26 07:25:28.501: INFO: stdout: "true"
  Apr 26 07:25:28.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods update-demo-nautilus-hzxq4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 26 07:25:28.564: INFO: stderr: ""
  Apr 26 07:25:28.564: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 26 07:25:28.564: INFO: validating pod update-demo-nautilus-hzxq4
  Apr 26 07:25:28.569: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 26 07:25:28.569: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 26 07:25:28.569: INFO: update-demo-nautilus-hzxq4 is verified up and running
  STEP: using delete to clean up resources @ 04/26/23 07:25:28.569
  Apr 26 07:25:28.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 delete --grace-period=0 --force -f -'
  Apr 26 07:25:28.638: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 26 07:25:28.638: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 26 07:25:28.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get rc,svc -l name=update-demo --no-headers'
  Apr 26 07:25:28.726: INFO: stderr: "No resources found in kubectl-5102 namespace.\n"
  Apr 26 07:25:28.727: INFO: stdout: ""
  Apr 26 07:25:28.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-5102 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 26 07:25:28.822: INFO: stderr: ""
  Apr 26 07:25:28.822: INFO: stdout: ""
  Apr 26 07:25:28.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5102" for this suite. @ 04/26/23 07:25:28.826
• [14.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/26/23 07:25:28.839
  Apr 26 07:25:28.839: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubelet-test @ 04/26/23 07:25:28.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:25:28.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:25:28.867
  Apr 26 07:25:28.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6945" for this suite. @ 04/26/23 07:25:28.933
• [0.105 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/26/23 07:25:28.945
  Apr 26 07:25:28.945: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 07:25:28.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:25:28.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:25:28.978
  STEP: Creating pod liveness-4988acac-b65d-49db-afee-e1d9daf3218b in namespace container-probe-3771 @ 04/26/23 07:25:28.983
  E0426 07:25:29.002766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:30.003217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:31.002: INFO: Started pod liveness-4988acac-b65d-49db-afee-e1d9daf3218b in namespace container-probe-3771
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/26/23 07:25:31.002
  E0426 07:25:31.003470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:31.004: INFO: Initial restart count of pod liveness-4988acac-b65d-49db-afee-e1d9daf3218b is 0
  E0426 07:25:32.004222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:33.004880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:34.005620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:35.005839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:36.006141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:37.006556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:38.007189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:39.008237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:40.008384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:41.008801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:42.008927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:43.009527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:44.010328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:45.011297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:46.011902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:47.012146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:48.012825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:49.013097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:50.013632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:51.013914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:25:51.045: INFO: Restart count of pod container-probe-3771/liveness-4988acac-b65d-49db-afee-e1d9daf3218b is now 1 (20.04106379s elapsed)
  E0426 07:25:52.014744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:53.015278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:54.016141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:55.016196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:56.017236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:57.017564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:58.018446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:25:59.019296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:00.020050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:01.020290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:02.021337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:03.022259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:04.022821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:05.023160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:06.023282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:07.023417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:08.023909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:09.024180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:10.024475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:11.024875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:26:11.098: INFO: Restart count of pod container-probe-3771/liveness-4988acac-b65d-49db-afee-e1d9daf3218b is now 2 (40.094021882s elapsed)
  E0426 07:26:12.025777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:13.026291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:14.027412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:15.027655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:16.027790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:17.028017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:18.028041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:19.028222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:20.028469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:21.029444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:22.029682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:23.030189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:24.031297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:25.031538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:26.031962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:27.032090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:28.032758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:29.032951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:30.033724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:31.034003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:26:31.140: INFO: Restart count of pod container-probe-3771/liveness-4988acac-b65d-49db-afee-e1d9daf3218b is now 3 (1m0.136048188s elapsed)
  E0426 07:26:32.034172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:33.034828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:34.035339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:35.035609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:36.036287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:37.036773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:38.037144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:39.037440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:40.037592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:41.038525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:42.039079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:43.039643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:44.040464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:45.040702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:46.040976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:47.041233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:48.042237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:49.042394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:50.042791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:51.042941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:26:51.243: INFO: Restart count of pod container-probe-3771/liveness-4988acac-b65d-49db-afee-e1d9daf3218b is now 4 (1m20.23913879s elapsed)
  E0426 07:26:52.043804      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:53.044448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:54.045130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:55.045330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:56.045965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:57.046199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:58.046342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:26:59.047472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:00.048536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:01.049540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:02.050258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:03.051243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:04.051829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:05.052019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:06.052542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:07.052848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:08.053446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:09.053712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:10.054229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:11.055313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:12.056363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:13.056554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:14.056965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:15.057104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:16.057316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:17.057383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:18.057532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:19.057792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:20.058807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:21.058946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:22.059077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:23.059521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:24.059982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:25.060209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:26.060914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:27.061180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:28.061273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:29.061885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:30.062123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:31.062342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:32.063253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:33.063620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:34.064553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:35.064820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:36.065258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:37.065443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:38.066300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:39.066542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:40.066672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:41.066905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:42.067027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:43.067650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:44.068055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:45.068346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:46.069232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:47.069460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:48.069941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:49.070244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:50.071283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:51.071894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:52.073007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:53.073890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:54.074872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:55.075137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:56.075355      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:57.075613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:58.076066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:27:59.076321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:00.077187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:01.077370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:02.077567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:03.078474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:04.078819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:05.079879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:05.491: INFO: Restart count of pod container-probe-3771/liveness-4988acac-b65d-49db-afee-e1d9daf3218b is now 5 (2m34.486741261s elapsed)
  Apr 26 07:28:05.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 07:28:05.494
  STEP: Destroying namespace "container-probe-3771" for this suite. @ 04/26/23 07:28:05.512
• [156.576 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/26/23 07:28:05.521
  Apr 26 07:28:05.521: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 07:28:05.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:28:05.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:28:05.549
  STEP: Creating configMap with name configmap-test-upd-8c0dad73-f330-4614-af55-fd10da1a4a34 @ 04/26/23 07:28:05.556
  STEP: Creating the pod @ 04/26/23 07:28:05.563
  E0426 07:28:06.080075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:07.080724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-8c0dad73-f330-4614-af55-fd10da1a4a34 @ 04/26/23 07:28:07.594
  STEP: waiting to observe update in volume @ 04/26/23 07:28:07.6
  E0426 07:28:08.080769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:09.081043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:09.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1951" for this suite. @ 04/26/23 07:28:09.614
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/26/23 07:28:09.622
  Apr 26 07:28:09.622: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:28:09.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:28:09.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:28:09.64
  Apr 26 07:28:09.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5446" for this suite. @ 04/26/23 07:28:09.647
• [0.032 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/26/23 07:28:09.654
  Apr 26 07:28:09.654: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename deployment @ 04/26/23 07:28:09.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:28:09.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:28:09.703
  Apr 26 07:28:09.715: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0426 07:28:10.081757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:11.082475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:12.083208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:13.083779      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:14.084035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:14.719: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/26/23 07:28:14.719
  Apr 26 07:28:14.719: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0426 07:28:15.084962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:16.085186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:16.730: INFO: Creating deployment "test-rollover-deployment"
  Apr 26 07:28:16.742: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0426 07:28:17.085527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:18.086267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:18.749: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 26 07:28:18.765: INFO: Ensure that both replica sets have 1 created replica
  Apr 26 07:28:18.770: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 26 07:28:18.779: INFO: Updating deployment test-rollover-deployment
  Apr 26 07:28:18.779: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0426 07:28:19.086887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:20.087134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:20.793: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Apr 26 07:28:20.798: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 26 07:28:20.803: INFO: all replica sets need to contain the pod-template-hash label
  Apr 26 07:28:20.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:21.087169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:22.087446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:22.827: INFO: all replica sets need to contain the pod-template-hash label
  Apr 26 07:28:22.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:23.087518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:24.087809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:24.811: INFO: all replica sets need to contain the pod-template-hash label
  Apr 26 07:28:24.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:25.088032      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:26.088207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:26.810: INFO: all replica sets need to contain the pod-template-hash label
  Apr 26 07:28:26.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:27.088303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:28.088768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:28.810: INFO: all replica sets need to contain the pod-template-hash label
  Apr 26 07:28:28.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:29.088960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:30.089180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:30.810: INFO: 
  Apr 26 07:28:30.810: INFO: Ensure that both old replica sets have no replicas
  Apr 26 07:28:30.817: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1435  814f7e2d-dcac-40e4-b49a-c312c794bcee 30250 2 2023-04-26 07:28:16 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-26 07:28:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 07:28:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e1f418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-26 07:28:16 +0000 UTC,LastTransitionTime:2023-04-26 07:28:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-04-26 07:28:30 +0000 UTC,LastTransitionTime:2023-04-26 07:28:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 26 07:28:30.821: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-1435  0a40e7c5-71ac-41d4-8528-672a315af1c1 30240 2 2023-04-26 07:28:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 814f7e2d-dcac-40e4-b49a-c312c794bcee 0xc003e1f907 0xc003e1f908}] [] [{kubelite Update apps/v1 2023-04-26 07:28:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"814f7e2d-dcac-40e4-b49a-c312c794bcee\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 07:28:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e1f9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 07:28:30.821: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 26 07:28:30.821: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1435  f3c35789-154e-4e79-9255-c625be795a0d 30249 2 2023-04-26 07:28:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 814f7e2d-dcac-40e4-b49a-c312c794bcee 0xc003e1fa37 0xc003e1fa38}] [] [{e2e.test Update apps/v1 2023-04-26 07:28:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 07:28:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"814f7e2d-dcac-40e4-b49a-c312c794bcee\"}":{}}},"f:spec":{"f:replicas":{}}} } {kubelite Update apps/v1 2023-04-26 07:28:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e1fb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 07:28:30.821: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-1435  f46ea724-51f5-401b-9845-9ec474868229 30203 2 2023-04-26 07:28:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 814f7e2d-dcac-40e4-b49a-c312c794bcee 0xc003e1f7d7 0xc003e1f7d8}] [] [{kubelite Update apps/v1 2023-04-26 07:28:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"814f7e2d-dcac-40e4-b49a-c312c794bcee\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 07:28:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e1f898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 26 07:28:30.824: INFO: Pod "test-rollover-deployment-57777854c9-mfq7m" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-mfq7m test-rollover-deployment-57777854c9- deployment-1435  7e56972f-2e57-448c-916d-c3b92064fe83 30218 0 2023-04-26 07:28:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:586030ed05b3bf758e38a033bf3c36f2bde88e86396cf1557c202666fb49c9f1 cni.projectcalico.org/podIP:10.1.96.7/32 cni.projectcalico.org/podIPs:10.1.96.7/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 0a40e7c5-71ac-41d4-8528-672a315af1c1 0xc003b14097 0xc003b14098}] [] [{kubelite Update v1 2023-04-26 07:28:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a40e7c5-71ac-41d4-8528-672a315af1c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 07:28:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 07:28:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4n5g5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4n5g5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 07:28:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 07:28:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 07:28:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 07:28:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.7,StartTime:2023-04-26 07:28:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 07:28:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e39e686d05900f156e1f8f9df61bf4020f388d8dcb3aa86e0baffe9e4a48a40a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.7,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 26 07:28:30.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1435" for this suite. @ 04/26/23 07:28:30.827
• [21.180 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 04/26/23 07:28:30.834
  Apr 26 07:28:30.834: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename daemonsets @ 04/26/23 07:28:30.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:28:30.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:28:30.855
  STEP: Creating simple DaemonSet "daemon-set" @ 04/26/23 07:28:30.872
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/26/23 07:28:30.879
  Apr 26 07:28:30.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:28:30.885: INFO: Node ip-172-31-3-127 is running 0 daemon pod, expected 1
  E0426 07:28:31.089701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:31.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:28:31.892: INFO: Node ip-172-31-3-127 is running 0 daemon pod, expected 1
  E0426 07:28:32.090338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:32.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 07:28:32.891: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/26/23 07:28:32.893
  STEP: DeleteCollection of the DaemonSets @ 04/26/23 07:28:32.897
  STEP: Verify that ReplicaSets have been deleted @ 04/26/23 07:28:32.905
  Apr 26 07:28:32.914: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30293"},"items":null}

  Apr 26 07:28:32.917: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30293"},"items":[{"metadata":{"name":"daemon-set-gj899","generateName":"daemon-set-","namespace":"daemonsets-8224","uid":"e2b9651a-0e5d-4b5d-95e3-06c8ec24c5e5","resourceVersion":"30289","creationTimestamp":"2023-04-26T07:28:30Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"54f05e3c6be2fd36ac7e38fabd7fce67d28bb8acbfbcf8e294cbba4549b7efde","cni.projectcalico.org/podIP":"10.1.96.6/32","cni.projectcalico.org/podIPs":"10.1.96.6/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"7cfd3e82-618c-4908-83d8-26c76c4fcf72","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-04-26T07:28:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7cfd3e82-618c-4908-83d8-26c76c4fcf72\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-26T07:28:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-04-26T07:28:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-h5mfd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-h5mfd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-3-127","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-3-127"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T07:28:30Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T07:28:32Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T07:28:32Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T07:28:30Z"}],"hostIP":"172.31.3.127","podIP":"10.1.96.6","podIPs":[{"ip":"10.1.96.6"}],"startTime":"2023-04-26T07:28:30Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T07:28:31Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ea10b1c1cb13dab308c5964690778cc83ca0f26c9ec9a3c37fdc836d2424f7bf","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wvscp","generateName":"daemon-set-","namespace":"daemonsets-8224","uid":"8f5e758b-4ff6-4a68-ba4a-fb49ff9147aa","resourceVersion":"30291","creationTimestamp":"2023-04-26T07:28:30Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1e392b43cdea18ff8c7955f045cfced9eaa19d61165e25b0adea90f1422bcef6","cni.projectcalico.org/podIP":"10.1.93.253/32","cni.projectcalico.org/podIPs":"10.1.93.253/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"7cfd3e82-618c-4908-83d8-26c76c4fcf72","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-04-26T07:28:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7cfd3e82-618c-4908-83d8-26c76c4fcf72\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-26T07:28:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-04-26T07:28:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.93.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bbwbr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bbwbr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-1-91","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-1-91"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T07:28:30Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T07:28:32Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T07:28:32Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-26T07:28:30Z"}],"hostIP":"172.31.1.91","podIP":"10.1.93.253","podIPs":[{"ip":"10.1.93.253"}],"startTime":"2023-04-26T07:28:30Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-26T07:28:31Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3f18f4654a4536e862b444e51911502b31aaea47d7742900fa3e1e863813cd33","started":true}],"qosClass":"BestEffort"}}]}

  Apr 26 07:28:32.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8224" for this suite. @ 04/26/23 07:28:32.934
• [2.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/26/23 07:28:32.945
  Apr 26 07:28:32.946: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:28:32.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:28:32.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:28:32.967
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-5367 @ 04/26/23 07:28:32.974
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/26/23 07:28:33.002
  STEP: creating service externalsvc in namespace services-5367 @ 04/26/23 07:28:33.002
  STEP: creating replication controller externalsvc in namespace services-5367 @ 04/26/23 07:28:33.023
  I0426 07:28:33.033032      22 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5367, replica count: 2
  E0426 07:28:33.091441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:34.092150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:35.092326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0426 07:28:36.084340      22 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/26/23 07:28:36.089
  E0426 07:28:36.092767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:36.128: INFO: Creating new exec pod
  E0426 07:28:37.093293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:38.094108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:38.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-5367 exec execpodx7zw7 -- /bin/sh -x -c nslookup nodeport-service.services-5367.svc.cluster.local'
  Apr 26 07:28:38.296: INFO: stderr: "+ nslookup nodeport-service.services-5367.svc.cluster.local\n"
  Apr 26 07:28:38.296: INFO: stdout: "Server:\t\t10.152.183.10\nAddress:\t10.152.183.10#53\n\nnodeport-service.services-5367.svc.cluster.local\tcanonical name = externalsvc.services-5367.svc.cluster.local.\nName:\texternalsvc.services-5367.svc.cluster.local\nAddress: 10.152.183.80\n\n"
  Apr 26 07:28:38.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5367, will wait for the garbage collector to delete the pods @ 04/26/23 07:28:38.3
  Apr 26 07:28:38.362: INFO: Deleting ReplicationController externalsvc took: 8.559633ms
  Apr 26 07:28:38.463: INFO: Terminating ReplicationController externalsvc pods took: 101.015248ms
  E0426 07:28:39.094544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:40.095050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:40.987: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-5367" for this suite. @ 04/26/23 07:28:41.001
• [8.064 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/26/23 07:28:41.01
  Apr 26 07:28:41.010: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:28:41.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:28:41.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:28:41.029
  STEP: Starting the proxy @ 04/26/23 07:28:41.032
  Apr 26 07:28:41.032: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2952 proxy --unix-socket=/tmp/kubectl-proxy-unix1204653363/test'
  STEP: retrieving proxy /api/ output @ 04/26/23 07:28:41.082
  Apr 26 07:28:41.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2952" for this suite. @ 04/26/23 07:28:41.086
• [0.083 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 04/26/23 07:28:41.093
  Apr 26 07:28:41.093: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename aggregator @ 04/26/23 07:28:41.094
  E0426 07:28:41.095997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:28:41.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:28:41.113
  Apr 26 07:28:41.116: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Registering the sample API server. @ 04/26/23 07:28:41.116
  Apr 26 07:28:41.982: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 26 07:28:42.012: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  E0426 07:28:42.096790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:43.097549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:44.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:44.097781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:45.098065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:46.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:46.098119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:47.098376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:48.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:48.099264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:49.099527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:50.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:50.100243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:51.100503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:52.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:52.100849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:53.101624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:54.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:54.102196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:55.102368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:56.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:56.102813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:57.102998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:28:58.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:28:58.103463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:28:59.103690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:00.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:29:00.103783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:01.103999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:02.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:29:02.104620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:03.105292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:04.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 26, 7, 28, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0426 07:29:04.106064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:05.106138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:06.107104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:06.178: INFO: Waited 110.209264ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/26/23 07:29:06.216
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/26/23 07:29:06.219
  STEP: List APIServices @ 04/26/23 07:29:06.226
  Apr 26 07:29:06.231: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/26/23 07:29:06.231
  Apr 26 07:29:06.250: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/26/23 07:29:06.25
  Apr 26 07:29:06.261: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.April, 26, 7, 29, 6, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/26/23 07:29:06.262
  Apr 26 07:29:06.264: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-04-26 07:29:06 +0000 UTC Passed all checks passed}
  Apr 26 07:29:06.264: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 26 07:29:06.264: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/26/23 07:29:06.264
  Apr 26 07:29:06.275: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1369018177" @ 04/26/23 07:29:06.275
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/26/23 07:29:06.29
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/26/23 07:29:06.296
  STEP: Patch APIService Status @ 04/26/23 07:29:06.299
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/26/23 07:29:06.306
  Apr 26 07:29:06.309: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-04-26 07:29:06 +0000 UTC Passed all checks passed}
  Apr 26 07:29:06.309: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 26 07:29:06.309: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 26 07:29:06.309: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/26/23 07:29:06.309
  STEP: Confirm that the generated APIService has been deleted @ 04/26/23 07:29:06.313
  Apr 26 07:29:06.313: INFO: Requesting list of APIServices to confirm quantity
  Apr 26 07:29:06.317: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 26 07:29:06.317: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 26 07:29:06.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-5814" for this suite. @ 04/26/23 07:29:06.469
• [25.389 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/26/23 07:29:06.483
  Apr 26 07:29:06.483: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/26/23 07:29:06.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:29:06.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:29:06.551
  STEP: create the container to handle the HTTPGet hook request. @ 04/26/23 07:29:06.628
  E0426 07:29:07.107848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:08.108275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:09.108607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:10.108876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/26/23 07:29:10.706
  E0426 07:29:11.109542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:12.109760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/26/23 07:29:12.722
  STEP: delete the pod with lifecycle hook @ 04/26/23 07:29:12.727
  E0426 07:29:13.110815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:14.111780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:15.112041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:16.112281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:16.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9871" for this suite. @ 04/26/23 07:29:16.749
• [10.274 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/26/23 07:29:16.757
  Apr 26 07:29:16.757: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 07:29:16.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:29:16.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:29:16.776
  STEP: Creating a pod to test downward api env vars @ 04/26/23 07:29:16.779
  E0426 07:29:17.113120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:18.113527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:19.114271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:20.114432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:29:20.799
  Apr 26 07:29:20.802: INFO: Trying to get logs from node ip-172-31-3-127 pod downward-api-cc7e4c93-8b1b-4da7-9b0f-2d6062c7ba06 container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 07:29:20.807
  Apr 26 07:29:20.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5194" for this suite. @ 04/26/23 07:29:20.826
• [4.076 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/26/23 07:29:20.834
  Apr 26 07:29:20.834: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 07:29:20.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:29:20.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:29:20.851
  STEP: Create set of pods @ 04/26/23 07:29:20.853
  Apr 26 07:29:20.862: INFO: created test-pod-1
  Apr 26 07:29:20.869: INFO: created test-pod-2
  Apr 26 07:29:20.876: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/26/23 07:29:20.876
  E0426 07:29:21.114628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:22.115227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 04/26/23 07:29:22.92
  Apr 26 07:29:22.922: INFO: Pod quantity 3 is different from expected quantity 0
  E0426 07:29:23.116109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:23.926: INFO: Pod quantity 3 is different from expected quantity 0
  E0426 07:29:24.116951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:24.926: INFO: Pod quantity 3 is different from expected quantity 0
  E0426 07:29:25.117502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:25.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9548" for this suite. @ 04/26/23 07:29:25.929
• [5.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/26/23 07:29:25.937
  Apr 26 07:29:25.937: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:29:25.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:29:25.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:29:25.955
  STEP: Setting up server cert @ 04/26/23 07:29:25.988
  E0426 07:29:26.118131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:29:26.442
  STEP: Deploying the webhook pod @ 04/26/23 07:29:26.451
  STEP: Wait for the deployment to be ready @ 04/26/23 07:29:26.464
  Apr 26 07:29:26.470: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:29:27.119062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:28.119594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:29:28.479
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:29:28.493
  E0426 07:29:29.120152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:29.493: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/26/23 07:29:29.496
  STEP: create a pod that should be denied by the webhook @ 04/26/23 07:29:29.512
  STEP: create a pod that causes the webhook to hang @ 04/26/23 07:29:29.521
  E0426 07:29:30.120286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:31.120747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:32.121781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:33.122199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:34.123391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:35.123648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:36.123880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:37.124296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:38.124643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:39.125010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 04/26/23 07:29:39.528
  STEP: create a configmap that should be admitted by the webhook @ 04/26/23 07:29:39.535
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/26/23 07:29:39.545
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/26/23 07:29:39.551
  STEP: create a namespace that bypass the webhook @ 04/26/23 07:29:39.555
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/26/23 07:29:39.571
  Apr 26 07:29:39.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4936" for this suite. @ 04/26/23 07:29:39.672
  STEP: Destroying namespace "webhook-markers-3909" for this suite. @ 04/26/23 07:29:39.688
  STEP: Destroying namespace "exempted-namespace-9027" for this suite. @ 04/26/23 07:29:39.696
• [13.769 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/26/23 07:29:39.71
  Apr 26 07:29:39.710: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:29:39.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:29:39.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:29:39.737
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 07:29:39.74
  E0426 07:29:40.125963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:41.127115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:42.127896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:43.128804      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:29:43.761
  Apr 26 07:29:43.764: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-0c2227c8-9ca3-4e3f-a6b5-e07198e081b7 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 07:29:43.769
  Apr 26 07:29:43.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5114" for this suite. @ 04/26/23 07:29:43.788
• [4.087 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/26/23 07:29:43.797
  Apr 26 07:29:43.797: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:29:43.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:29:43.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:29:43.816
  STEP: validating cluster-info @ 04/26/23 07:29:43.818
  Apr 26 07:29:43.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-1819 cluster-info'
  Apr 26 07:29:43.885: INFO: stderr: ""
  Apr 26 07:29:43.885: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 26 07:29:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1819" for this suite. @ 04/26/23 07:29:43.888
• [0.099 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 04/26/23 07:29:43.896
  Apr 26 07:29:43.896: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename statefulset @ 04/26/23 07:29:43.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:29:43.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:29:43.915
  STEP: Creating service test in namespace statefulset-5506 @ 04/26/23 07:29:43.918
  STEP: Creating statefulset ss in namespace statefulset-5506 @ 04/26/23 07:29:43.931
  Apr 26 07:29:43.949: INFO: Found 0 stateful pods, waiting for 1
  E0426 07:29:44.128897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:45.129150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:46.130039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:47.130190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:48.131276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:49.131514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:50.131694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:51.131919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:52.132207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:53.132837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:29:53.953: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/26/23 07:29:53.958
  STEP: updating a scale subresource @ 04/26/23 07:29:53.96
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/26/23 07:29:53.967
  STEP: Patch a scale subresource @ 04/26/23 07:29:53.969
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/26/23 07:29:53.98
  Apr 26 07:29:53.982: INFO: Deleting all statefulset in ns statefulset-5506
  Apr 26 07:29:53.985: INFO: Scaling statefulset ss to 0
  E0426 07:29:54.133878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:55.134149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:56.134308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:57.135257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:58.135730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:29:59.135986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:00.136303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:01.136508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:02.136773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:03.137739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:04.005: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 07:30:04.007: INFO: Deleting statefulset ss
  Apr 26 07:30:04.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5506" for this suite. @ 04/26/23 07:30:04.025
• [20.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/26/23 07:30:04.034
  Apr 26 07:30:04.034: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename conformance-tests @ 04/26/23 07:30:04.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:30:04.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:30:04.057
  STEP: Getting node addresses @ 04/26/23 07:30:04.059
  Apr 26 07:30:04.059: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 26 07:30:04.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-6287" for this suite. @ 04/26/23 07:30:04.067
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/26/23 07:30:04.081
  Apr 26 07:30:04.081: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename deployment @ 04/26/23 07:30:04.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:30:04.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:30:04.1
  STEP: creating a Deployment @ 04/26/23 07:30:04.106
  STEP: waiting for Deployment to be created @ 04/26/23 07:30:04.114
  STEP: waiting for all Replicas to be Ready @ 04/26/23 07:30:04.115
  Apr 26 07:30:04.116: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 26 07:30:04.116: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 26 07:30:04.131: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 26 07:30:04.131: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0426 07:30:04.138236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:04.142: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 26 07:30:04.142: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 26 07:30:04.247: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 26 07:30:04.247: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0426 07:30:05.139400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:05.492: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 26 07:30:05.492: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 26 07:30:05.751: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/26/23 07:30:05.751
  W0426 07:30:05.759676      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 26 07:30:05.761: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/26/23 07:30:05.761
  Apr 26 07:30:05.762: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0
  Apr 26 07:30:05.762: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 0
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:05.763: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:05.779: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:05.779: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:05.839: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:05.839: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:05.864: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:05.864: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:05.880: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:05.880: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  E0426 07:30:06.139444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:07.140286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:07.527: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:07.527: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:07.606: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  STEP: listing Deployments @ 04/26/23 07:30:07.606
  Apr 26 07:30:07.611: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/26/23 07:30:07.611
  Apr 26 07:30:07.625: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/26/23 07:30:07.625
  Apr 26 07:30:07.631: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 26 07:30:07.645: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 26 07:30:07.682: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 26 07:30:07.721: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 26 07:30:07.742: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 26 07:30:07.754: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0426 07:30:08.140956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:08.551: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 26 07:30:08.637: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 26 07:30:08.657: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0426 07:30:09.141447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:10.141695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:10.280: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/26/23 07:30:10.387
  STEP: fetching the DeploymentStatus @ 04/26/23 07:30:10.392
  Apr 26 07:30:10.400: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:10.400: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:10.400: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:10.400: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:10.400: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:10.400: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 1
  Apr 26 07:30:10.401: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:10.401: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:10.401: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 2
  Apr 26 07:30:10.401: INFO: observed Deployment test-deployment in namespace deployment-9633 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/26/23 07:30:10.401
  Apr 26 07:30:10.411: INFO: observed event type MODIFIED
  Apr 26 07:30:10.411: INFO: observed event type MODIFIED
  Apr 26 07:30:10.411: INFO: observed event type MODIFIED
  Apr 26 07:30:10.411: INFO: observed event type MODIFIED
  Apr 26 07:30:10.411: INFO: observed event type MODIFIED
  Apr 26 07:30:10.412: INFO: observed event type MODIFIED
  Apr 26 07:30:10.412: INFO: observed event type MODIFIED
  Apr 26 07:30:10.412: INFO: observed event type MODIFIED
  Apr 26 07:30:10.412: INFO: observed event type MODIFIED
  Apr 26 07:30:10.412: INFO: observed event type MODIFIED
  Apr 26 07:30:10.412: INFO: observed event type MODIFIED
  Apr 26 07:30:10.426: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 26 07:30:10.431: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-9633  8f4cf309-1f67-459e-95f2-ebb26afe479c 31303 4 2023-04-26 07:30:05 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ffb78314-9ba2-4218-af1d-4c2eeb71a765 0xc002b5ad17 0xc002b5ad18}] [] [{kubelite Update apps/v1 2023-04-26 07:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ffb78314-9ba2-4218-af1d-4c2eeb71a765\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-04-26 07:30:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b5adb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 26 07:30:10.436: INFO: pod: "test-deployment-5b5dcbcd95-k72jf":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-k72jf test-deployment-5b5dcbcd95- deployment-9633  46bb0208-38a6-4830-afe4-37719f433ac8 31297 0 2023-04-26 07:30:05 +0000 UTC 2023-04-26 07:30:11 +0000 UTC 0xc002b5b098 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:dd68a0b4154f3f056fbd7ced1d4ddc02761df7742912a4d4027bd438dd95ef07 cni.projectcalico.org/podIP:10.1.96.26/32 cni.projectcalico.org/podIPs:10.1.96.26/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 8f4cf309-1f67-459e-95f2-ebb26afe479c 0xc002b5b0e7 0xc002b5b0e8}] [] [{kubelite Update v1 2023-04-26 07:30:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f4cf309-1f67-459e-95f2-ebb26afe479c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-26 07:30:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-04-26 07:30:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.96.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vpgcm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vpgcm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-127,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 07:30:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 07:30:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 07:30:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-26 07:30:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.127,PodIP:10.1.96.26,StartTime:2023-04-26 07:30:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-26 07:30:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://79ddcddb20093630ab7883e09760470038da230c036fa9828e88f406b5c0abdc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.96.26,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 26 07:30:10.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9633" for this suite. @ 04/26/23 07:30:10.439
• [6.369 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/26/23 07:30:10.451
  Apr 26 07:30:10.451: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename gc @ 04/26/23 07:30:10.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:30:10.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:30:10.494
  STEP: create the rc1 @ 04/26/23 07:30:10.5
  STEP: create the rc2 @ 04/26/23 07:30:10.507
  E0426 07:30:11.142003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:12.142143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:13.146693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:14.147322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:15.148035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:16.148279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/26/23 07:30:16.551
  E0426 07:30:17.149694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:18.150455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:19.151166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/26/23 07:30:19.644
  STEP: wait for the rc to be deleted @ 04/26/23 07:30:19.667
  E0426 07:30:20.151362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:21.152318      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:22.152637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:23.154213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:24.154269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:24.741: INFO: 72 pods remaining
  Apr 26 07:30:24.741: INFO: 72 pods has nil DeletionTimestamp
  Apr 26 07:30:24.741: INFO: 
  E0426 07:30:25.154432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:26.155288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:27.155318      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:28.155449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:29.155938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/26/23 07:30:29.697
  W0426 07:30:29.703220      22 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 26 07:30:29.703: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 26 07:30:29.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-q5lzf" in namespace "gc-1047"
  Apr 26 07:30:29.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9p7k" in namespace "gc-1047"
  Apr 26 07:30:29.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5cs5" in namespace "gc-1047"
  Apr 26 07:30:29.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsmx6" in namespace "gc-1047"
  Apr 26 07:30:29.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lfxt" in namespace "gc-1047"
  Apr 26 07:30:29.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-p9qcp" in namespace "gc-1047"
  Apr 26 07:30:29.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-t9js5" in namespace "gc-1047"
  Apr 26 07:30:29.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccgws" in namespace "gc-1047"
  Apr 26 07:30:30.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-5d96t" in namespace "gc-1047"
  Apr 26 07:30:30.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-xzmf6" in namespace "gc-1047"
  Apr 26 07:30:30.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-clss8" in namespace "gc-1047"
  Apr 26 07:30:30.086: INFO: Deleting pod "simpletest-rc-to-be-deleted-74bwp" in namespace "gc-1047"
  Apr 26 07:30:30.134: INFO: Deleting pod "simpletest-rc-to-be-deleted-nj4fl" in namespace "gc-1047"
  Apr 26 07:30:30.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw5rv" in namespace "gc-1047"
  E0426 07:30:30.156266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:30.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtcwt" in namespace "gc-1047"
  Apr 26 07:30:30.192: INFO: Deleting pod "simpletest-rc-to-be-deleted-wt2rt" in namespace "gc-1047"
  Apr 26 07:30:30.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-9g49p" in namespace "gc-1047"
  Apr 26 07:30:30.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-wzvdh" in namespace "gc-1047"
  Apr 26 07:30:30.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc95h" in namespace "gc-1047"
  Apr 26 07:30:30.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-zvcqv" in namespace "gc-1047"
  Apr 26 07:30:30.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6p99" in namespace "gc-1047"
  Apr 26 07:30:30.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6xks" in namespace "gc-1047"
  Apr 26 07:30:30.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zmfd" in namespace "gc-1047"
  Apr 26 07:30:30.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6lp4" in namespace "gc-1047"
  Apr 26 07:30:30.758: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qcsc" in namespace "gc-1047"
  Apr 26 07:30:30.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-7c5mj" in namespace "gc-1047"
  Apr 26 07:30:30.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-96dxm" in namespace "gc-1047"
  Apr 26 07:30:30.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5dw9" in namespace "gc-1047"
  Apr 26 07:30:30.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-kh8lp" in namespace "gc-1047"
  Apr 26 07:30:30.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-r8f6d" in namespace "gc-1047"
  Apr 26 07:30:30.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnv9k" in namespace "gc-1047"
  Apr 26 07:30:31.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-r62wp" in namespace "gc-1047"
  Apr 26 07:30:31.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-n55xd" in namespace "gc-1047"
  Apr 26 07:30:31.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w9df" in namespace "gc-1047"
  E0426 07:30:31.157524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:31.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c7bw" in namespace "gc-1047"
  Apr 26 07:30:31.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cgg5" in namespace "gc-1047"
  Apr 26 07:30:31.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-9l95k" in namespace "gc-1047"
  Apr 26 07:30:31.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-vpksk" in namespace "gc-1047"
  Apr 26 07:30:31.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghf74" in namespace "gc-1047"
  Apr 26 07:30:31.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-nmrtx" in namespace "gc-1047"
  Apr 26 07:30:31.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgfcx" in namespace "gc-1047"
  Apr 26 07:30:31.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kh92" in namespace "gc-1047"
  Apr 26 07:30:31.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdhgg" in namespace "gc-1047"
  Apr 26 07:30:31.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-qrfrr" in namespace "gc-1047"
  Apr 26 07:30:31.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-xr5vg" in namespace "gc-1047"
  Apr 26 07:30:31.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-26n84" in namespace "gc-1047"
  Apr 26 07:30:31.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-s8t8s" in namespace "gc-1047"
  Apr 26 07:30:32.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-92jlr" in namespace "gc-1047"
  Apr 26 07:30:32.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-rttch" in namespace "gc-1047"
  Apr 26 07:30:32.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-zrfn2" in namespace "gc-1047"
  E0426 07:30:32.158286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:32.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1047" for this suite. @ 04/26/23 07:30:32.176
• [21.751 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/26/23 07:30:32.222
  Apr 26 07:30:32.222: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:30:32.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:30:32.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:30:32.272
  STEP: starting the proxy server @ 04/26/23 07:30:32.275
  Apr 26 07:30:32.275: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-306 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/26/23 07:30:32.387
  Apr 26 07:30:32.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-306" for this suite. @ 04/26/23 07:30:32.415
• [0.205 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/26/23 07:30:32.427
  Apr 26 07:30:32.427: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:30:32.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:30:32.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:30:32.473
  STEP: Creating configMap with name projected-configmap-test-volume-dc50ccea-f884-4cbe-b1b6-25336324c410 @ 04/26/23 07:30:32.476
  STEP: Creating a pod to test consume configMaps @ 04/26/23 07:30:32.484
  E0426 07:30:33.159055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:34.159124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:35.159350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:36.159459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:30:36.512
  Apr 26 07:30:36.515: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-configmaps-10ea162d-9bb4-4704-9260-741fc3fc4064 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:30:36.52
  Apr 26 07:30:36.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6965" for this suite. @ 04/26/23 07:30:36.547
• [4.126 seconds]
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/26/23 07:30:36.554
  Apr 26 07:30:36.554: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename init-container @ 04/26/23 07:30:36.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:30:36.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:30:36.573
  STEP: creating the pod @ 04/26/23 07:30:36.576
  Apr 26 07:30:36.576: INFO: PodSpec: initContainers in spec.initContainers
  E0426 07:30:37.160461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:38.161494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:39.161732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:39.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-823" for this suite. @ 04/26/23 07:30:39.89
• [3.344 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/26/23 07:30:39.899
  Apr 26 07:30:39.899: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 07:30:39.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:30:39.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:30:39.924
  STEP: Creating pod liveness-3f510b8d-c4d3-4a0a-9229-6f9218b531d6 in namespace container-probe-6949 @ 04/26/23 07:30:39.927
  E0426 07:30:40.161910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:41.162168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:30:41.947: INFO: Started pod liveness-3f510b8d-c4d3-4a0a-9229-6f9218b531d6 in namespace container-probe-6949
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/26/23 07:30:41.947
  Apr 26 07:30:41.949: INFO: Initial restart count of pod liveness-3f510b8d-c4d3-4a0a-9229-6f9218b531d6 is 0
  E0426 07:30:42.163278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:43.163886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:44.164343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:45.164619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:46.165190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:47.165437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:48.166315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:49.166613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:50.167655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:51.167902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:52.168588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:53.168657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:54.169584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:55.170106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:56.170812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:57.170945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:58.171253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:30:59.171471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:00.172502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:01.172593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:01.988: INFO: Restart count of pod container-probe-6949/liveness-3f510b8d-c4d3-4a0a-9229-6f9218b531d6 is now 1 (20.038532152s elapsed)
  Apr 26 07:31:01.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 07:31:01.991
  STEP: Destroying namespace "container-probe-6949" for this suite. @ 04/26/23 07:31:02.011
• [22.122 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/26/23 07:31:02.022
  Apr 26 07:31:02.022: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename dns @ 04/26/23 07:31:02.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:02.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:02.054
  STEP: Creating a test headless service @ 04/26/23 07:31:02.056
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3514 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3514;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3514 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3514;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3514.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3514.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3514.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3514.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3514.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3514.svc;check="$$(dig +notcp +noall +answer +search 180.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.180_udp@PTR;check="$$(dig +tcp +noall +answer +search 180.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.180_tcp@PTR;sleep 1; done
   @ 04/26/23 07:31:02.087
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3514 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3514;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3514 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3514;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3514.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3514.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3514.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3514.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3514.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3514.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3514.svc;check="$$(dig +notcp +noall +answer +search 180.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.180_udp@PTR;check="$$(dig +tcp +noall +answer +search 180.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.180_tcp@PTR;sleep 1; done
   @ 04/26/23 07:31:02.087
  STEP: creating a pod to probe DNS @ 04/26/23 07:31:02.087
  STEP: submitting the pod to kubernetes @ 04/26/23 07:31:02.087
  E0426 07:31:02.173180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:03.173701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/26/23 07:31:04.113
  STEP: looking for the results for each expected name from probers @ 04/26/23 07:31:04.157
  Apr 26 07:31:04.161: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.163: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.166: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.168: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.172: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  E0426 07:31:04.174126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:04.175: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.179: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.181: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.195: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.198: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.201: INFO: Unable to read jessie_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.203: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.206: INFO: Unable to read jessie_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.211: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.214: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a: the server could not find the requested resource (get pods dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a)
  Apr 26 07:31:04.225: INFO: Lookups using dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3514 wheezy_tcp@dns-test-service.dns-3514 wheezy_udp@dns-test-service.dns-3514.svc wheezy_tcp@dns-test-service.dns-3514.svc wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3514 jessie_tcp@dns-test-service.dns-3514 jessie_udp@dns-test-service.dns-3514.svc jessie_tcp@dns-test-service.dns-3514.svc jessie_udp@_http._tcp.dns-test-service.dns-3514.svc jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc]

  E0426 07:31:05.175216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:06.175426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:07.175581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:08.175769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:09.176078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:09.291: INFO: DNS probes using dns-3514/dns-test-bb82db71-c599-4a19-9457-dfc177d8f24a succeeded

  Apr 26 07:31:09.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 07:31:09.295
  STEP: deleting the test service @ 04/26/23 07:31:09.338
  STEP: deleting the test headless service @ 04/26/23 07:31:09.401
  STEP: Destroying namespace "dns-3514" for this suite. @ 04/26/23 07:31:09.444
• [7.447 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/26/23 07:31:09.469
  Apr 26 07:31:09.469: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename ingress @ 04/26/23 07:31:09.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:09.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:09.501
  STEP: getting /apis @ 04/26/23 07:31:09.504
  STEP: getting /apis/networking.k8s.io @ 04/26/23 07:31:09.511
  STEP: getting /apis/networking.k8s.iov1 @ 04/26/23 07:31:09.512
  STEP: creating @ 04/26/23 07:31:09.513
  STEP: getting @ 04/26/23 07:31:09.537
  STEP: listing @ 04/26/23 07:31:09.54
  STEP: watching @ 04/26/23 07:31:09.543
  Apr 26 07:31:09.543: INFO: starting watch
  STEP: cluster-wide listing @ 04/26/23 07:31:09.544
  STEP: cluster-wide watching @ 04/26/23 07:31:09.547
  Apr 26 07:31:09.547: INFO: starting watch
  STEP: patching @ 04/26/23 07:31:09.548
  STEP: updating @ 04/26/23 07:31:09.555
  Apr 26 07:31:09.571: INFO: waiting for watch events with expected annotations
  Apr 26 07:31:09.571: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/26/23 07:31:09.572
  STEP: updating /status @ 04/26/23 07:31:09.608
  STEP: get /status @ 04/26/23 07:31:09.618
  STEP: deleting @ 04/26/23 07:31:09.62
  STEP: deleting a collection @ 04/26/23 07:31:09.633
  Apr 26 07:31:09.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-813" for this suite. @ 04/26/23 07:31:09.654
• [0.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/26/23 07:31:09.673
  Apr 26 07:31:09.673: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename endpointslice @ 04/26/23 07:31:09.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:09.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:09.696
  E0426 07:31:10.176980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:11.177307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:11.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9317" for this suite. @ 04/26/23 07:31:11.757
• [2.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/26/23 07:31:11.765
  Apr 26 07:31:11.765: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 07:31:11.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:11.785
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:11.788
  Apr 26 07:31:11.791: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: creating the pod @ 04/26/23 07:31:11.792
  STEP: submitting the pod to kubernetes @ 04/26/23 07:31:11.792
  E0426 07:31:12.178098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:13.178863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:13.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5625" for this suite. @ 04/26/23 07:31:13.913
• [2.155 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/26/23 07:31:13.921
  Apr 26 07:31:13.921: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 07:31:13.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:13.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:13.942
  STEP: Creating configMap with name configmap-test-volume-20d478e3-53c9-45f9-b884-86e1d6ca4682 @ 04/26/23 07:31:13.944
  STEP: Creating a pod to test consume configMaps @ 04/26/23 07:31:13.95
  E0426 07:31:14.179962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:15.180194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:16.180317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:17.180869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:31:17.973
  Apr 26 07:31:17.976: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-41536614-7965-4ae1-b3ca-9c58765a567b container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:31:17.981
  Apr 26 07:31:17.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7248" for this suite. @ 04/26/23 07:31:18.001
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/26/23 07:31:18.008
  Apr 26 07:31:18.008: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/26/23 07:31:18.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:18.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:18.028
  STEP: create the container to handle the HTTPGet hook request. @ 04/26/23 07:31:18.033
  E0426 07:31:18.181854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:19.182126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/26/23 07:31:20.052
  E0426 07:31:20.182497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:21.182713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/26/23 07:31:22.066
  E0426 07:31:22.183096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:23.183838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:24.184669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:25.184938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/26/23 07:31:26.083
  Apr 26 07:31:26.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9378" for this suite. @ 04/26/23 07:31:26.112
• [8.110 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/26/23 07:31:26.119
  Apr 26 07:31:26.119: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:31:26.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:26.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:26.137
  STEP: Setting up server cert @ 04/26/23 07:31:26.161
  E0426 07:31:26.185722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:31:26.556
  STEP: Deploying the webhook pod @ 04/26/23 07:31:26.568
  STEP: Wait for the deployment to be ready @ 04/26/23 07:31:26.587
  Apr 26 07:31:26.594: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:31:27.186129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:28.186624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:31:28.603
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:31:28.618
  E0426 07:31:29.187421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:29.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/26/23 07:31:29.629
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/26/23 07:31:29.63
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/26/23 07:31:29.63
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/26/23 07:31:29.63
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/26/23 07:31:29.631
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/26/23 07:31:29.631
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/26/23 07:31:29.632
  Apr 26 07:31:29.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5488" for this suite. @ 04/26/23 07:31:29.688
  STEP: Destroying namespace "webhook-markers-7401" for this suite. @ 04/26/23 07:31:29.702
• [3.592 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/26/23 07:31:29.713
  Apr 26 07:31:29.713: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename svcaccounts @ 04/26/23 07:31:29.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:29.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:29.735
  STEP: Creating ServiceAccount "e2e-sa-dw88c"  @ 04/26/23 07:31:29.738
  Apr 26 07:31:29.745: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-dw88c"  @ 04/26/23 07:31:29.745
  Apr 26 07:31:29.754: INFO: AutomountServiceAccountToken: true
  Apr 26 07:31:29.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2866" for this suite. @ 04/26/23 07:31:29.757
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/26/23 07:31:29.766
  Apr 26 07:31:29.766: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename gc @ 04/26/23 07:31:29.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:29.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:29.784
  STEP: create the deployment @ 04/26/23 07:31:29.786
  W0426 07:31:29.793030      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/26/23 07:31:29.793
  E0426 07:31:30.187700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 04/26/23 07:31:30.302
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/26/23 07:31:30.31
  STEP: Gathering metrics @ 04/26/23 07:31:30.827
  W0426 07:31:30.831412      22 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 26 07:31:30.831: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 26 07:31:30.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9377" for this suite. @ 04/26/23 07:31:30.834
• [1.075 seconds]
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/26/23 07:31:30.841
  Apr 26 07:31:30.841: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename pods @ 04/26/23 07:31:30.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:30.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:30.861
  STEP: creating the pod @ 04/26/23 07:31:30.863
  STEP: submitting the pod to kubernetes @ 04/26/23 07:31:30.864
  W0426 07:31:30.873088      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0426 07:31:31.187838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:32.188221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/26/23 07:31:32.881
  STEP: updating the pod @ 04/26/23 07:31:32.883
  E0426 07:31:33.189175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:33.397: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9470353a-0caf-4bf0-8f8f-d21e0fa37d3a"
  E0426 07:31:34.190050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:35.190186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:36.191216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:37.191458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:37.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9675" for this suite. @ 04/26/23 07:31:37.411
• [6.578 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/26/23 07:31:37.42
  Apr 26 07:31:37.420: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/26/23 07:31:37.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:37.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:37.443
  STEP: set up a multi version CRD @ 04/26/23 07:31:37.445
  Apr 26 07:31:37.446: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:31:38.192348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:39.193003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:40.194137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:41.195270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 04/26/23 07:31:41.229
  STEP: check the unserved version gets removed @ 04/26/23 07:31:41.249
  E0426 07:31:42.195974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/26/23 07:31:42.628
  E0426 07:31:43.196405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:44.197071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:45.197954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:45.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4389" for this suite. @ 04/26/23 07:31:45.568
• [8.157 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/26/23 07:31:45.577
  Apr 26 07:31:45.577: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:31:45.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:45.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:45.596
  STEP: creating Agnhost RC @ 04/26/23 07:31:45.599
  Apr 26 07:31:45.599: INFO: namespace kubectl-2721
  Apr 26 07:31:45.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2721 create -f -'
  Apr 26 07:31:45.910: INFO: stderr: ""
  Apr 26 07:31:45.910: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/26/23 07:31:45.911
  E0426 07:31:46.198499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:46.914: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 07:31:46.914: INFO: Found 0 / 1
  E0426 07:31:47.199148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:47.914: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 07:31:47.914: INFO: Found 1 / 1
  Apr 26 07:31:47.914: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 26 07:31:47.928: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 26 07:31:47.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 26 07:31:47.928: INFO: wait on agnhost-primary startup in kubectl-2721 
  Apr 26 07:31:47.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2721 logs agnhost-primary-hcwwq agnhost-primary'
  Apr 26 07:31:47.998: INFO: stderr: ""
  Apr 26 07:31:47.998: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/26/23 07:31:47.998
  Apr 26 07:31:47.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2721 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 26 07:31:48.075: INFO: stderr: ""
  Apr 26 07:31:48.075: INFO: stdout: "service/rm2 exposed\n"
  Apr 26 07:31:48.078: INFO: Service rm2 in namespace kubectl-2721 found.
  E0426 07:31:48.199491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:49.200482      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 04/26/23 07:31:50.084
  Apr 26 07:31:50.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-2721 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Apr 26 07:31:50.171: INFO: stderr: ""
  Apr 26 07:31:50.171: INFO: stdout: "service/rm3 exposed\n"
  Apr 26 07:31:50.174: INFO: Service rm3 in namespace kubectl-2721 found.
  E0426 07:31:50.201306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:51.202245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:31:52.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2721" for this suite. @ 04/26/23 07:31:52.184
• [6.613 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/26/23 07:31:52.191
  Apr 26 07:31:52.191: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 07:31:52.192
  E0426 07:31:52.202516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:52.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:52.216
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/26/23 07:31:52.218
  E0426 07:31:53.203217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:54.203446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:55.204268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:56.204648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:31:56.246
  Apr 26 07:31:56.251: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-dcdcac0e-e403-43d8-94d2-187559ad8de5 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 07:31:56.258
  Apr 26 07:31:56.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-411" for this suite. @ 04/26/23 07:31:56.282
• [4.098 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/26/23 07:31:56.29
  Apr 26 07:31:56.290: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename job @ 04/26/23 07:31:56.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:31:56.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:31:56.351
  STEP: Creating a suspended job @ 04/26/23 07:31:56.356
  STEP: Patching the Job @ 04/26/23 07:31:56.363
  STEP: Watching for Job to be patched @ 04/26/23 07:31:56.374
  Apr 26 07:31:56.375: INFO: Event ADDED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 26 07:31:56.375: INFO: Event MODIFIED found for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/26/23 07:31:56.375
  STEP: Watching for Job to be updated @ 04/26/23 07:31:56.386
  Apr 26 07:31:56.387: INFO: Event MODIFIED found for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:31:56.388: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/26/23 07:31:56.388
  Apr 26 07:31:56.390: INFO: Job: e2e-cmmg2 as labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2]
  STEP: Waiting for job to complete @ 04/26/23 07:31:56.39
  E0426 07:31:57.204820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:58.205306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:31:59.205675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:00.205885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:01.206142      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:02.206372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:03.207244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:04.207403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 04/26/23 07:32:04.393
  STEP: Watching for Job to be deleted @ 04/26/23 07:32:04.403
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event MODIFIED observed for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 26 07:32:04.405: INFO: Event DELETED found for Job e2e-cmmg2 in namespace job-3340 with labels: map[e2e-cmmg2:patched e2e-job-label:e2e-cmmg2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/26/23 07:32:04.405
  Apr 26 07:32:04.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3340" for this suite. @ 04/26/23 07:32:04.412
• [8.139 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/26/23 07:32:04.429
  Apr 26 07:32:04.429: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replication-controller @ 04/26/23 07:32:04.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:32:04.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:32:04.47
  STEP: Creating replication controller my-hostname-basic-0dadc1d6-0751-4b40-a0a9-e46492bb47b2 @ 04/26/23 07:32:04.472
  Apr 26 07:32:04.485: INFO: Pod name my-hostname-basic-0dadc1d6-0751-4b40-a0a9-e46492bb47b2: Found 0 pods out of 1
  E0426 07:32:05.207809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:06.208317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:07.208464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:08.208942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:09.209158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:32:09.488: INFO: Pod name my-hostname-basic-0dadc1d6-0751-4b40-a0a9-e46492bb47b2: Found 1 pods out of 1
  Apr 26 07:32:09.489: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0dadc1d6-0751-4b40-a0a9-e46492bb47b2" are running
  Apr 26 07:32:09.492: INFO: Pod "my-hostname-basic-0dadc1d6-0751-4b40-a0a9-e46492bb47b2-pvf9j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 07:32:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 07:32:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 07:32:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-26 07:32:04 +0000 UTC Reason: Message:}])
  Apr 26 07:32:09.492: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/26/23 07:32:09.492
  Apr 26 07:32:09.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7202" for this suite. @ 04/26/23 07:32:09.504
• [5.086 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/26/23 07:32:09.516
  Apr 26 07:32:09.516: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubelet-test @ 04/26/23 07:32:09.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:32:09.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:32:09.548
  E0426 07:32:10.209683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:11.209851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:32:11.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8623" for this suite. @ 04/26/23 07:32:11.589
• [2.084 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/26/23 07:32:11.6
  Apr 26 07:32:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 07:32:11.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:32:11.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:32:11.623
  STEP: Creating configMap with name configmap-test-volume-8146fb41-6d12-46a0-82ae-9f27fe3a62e5 @ 04/26/23 07:32:11.625
  STEP: Creating a pod to test consume configMaps @ 04/26/23 07:32:11.633
  E0426 07:32:12.209943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:13.210886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:14.211267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:15.211472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:32:15.654
  Apr 26 07:32:15.657: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-04b5bba0-c6ff-4652-b837-85db7d37d75f container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:32:15.663
  Apr 26 07:32:15.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1514" for this suite. @ 04/26/23 07:32:15.718
• [4.127 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/26/23 07:32:15.727
  Apr 26 07:32:15.727: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename init-container @ 04/26/23 07:32:15.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:32:15.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:32:15.756
  STEP: creating the pod @ 04/26/23 07:32:15.758
  Apr 26 07:32:15.758: INFO: PodSpec: initContainers in spec.initContainers
  E0426 07:32:16.211936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:17.212554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:18.212798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:19.213696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:20.214150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:32:20.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5796" for this suite. @ 04/26/23 07:32:20.659
• [4.946 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 04/26/23 07:32:20.673
  Apr 26 07:32:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename field-validation @ 04/26/23 07:32:20.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:32:20.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:32:20.705
  Apr 26 07:32:20.707: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  W0426 07:32:20.708380      22 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc005a6cb50 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0426 07:32:21.215252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:22.215784      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:23.216803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0426 07:32:23.235458      22 warnings.go:70] unknown field "alpha"
  W0426 07:32:23.235480      22 warnings.go:70] unknown field "beta"
  W0426 07:32:23.235486      22 warnings.go:70] unknown field "delta"
  W0426 07:32:23.235493      22 warnings.go:70] unknown field "epsilon"
  W0426 07:32:23.235499      22 warnings.go:70] unknown field "gamma"
  Apr 26 07:32:23.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4542" for this suite. @ 04/26/23 07:32:23.282
• [2.615 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 04/26/23 07:32:23.289
  Apr 26 07:32:23.289: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename endpointslice @ 04/26/23 07:32:23.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:32:23.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:32:23.306
  E0426 07:32:24.216998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:25.217216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:26.217440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:27.218020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:28.218153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 04/26/23 07:32:28.426
  E0426 07:32:29.218182      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:30.219273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:31.219590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:32.219876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:33.220421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 04/26/23 07:32:33.432
  E0426 07:32:34.221175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:35.221420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:36.221549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:37.221764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:38.222122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/26/23 07:32:38.439
  E0426 07:32:39.222750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:40.223009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:41.223268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:42.223483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:43.223694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 04/26/23 07:32:43.446
  Apr 26 07:32:43.475: INFO: EndpointSlice for Service endpointslice-2452/example-named-port not found
  E0426 07:32:44.223760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:45.224025      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:46.224207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:47.224449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:48.224842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:49.225027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:50.225373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:51.225657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:52.225873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:53.226138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:32:53.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2452" for this suite. @ 04/26/23 07:32:53.487
• [30.206 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/26/23 07:32:53.495
  Apr 26 07:32:53.495: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename subpath @ 04/26/23 07:32:53.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:32:53.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:32:53.517
  STEP: Setting up data @ 04/26/23 07:32:53.519
  STEP: Creating pod pod-subpath-test-configmap-md65 @ 04/26/23 07:32:53.532
  STEP: Creating a pod to test atomic-volume-subpath @ 04/26/23 07:32:53.532
  E0426 07:32:54.226886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:55.227186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:56.227834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:57.228048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:58.228438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:32:59.228687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:00.229601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:01.229841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:02.230153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:03.230193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:04.230823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:05.231255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:06.232297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:07.232642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:08.233248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:09.233510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:10.234632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:11.235228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:12.236105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:13.236747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:14.236879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:15.237058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:16.238122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:17.238314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:33:17.59
  Apr 26 07:33:17.593: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-subpath-test-configmap-md65 container test-container-subpath-configmap-md65: <nil>
  STEP: delete the pod @ 04/26/23 07:33:17.598
  STEP: Deleting pod pod-subpath-test-configmap-md65 @ 04/26/23 07:33:17.613
  Apr 26 07:33:17.613: INFO: Deleting pod "pod-subpath-test-configmap-md65" in namespace "subpath-4913"
  Apr 26 07:33:17.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4913" for this suite. @ 04/26/23 07:33:17.618
• [24.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/26/23 07:33:17.626
  Apr 26 07:33:17.626: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 07:33:17.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:33:17.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:33:17.65
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/26/23 07:33:17.652
  E0426 07:33:18.239416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:19.239649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:20.240533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:21.240749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:33:21.671
  Apr 26 07:33:21.674: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-0a500b05-9fce-4a0e-a39c-e64a2a9ef2d8 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 07:33:21.679
  Apr 26 07:33:21.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8081" for this suite. @ 04/26/23 07:33:21.697
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/26/23 07:33:21.705
  Apr 26 07:33:21.705: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename dns @ 04/26/23 07:33:21.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:33:21.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:33:21.725
  STEP: Creating a test headless service @ 04/26/23 07:33:21.727
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9146.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9146.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9146.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9146.svc.cluster.local;sleep 1; done
   @ 04/26/23 07:33:21.733
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9146.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9146.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9146.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9146.svc.cluster.local;sleep 1; done
   @ 04/26/23 07:33:21.733
  STEP: creating a pod to probe DNS @ 04/26/23 07:33:21.733
  STEP: submitting the pod to kubernetes @ 04/26/23 07:33:21.733
  E0426 07:33:22.241623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:23.242067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/26/23 07:33:23.751
  STEP: looking for the results for each expected name from probers @ 04/26/23 07:33:23.754
  Apr 26 07:33:23.757: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local from pod dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665: the server could not find the requested resource (get pods dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665)
  Apr 26 07:33:23.759: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local from pod dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665: the server could not find the requested resource (get pods dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665)
  Apr 26 07:33:23.762: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9146.svc.cluster.local from pod dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665: the server could not find the requested resource (get pods dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665)
  Apr 26 07:33:23.764: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9146.svc.cluster.local from pod dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665: the server could not find the requested resource (get pods dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665)
  Apr 26 07:33:23.767: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local from pod dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665: the server could not find the requested resource (get pods dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665)
  Apr 26 07:33:23.769: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local from pod dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665: the server could not find the requested resource (get pods dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665)
  Apr 26 07:33:23.772: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9146.svc.cluster.local from pod dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665: the server could not find the requested resource (get pods dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665)
  Apr 26 07:33:23.774: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9146.svc.cluster.local from pod dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665: the server could not find the requested resource (get pods dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665)
  Apr 26 07:33:23.774: INFO: Lookups using dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9146.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9146.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9146.svc.cluster.local jessie_udp@dns-test-service-2.dns-9146.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9146.svc.cluster.local]

  E0426 07:33:24.242173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:25.242302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:26.242417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:27.242676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:28.243080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:33:28.797: INFO: DNS probes using dns-9146/dns-test-9d9c387c-eb49-4435-b3bf-d524491b0665 succeeded

  Apr 26 07:33:28.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 07:33:28.8
  STEP: deleting the test headless service @ 04/26/23 07:33:28.819
  STEP: Destroying namespace "dns-9146" for this suite. @ 04/26/23 07:33:28.874
• [7.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/26/23 07:33:28.899
  Apr 26 07:33:28.899: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename events @ 04/26/23 07:33:28.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:33:28.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:33:28.92
  STEP: creating a test event @ 04/26/23 07:33:28.922
  STEP: listing all events in all namespaces @ 04/26/23 07:33:28.929
  STEP: patching the test event @ 04/26/23 07:33:28.938
  STEP: fetching the test event @ 04/26/23 07:33:28.945
  STEP: updating the test event @ 04/26/23 07:33:28.947
  STEP: getting the test event @ 04/26/23 07:33:29.003
  STEP: deleting the test event @ 04/26/23 07:33:29.005
  STEP: listing all events in all namespaces @ 04/26/23 07:33:29.012
  Apr 26 07:33:29.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2509" for this suite. @ 04/26/23 07:33:29.024
• [0.132 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/26/23 07:33:29.032
  Apr 26 07:33:29.032: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename job @ 04/26/23 07:33:29.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:33:29.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:33:29.05
  STEP: Creating a job @ 04/26/23 07:33:29.052
  STEP: Ensuring active pods == parallelism @ 04/26/23 07:33:29.059
  E0426 07:33:29.243331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:30.243516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 04/26/23 07:33:31.064
  STEP: deleting Job.batch foo in namespace job-1010, will wait for the garbage collector to delete the pods @ 04/26/23 07:33:31.064
  Apr 26 07:33:31.127: INFO: Deleting Job.batch foo took: 9.422214ms
  Apr 26 07:33:31.228: INFO: Terminating Job.batch foo pods took: 100.975946ms
  E0426 07:33:31.243767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:32.244579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:33.245442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:34.246117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:35.246866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:36.247505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:37.248298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:38.248870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:39.249402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:40.249992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:41.250465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:42.251129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:43.252182      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:44.252821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:45.253573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:46.253730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:47.254506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:48.255447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:49.255533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:50.256343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:51.256781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:52.257117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:53.258155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:54.258804      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:55.259213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:56.259793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:57.260381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:58.261511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:33:59.262220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:00.262803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:01.262829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:02.263164      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:03.263975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 04/26/23 07:34:03.729
  Apr 26 07:34:03.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1010" for this suite. @ 04/26/23 07:34:03.736
• [34.712 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 04/26/23 07:34:03.744
  Apr 26 07:34:03.744: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename field-validation @ 04/26/23 07:34:03.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:03.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:03.776
  Apr 26 07:34:03.779: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  E0426 07:34:04.264278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:05.264507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:06.265215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0426 07:34:06.305798      22 warnings.go:70] unknown field "alpha"
  W0426 07:34:06.305838      22 warnings.go:70] unknown field "beta"
  W0426 07:34:06.305846      22 warnings.go:70] unknown field "delta"
  W0426 07:34:06.305854      22 warnings.go:70] unknown field "epsilon"
  W0426 07:34:06.305860      22 warnings.go:70] unknown field "gamma"
  Apr 26 07:34:06.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8071" for this suite. @ 04/26/23 07:34:06.331
• [2.594 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/26/23 07:34:06.34
  Apr 26 07:34:06.340: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:34:06.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:06.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:06.358
  STEP: Creating configMap with name projected-configmap-test-volume-map-b78a0752-81d8-4c4c-b781-fc4036f92259 @ 04/26/23 07:34:06.361
  STEP: Creating a pod to test consume configMaps @ 04/26/23 07:34:06.367
  E0426 07:34:07.266295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:08.267241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:09.267389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:10.267691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:34:10.385
  Apr 26 07:34:10.387: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-configmaps-ce465232-67be-4e7b-aaa2-80c6034ef333 container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:34:10.392
  Apr 26 07:34:10.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2060" for this suite. @ 04/26/23 07:34:10.414
• [4.081 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/26/23 07:34:10.422
  Apr 26 07:34:10.422: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:34:10.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:10.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:10.446
  STEP: creating service in namespace services-7874 @ 04/26/23 07:34:10.448
  STEP: creating service affinity-nodeport-transition in namespace services-7874 @ 04/26/23 07:34:10.448
  STEP: creating replication controller affinity-nodeport-transition in namespace services-7874 @ 04/26/23 07:34:10.465
  I0426 07:34:10.474856      22 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-7874, replica count: 3
  E0426 07:34:11.268560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:12.269554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:13.270183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0426 07:34:13.525733      22 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 07:34:13.533: INFO: Creating new exec pod
  E0426 07:34:14.270299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:15.271238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:16.272246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:34:16.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-7874 exec execpod-affinityqslfl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 26 07:34:16.691: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 26 07:34:16.691: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:34:16.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-7874 exec execpod-affinityqslfl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.97 80'
  Apr 26 07:34:16.829: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.97 80\nConnection to 10.152.183.97 80 port [tcp/http] succeeded!\n"
  Apr 26 07:34:16.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:34:16.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-7874 exec execpod-affinityqslfl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.127 30629'
  Apr 26 07:34:16.959: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.3.127 30629\nConnection to 172.31.3.127 30629 port [tcp/*] succeeded!\n"
  Apr 26 07:34:16.959: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:34:16.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-7874 exec execpod-affinityqslfl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.1.91 30629'
  Apr 26 07:34:17.094: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.1.91 30629\nConnection to 172.31.1.91 30629 port [tcp/*] succeeded!\n"
  Apr 26 07:34:17.094: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:34:17.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-7874 exec execpod-affinityqslfl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.127:30629/ ; done'
  E0426 07:34:17.272344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:34:17.308: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n"
  Apr 26 07:34:17.308: INFO: stdout: "\naffinity-nodeport-transition-tptnp\naffinity-nodeport-transition-5ktkw\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-5ktkw\naffinity-nodeport-transition-tptnp\naffinity-nodeport-transition-tptnp\naffinity-nodeport-transition-tptnp\naffinity-nodeport-transition-5ktkw\naffinity-nodeport-transition-tptnp\naffinity-nodeport-transition-5ktkw\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9"
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-tptnp
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-5ktkw
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-5ktkw
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-tptnp
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-tptnp
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-tptnp
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-5ktkw
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-tptnp
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-5ktkw
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.308: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-7874 exec execpod-affinityqslfl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.127:30629/ ; done'
  Apr 26 07:34:17.499: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.127:30629/\n"
  Apr 26 07:34:17.499: INFO: stdout: "\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9\naffinity-nodeport-transition-dbnq9"
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.499: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.500: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.500: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.500: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.500: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.500: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.500: INFO: Received response from host: affinity-nodeport-transition-dbnq9
  Apr 26 07:34:17.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 07:34:17.503: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7874, will wait for the garbage collector to delete the pods @ 04/26/23 07:34:17.517
  Apr 26 07:34:17.586: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.727256ms
  Apr 26 07:34:17.686: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.149121ms
  E0426 07:34:18.273093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:19.273779      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7874" for this suite. @ 04/26/23 07:34:20.066
• [9.651 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/26/23 07:34:20.075
  Apr 26 07:34:20.075: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 07:34:20.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:20.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:20.1
  STEP: Creating configMap that has name configmap-test-emptyKey-c8d2bd23-cea8-4315-94f9-f8a88d6fc9b6 @ 04/26/23 07:34:20.102
  Apr 26 07:34:20.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5436" for this suite. @ 04/26/23 07:34:20.107
• [0.038 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/26/23 07:34:20.114
  Apr 26 07:34:20.114: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename job @ 04/26/23 07:34:20.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:20.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:20.136
  STEP: Creating Indexed job @ 04/26/23 07:34:20.139
  STEP: Ensuring job reaches completions @ 04/26/23 07:34:20.145
  E0426 07:34:20.274666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:21.275102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:22.275609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:23.276422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:24.276940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:25.277291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:26.277704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:27.277945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 04/26/23 07:34:28.149
  Apr 26 07:34:28.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3502" for this suite. @ 04/26/23 07:34:28.156
• [8.050 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/26/23 07:34:28.164
  Apr 26 07:34:28.164: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:34:28.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:28.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:28.184
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 07:34:28.187
  E0426 07:34:28.278681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:29.279604      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:30.279743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:31.279897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:34:32.206
  Apr 26 07:34:32.208: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-39c17836-e59c-4720-bd73-64b99fdc0831 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 07:34:32.213
  Apr 26 07:34:32.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7615" for this suite. @ 04/26/23 07:34:32.234
• [4.080 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/26/23 07:34:32.245
  Apr 26 07:34:32.245: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 07:34:32.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:32.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:32.277
  E0426 07:34:32.280204      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating configMap with name configmap-test-volume-map-0b60e49f-812a-4314-8d1c-44284eb1548d @ 04/26/23 07:34:32.28
  STEP: Creating a pod to test consume configMaps @ 04/26/23 07:34:32.288
  E0426 07:34:33.280471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:34.280723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:35.280931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:36.281167      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:34:36.309
  Apr 26 07:34:36.311: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-configmaps-4b7bb109-5227-4d1f-bb7c-6b197bd5786f container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:34:36.316
  Apr 26 07:34:36.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1058" for this suite. @ 04/26/23 07:34:36.339
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/26/23 07:34:36.347
  Apr 26 07:34:36.347: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 07:34:36.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:36.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:36.368
  STEP: Creating secret with name secret-test-2cda2ce7-8419-4854-9dc8-4ef86931d5a9 @ 04/26/23 07:34:36.371
  STEP: Creating a pod to test consume secrets @ 04/26/23 07:34:36.378
  E0426 07:34:37.282063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:38.282159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:39.283255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:40.283696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:34:40.396
  Apr 26 07:34:40.399: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-secrets-e599009b-61c1-4d9d-b129-82d1b08249b2 container secret-env-test: <nil>
  STEP: delete the pod @ 04/26/23 07:34:40.404
  Apr 26 07:34:40.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5956" for this suite. @ 04/26/23 07:34:40.423
• [4.082 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/26/23 07:34:40.43
  Apr 26 07:34:40.430: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename secrets @ 04/26/23 07:34:40.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:40.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:40.447
  STEP: Creating secret with name secret-test-map-c44581eb-c231-4605-a145-3a453be9316d @ 04/26/23 07:34:40.45
  STEP: Creating a pod to test consume secrets @ 04/26/23 07:34:40.455
  E0426 07:34:41.284104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:42.284484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:43.285575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:44.285622      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:34:44.474
  Apr 26 07:34:44.478: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-secrets-69e57af2-8f89-44b8-8bfa-9e48f3faa60c container secret-volume-test: <nil>
  STEP: delete the pod @ 04/26/23 07:34:44.485
  Apr 26 07:34:44.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3633" for this suite. @ 04/26/23 07:34:44.506
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/26/23 07:34:44.514
  Apr 26 07:34:44.514: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename events @ 04/26/23 07:34:44.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:44.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:44.539
  STEP: Create set of events @ 04/26/23 07:34:44.541
  Apr 26 07:34:44.548: INFO: created test-event-1
  Apr 26 07:34:44.554: INFO: created test-event-2
  Apr 26 07:34:44.561: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/26/23 07:34:44.561
  STEP: delete collection of events @ 04/26/23 07:34:44.563
  Apr 26 07:34:44.563: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/26/23 07:34:44.596
  Apr 26 07:34:44.596: INFO: requesting list of events to confirm quantity
  Apr 26 07:34:44.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1900" for this suite. @ 04/26/23 07:34:44.602
• [0.100 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/26/23 07:34:44.614
  Apr 26 07:34:44.614: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename subjectreview @ 04/26/23 07:34:44.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:44.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:44.642
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-2501" @ 04/26/23 07:34:44.644
  Apr 26 07:34:44.650: INFO: saUsername: "system:serviceaccount:subjectreview-2501:e2e"
  Apr 26 07:34:44.650: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-2501"}
  Apr 26 07:34:44.650: INFO: saUID: "93b5113f-2a70-4c63-a34f-d58a47f8c6e8"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-2501:e2e" @ 04/26/23 07:34:44.65
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-2501:e2e" @ 04/26/23 07:34:44.651
  Apr 26 07:34:44.653: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:true, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-2501:e2e" api 'list' configmaps in "subjectreview-2501" namespace @ 04/26/23 07:34:44.653
  Apr 26 07:34:44.655: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-2501:e2e" @ 04/26/23 07:34:44.655
  Apr 26 07:34:44.657: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:true, Denied:false, Reason:"", EvaluationError:""}
  Apr 26 07:34:44.657: INFO: LocalSubjectAccessReview has been verified
  Apr 26 07:34:44.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-2501" for this suite. @ 04/26/23 07:34:44.66
• [0.053 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/26/23 07:34:44.668
  Apr 26 07:34:44.668: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:34:44.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:44.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:44.687
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 07:34:44.69
  E0426 07:34:45.285894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:46.286169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:47.287235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:48.287749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:34:48.71
  Apr 26 07:34:48.712: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-38f10b24-a013-43d0-9029-8ad9d36b21ef container client-container: <nil>
  STEP: delete the pod @ 04/26/23 07:34:48.717
  Apr 26 07:34:48.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7532" for this suite. @ 04/26/23 07:34:48.736
• [4.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/26/23 07:34:48.744
  Apr 26 07:34:48.744: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 07:34:48.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:48.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:48.763
  STEP: Creating Pod @ 04/26/23 07:34:48.766
  E0426 07:34:49.287922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:50.288140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 04/26/23 07:34:50.782
  Apr 26 07:34:50.782: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-455 PodName:pod-sharedvolume-8b03666c-d3b4-42f2-9b7e-a644f8fcb9b0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 26 07:34:50.782: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  Apr 26 07:34:50.782: INFO: ExecWithOptions: Clientset creation
  Apr 26 07:34:50.782: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-455/pods/pod-sharedvolume-8b03666c-d3b4-42f2-9b7e-a644f8fcb9b0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Apr 26 07:34:50.849: INFO: Exec stderr: ""
  Apr 26 07:34:50.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-455" for this suite. @ 04/26/23 07:34:50.852
• [2.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/26/23 07:34:50.861
  Apr 26 07:34:50.861: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename discovery @ 04/26/23 07:34:50.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:50.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:50.906
  STEP: Setting up server cert @ 04/26/23 07:34:50.91
  Apr 26 07:34:51.224: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 26 07:34:51.225: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 26 07:34:51.225: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 26 07:34:51.225: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 26 07:34:51.225: INFO: Checking APIGroup: apps
  Apr 26 07:34:51.226: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 26 07:34:51.226: INFO: Versions found [{apps/v1 v1}]
  Apr 26 07:34:51.226: INFO: apps/v1 matches apps/v1
  Apr 26 07:34:51.226: INFO: Checking APIGroup: events.k8s.io
  Apr 26 07:34:51.227: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 26 07:34:51.227: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 26 07:34:51.227: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 26 07:34:51.227: INFO: Checking APIGroup: authentication.k8s.io
  Apr 26 07:34:51.228: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 26 07:34:51.228: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 26 07:34:51.228: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 26 07:34:51.228: INFO: Checking APIGroup: authorization.k8s.io
  Apr 26 07:34:51.229: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 26 07:34:51.229: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 26 07:34:51.229: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 26 07:34:51.229: INFO: Checking APIGroup: autoscaling
  Apr 26 07:34:51.230: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 26 07:34:51.230: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 26 07:34:51.230: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 26 07:34:51.230: INFO: Checking APIGroup: batch
  Apr 26 07:34:51.231: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 26 07:34:51.231: INFO: Versions found [{batch/v1 v1}]
  Apr 26 07:34:51.231: INFO: batch/v1 matches batch/v1
  Apr 26 07:34:51.231: INFO: Checking APIGroup: certificates.k8s.io
  Apr 26 07:34:51.232: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 26 07:34:51.232: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 26 07:34:51.232: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 26 07:34:51.232: INFO: Checking APIGroup: networking.k8s.io
  Apr 26 07:34:51.233: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 26 07:34:51.233: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 26 07:34:51.233: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 26 07:34:51.233: INFO: Checking APIGroup: policy
  Apr 26 07:34:51.234: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 26 07:34:51.234: INFO: Versions found [{policy/v1 v1}]
  Apr 26 07:34:51.234: INFO: policy/v1 matches policy/v1
  Apr 26 07:34:51.234: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 26 07:34:51.235: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 26 07:34:51.235: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 26 07:34:51.235: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 26 07:34:51.235: INFO: Checking APIGroup: storage.k8s.io
  Apr 26 07:34:51.236: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 26 07:34:51.236: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 26 07:34:51.236: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 26 07:34:51.236: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 26 07:34:51.237: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 26 07:34:51.237: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 26 07:34:51.237: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 26 07:34:51.237: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 26 07:34:51.238: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 26 07:34:51.238: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 26 07:34:51.238: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 26 07:34:51.238: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 26 07:34:51.239: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 26 07:34:51.239: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 26 07:34:51.239: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 26 07:34:51.239: INFO: Checking APIGroup: coordination.k8s.io
  Apr 26 07:34:51.240: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 26 07:34:51.240: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 26 07:34:51.240: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 26 07:34:51.240: INFO: Checking APIGroup: node.k8s.io
  Apr 26 07:34:51.240: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 26 07:34:51.241: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 26 07:34:51.241: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 26 07:34:51.241: INFO: Checking APIGroup: discovery.k8s.io
  Apr 26 07:34:51.242: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 26 07:34:51.242: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 26 07:34:51.242: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 26 07:34:51.242: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 26 07:34:51.243: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 26 07:34:51.243: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 26 07:34:51.243: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 26 07:34:51.243: INFO: Checking APIGroup: crd.projectcalico.org
  Apr 26 07:34:51.243: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  Apr 26 07:34:51.243: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  Apr 26 07:34:51.243: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  Apr 26 07:34:51.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-1811" for this suite. @ 04/26/23 07:34:51.247
• [0.393 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/26/23 07:34:51.257
  Apr 26 07:34:51.257: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:34:51.258
  E0426 07:34:51.288207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:34:51.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:34:51.3
  STEP: Setting up server cert @ 04/26/23 07:34:51.328
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:34:51.816
  STEP: Deploying the webhook pod @ 04/26/23 07:34:51.826
  STEP: Wait for the deployment to be ready @ 04/26/23 07:34:51.839
  Apr 26 07:34:51.845: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:34:52.288921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:53.289573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:34:53.854
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:34:53.866
  E0426 07:34:54.290244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:34:54.866: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/26/23 07:34:54.869
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/26/23 07:34:54.869
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/26/23 07:34:54.884
  E0426 07:34:55.291280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/26/23 07:34:55.896
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/26/23 07:34:55.896
  E0426 07:34:56.291705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 04/26/23 07:34:56.928
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/26/23 07:34:56.928
  E0426 07:34:57.292368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:58.292579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:34:59.292860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:00.293019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:01.293261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/26/23 07:35:01.964
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/26/23 07:35:01.964
  E0426 07:35:02.293536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:03.294156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:04.294174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:05.294392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:06.295240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:06.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1911" for this suite. @ 04/26/23 07:35:07.054
  STEP: Destroying namespace "webhook-markers-5454" for this suite. @ 04/26/23 07:35:07.074
• [15.827 seconds]
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/26/23 07:35:07.084
  Apr 26 07:35:07.084: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename security-context-test @ 04/26/23 07:35:07.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:07.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:07.116
  E0426 07:35:07.295371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:08.295950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:09.296975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:10.297771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:11.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7470" for this suite. @ 04/26/23 07:35:11.142
• [4.066 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/26/23 07:35:11.151
  Apr 26 07:35:11.151: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:35:11.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:11.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:11.17
  STEP: Creating configMap with name projected-configmap-test-volume-map-e05f0144-f99c-4f2a-94e6-9d96ce0f5854 @ 04/26/23 07:35:11.173
  STEP: Creating a pod to test consume configMaps @ 04/26/23 07:35:11.178
  E0426 07:35:11.298246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:12.299218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:13.299596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:14.299904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:35:15.198
  Apr 26 07:35:15.201: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-projected-configmaps-22f5a3f2-85e3-4859-ae57-fbcffe1d429b container agnhost-container: <nil>
  STEP: delete the pod @ 04/26/23 07:35:15.206
  Apr 26 07:35:15.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3786" for this suite. @ 04/26/23 07:35:15.226
• [4.083 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 04/26/23 07:35:15.234
  Apr 26 07:35:15.234: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename daemonsets @ 04/26/23 07:35:15.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:15.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:15.254
  Apr 26 07:35:15.276: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/26/23 07:35:15.282
  Apr 26 07:35:15.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:15.285: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/26/23 07:35:15.285
  E0426 07:35:15.299978      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:15.312: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:15.312: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 07:35:16.300435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:16.316: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:16.316: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 07:35:17.300630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:17.316: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 07:35:17.316: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/26/23 07:35:17.319
  Apr 26 07:35:17.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 07:35:17.339: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0426 07:35:18.301163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:18.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:18.343: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/26/23 07:35:18.343
  Apr 26 07:35:18.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:18.354: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 07:35:19.302273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:19.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:19.357: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 07:35:20.302451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:20.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:20.357: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 07:35:21.303286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:21.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 07:35:21.357: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/26/23 07:35:21.362
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4074, will wait for the garbage collector to delete the pods @ 04/26/23 07:35:21.362
  Apr 26 07:35:21.435: INFO: Deleting DaemonSet.extensions daemon-set took: 19.88856ms
  Apr 26 07:35:21.536: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.931184ms
  E0426 07:35:22.304124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:23.304969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:24.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:24.240: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 26 07:35:24.243: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36318"},"items":null}

  Apr 26 07:35:24.246: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36318"},"items":null}

  Apr 26 07:35:24.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4074" for this suite. @ 04/26/23 07:35:24.271
• [9.044 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/26/23 07:35:24.279
  Apr 26 07:35:24.279: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-runtime @ 04/26/23 07:35:24.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:24.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:24.304
  E0426 07:35:24.305943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the container @ 04/26/23 07:35:24.307
  W0426 07:35:24.315938      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/26/23 07:35:24.316
  E0426 07:35:25.306324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:26.307251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:27.307473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/26/23 07:35:27.329
  STEP: the container should be terminated @ 04/26/23 07:35:27.331
  STEP: the termination message should be set @ 04/26/23 07:35:27.331
  Apr 26 07:35:27.331: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/26/23 07:35:27.332
  Apr 26 07:35:27.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5920" for this suite. @ 04/26/23 07:35:27.354
• [3.082 seconds]
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/26/23 07:35:27.361
  Apr 26 07:35:27.361: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename runtimeclass @ 04/26/23 07:35:27.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:27.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:27.383
  Apr 26 07:35:27.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7752" for this suite. @ 04/26/23 07:35:27.395
• [0.042 seconds]
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/26/23 07:35:27.403
  Apr 26 07:35:27.403: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename endpointslice @ 04/26/23 07:35:27.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:27.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:27.421
  Apr 26 07:35:27.430: INFO: Endpoints addresses: [172.31.1.91 172.31.3.127] , ports: [16443]
  Apr 26 07:35:27.430: INFO: EndpointSlices addresses: [172.31.1.91 172.31.3.127] , ports: [16443]
  Apr 26 07:35:27.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1976" for this suite. @ 04/26/23 07:35:27.433
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/26/23 07:35:27.44
  Apr 26 07:35:27.440: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename namespaces @ 04/26/23 07:35:27.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:27.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:27.463
  STEP: creating a Namespace @ 04/26/23 07:35:27.465
  STEP: patching the Namespace @ 04/26/23 07:35:27.48
  STEP: get the Namespace and ensuring it has the label @ 04/26/23 07:35:27.486
  Apr 26 07:35:27.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-13" for this suite. @ 04/26/23 07:35:27.491
  STEP: Destroying namespace "nspatchtest-d5bf7398-139d-4a75-90ef-706cbe83162a-7865" for this suite. @ 04/26/23 07:35:27.502
• [0.068 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 04/26/23 07:35:27.51
  Apr 26 07:35:27.510: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename daemonsets @ 04/26/23 07:35:27.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:27.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:27.527
  Apr 26 07:35:27.542: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/26/23 07:35:27.549
  Apr 26 07:35:27.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:27.554: INFO: Node ip-172-31-3-127 is running 0 daemon pod, expected 1
  E0426 07:35:28.307609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:28.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:28.561: INFO: Node ip-172-31-3-127 is running 0 daemon pod, expected 1
  E0426 07:35:29.308533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:29.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 07:35:29.570: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/26/23 07:35:29.58
  STEP: Check that daemon pods images are updated. @ 04/26/23 07:35:29.59
  Apr 26 07:35:29.593: INFO: Wrong image for pod: daemon-set-87sln. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 26 07:35:29.593: INFO: Wrong image for pod: daemon-set-t5dtm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0426 07:35:30.308796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:30.601: INFO: Wrong image for pod: daemon-set-t5dtm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0426 07:35:31.309861      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:31.601: INFO: Wrong image for pod: daemon-set-t5dtm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0426 07:35:32.310137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:32.607: INFO: Wrong image for pod: daemon-set-t5dtm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 26 07:35:32.607: INFO: Pod daemon-set-8rpgn is not available
  E0426 07:35:33.311266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:34.311841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:34.602: INFO: Pod daemon-set-pdhf8 is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/26/23 07:35:34.606
  Apr 26 07:35:34.617: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 26 07:35:34.617: INFO: Node ip-172-31-1-91 is running 0 daemon pod, expected 1
  E0426 07:35:35.312783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:35.624: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 26 07:35:35.624: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/26/23 07:35:35.637
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5912, will wait for the garbage collector to delete the pods @ 04/26/23 07:35:35.637
  Apr 26 07:35:35.697: INFO: Deleting DaemonSet.extensions daemon-set took: 7.716413ms
  Apr 26 07:35:35.798: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.749451ms
  E0426 07:35:36.313801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:37.314226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:38.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 26 07:35:38.203: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 26 07:35:38.206: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36536"},"items":null}

  Apr 26 07:35:38.209: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36536"},"items":null}

  Apr 26 07:35:38.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5912" for this suite. @ 04/26/23 07:35:38.22
• [10.718 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/26/23 07:35:38.231
  Apr 26 07:35:38.231: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replication-controller @ 04/26/23 07:35:38.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:38.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:38.253
  Apr 26 07:35:38.256: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0426 07:35:38.314547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/26/23 07:35:39.267
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/26/23 07:35:39.274
  E0426 07:35:39.314998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/26/23 07:35:40.28
  Apr 26 07:35:40.289: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/26/23 07:35:40.289
  E0426 07:35:40.315468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:41.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8087" for this suite. @ 04/26/23 07:35:41.297
• [3.073 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/26/23 07:35:41.304
  Apr 26 07:35:41.304: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 07:35:41.305
  E0426 07:35:41.315880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:41.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:41.322
  STEP: Creating a pod to test downward API volume plugin @ 04/26/23 07:35:41.325
  E0426 07:35:42.316197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:43.316753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:44.316909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:45.317191      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:35:45.343
  Apr 26 07:35:45.345: INFO: Trying to get logs from node ip-172-31-3-127 pod downwardapi-volume-1cda8470-5465-412c-a9e6-df714cc54371 container client-container: <nil>
  STEP: delete the pod @ 04/26/23 07:35:45.35
  Apr 26 07:35:45.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9663" for this suite. @ 04/26/23 07:35:45.371
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/26/23 07:35:45.379
  Apr 26 07:35:45.379: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 07:35:45.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:45.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:45.398
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/26/23 07:35:45.4
  E0426 07:35:46.317559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:47.317969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:48.318029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:49.318471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:35:49.419
  Apr 26 07:35:49.422: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-2f86c580-26ad-4499-b189-dda226084935 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 07:35:49.427
  Apr 26 07:35:49.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9211" for this suite. @ 04/26/23 07:35:49.446
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 04/26/23 07:35:49.454
  Apr 26 07:35:49.454: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename statefulset @ 04/26/23 07:35:49.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:35:49.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:35:49.476
  STEP: Creating service test in namespace statefulset-8169 @ 04/26/23 07:35:49.479
  STEP: Creating a new StatefulSet @ 04/26/23 07:35:49.485
  Apr 26 07:35:49.495: INFO: Found 0 stateful pods, waiting for 3
  E0426 07:35:50.319298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:51.319686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:52.319888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:53.320651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:54.320869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:55.321165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:56.321302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:57.321544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:58.322054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:35:59.322338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:35:59.500: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:35:59.500: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:35:59.500: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 26 07:35:59.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-8169 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 07:35:59.646: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 07:35:59.646: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 07:35:59.646: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0426 07:36:00.323036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:01.323229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:02.324112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:03.324771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:04.325026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:05.325297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:06.325479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:07.325723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:08.326140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:09.327251      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/26/23 07:36:09.659
  Apr 26 07:36:09.679: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/26/23 07:36:09.679
  E0426 07:36:10.327414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:11.328613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:12.329629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:13.330177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:14.330720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:15.331254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:16.331416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:17.331542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:18.332541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:19.332801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 04/26/23 07:36:19.692
  Apr 26 07:36:19.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-8169 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 07:36:19.860: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 26 07:36:19.860: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 07:36:19.860: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0426 07:36:20.333388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:21.333632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:22.333951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:23.334135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:24.334169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:25.335305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:26.335529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:27.335745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:28.336178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:29.336413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 04/26/23 07:36:29.889
  Apr 26 07:36:29.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-8169 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 26 07:36:30.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 26 07:36:30.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 26 07:36:30.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0426 07:36:30.337285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:31.337726      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:32.337846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:33.338574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:34.338976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:35.339550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:36.339809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:37.340065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:38.340547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:39.340803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:36:40.051: INFO: Updating stateful set ss2
  E0426 07:36:40.341410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:41.341668      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:42.341831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:43.342124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:44.342173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:45.343220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:46.343435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:47.343654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:48.343845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:49.344057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 04/26/23 07:36:50.065
  Apr 26 07:36:50.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=statefulset-8169 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 26 07:36:50.202: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 26 07:36:50.202: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 26 07:36:50.202: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0426 07:36:50.344790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:51.344983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:52.345065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:53.345669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:54.346130      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:55.346239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:56.347276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:57.347438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:58.347579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:36:59.348575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:00.273: INFO: Deleting all statefulset in ns statefulset-8169
  Apr 26 07:37:00.276: INFO: Scaling statefulset ss2 to 0
  E0426 07:37:00.348797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:01.349394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:02.349623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:03.350143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:04.350312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:05.351263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:06.351514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:07.351683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:08.352275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:09.352517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:10.294: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 26 07:37:10.297: INFO: Deleting statefulset ss2
  Apr 26 07:37:10.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8169" for this suite. @ 04/26/23 07:37:10.313
• [80.870 seconds]
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/26/23 07:37:10.324
  Apr 26 07:37:10.324: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename lease-test @ 04/26/23 07:37:10.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:10.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:10.345
  E0426 07:37:10.353337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:10.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-2482" for this suite. @ 04/26/23 07:37:10.436
• [0.119 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/26/23 07:37:10.443
  Apr 26 07:37:10.443: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename replication-controller @ 04/26/23 07:37:10.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:10.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:10.464
  STEP: Creating ReplicationController "e2e-rc-p25rn" @ 04/26/23 07:37:10.467
  Apr 26 07:37:10.474: INFO: Get Replication Controller "e2e-rc-p25rn" to confirm replicas
  E0426 07:37:11.354152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:11.480: INFO: Get Replication Controller "e2e-rc-p25rn" to confirm replicas
  Apr 26 07:37:11.483: INFO: Found 1 replicas for "e2e-rc-p25rn" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-p25rn" @ 04/26/23 07:37:11.483
  STEP: Updating a scale subresource @ 04/26/23 07:37:11.486
  STEP: Verifying replicas where modified for replication controller "e2e-rc-p25rn" @ 04/26/23 07:37:11.493
  Apr 26 07:37:11.493: INFO: Get Replication Controller "e2e-rc-p25rn" to confirm replicas
  E0426 07:37:12.355221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:12.496: INFO: Get Replication Controller "e2e-rc-p25rn" to confirm replicas
  Apr 26 07:37:12.499: INFO: Found 2 replicas for "e2e-rc-p25rn" replication controller
  Apr 26 07:37:12.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6335" for this suite. @ 04/26/23 07:37:12.502
• [2.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/26/23 07:37:12.512
  Apr 26 07:37:12.512: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename projected @ 04/26/23 07:37:12.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:12.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:12.533
  STEP: Creating configMap with name cm-test-opt-del-9f360370-0660-423e-a708-94c76a0c7e06 @ 04/26/23 07:37:12.539
  STEP: Creating configMap with name cm-test-opt-upd-3650e5e1-c16c-4597-ab91-7b27fd7d1117 @ 04/26/23 07:37:12.544
  STEP: Creating the pod @ 04/26/23 07:37:12.549
  E0426 07:37:13.355394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:14.355860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-9f360370-0660-423e-a708-94c76a0c7e06 @ 04/26/23 07:37:14.585
  STEP: Updating configmap cm-test-opt-upd-3650e5e1-c16c-4597-ab91-7b27fd7d1117 @ 04/26/23 07:37:14.593
  STEP: Creating configMap with name cm-test-opt-create-0b5ef509-e03c-486e-a024-ffcefe816a96 @ 04/26/23 07:37:14.599
  STEP: waiting to observe update in volume @ 04/26/23 07:37:14.605
  E0426 07:37:15.356072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:16.356317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:16.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3934" for this suite. @ 04/26/23 07:37:16.629
• [4.123 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/26/23 07:37:16.636
  Apr 26 07:37:16.636: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename services @ 04/26/23 07:37:16.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:16.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:16.657
  STEP: creating service in namespace services-8310 @ 04/26/23 07:37:16.659
  STEP: creating service affinity-clusterip in namespace services-8310 @ 04/26/23 07:37:16.659
  STEP: creating replication controller affinity-clusterip in namespace services-8310 @ 04/26/23 07:37:16.67
  I0426 07:37:16.678939      22 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-8310, replica count: 3
  E0426 07:37:17.356904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:18.357172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:19.357347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0426 07:37:19.729910      22 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 26 07:37:19.735: INFO: Creating new exec pod
  E0426 07:37:20.357362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:21.357545      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:22.358013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:22.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8310 exec execpod-affinitykzspq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Apr 26 07:37:22.897: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 26 07:37:22.897: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:37:22.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8310 exec execpod-affinitykzspq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.205 80'
  Apr 26 07:37:23.039: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.205 80\nConnection to 10.152.183.205 80 port [tcp/http] succeeded!\n"
  Apr 26 07:37:23.039: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 26 07:37:23.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=services-8310 exec execpod-affinitykzspq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.205:80/ ; done'
  Apr 26 07:37:23.221: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.205:80/\n"
  Apr 26 07:37:23.221: INFO: stdout: "\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q\naffinity-clusterip-zwc4q"
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.221: INFO: Received response from host: affinity-clusterip-zwc4q
  Apr 26 07:37:23.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 26 07:37:23.225: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-8310, will wait for the garbage collector to delete the pods @ 04/26/23 07:37:23.243
  Apr 26 07:37:23.306: INFO: Deleting ReplicationController affinity-clusterip took: 8.436411ms
  E0426 07:37:23.358837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:23.407: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.327299ms
  E0426 07:37:24.359220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:25.360219      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8310" for this suite. @ 04/26/23 07:37:26.329
• [9.702 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/26/23 07:37:26.339
  Apr 26 07:37:26.339: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename kubectl @ 04/26/23 07:37:26.34
  E0426 07:37:26.360843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:26.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:26.364
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/26/23 07:37:26.366
  Apr 26 07:37:26.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-1409 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 26 07:37:26.500: INFO: stderr: ""
  Apr 26 07:37:26.500: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/26/23 07:37:26.5
  Apr 26 07:37:26.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-1409 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Apr 26 07:37:26.569: INFO: stderr: ""
  Apr 26 07:37:26.569: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/26/23 07:37:26.569
  Apr 26 07:37:26.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-793693479 --namespace=kubectl-1409 delete pods e2e-test-httpd-pod'
  E0426 07:37:27.361096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:28.361843      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:29.191: INFO: stderr: ""
  Apr 26 07:37:29.191: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 26 07:37:29.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1409" for this suite. @ 04/26/23 07:37:29.195
• [2.872 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/26/23 07:37:29.211
  Apr 26 07:37:29.211: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename configmap @ 04/26/23 07:37:29.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:29.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:29.252
  STEP: Creating configMap with name configmap-test-upd-1390f333-0367-4831-8b7c-66e9ee28576f @ 04/26/23 07:37:29.258
  STEP: Creating the pod @ 04/26/23 07:37:29.269
  E0426 07:37:29.362846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:30.363484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 04/26/23 07:37:31.291
  STEP: Waiting for pod with binary data @ 04/26/23 07:37:31.3
  Apr 26 07:37:31.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1381" for this suite. @ 04/26/23 07:37:31.308
• [2.113 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/26/23 07:37:31.325
  Apr 26 07:37:31.325: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename deployment @ 04/26/23 07:37:31.325
  E0426 07:37:31.363805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:31.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:31.395
  Apr 26 07:37:31.426: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0426 07:37:32.364353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:33.365201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:34.365427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:35.365679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:36.366096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:36.430: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/26/23 07:37:36.43
  Apr 26 07:37:36.430: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/26/23 07:37:36.463
  Apr 26 07:37:36.470: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6841  61e22cd1-b378-4e10-88b4-af08291725f3 37716 1 2023-04-26 07:37:36 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-26 07:37:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005125a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 26 07:37:36.473: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Apr 26 07:37:36.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6841" for this suite. @ 04/26/23 07:37:36.478
• [5.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/26/23 07:37:36.509
  Apr 26 07:37:36.509: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename controllerrevisions @ 04/26/23 07:37:36.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:36.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:36.608
  STEP: Creating DaemonSet "e2e-h8rws-daemon-set" @ 04/26/23 07:37:36.663
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/26/23 07:37:36.679
  Apr 26 07:37:36.687: INFO: Number of nodes with available pods controlled by daemonset e2e-h8rws-daemon-set: 0
  Apr 26 07:37:36.687: INFO: Node ip-172-31-3-127 is running 0 daemon pod, expected 1
  E0426 07:37:37.367065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:37.693: INFO: Number of nodes with available pods controlled by daemonset e2e-h8rws-daemon-set: 0
  Apr 26 07:37:37.693: INFO: Node ip-172-31-3-127 is running 0 daemon pod, expected 1
  E0426 07:37:38.367178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:38.694: INFO: Number of nodes with available pods controlled by daemonset e2e-h8rws-daemon-set: 2
  Apr 26 07:37:38.694: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-h8rws-daemon-set
  STEP: Confirm DaemonSet "e2e-h8rws-daemon-set" successfully created with "daemonset-name=e2e-h8rws-daemon-set" label @ 04/26/23 07:37:38.696
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-h8rws-daemon-set" @ 04/26/23 07:37:38.705
  Apr 26 07:37:38.709: INFO: Located ControllerRevision: "e2e-h8rws-daemon-set-75cc7b967c"
  STEP: Patching ControllerRevision "e2e-h8rws-daemon-set-75cc7b967c" @ 04/26/23 07:37:38.711
  Apr 26 07:37:38.721: INFO: e2e-h8rws-daemon-set-75cc7b967c has been patched
  STEP: Create a new ControllerRevision @ 04/26/23 07:37:38.721
  Apr 26 07:37:38.733: INFO: Created ControllerRevision: e2e-h8rws-daemon-set-74b6b56585
  STEP: Confirm that there are two ControllerRevisions @ 04/26/23 07:37:38.733
  Apr 26 07:37:38.733: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 26 07:37:38.735: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-h8rws-daemon-set-75cc7b967c" @ 04/26/23 07:37:38.735
  STEP: Confirm that there is only one ControllerRevision @ 04/26/23 07:37:38.746
  Apr 26 07:37:38.746: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 26 07:37:38.754: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-h8rws-daemon-set-74b6b56585" @ 04/26/23 07:37:38.756
  Apr 26 07:37:38.768: INFO: e2e-h8rws-daemon-set-74b6b56585 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/26/23 07:37:38.768
  W0426 07:37:38.782710      22 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/26/23 07:37:38.782
  Apr 26 07:37:38.782: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0426 07:37:39.368004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:39.786: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 26 07:37:39.789: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-h8rws-daemon-set-74b6b56585=updated" @ 04/26/23 07:37:39.789
  STEP: Confirm that there is only one ControllerRevision @ 04/26/23 07:37:39.797
  Apr 26 07:37:39.798: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 26 07:37:39.800: INFO: Found 1 ControllerRevisions
  Apr 26 07:37:39.802: INFO: ControllerRevision "e2e-h8rws-daemon-set-7cdb7d9fb8" has revision 3
  STEP: Deleting DaemonSet "e2e-h8rws-daemon-set" @ 04/26/23 07:37:39.804
  STEP: deleting DaemonSet.extensions e2e-h8rws-daemon-set in namespace controllerrevisions-230, will wait for the garbage collector to delete the pods @ 04/26/23 07:37:39.804
  Apr 26 07:37:39.868: INFO: Deleting DaemonSet.extensions e2e-h8rws-daemon-set took: 10.460581ms
  Apr 26 07:37:39.969: INFO: Terminating DaemonSet.extensions e2e-h8rws-daemon-set pods took: 100.881808ms
  E0426 07:37:40.368714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:41.369422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:37:41.573: INFO: Number of nodes with available pods controlled by daemonset e2e-h8rws-daemon-set: 0
  Apr 26 07:37:41.573: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-h8rws-daemon-set
  Apr 26 07:37:41.576: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37809"},"items":null}

  Apr 26 07:37:41.579: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37809"},"items":null}

  Apr 26 07:37:41.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-230" for this suite. @ 04/26/23 07:37:41.59
• [5.098 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/26/23 07:37:41.608
  Apr 26 07:37:41.608: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-runtime @ 04/26/23 07:37:41.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:37:41.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:37:41.639
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/26/23 07:37:41.657
  E0426 07:37:42.369764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:43.370129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:44.371275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:45.371655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:46.372124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:47.372666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:48.373151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:49.373291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:50.373620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:51.373735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:52.374063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:53.374174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:54.375295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:55.375770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:56.376170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:57.376431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:58.376760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:37:59.376849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:00.376919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/26/23 07:38:00.726
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/26/23 07:38:00.729
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/26/23 07:38:00.733
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/26/23 07:38:00.733
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/26/23 07:38:00.771
  E0426 07:38:01.378047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:02.378937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:03.380003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/26/23 07:38:03.785
  E0426 07:38:04.380962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/26/23 07:38:04.791
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/26/23 07:38:04.795
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/26/23 07:38:04.795
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/26/23 07:38:04.84
  E0426 07:38:05.381802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/26/23 07:38:05.847
  E0426 07:38:06.382166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:07.383257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/26/23 07:38:07.857
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/26/23 07:38:07.862
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/26/23 07:38:07.862
  Apr 26 07:38:07.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-260" for this suite. @ 04/26/23 07:38:07.889
• [26.288 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/26/23 07:38:07.897
  Apr 26 07:38:07.897: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 07:38:07.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:38:07.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:38:07.918
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/26/23 07:38:07.921
  E0426 07:38:08.383899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:09.384006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:10.384387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:11.384611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:38:11.941
  Apr 26 07:38:11.943: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-18100888-07a7-45fc-86d0-6e5fca84d30f container test-container: <nil>
  STEP: delete the pod @ 04/26/23 07:38:11.948
  Apr 26 07:38:11.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-500" for this suite. @ 04/26/23 07:38:11.97
• [4.079 seconds]
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/26/23 07:38:11.976
  Apr 26 07:38:11.976: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename downward-api @ 04/26/23 07:38:11.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:38:11.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:38:11.998
  STEP: Creating a pod to test downward api env vars @ 04/26/23 07:38:12.001
  E0426 07:38:12.385319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:13.386067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:14.386170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:15.386371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:38:16.018
  Apr 26 07:38:16.020: INFO: Trying to get logs from node ip-172-31-3-127 pod downward-api-2f953cca-e6e0-4670-93fa-74a029d33e3a container dapi-container: <nil>
  STEP: delete the pod @ 04/26/23 07:38:16.026
  Apr 26 07:38:16.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-156" for this suite. @ 04/26/23 07:38:16.046
• [4.077 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/26/23 07:38:16.053
  Apr 26 07:38:16.053: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename webhook @ 04/26/23 07:38:16.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:38:16.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:38:16.075
  STEP: Setting up server cert @ 04/26/23 07:38:16.103
  E0426 07:38:16.386969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/26/23 07:38:16.677
  STEP: Deploying the webhook pod @ 04/26/23 07:38:16.688
  STEP: Wait for the deployment to be ready @ 04/26/23 07:38:16.704
  Apr 26 07:38:16.709: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0426 07:38:17.387214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:18.387740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/26/23 07:38:18.718
  STEP: Verifying the service has paired with the endpoint @ 04/26/23 07:38:18.732
  E0426 07:38:19.388099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:38:19.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 26 07:38:19.735: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-621-crds.webhook.example.com via the AdmissionRegistration API @ 04/26/23 07:38:20.248
  STEP: Creating a custom resource while v1 is storage version @ 04/26/23 07:38:20.264
  E0426 07:38:20.388607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:21.389148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/26/23 07:38:22.291
  STEP: Patching the custom resource while v2 is storage version @ 04/26/23 07:38:22.3
  Apr 26 07:38:22.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0426 07:38:22.390089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-176" for this suite. @ 04/26/23 07:38:22.917
  STEP: Destroying namespace "webhook-markers-453" for this suite. @ 04/26/23 07:38:22.932
• [6.887 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/26/23 07:38:22.945
  Apr 26 07:38:22.945: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename watch @ 04/26/23 07:38:22.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:38:22.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:38:22.967
  STEP: creating a watch on configmaps with a certain label @ 04/26/23 07:38:22.969
  STEP: creating a new configmap @ 04/26/23 07:38:22.97
  STEP: modifying the configmap once @ 04/26/23 07:38:22.979
  STEP: changing the label value of the configmap @ 04/26/23 07:38:22.987
  STEP: Expecting to observe a delete notification for the watched object @ 04/26/23 07:38:22.997
  Apr 26 07:38:22.997: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2639  b9fe55a2-978f-4706-9288-fd4d12575a2a 38135 0 2023-04-26 07:38:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 07:38:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:38:22.997: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2639  b9fe55a2-978f-4706-9288-fd4d12575a2a 38136 0 2023-04-26 07:38:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 07:38:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:38:22.997: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2639  b9fe55a2-978f-4706-9288-fd4d12575a2a 38137 0 2023-04-26 07:38:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 07:38:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/26/23 07:38:22.997
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/26/23 07:38:23.009
  E0426 07:38:23.390172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:24.390792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:25.390944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:26.391260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:27.391494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:28.392000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:29.392194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:30.393094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:31.393307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:32.394418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 04/26/23 07:38:33.01
  STEP: modifying the configmap a third time @ 04/26/23 07:38:33.02
  STEP: deleting the configmap @ 04/26/23 07:38:33.031
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/26/23 07:38:33.038
  Apr 26 07:38:33.038: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2639  b9fe55a2-978f-4706-9288-fd4d12575a2a 38173 0 2023-04-26 07:38:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 07:38:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:38:33.039: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2639  b9fe55a2-978f-4706-9288-fd4d12575a2a 38174 0 2023-04-26 07:38:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 07:38:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:38:33.039: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2639  b9fe55a2-978f-4706-9288-fd4d12575a2a 38175 0 2023-04-26 07:38:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-26 07:38:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 26 07:38:33.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2639" for this suite. @ 04/26/23 07:38:33.043
• [10.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/26/23 07:38:33.053
  Apr 26 07:38:33.053: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename emptydir @ 04/26/23 07:38:33.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:38:33.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:38:33.075
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/26/23 07:38:33.078
  E0426 07:38:33.395265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:34.395525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:35.395937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:36.396218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/26/23 07:38:37.098
  Apr 26 07:38:37.101: INFO: Trying to get logs from node ip-172-31-3-127 pod pod-45d6b204-6c4e-496d-95d8-c1bbf44af596 container test-container: <nil>
  STEP: delete the pod @ 04/26/23 07:38:37.105
  Apr 26 07:38:37.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6298" for this suite. @ 04/26/23 07:38:37.124
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/26/23 07:38:37.132
  Apr 26 07:38:37.132: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename dns @ 04/26/23 07:38:37.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:38:37.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:38:37.15
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/26/23 07:38:37.153
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/26/23 07:38:37.153
  STEP: creating a pod to probe DNS @ 04/26/23 07:38:37.153
  STEP: submitting the pod to kubernetes @ 04/26/23 07:38:37.153
  E0426 07:38:37.396701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:38.396849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/26/23 07:38:39.175
  STEP: looking for the results for each expected name from probers @ 04/26/23 07:38:39.177
  Apr 26 07:38:39.188: INFO: DNS probes using dns-1642/dns-test-6e72ba1a-bb07-4080-ab92-b71ca8542107 succeeded

  Apr 26 07:38:39.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/26/23 07:38:39.191
  STEP: Destroying namespace "dns-1642" for this suite. @ 04/26/23 07:38:39.212
• [2.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/26/23 07:38:39.226
  Apr 26 07:38:39.226: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename container-probe @ 04/26/23 07:38:39.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:38:39.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:38:39.248
  E0426 07:38:39.397052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:40.397339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:41.397564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:42.398082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:43.398560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:44.399629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:45.400303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:46.400564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:47.401532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:48.401948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:49.402873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:50.403315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:51.404322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:52.405408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:53.405541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:54.405822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:55.406293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:56.406543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:57.407068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:58.407642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:38:59.408320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:39:00.408574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:39:01.310: INFO: Container started at 2023-04-26 07:38:39 +0000 UTC, pod became ready at 2023-04-26 07:38:59 +0000 UTC
  Apr 26 07:39:01.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5731" for this suite. @ 04/26/23 07:39:01.314
• [22.095 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/26/23 07:39:01.321
  Apr 26 07:39:01.321: INFO: >>> kubeConfig: /tmp/kubeconfig-793693479
  STEP: Building a namespace api object, basename job @ 04/26/23 07:39:01.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/26/23 07:39:01.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/26/23 07:39:01.341
  STEP: Creating a job @ 04/26/23 07:39:01.344
  STEP: Ensuring active pods == parallelism @ 04/26/23 07:39:01.352
  E0426 07:39:01.408892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:39:02.409403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 04/26/23 07:39:03.357
  E0426 07:39:03.410080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:39:03.874: INFO: Successfully updated pod "adopt-release-87p8z"
  STEP: Checking that the Job readopts the Pod @ 04/26/23 07:39:03.874
  E0426 07:39:04.410469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:39:05.411304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 04/26/23 07:39:05.883
  Apr 26 07:39:06.409: INFO: Successfully updated pod "adopt-release-87p8z"
  STEP: Checking that the Job releases the Pod @ 04/26/23 07:39:06.409
  E0426 07:39:06.411476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:39:07.411833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0426 07:39:08.411984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 26 07:39:08.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8278" for this suite. @ 04/26/23 07:39:08.419
• [7.107 seconds]
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 26 07:39:08.428: INFO: Running AfterSuite actions on node 1
  Apr 26 07:39:08.428: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.054 seconds]
------------------------------

Ran 378 of 7207 Specs in 5605.803 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h33m26.182833158s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

