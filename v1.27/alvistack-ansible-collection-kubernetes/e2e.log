  I0423 15:00:42.534281      15 e2e.go:117] Starting e2e run "88793e81-66ba-458e-8c2f-bc082b701f21" on Ginkgo node 1
  Apr 23 15:00:42.604: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682262042 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 23 15:00:42.977: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:00:42.984: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 23 15:00:43.080: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 23 15:00:43.096: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
  Apr 23 15:00:43.096: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
  Apr 23 15:00:43.096: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Apr 23 15:00:43.096: INFO: e2e test version: v1.27.1
  Apr 23 15:00:43.098: INFO: kube-apiserver version: v1.27.1
  Apr 23 15:00:43.098: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:00:43.107: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.131 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/23/23 15:00:43.676
  Apr 23 15:00:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename proxy @ 04/23/23 15:00:43.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:00:43.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:00:43.723
  Apr 23 15:00:43.728: INFO: Creating pod...
  Apr 23 15:00:55.810: INFO: Creating service...
  Apr 23 15:00:55.855: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/DELETE
  Apr 23 15:00:55.871: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 23 15:00:55.871: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/GET
  Apr 23 15:00:55.880: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 23 15:00:55.880: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/HEAD
  Apr 23 15:00:55.887: INFO: http.Client request:HEAD | StatusCode:200
  Apr 23 15:00:55.887: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 23 15:00:55.894: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 23 15:00:55.895: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/PATCH
  Apr 23 15:00:55.901: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 23 15:00:55.901: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/POST
  Apr 23 15:00:55.910: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 23 15:00:55.910: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/pods/agnhost/proxy/some/path/with/PUT
  Apr 23 15:00:55.916: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 23 15:00:55.916: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/DELETE
  Apr 23 15:00:55.929: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 23 15:00:55.930: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/GET
  Apr 23 15:00:55.946: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 23 15:00:55.947: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/HEAD
  Apr 23 15:00:55.955: INFO: http.Client request:HEAD | StatusCode:200
  Apr 23 15:00:55.955: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/OPTIONS
  Apr 23 15:00:55.966: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 23 15:00:55.966: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/PATCH
  Apr 23 15:00:55.976: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 23 15:00:55.976: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/POST
  Apr 23 15:00:55.987: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 23 15:00:55.987: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6422/services/test-service/proxy/some/path/with/PUT
  Apr 23 15:00:55.995: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 23 15:00:55.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6422" for this suite. @ 04/23/23 15:00:56.005
• [12.343 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/23/23 15:00:56.019
  Apr 23 15:00:56.019: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 15:00:56.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:00:56.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:00:56.064
  STEP: Creating configMap configmap-925/configmap-test-32c1f2d2-8ac0-4ba6-99e3-37945ce6b642 @ 04/23/23 15:00:56.07
  STEP: Creating a pod to test consume configMaps @ 04/23/23 15:00:56.079
  STEP: Saw pod success @ 04/23/23 15:01:00.116
  Apr 23 15:01:00.123: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-fd70b635-f0a6-45da-921d-4511d808b436 container env-test: <nil>
  STEP: delete the pod @ 04/23/23 15:01:00.157
  Apr 23 15:01:00.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-925" for this suite. @ 04/23/23 15:01:00.195
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/23/23 15:01:00.214
  Apr 23 15:01:00.214: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 15:01:00.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:01:00.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:01:00.253
  STEP: create the container @ 04/23/23 15:01:00.257
  W0423 15:01:00.271445      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/23/23 15:01:00.272
  STEP: get the container status @ 04/23/23 15:01:03.332
  STEP: the container should be terminated @ 04/23/23 15:01:03.341
  STEP: the termination message should be set @ 04/23/23 15:01:03.341
  Apr 23 15:01:03.341: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/23/23 15:01:03.341
  Apr 23 15:01:03.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9839" for this suite. @ 04/23/23 15:01:03.391
• [3.193 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/23/23 15:01:03.409
  Apr 23 15:01:03.410: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 15:01:03.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:01:03.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:01:03.454
  STEP: reading a file in the container @ 04/23/23 15:01:05.511
  Apr 23 15:01:05.512: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1627 pod-service-account-1b032737-3dc0-4bf6-8216-5a00d835e6ea -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 04/23/23 15:01:05.999
  Apr 23 15:01:06.000: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1627 pod-service-account-1b032737-3dc0-4bf6-8216-5a00d835e6ea -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 04/23/23 15:01:06.271
  Apr 23 15:01:06.271: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1627 pod-service-account-1b032737-3dc0-4bf6-8216-5a00d835e6ea -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Apr 23 15:01:06.507: INFO: Got root ca configmap in namespace "svcaccounts-1627"
  Apr 23 15:01:06.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1627" for this suite. @ 04/23/23 15:01:06.523
• [3.125 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/23/23 15:01:06.536
  Apr 23 15:01:06.536: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename taint-single-pod @ 04/23/23 15:01:06.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:01:06.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:01:06.59
  Apr 23 15:01:06.595: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 23 15:02:06.675: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 15:02:06.701: INFO: Starting informer...
  STEP: Starting pod... @ 04/23/23 15:02:06.702
  Apr 23 15:02:07.008: INFO: Pod is running on soodi4ja4shi-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/23/23 15:02:07.008
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/23/23 15:02:07.043
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/23/23 15:02:07.06
  Apr 23 15:02:07.061: INFO: Pod wasn't evicted. Proceeding
  Apr 23 15:02:07.061: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/23/23 15:02:07.098
  STEP: Waiting some time to make sure that toleration time passed. @ 04/23/23 15:02:07.123
  Apr 23 15:03:22.124: INFO: Pod wasn't evicted. Test successful
  Apr 23 15:03:22.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-227" for this suite. @ 04/23/23 15:03:22.14
• [135.617 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/23/23 15:03:22.16
  Apr 23 15:03:22.160: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename endpointslice @ 04/23/23 15:03:22.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:03:22.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:03:22.201
  Apr 23 15:03:22.221: INFO: Endpoints addresses: [192.168.121.106 192.168.121.241] , ports: [6443]
  Apr 23 15:03:22.221: INFO: EndpointSlices addresses: [192.168.121.106 192.168.121.241] , ports: [6443]
  Apr 23 15:03:22.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7670" for this suite. @ 04/23/23 15:03:22.231
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/23/23 15:03:22.255
  Apr 23 15:03:22.255: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 15:03:22.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:03:22.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:03:22.306
  Apr 23 15:03:22.310: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  W0423 15:03:25.040964      15 warnings.go:70] unknown field "alpha"
  W0423 15:03:25.041533      15 warnings.go:70] unknown field "beta"
  W0423 15:03:25.041861      15 warnings.go:70] unknown field "delta"
  W0423 15:03:25.042194      15 warnings.go:70] unknown field "epsilon"
  W0423 15:03:25.042529      15 warnings.go:70] unknown field "gamma"
  Apr 23 15:03:25.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7993" for this suite. @ 04/23/23 15:03:25.12
• [2.876 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/23/23 15:03:25.136
  Apr 23 15:03:25.136: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 15:03:25.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:03:25.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:03:25.174
  STEP: Creating a pod to test substitution in volume subpath @ 04/23/23 15:03:25.179
  STEP: Saw pod success @ 04/23/23 15:03:29.224
  Apr 23 15:03:29.231: INFO: Trying to get logs from node soodi4ja4shi-3 pod var-expansion-f9e16879-5318-42b7-a2b4-05e61a7e74cc container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 15:03:29.267
  Apr 23 15:03:29.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2393" for this suite. @ 04/23/23 15:03:29.311
• [4.188 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/23/23 15:03:29.324
  Apr 23 15:03:29.324: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 15:03:29.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:03:29.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:03:29.369
  STEP: creating Agnhost RC @ 04/23/23 15:03:29.376
  Apr 23 15:03:29.377: INFO: namespace kubectl-2970
  Apr 23 15:03:29.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2970 create -f -'
  Apr 23 15:03:31.248: INFO: stderr: ""
  Apr 23 15:03:31.249: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/23/23 15:03:31.249
  Apr 23 15:03:32.269: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 15:03:32.269: INFO: Found 0 / 1
  Apr 23 15:03:33.259: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 15:03:33.259: INFO: Found 1 / 1
  Apr 23 15:03:33.260: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 23 15:03:33.267: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 15:03:33.268: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 23 15:03:33.268: INFO: wait on agnhost-primary startup in kubectl-2970 
  Apr 23 15:03:33.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2970 logs agnhost-primary-h2gdx agnhost-primary'
  Apr 23 15:03:33.486: INFO: stderr: ""
  Apr 23 15:03:33.486: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/23/23 15:03:33.486
  Apr 23 15:03:33.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2970 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 23 15:03:33.674: INFO: stderr: ""
  Apr 23 15:03:33.674: INFO: stdout: "service/rm2 exposed\n"
  Apr 23 15:03:33.683: INFO: Service rm2 in namespace kubectl-2970 found.
  STEP: exposing service @ 04/23/23 15:03:35.694
  Apr 23 15:03:35.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2970 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Apr 23 15:03:35.848: INFO: stderr: ""
  Apr 23 15:03:35.848: INFO: stdout: "service/rm3 exposed\n"
  Apr 23 15:03:35.861: INFO: Service rm3 in namespace kubectl-2970 found.
  Apr 23 15:03:37.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2970" for this suite. @ 04/23/23 15:03:37.885
• [8.574 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/23/23 15:03:37.899
  Apr 23 15:03:37.899: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename dns @ 04/23/23 15:03:37.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:03:37.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:03:37.938
  STEP: Creating a test headless service @ 04/23/23 15:03:37.943
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2569.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2569.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2569.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2569.svc.cluster.local;sleep 1; done
   @ 04/23/23 15:03:37.955
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2569.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2569.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2569.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2569.svc.cluster.local;sleep 1; done
   @ 04/23/23 15:03:37.956
  STEP: creating a pod to probe DNS @ 04/23/23 15:03:37.956
  STEP: submitting the pod to kubernetes @ 04/23/23 15:03:37.957
  STEP: retrieving the pod @ 04/23/23 15:04:02.134
  STEP: looking for the results for each expected name from probers @ 04/23/23 15:04:02.143
  Apr 23 15:04:02.161: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:02.171: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:02.179: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:02.191: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:02.203: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:02.213: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:02.222: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:02.230: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:02.230: INFO: Lookups using dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2569.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2569.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local jessie_udp@dns-test-service-2.dns-2569.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2569.svc.cluster.local]

  Apr 23 15:04:07.248: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:07.269: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:07.294: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:07.312: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:07.325: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:07.333: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:07.348: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:07.359: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2569.svc.cluster.local from pod dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a: the server could not find the requested resource (get pods dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a)
  Apr 23 15:04:07.359: INFO: Lookups using dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2569.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2569.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2569.svc.cluster.local jessie_udp@dns-test-service-2.dns-2569.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2569.svc.cluster.local]

  Apr 23 15:04:12.307: INFO: DNS probes using dns-2569/dns-test-defc6616-0c48-4345-8ce6-f372f0d9109a succeeded

  Apr 23 15:04:12.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 15:04:12.325
  STEP: deleting the test headless service @ 04/23/23 15:04:12.377
  STEP: Destroying namespace "dns-2569" for this suite. @ 04/23/23 15:04:12.426
• [34.542 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/23/23 15:04:12.442
  Apr 23 15:04:12.442: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 15:04:12.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:04:12.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:04:12.536
  STEP: Setting up server cert @ 04/23/23 15:04:12.592
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 15:04:13.393
  STEP: Deploying the webhook pod @ 04/23/23 15:04:13.417
  STEP: Wait for the deployment to be ready @ 04/23/23 15:04:13.445
  Apr 23 15:04:13.501: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/23/23 15:04:15.537
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 15:04:15.575
  Apr 23 15:04:16.575: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/23/23 15:04:16.731
  STEP: Creating a configMap that should be mutated @ 04/23/23 15:04:16.787
  STEP: Deleting the collection of validation webhooks @ 04/23/23 15:04:16.844
  STEP: Creating a configMap that should not be mutated @ 04/23/23 15:04:16.96
  Apr 23 15:04:16.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9106" for this suite. @ 04/23/23 15:04:17.063
  STEP: Destroying namespace "webhook-markers-3111" for this suite. @ 04/23/23 15:04:17.079
• [4.649 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 04/23/23 15:04:17.095
  Apr 23 15:04:17.095: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 15:04:17.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:04:17.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:04:17.153
  Apr 23 15:04:17.214: INFO: Create a RollingUpdate DaemonSet
  Apr 23 15:04:17.240: INFO: Check that daemon pods launch on every node of the cluster
  Apr 23 15:04:17.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:17.259: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:18.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:18.301: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:19.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:19.275: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:20.306: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:20.306: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:21.281: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:21.282: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:22.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:22.274: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:23.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:23.278: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:24.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:24.315: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:25.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:25.274: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:26.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:26.277: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:27.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:27.293: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:28.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:28.277: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:29.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:29.280: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:30.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:30.279: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:31.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:31.279: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:32.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 15:04:32.278: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:33.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 15:04:33.278: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:04:34.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:04:34.279: INFO: Node soodi4ja4shi-2 is running 0 daemon pod, expected 1
  Apr 23 15:04:35.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:04:35.276: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Apr 23 15:04:35.276: INFO: Update the DaemonSet to trigger a rollout
  Apr 23 15:04:35.295: INFO: Updating DaemonSet daemon-set
  Apr 23 15:04:38.332: INFO: Roll back the DaemonSet before rollout is complete
  Apr 23 15:04:38.355: INFO: Updating DaemonSet daemon-set
  Apr 23 15:04:38.355: INFO: Make sure DaemonSet rollback is complete
  Apr 23 15:04:38.364: INFO: Wrong image for pod: daemon-set-jqffc. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 23 15:04:38.365: INFO: Pod daemon-set-jqffc is not available
  Apr 23 15:04:44.390: INFO: Pod daemon-set-g2fx8 is not available
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 15:04:44.416
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9210, will wait for the garbage collector to delete the pods @ 04/23/23 15:04:44.416
  Apr 23 15:04:44.490: INFO: Deleting DaemonSet.extensions daemon-set took: 16.305965ms
  Apr 23 15:04:44.691: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.656737ms
  Apr 23 15:04:46.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:04:46.998: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 15:04:47.009: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2744"},"items":null}

  Apr 23 15:04:47.018: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2744"},"items":null}

  Apr 23 15:04:47.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9210" for this suite. @ 04/23/23 15:04:47.057
• [29.974 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/23/23 15:04:47.071
  Apr 23 15:04:47.071: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 15:04:47.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:04:47.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:04:47.109
  STEP: Creating a test namespace @ 04/23/23 15:04:47.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:04:47.145
  STEP: Creating a pod in the namespace @ 04/23/23 15:04:47.15
  STEP: Waiting for the pod to have running status @ 04/23/23 15:04:47.165
  STEP: Deleting the namespace @ 04/23/23 15:04:49.18
  STEP: Waiting for the namespace to be removed. @ 04/23/23 15:04:49.196
  STEP: Recreating the namespace @ 04/23/23 15:05:01.203
  STEP: Verifying there are no pods in the namespace @ 04/23/23 15:05:01.247
  Apr 23 15:05:01.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1184" for this suite. @ 04/23/23 15:05:01.263
  STEP: Destroying namespace "nsdeletetest-2570" for this suite. @ 04/23/23 15:05:01.276
  Apr 23 15:05:01.283: INFO: Namespace nsdeletetest-2570 was already deleted
  STEP: Destroying namespace "nsdeletetest-3136" for this suite. @ 04/23/23 15:05:01.284
• [14.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/23/23 15:05:01.311
  Apr 23 15:05:01.311: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 15:05:01.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:05:01.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:05:01.354
  STEP: Creating pod liveness-c207ee67-cbc9-4a0e-bec6-ba878b84845f in namespace container-probe-5138 @ 04/23/23 15:05:01.36
  Apr 23 15:05:03.395: INFO: Started pod liveness-c207ee67-cbc9-4a0e-bec6-ba878b84845f in namespace container-probe-5138
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 15:05:03.396
  Apr 23 15:05:03.402: INFO: Initial restart count of pod liveness-c207ee67-cbc9-4a0e-bec6-ba878b84845f is 0
  Apr 23 15:05:23.521: INFO: Restart count of pod container-probe-5138/liveness-c207ee67-cbc9-4a0e-bec6-ba878b84845f is now 1 (20.118826077s elapsed)
  Apr 23 15:05:43.621: INFO: Restart count of pod container-probe-5138/liveness-c207ee67-cbc9-4a0e-bec6-ba878b84845f is now 2 (40.218104241s elapsed)
  Apr 23 15:06:03.760: INFO: Restart count of pod container-probe-5138/liveness-c207ee67-cbc9-4a0e-bec6-ba878b84845f is now 3 (1m0.357003855s elapsed)
  Apr 23 15:06:23.868: INFO: Restart count of pod container-probe-5138/liveness-c207ee67-cbc9-4a0e-bec6-ba878b84845f is now 4 (1m20.465298185s elapsed)
  Apr 23 15:07:36.268: INFO: Restart count of pod container-probe-5138/liveness-c207ee67-cbc9-4a0e-bec6-ba878b84845f is now 5 (2m32.865771547s elapsed)
  Apr 23 15:07:36.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 15:07:36.279
  STEP: Destroying namespace "container-probe-5138" for this suite. @ 04/23/23 15:07:36.303
• [155.004 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/23/23 15:07:36.319
  Apr 23 15:07:36.320: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename init-container @ 04/23/23 15:07:36.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:07:36.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:07:36.365
  STEP: creating the pod @ 04/23/23 15:07:36.369
  Apr 23 15:07:36.369: INFO: PodSpec: initContainers in spec.initContainers
  Apr 23 15:08:23.890: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d2d3c770-0fbc-4355-992f-1d49ecedcef3", GenerateName:"", Namespace:"init-container-5816", SelfLink:"", UID:"e3e35356-2a32-494e-8e2f-5800cc35e3b6", ResourceVersion:"3386", Generation:0, CreationTimestamp:time.Date(2023, time.April, 23, 15, 7, 36, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"369652807"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 23, 15, 7, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00119c048), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 23, 15, 8, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00119c078), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-hfr75", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004038020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hfr75", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hfr75", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hfr75", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000668518), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"soodi4ja4shi-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000ac8000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0006685a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0006685c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0006685c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0006685cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00136a0c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 23, 15, 7, 36, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 23, 15, 7, 36, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 23, 15, 7, 36, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 23, 15, 7, 36, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.96", PodIP:"10.233.66.101", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.101"}}, StartTime:time.Date(2023, time.April, 23, 15, 7, 36, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ac80e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ac8150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://871b36881448addbcccae81743c441f3a643d0eef9c22242416a8dd8d11c5dd8", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0040380a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004038080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc000668644), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 23 15:08:23.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5816" for this suite. @ 04/23/23 15:08:23.911
• [47.609 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/23/23 15:08:23.93
  Apr 23 15:08:23.930: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 15:08:23.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:08:23.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:08:23.97
  STEP: creating a replication controller @ 04/23/23 15:08:23.976
  Apr 23 15:08:23.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 create -f -'
  Apr 23 15:08:24.744: INFO: stderr: ""
  Apr 23 15:08:24.744: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/23/23 15:08:24.744
  Apr 23 15:08:24.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 15:08:25.014: INFO: stderr: ""
  Apr 23 15:08:25.014: INFO: stdout: "update-demo-nautilus-6tpfm update-demo-nautilus-qb4wl "
  Apr 23 15:08:25.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods update-demo-nautilus-6tpfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 15:08:25.141: INFO: stderr: ""
  Apr 23 15:08:25.141: INFO: stdout: ""
  Apr 23 15:08:25.141: INFO: update-demo-nautilus-6tpfm is created but not running
  Apr 23 15:08:30.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 15:08:30.353: INFO: stderr: ""
  Apr 23 15:08:30.353: INFO: stdout: "update-demo-nautilus-6tpfm update-demo-nautilus-qb4wl "
  Apr 23 15:08:30.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods update-demo-nautilus-6tpfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 15:08:30.544: INFO: stderr: ""
  Apr 23 15:08:30.544: INFO: stdout: ""
  Apr 23 15:08:30.544: INFO: update-demo-nautilus-6tpfm is created but not running
  Apr 23 15:08:35.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 15:08:35.791: INFO: stderr: ""
  Apr 23 15:08:35.791: INFO: stdout: "update-demo-nautilus-6tpfm update-demo-nautilus-qb4wl "
  Apr 23 15:08:35.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods update-demo-nautilus-6tpfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 15:08:35.941: INFO: stderr: ""
  Apr 23 15:08:35.942: INFO: stdout: ""
  Apr 23 15:08:35.942: INFO: update-demo-nautilus-6tpfm is created but not running
  Apr 23 15:08:40.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 15:08:41.152: INFO: stderr: ""
  Apr 23 15:08:41.152: INFO: stdout: "update-demo-nautilus-6tpfm update-demo-nautilus-qb4wl "
  Apr 23 15:08:41.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods update-demo-nautilus-6tpfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 15:08:41.293: INFO: stderr: ""
  Apr 23 15:08:41.293: INFO: stdout: "true"
  Apr 23 15:08:41.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods update-demo-nautilus-6tpfm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 15:08:41.445: INFO: stderr: ""
  Apr 23 15:08:41.445: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 15:08:41.446: INFO: validating pod update-demo-nautilus-6tpfm
  Apr 23 15:08:41.479: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 15:08:41.479: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 15:08:41.479: INFO: update-demo-nautilus-6tpfm is verified up and running
  Apr 23 15:08:41.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods update-demo-nautilus-qb4wl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 15:08:41.628: INFO: stderr: ""
  Apr 23 15:08:41.628: INFO: stdout: "true"
  Apr 23 15:08:41.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods update-demo-nautilus-qb4wl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 15:08:41.774: INFO: stderr: ""
  Apr 23 15:08:41.774: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 15:08:41.774: INFO: validating pod update-demo-nautilus-qb4wl
  Apr 23 15:08:41.791: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 15:08:41.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 15:08:41.791: INFO: update-demo-nautilus-qb4wl is verified up and running
  STEP: using delete to clean up resources @ 04/23/23 15:08:41.791
  Apr 23 15:08:41.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 delete --grace-period=0 --force -f -'
  Apr 23 15:08:41.918: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 15:08:41.918: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 23 15:08:41.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get rc,svc -l name=update-demo --no-headers'
  Apr 23 15:08:42.122: INFO: stderr: "No resources found in kubectl-7717 namespace.\n"
  Apr 23 15:08:42.122: INFO: stdout: ""
  Apr 23 15:08:42.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-7717 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 23 15:08:42.378: INFO: stderr: ""
  Apr 23 15:08:42.379: INFO: stdout: ""
  Apr 23 15:08:42.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7717" for this suite. @ 04/23/23 15:08:42.396
• [18.484 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/23/23 15:08:42.416
  Apr 23 15:08:42.416: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 15:08:42.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:08:42.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:08:42.467
  STEP: creating a Service @ 04/23/23 15:08:42.481
  STEP: watching for the Service to be added @ 04/23/23 15:08:42.507
  Apr 23 15:08:42.511: INFO: Found Service test-service-6lb4b in namespace services-4849 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 23 15:08:42.511: INFO: Service test-service-6lb4b created
  STEP: Getting /status @ 04/23/23 15:08:42.511
  Apr 23 15:08:42.519: INFO: Service test-service-6lb4b has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/23/23 15:08:42.519
  STEP: watching for the Service to be patched @ 04/23/23 15:08:42.542
  Apr 23 15:08:42.547: INFO: observed Service test-service-6lb4b in namespace services-4849 with annotations: map[] & LoadBalancer: {[]}
  Apr 23 15:08:42.547: INFO: Found Service test-service-6lb4b in namespace services-4849 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 23 15:08:42.547: INFO: Service test-service-6lb4b has service status patched
  STEP: updating the ServiceStatus @ 04/23/23 15:08:42.547
  Apr 23 15:08:42.592: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/23/23 15:08:42.593
  Apr 23 15:08:42.603: INFO: Observed Service test-service-6lb4b in namespace services-4849 with annotations: map[] & Conditions: {[]}
  Apr 23 15:08:42.604: INFO: Observed event: &Service{ObjectMeta:{test-service-6lb4b  services-4849  30ca4380-f915-4c3b-bc91-6a6ca74f1529 3495 0 2023-04-23 15:08:42 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-23 15:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-23 15:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.13.73,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.13.73],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 23 15:08:42.604: INFO: Found Service test-service-6lb4b in namespace services-4849 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 23 15:08:42.605: INFO: Service test-service-6lb4b has service status updated
  STEP: patching the service @ 04/23/23 15:08:42.605
  STEP: watching for the Service to be patched @ 04/23/23 15:08:42.629
  Apr 23 15:08:42.632: INFO: observed Service test-service-6lb4b in namespace services-4849 with labels: map[test-service-static:true]
  Apr 23 15:08:42.632: INFO: observed Service test-service-6lb4b in namespace services-4849 with labels: map[test-service-static:true]
  Apr 23 15:08:42.632: INFO: observed Service test-service-6lb4b in namespace services-4849 with labels: map[test-service-static:true]
  Apr 23 15:08:42.633: INFO: Found Service test-service-6lb4b in namespace services-4849 with labels: map[test-service:patched test-service-static:true]
  Apr 23 15:08:42.633: INFO: Service test-service-6lb4b patched
  STEP: deleting the service @ 04/23/23 15:08:42.633
  STEP: watching for the Service to be deleted @ 04/23/23 15:08:42.663
  Apr 23 15:08:42.666: INFO: Observed event: ADDED
  Apr 23 15:08:42.666: INFO: Observed event: MODIFIED
  Apr 23 15:08:42.666: INFO: Observed event: MODIFIED
  Apr 23 15:08:42.666: INFO: Observed event: MODIFIED
  Apr 23 15:08:42.666: INFO: Found Service test-service-6lb4b in namespace services-4849 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 23 15:08:42.667: INFO: Service test-service-6lb4b deleted
  Apr 23 15:08:42.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4849" for this suite. @ 04/23/23 15:08:42.686
• [0.282 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/23/23 15:08:42.7
  Apr 23 15:08:42.700: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 15:08:42.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:08:42.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:08:42.755
  Apr 23 15:08:42.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5861" for this suite. @ 04/23/23 15:08:42.845
• [0.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/23/23 15:08:42.883
  Apr 23 15:08:42.883: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pod-network-test @ 04/23/23 15:08:42.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:08:42.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:08:42.977
  STEP: Performing setup for networking test in namespace pod-network-test-9439 @ 04/23/23 15:08:42.983
  STEP: creating a selector @ 04/23/23 15:08:42.983
  STEP: Creating the service pods in kubernetes @ 04/23/23 15:08:42.983
  Apr 23 15:08:42.983: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/23/23 15:09:15.27
  Apr 23 15:09:17.342: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 23 15:09:17.342: INFO: Going to poll 10.233.64.195 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 15:09:17.348: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.195:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9439 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:09:17.348: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:09:17.349: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:09:17.349: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9439/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.195%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 15:09:17.555: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 23 15:09:17.555: INFO: Going to poll 10.233.65.203 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 15:09:17.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.203:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9439 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:09:17.561: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:09:17.562: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:09:17.562: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9439/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.203%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 15:09:17.685: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 23 15:09:17.685: INFO: Going to poll 10.233.66.76 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 15:09:17.694: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.76:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9439 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:09:17.694: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:09:17.695: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:09:17.695: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9439/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.76%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 15:09:17.792: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 23 15:09:17.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9439" for this suite. @ 04/23/23 15:09:17.802
• [34.934 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 04/23/23 15:09:17.819
  Apr 23 15:09:17.819: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 15:09:17.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:09:17.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:09:17.864
  STEP: Creating simple DaemonSet "daemon-set" @ 04/23/23 15:09:17.912
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 15:09:17.923
  Apr 23 15:09:17.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:09:17.937: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:09:18.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:09:18.982: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:09:19.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 15:09:19.957: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:09:20.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:09:20.956: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/23/23 15:09:20.962
  Apr 23 15:09:21.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:09:21.005: INFO: Node soodi4ja4shi-2 is running 0 daemon pod, expected 1
  Apr 23 15:09:22.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:09:22.022: INFO: Node soodi4ja4shi-2 is running 0 daemon pod, expected 1
  Apr 23 15:09:23.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:09:23.036: INFO: Node soodi4ja4shi-2 is running 0 daemon pod, expected 1
  Apr 23 15:09:24.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:09:24.023: INFO: Node soodi4ja4shi-2 is running 0 daemon pod, expected 1
  Apr 23 15:09:25.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:09:25.028: INFO: Node soodi4ja4shi-2 is running 0 daemon pod, expected 1
  Apr 23 15:09:26.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:09:26.026: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 15:09:26.034
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1597, will wait for the garbage collector to delete the pods @ 04/23/23 15:09:26.034
  Apr 23 15:09:26.108: INFO: Deleting DaemonSet.extensions daemon-set took: 14.818193ms
  Apr 23 15:09:26.208: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.798434ms
  Apr 23 15:09:27.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:09:27.417: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 15:09:27.423: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3831"},"items":null}

  Apr 23 15:09:27.431: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3831"},"items":null}

  Apr 23 15:09:27.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1597" for this suite. @ 04/23/23 15:09:27.472
• [9.664 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/23/23 15:09:27.494
  Apr 23 15:09:27.494: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename svc-latency @ 04/23/23 15:09:27.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:09:27.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:09:27.535
  Apr 23 15:09:27.540: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-3661 @ 04/23/23 15:09:27.544
  I0423 15:09:27.558471      15 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3661, replica count: 1
  I0423 15:09:28.611014      15 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0423 15:09:29.612316      15 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 15:09:29.734: INFO: Created: latency-svc-llq2j
  Apr 23 15:09:29.746: INFO: Got endpoints: latency-svc-llq2j [32.865295ms]
  Apr 23 15:09:29.789: INFO: Created: latency-svc-22hz5
  Apr 23 15:09:29.792: INFO: Got endpoints: latency-svc-22hz5 [45.667018ms]
  Apr 23 15:09:29.805: INFO: Created: latency-svc-xkdtp
  Apr 23 15:09:29.807: INFO: Created: latency-svc-nkxm7
  Apr 23 15:09:29.819: INFO: Got endpoints: latency-svc-nkxm7 [71.502026ms]
  Apr 23 15:09:29.820: INFO: Got endpoints: latency-svc-xkdtp [72.342573ms]
  Apr 23 15:09:29.843: INFO: Created: latency-svc-ff9qr
  Apr 23 15:09:29.850: INFO: Got endpoints: latency-svc-ff9qr [102.877304ms]
  Apr 23 15:09:29.872: INFO: Created: latency-svc-5zjfg
  Apr 23 15:09:29.872: INFO: Got endpoints: latency-svc-5zjfg [124.439021ms]
  Apr 23 15:09:29.878: INFO: Created: latency-svc-t6bx8
  Apr 23 15:09:29.887: INFO: Created: latency-svc-kn4rh
  Apr 23 15:09:29.892: INFO: Got endpoints: latency-svc-t6bx8 [144.395398ms]
  Apr 23 15:09:29.901: INFO: Got endpoints: latency-svc-kn4rh [153.331592ms]
  Apr 23 15:09:30.028: INFO: Created: latency-svc-vtx9m
  Apr 23 15:09:30.029: INFO: Created: latency-svc-q4nw8
  Apr 23 15:09:30.030: INFO: Created: latency-svc-ps28m
  Apr 23 15:09:30.042: INFO: Created: latency-svc-b2k4x
  Apr 23 15:09:30.043: INFO: Created: latency-svc-w54cl
  Apr 23 15:09:30.042: INFO: Created: latency-svc-6d8bs
  Apr 23 15:09:30.043: INFO: Created: latency-svc-fjzmp
  Apr 23 15:09:30.044: INFO: Created: latency-svc-9qgmq
  Apr 23 15:09:30.044: INFO: Created: latency-svc-tnb82
  Apr 23 15:09:30.044: INFO: Created: latency-svc-b8qx6
  Apr 23 15:09:30.051: INFO: Created: latency-svc-svr2p
  Apr 23 15:09:30.051: INFO: Created: latency-svc-g4crq
  Apr 23 15:09:30.051: INFO: Created: latency-svc-qgq75
  Apr 23 15:09:30.053: INFO: Created: latency-svc-b877n
  Apr 23 15:09:30.055: INFO: Created: latency-svc-k6fhs
  Apr 23 15:09:30.070: INFO: Got endpoints: latency-svc-vtx9m [197.311594ms]
  Apr 23 15:09:30.071: INFO: Got endpoints: latency-svc-q4nw8 [321.261897ms]
  Apr 23 15:09:30.071: INFO: Got endpoints: latency-svc-w54cl [279.172093ms]
  Apr 23 15:09:30.078: INFO: Got endpoints: latency-svc-b2k4x [329.817641ms]
  Apr 23 15:09:30.079: INFO: Got endpoints: latency-svc-ps28m [330.065562ms]
  Apr 23 15:09:30.113: INFO: Got endpoints: latency-svc-qgq75 [220.61357ms]
  Apr 23 15:09:30.115: INFO: Got endpoints: latency-svc-b877n [295.115992ms]
  Apr 23 15:09:30.115: INFO: Got endpoints: latency-svc-svr2p [366.814959ms]
  Apr 23 15:09:30.115: INFO: Got endpoints: latency-svc-k6fhs [367.038667ms]
  Apr 23 15:09:30.126: INFO: Got endpoints: latency-svc-g4crq [307.152172ms]
  Apr 23 15:09:30.126: INFO: Got endpoints: latency-svc-tnb82 [224.854697ms]
  Apr 23 15:09:30.143: INFO: Created: latency-svc-kkxlg
  Apr 23 15:09:30.149: INFO: Got endpoints: latency-svc-fjzmp [400.875334ms]
  Apr 23 15:09:30.152: INFO: Got endpoints: latency-svc-9qgmq [403.140174ms]
  Apr 23 15:09:30.160: INFO: Created: latency-svc-q2zz9
  Apr 23 15:09:30.166: INFO: Got endpoints: latency-svc-b8qx6 [417.312651ms]
  Apr 23 15:09:30.168: INFO: Got endpoints: latency-svc-6d8bs [317.140414ms]
  Apr 23 15:09:30.191: INFO: Created: latency-svc-kgsjz
  Apr 23 15:09:30.195: INFO: Got endpoints: latency-svc-kkxlg [125.418976ms]
  Apr 23 15:09:30.195: INFO: Got endpoints: latency-svc-q2zz9 [124.396394ms]
  Apr 23 15:09:30.204: INFO: Got endpoints: latency-svc-kgsjz [125.781812ms]
  Apr 23 15:09:30.213: INFO: Created: latency-svc-hgmwf
  Apr 23 15:09:30.217: INFO: Got endpoints: latency-svc-hgmwf [137.829477ms]
  Apr 23 15:09:30.221: INFO: Created: latency-svc-6w8l6
  Apr 23 15:09:30.233: INFO: Created: latency-svc-w5xkj
  Apr 23 15:09:30.247: INFO: Got endpoints: latency-svc-6w8l6 [175.785472ms]
  Apr 23 15:09:30.248: INFO: Created: latency-svc-6wglk
  Apr 23 15:09:30.262: INFO: Created: latency-svc-7lwk8
  Apr 23 15:09:30.264: INFO: Got endpoints: latency-svc-6wglk [147.455613ms]
  Apr 23 15:09:30.271: INFO: Created: latency-svc-rkr7l
  Apr 23 15:09:30.289: INFO: Got endpoints: latency-svc-7lwk8 [173.501751ms]
  Apr 23 15:09:30.299: INFO: Got endpoints: latency-svc-w5xkj [185.908958ms]
  Apr 23 15:09:30.301: INFO: Got endpoints: latency-svc-rkr7l [181.612167ms]
  Apr 23 15:09:30.305: INFO: Created: latency-svc-5glss
  Apr 23 15:09:30.321: INFO: Created: latency-svc-rrqtv
  Apr 23 15:09:30.328: INFO: Created: latency-svc-pxqc8
  Apr 23 15:09:30.355: INFO: Got endpoints: latency-svc-5glss [228.743293ms]
  Apr 23 15:09:30.374: INFO: Created: latency-svc-xj966
  Apr 23 15:09:30.396: INFO: Got endpoints: latency-svc-rrqtv [269.377956ms]
  Apr 23 15:09:30.397: INFO: Got endpoints: latency-svc-pxqc8 [246.789526ms]
  Apr 23 15:09:30.399: INFO: Created: latency-svc-hhw5s
  Apr 23 15:09:30.404: INFO: Created: latency-svc-xnw8g
  Apr 23 15:09:30.426: INFO: Created: latency-svc-mn74x
  Apr 23 15:09:30.438: INFO: Created: latency-svc-vpxsk
  Apr 23 15:09:30.445: INFO: Created: latency-svc-ctrlj
  Apr 23 15:09:30.448: INFO: Got endpoints: latency-svc-hhw5s [281.507876ms]
  Apr 23 15:09:30.463: INFO: Got endpoints: latency-svc-xj966 [310.425124ms]
  Apr 23 15:09:30.464: INFO: Got endpoints: latency-svc-xnw8g [296.296251ms]
  Apr 23 15:09:30.478: INFO: Created: latency-svc-2vtjd
  Apr 23 15:09:30.483: INFO: Got endpoints: latency-svc-vpxsk [287.180526ms]
  Apr 23 15:09:30.496: INFO: Created: latency-svc-wfw4m
  Apr 23 15:09:30.496: INFO: Created: latency-svc-l42ph
  Apr 23 15:09:30.496: INFO: Got endpoints: latency-svc-mn74x [301.00734ms]
  Apr 23 15:09:30.513: INFO: Created: latency-svc-4swp6
  Apr 23 15:09:30.525: INFO: Created: latency-svc-vxhph
  Apr 23 15:09:30.529: INFO: Created: latency-svc-m4sdd
  Apr 23 15:09:30.532: INFO: Got endpoints: latency-svc-ctrlj [327.043362ms]
  Apr 23 15:09:30.534: INFO: Got endpoints: latency-svc-2vtjd [315.902131ms]
  Apr 23 15:09:30.552: INFO: Created: latency-svc-5h58n
  Apr 23 15:09:30.570: INFO: Created: latency-svc-5dlvh
  Apr 23 15:09:30.581: INFO: Got endpoints: latency-svc-4swp6 [291.445301ms]
  Apr 23 15:09:30.583: INFO: Created: latency-svc-4zb7w
  Apr 23 15:09:30.631: INFO: Created: latency-svc-std2p
  Apr 23 15:09:30.636: INFO: Got endpoints: latency-svc-m4sdd [336.381525ms]
  Apr 23 15:09:30.659: INFO: Got endpoints: latency-svc-vxhph [357.385527ms]
  Apr 23 15:09:30.660: INFO: Got endpoints: latency-svc-wfw4m [395.588941ms]
  Apr 23 15:09:30.661: INFO: Got endpoints: latency-svc-l42ph [413.636465ms]
  Apr 23 15:09:30.698: INFO: Got endpoints: latency-svc-5h58n [343.160588ms]
  Apr 23 15:09:30.708: INFO: Got endpoints: latency-svc-5dlvh [311.843907ms]
  Apr 23 15:09:30.739: INFO: Created: latency-svc-9l2s9
  Apr 23 15:09:30.742: INFO: Created: latency-svc-jdttm
  Apr 23 15:09:30.744: INFO: Got endpoints: latency-svc-4zb7w [347.31789ms]
  Apr 23 15:09:30.761: INFO: Created: latency-svc-j8jcn
  Apr 23 15:09:30.779: INFO: Created: latency-svc-m47r9
  Apr 23 15:09:30.783: INFO: Created: latency-svc-8scbn
  Apr 23 15:09:30.797: INFO: Got endpoints: latency-svc-std2p [348.955659ms]
  Apr 23 15:09:30.817: INFO: Created: latency-svc-mg7g4
  Apr 23 15:09:30.826: INFO: Created: latency-svc-m5ksl
  Apr 23 15:09:30.836: INFO: Created: latency-svc-ln7xh
  Apr 23 15:09:30.850: INFO: Created: latency-svc-px46k
  Apr 23 15:09:30.859: INFO: Got endpoints: latency-svc-9l2s9 [394.521843ms]
  Apr 23 15:09:30.869: INFO: Created: latency-svc-lk28r
  Apr 23 15:09:30.878: INFO: Created: latency-svc-25qv6
  Apr 23 15:09:30.905: INFO: Got endpoints: latency-svc-jdttm [440.736843ms]
  Apr 23 15:09:30.905: INFO: Created: latency-svc-xlr4x
  Apr 23 15:09:30.917: INFO: Created: latency-svc-rwzwp
  Apr 23 15:09:30.923: INFO: Created: latency-svc-g4km5
  Apr 23 15:09:30.944: INFO: Got endpoints: latency-svc-j8jcn [460.314404ms]
  Apr 23 15:09:30.961: INFO: Created: latency-svc-pcfvb
  Apr 23 15:09:30.962: INFO: Created: latency-svc-8tjql
  Apr 23 15:09:30.991: INFO: Created: latency-svc-8h6fs
  Apr 23 15:09:30.998: INFO: Created: latency-svc-6h9bb
  Apr 23 15:09:30.999: INFO: Got endpoints: latency-svc-m47r9 [502.676019ms]
  Apr 23 15:09:31.016: INFO: Created: latency-svc-vc97v
  Apr 23 15:09:31.051: INFO: Got endpoints: latency-svc-8scbn [518.374115ms]
  Apr 23 15:09:31.073: INFO: Created: latency-svc-jlzt9
  Apr 23 15:09:31.100: INFO: Got endpoints: latency-svc-mg7g4 [565.105881ms]
  Apr 23 15:09:31.125: INFO: Created: latency-svc-bngnx
  Apr 23 15:09:31.147: INFO: Got endpoints: latency-svc-m5ksl [566.276778ms]
  Apr 23 15:09:31.166: INFO: Created: latency-svc-cm6tq
  Apr 23 15:09:31.200: INFO: Got endpoints: latency-svc-ln7xh [563.11422ms]
  Apr 23 15:09:31.242: INFO: Created: latency-svc-zcmwb
  Apr 23 15:09:31.257: INFO: Got endpoints: latency-svc-px46k [598.239137ms]
  Apr 23 15:09:31.287: INFO: Created: latency-svc-r7qzg
  Apr 23 15:09:31.304: INFO: Got endpoints: latency-svc-lk28r [643.240898ms]
  Apr 23 15:09:31.331: INFO: Created: latency-svc-pnffc
  Apr 23 15:09:31.343: INFO: Got endpoints: latency-svc-25qv6 [681.429101ms]
  Apr 23 15:09:31.374: INFO: Created: latency-svc-r82pq
  Apr 23 15:09:31.399: INFO: Got endpoints: latency-svc-xlr4x [699.334111ms]
  Apr 23 15:09:31.419: INFO: Created: latency-svc-fcm4p
  Apr 23 15:09:31.444: INFO: Got endpoints: latency-svc-rwzwp [735.784107ms]
  Apr 23 15:09:31.463: INFO: Created: latency-svc-tpk8b
  Apr 23 15:09:31.494: INFO: Got endpoints: latency-svc-g4km5 [749.031775ms]
  Apr 23 15:09:31.515: INFO: Created: latency-svc-h8nvt
  Apr 23 15:09:31.547: INFO: Got endpoints: latency-svc-8tjql [750.565149ms]
  Apr 23 15:09:31.566: INFO: Created: latency-svc-xrftj
  Apr 23 15:09:31.597: INFO: Got endpoints: latency-svc-pcfvb [737.624587ms]
  Apr 23 15:09:31.615: INFO: Created: latency-svc-bcgcz
  Apr 23 15:09:31.646: INFO: Got endpoints: latency-svc-8h6fs [741.762549ms]
  Apr 23 15:09:31.670: INFO: Created: latency-svc-95zkz
  Apr 23 15:09:31.700: INFO: Got endpoints: latency-svc-6h9bb [756.420789ms]
  Apr 23 15:09:31.736: INFO: Created: latency-svc-5t7gv
  Apr 23 15:09:31.757: INFO: Got endpoints: latency-svc-vc97v [757.860667ms]
  Apr 23 15:09:31.781: INFO: Created: latency-svc-h85jq
  Apr 23 15:09:31.795: INFO: Got endpoints: latency-svc-jlzt9 [743.67303ms]
  Apr 23 15:09:31.815: INFO: Created: latency-svc-tfrtm
  Apr 23 15:09:31.866: INFO: Got endpoints: latency-svc-bngnx [765.649523ms]
  Apr 23 15:09:31.890: INFO: Created: latency-svc-242lz
  Apr 23 15:09:31.905: INFO: Got endpoints: latency-svc-cm6tq [757.380206ms]
  Apr 23 15:09:31.924: INFO: Created: latency-svc-wc9bk
  Apr 23 15:09:31.952: INFO: Got endpoints: latency-svc-zcmwb [751.741149ms]
  Apr 23 15:09:31.968: INFO: Created: latency-svc-s2x6j
  Apr 23 15:09:32.001: INFO: Got endpoints: latency-svc-r7qzg [744.038685ms]
  Apr 23 15:09:32.024: INFO: Created: latency-svc-2bz5h
  Apr 23 15:09:32.055: INFO: Got endpoints: latency-svc-pnffc [750.496937ms]
  Apr 23 15:09:32.069: INFO: Created: latency-svc-5v7k4
  Apr 23 15:09:32.095: INFO: Got endpoints: latency-svc-r82pq [751.499669ms]
  Apr 23 15:09:32.112: INFO: Created: latency-svc-h8m2f
  Apr 23 15:09:32.145: INFO: Got endpoints: latency-svc-fcm4p [746.049403ms]
  Apr 23 15:09:32.167: INFO: Created: latency-svc-qvqmp
  Apr 23 15:09:32.195: INFO: Got endpoints: latency-svc-tpk8b [750.364258ms]
  Apr 23 15:09:32.216: INFO: Created: latency-svc-dxkvs
  Apr 23 15:09:32.245: INFO: Got endpoints: latency-svc-h8nvt [750.940702ms]
  Apr 23 15:09:32.267: INFO: Created: latency-svc-98qh7
  Apr 23 15:09:32.307: INFO: Got endpoints: latency-svc-xrftj [759.581431ms]
  Apr 23 15:09:32.327: INFO: Created: latency-svc-m6j2j
  Apr 23 15:09:32.348: INFO: Got endpoints: latency-svc-bcgcz [750.701562ms]
  Apr 23 15:09:32.366: INFO: Created: latency-svc-4x94n
  Apr 23 15:09:32.393: INFO: Got endpoints: latency-svc-95zkz [746.76193ms]
  Apr 23 15:09:32.420: INFO: Created: latency-svc-7gnc8
  Apr 23 15:09:32.449: INFO: Got endpoints: latency-svc-5t7gv [748.641468ms]
  Apr 23 15:09:32.467: INFO: Created: latency-svc-bfqc4
  Apr 23 15:09:32.497: INFO: Got endpoints: latency-svc-h85jq [739.284797ms]
  Apr 23 15:09:32.514: INFO: Created: latency-svc-q8kbs
  Apr 23 15:09:32.542: INFO: Got endpoints: latency-svc-tfrtm [747.563067ms]
  Apr 23 15:09:32.565: INFO: Created: latency-svc-9qvsf
  Apr 23 15:09:32.596: INFO: Got endpoints: latency-svc-242lz [729.437523ms]
  Apr 23 15:09:32.615: INFO: Created: latency-svc-wbswm
  Apr 23 15:09:32.643: INFO: Got endpoints: latency-svc-wc9bk [737.322791ms]
  Apr 23 15:09:32.660: INFO: Created: latency-svc-g59tt
  Apr 23 15:09:32.698: INFO: Got endpoints: latency-svc-s2x6j [745.699633ms]
  Apr 23 15:09:32.719: INFO: Created: latency-svc-jqlv9
  Apr 23 15:09:32.748: INFO: Got endpoints: latency-svc-2bz5h [746.256109ms]
  Apr 23 15:09:32.764: INFO: Created: latency-svc-b66xj
  Apr 23 15:09:32.801: INFO: Got endpoints: latency-svc-5v7k4 [746.028467ms]
  Apr 23 15:09:32.825: INFO: Created: latency-svc-99jjc
  Apr 23 15:09:32.847: INFO: Got endpoints: latency-svc-h8m2f [752.45186ms]
  Apr 23 15:09:32.871: INFO: Created: latency-svc-wc58s
  Apr 23 15:09:32.900: INFO: Got endpoints: latency-svc-qvqmp [755.047696ms]
  Apr 23 15:09:32.926: INFO: Created: latency-svc-mq6gg
  Apr 23 15:09:32.949: INFO: Got endpoints: latency-svc-dxkvs [754.126089ms]
  Apr 23 15:09:32.971: INFO: Created: latency-svc-mgccn
  Apr 23 15:09:32.999: INFO: Got endpoints: latency-svc-98qh7 [753.384572ms]
  Apr 23 15:09:33.023: INFO: Created: latency-svc-wp7t4
  Apr 23 15:09:33.047: INFO: Got endpoints: latency-svc-m6j2j [740.136234ms]
  Apr 23 15:09:33.070: INFO: Created: latency-svc-bhrhx
  Apr 23 15:09:33.108: INFO: Got endpoints: latency-svc-4x94n [759.549775ms]
  Apr 23 15:09:33.145: INFO: Created: latency-svc-k2lc4
  Apr 23 15:09:33.155: INFO: Got endpoints: latency-svc-7gnc8 [761.916982ms]
  Apr 23 15:09:33.178: INFO: Created: latency-svc-4xcfh
  Apr 23 15:09:33.193: INFO: Got endpoints: latency-svc-bfqc4 [744.667015ms]
  Apr 23 15:09:33.212: INFO: Created: latency-svc-ktjn7
  Apr 23 15:09:33.274: INFO: Got endpoints: latency-svc-q8kbs [777.452917ms]
  Apr 23 15:09:33.293: INFO: Created: latency-svc-2nx9g
  Apr 23 15:09:33.310: INFO: Got endpoints: latency-svc-9qvsf [767.000806ms]
  Apr 23 15:09:33.331: INFO: Created: latency-svc-h4vd8
  Apr 23 15:09:33.349: INFO: Got endpoints: latency-svc-wbswm [752.698196ms]
  Apr 23 15:09:33.373: INFO: Created: latency-svc-dsdww
  Apr 23 15:09:33.406: INFO: Got endpoints: latency-svc-g59tt [762.558014ms]
  Apr 23 15:09:33.462: INFO: Created: latency-svc-4h5kv
  Apr 23 15:09:33.463: INFO: Got endpoints: latency-svc-jqlv9 [764.633488ms]
  Apr 23 15:09:33.490: INFO: Created: latency-svc-6lmhr
  Apr 23 15:09:33.497: INFO: Got endpoints: latency-svc-b66xj [748.464627ms]
  Apr 23 15:09:33.516: INFO: Created: latency-svc-x4bb4
  Apr 23 15:09:33.544: INFO: Got endpoints: latency-svc-99jjc [742.348708ms]
  Apr 23 15:09:33.561: INFO: Created: latency-svc-4z9mt
  Apr 23 15:09:33.594: INFO: Got endpoints: latency-svc-wc58s [747.135821ms]
  Apr 23 15:09:33.616: INFO: Created: latency-svc-88rdp
  Apr 23 15:09:33.650: INFO: Got endpoints: latency-svc-mq6gg [749.302953ms]
  Apr 23 15:09:33.671: INFO: Created: latency-svc-fcvnl
  Apr 23 15:09:33.695: INFO: Got endpoints: latency-svc-mgccn [745.430594ms]
  Apr 23 15:09:33.719: INFO: Created: latency-svc-rrv5m
  Apr 23 15:09:33.750: INFO: Got endpoints: latency-svc-wp7t4 [750.987143ms]
  Apr 23 15:09:33.775: INFO: Created: latency-svc-7whm7
  Apr 23 15:09:33.794: INFO: Got endpoints: latency-svc-bhrhx [746.364254ms]
  Apr 23 15:09:33.827: INFO: Created: latency-svc-rc7gp
  Apr 23 15:09:33.845: INFO: Got endpoints: latency-svc-k2lc4 [736.239209ms]
  Apr 23 15:09:33.865: INFO: Created: latency-svc-lv7wv
  Apr 23 15:09:33.897: INFO: Got endpoints: latency-svc-4xcfh [741.576265ms]
  Apr 23 15:09:33.916: INFO: Created: latency-svc-7fffc
  Apr 23 15:09:33.946: INFO: Got endpoints: latency-svc-ktjn7 [752.658331ms]
  Apr 23 15:09:33.966: INFO: Created: latency-svc-2zvsn
  Apr 23 15:09:33.997: INFO: Got endpoints: latency-svc-2nx9g [722.091726ms]
  Apr 23 15:09:34.014: INFO: Created: latency-svc-c4rjd
  Apr 23 15:09:34.048: INFO: Got endpoints: latency-svc-h4vd8 [738.316016ms]
  Apr 23 15:09:34.073: INFO: Created: latency-svc-bt56b
  Apr 23 15:09:34.100: INFO: Got endpoints: latency-svc-dsdww [750.598649ms]
  Apr 23 15:09:34.125: INFO: Created: latency-svc-cv279
  Apr 23 15:09:34.145: INFO: Got endpoints: latency-svc-4h5kv [739.165091ms]
  Apr 23 15:09:34.162: INFO: Created: latency-svc-9kpbz
  Apr 23 15:09:34.196: INFO: Got endpoints: latency-svc-6lmhr [732.853007ms]
  Apr 23 15:09:34.219: INFO: Created: latency-svc-h2vfj
  Apr 23 15:09:34.243: INFO: Got endpoints: latency-svc-x4bb4 [745.331906ms]
  Apr 23 15:09:34.260: INFO: Created: latency-svc-lxx95
  Apr 23 15:09:34.312: INFO: Got endpoints: latency-svc-4z9mt [768.263951ms]
  Apr 23 15:09:34.332: INFO: Created: latency-svc-slcgn
  Apr 23 15:09:34.344: INFO: Got endpoints: latency-svc-88rdp [748.986222ms]
  Apr 23 15:09:34.362: INFO: Created: latency-svc-dwz9w
  Apr 23 15:09:34.394: INFO: Got endpoints: latency-svc-fcvnl [741.788745ms]
  Apr 23 15:09:34.413: INFO: Created: latency-svc-gg9ss
  Apr 23 15:09:34.450: INFO: Got endpoints: latency-svc-rrv5m [755.034137ms]
  Apr 23 15:09:34.473: INFO: Created: latency-svc-x8p8g
  Apr 23 15:09:34.498: INFO: Got endpoints: latency-svc-7whm7 [747.629802ms]
  Apr 23 15:09:34.522: INFO: Created: latency-svc-fgpbh
  Apr 23 15:09:34.548: INFO: Got endpoints: latency-svc-rc7gp [754.109363ms]
  Apr 23 15:09:34.578: INFO: Created: latency-svc-5dlzm
  Apr 23 15:09:34.633: INFO: Got endpoints: latency-svc-lv7wv [787.520227ms]
  Apr 23 15:09:34.647: INFO: Got endpoints: latency-svc-7fffc [749.650262ms]
  Apr 23 15:09:34.664: INFO: Created: latency-svc-6z4n8
  Apr 23 15:09:34.700: INFO: Created: latency-svc-svkhg
  Apr 23 15:09:34.705: INFO: Got endpoints: latency-svc-2zvsn [758.02828ms]
  Apr 23 15:09:34.750: INFO: Got endpoints: latency-svc-c4rjd [752.839378ms]
  Apr 23 15:09:34.759: INFO: Created: latency-svc-fdbzd
  Apr 23 15:09:34.789: INFO: Created: latency-svc-rbw6r
  Apr 23 15:09:34.803: INFO: Got endpoints: latency-svc-bt56b [754.551775ms]
  Apr 23 15:09:34.833: INFO: Created: latency-svc-hshgj
  Apr 23 15:09:34.848: INFO: Got endpoints: latency-svc-cv279 [747.174302ms]
  Apr 23 15:09:34.875: INFO: Created: latency-svc-gzmqt
  Apr 23 15:09:34.898: INFO: Got endpoints: latency-svc-9kpbz [752.291161ms]
  Apr 23 15:09:34.921: INFO: Created: latency-svc-j9p7l
  Apr 23 15:09:34.945: INFO: Got endpoints: latency-svc-h2vfj [748.991771ms]
  Apr 23 15:09:34.972: INFO: Created: latency-svc-rmwpd
  Apr 23 15:09:34.996: INFO: Got endpoints: latency-svc-lxx95 [753.012585ms]
  Apr 23 15:09:35.022: INFO: Created: latency-svc-49bmt
  Apr 23 15:09:35.047: INFO: Got endpoints: latency-svc-slcgn [734.604717ms]
  Apr 23 15:09:35.068: INFO: Created: latency-svc-zvxb4
  Apr 23 15:09:35.095: INFO: Got endpoints: latency-svc-dwz9w [751.474179ms]
  Apr 23 15:09:35.115: INFO: Created: latency-svc-x7c6k
  Apr 23 15:09:35.148: INFO: Got endpoints: latency-svc-gg9ss [752.691255ms]
  Apr 23 15:09:35.173: INFO: Created: latency-svc-pm6z6
  Apr 23 15:09:35.195: INFO: Got endpoints: latency-svc-x8p8g [744.873698ms]
  Apr 23 15:09:35.231: INFO: Created: latency-svc-hgcs8
  Apr 23 15:09:35.246: INFO: Got endpoints: latency-svc-fgpbh [747.496268ms]
  Apr 23 15:09:35.276: INFO: Created: latency-svc-6rn7k
  Apr 23 15:09:35.295: INFO: Got endpoints: latency-svc-5dlzm [746.068362ms]
  Apr 23 15:09:35.313: INFO: Created: latency-svc-dbkvz
  Apr 23 15:09:35.348: INFO: Got endpoints: latency-svc-6z4n8 [714.29182ms]
  Apr 23 15:09:35.366: INFO: Created: latency-svc-v4xk5
  Apr 23 15:09:35.398: INFO: Got endpoints: latency-svc-svkhg [751.016321ms]
  Apr 23 15:09:35.424: INFO: Created: latency-svc-g89tz
  Apr 23 15:09:35.447: INFO: Got endpoints: latency-svc-fdbzd [742.207313ms]
  Apr 23 15:09:35.470: INFO: Created: latency-svc-8bnmp
  Apr 23 15:09:35.496: INFO: Got endpoints: latency-svc-rbw6r [745.157144ms]
  Apr 23 15:09:35.517: INFO: Created: latency-svc-7qwpf
  Apr 23 15:09:35.542: INFO: Got endpoints: latency-svc-hshgj [739.037864ms]
  Apr 23 15:09:35.567: INFO: Created: latency-svc-nq9hs
  Apr 23 15:09:35.597: INFO: Got endpoints: latency-svc-gzmqt [749.249021ms]
  Apr 23 15:09:35.617: INFO: Created: latency-svc-fm5dk
  Apr 23 15:09:35.646: INFO: Got endpoints: latency-svc-j9p7l [747.737372ms]
  Apr 23 15:09:35.670: INFO: Created: latency-svc-scn59
  Apr 23 15:09:35.700: INFO: Got endpoints: latency-svc-rmwpd [754.327203ms]
  Apr 23 15:09:35.723: INFO: Created: latency-svc-n476x
  Apr 23 15:09:35.745: INFO: Got endpoints: latency-svc-49bmt [748.857191ms]
  Apr 23 15:09:35.767: INFO: Created: latency-svc-kz6b6
  Apr 23 15:09:35.797: INFO: Got endpoints: latency-svc-zvxb4 [748.900681ms]
  Apr 23 15:09:35.817: INFO: Created: latency-svc-8wdqm
  Apr 23 15:09:35.846: INFO: Got endpoints: latency-svc-x7c6k [750.32367ms]
  Apr 23 15:09:35.865: INFO: Created: latency-svc-k2hdd
  Apr 23 15:09:35.899: INFO: Got endpoints: latency-svc-pm6z6 [751.053136ms]
  Apr 23 15:09:35.916: INFO: Created: latency-svc-h9njd
  Apr 23 15:09:35.945: INFO: Got endpoints: latency-svc-hgcs8 [749.273213ms]
  Apr 23 15:09:35.965: INFO: Created: latency-svc-wl98r
  Apr 23 15:09:35.998: INFO: Got endpoints: latency-svc-6rn7k [751.84494ms]
  Apr 23 15:09:36.017: INFO: Created: latency-svc-znzrf
  Apr 23 15:09:36.044: INFO: Got endpoints: latency-svc-dbkvz [748.795185ms]
  Apr 23 15:09:36.069: INFO: Created: latency-svc-pt9j5
  Apr 23 15:09:36.097: INFO: Got endpoints: latency-svc-v4xk5 [748.963113ms]
  Apr 23 15:09:36.116: INFO: Created: latency-svc-vt6rk
  Apr 23 15:09:36.147: INFO: Got endpoints: latency-svc-g89tz [748.2369ms]
  Apr 23 15:09:36.166: INFO: Created: latency-svc-6fmlq
  Apr 23 15:09:36.197: INFO: Got endpoints: latency-svc-8bnmp [750.215224ms]
  Apr 23 15:09:36.227: INFO: Created: latency-svc-rzh87
  Apr 23 15:09:36.251: INFO: Got endpoints: latency-svc-7qwpf [755.760228ms]
  Apr 23 15:09:36.268: INFO: Created: latency-svc-9jrnq
  Apr 23 15:09:36.294: INFO: Got endpoints: latency-svc-nq9hs [751.579076ms]
  Apr 23 15:09:36.323: INFO: Created: latency-svc-rb68k
  Apr 23 15:09:36.349: INFO: Got endpoints: latency-svc-fm5dk [751.622134ms]
  Apr 23 15:09:36.370: INFO: Created: latency-svc-wg6rq
  Apr 23 15:09:36.395: INFO: Got endpoints: latency-svc-scn59 [749.319652ms]
  Apr 23 15:09:36.414: INFO: Created: latency-svc-fkdwm
  Apr 23 15:09:36.442: INFO: Got endpoints: latency-svc-n476x [741.932855ms]
  Apr 23 15:09:36.463: INFO: Created: latency-svc-49ts7
  Apr 23 15:09:36.499: INFO: Got endpoints: latency-svc-kz6b6 [753.581876ms]
  Apr 23 15:09:36.523: INFO: Created: latency-svc-kxxdp
  Apr 23 15:09:36.547: INFO: Got endpoints: latency-svc-8wdqm [750.095083ms]
  Apr 23 15:09:36.564: INFO: Created: latency-svc-cdwsm
  Apr 23 15:09:36.596: INFO: Got endpoints: latency-svc-k2hdd [749.454558ms]
  Apr 23 15:09:36.612: INFO: Created: latency-svc-vlvtc
  Apr 23 15:09:36.648: INFO: Got endpoints: latency-svc-h9njd [748.899233ms]
  Apr 23 15:09:36.669: INFO: Created: latency-svc-xph2r
  Apr 23 15:09:36.696: INFO: Got endpoints: latency-svc-wl98r [751.401265ms]
  Apr 23 15:09:36.719: INFO: Created: latency-svc-lmxlf
  Apr 23 15:09:36.751: INFO: Got endpoints: latency-svc-znzrf [752.469696ms]
  Apr 23 15:09:36.771: INFO: Created: latency-svc-cq597
  Apr 23 15:09:36.799: INFO: Got endpoints: latency-svc-pt9j5 [754.90658ms]
  Apr 23 15:09:36.820: INFO: Created: latency-svc-cmm9c
  Apr 23 15:09:36.844: INFO: Got endpoints: latency-svc-vt6rk [746.392202ms]
  Apr 23 15:09:36.860: INFO: Created: latency-svc-hldk8
  Apr 23 15:09:36.897: INFO: Got endpoints: latency-svc-6fmlq [749.80633ms]
  Apr 23 15:09:36.919: INFO: Created: latency-svc-82gl9
  Apr 23 15:09:36.946: INFO: Got endpoints: latency-svc-rzh87 [747.863846ms]
  Apr 23 15:09:36.964: INFO: Created: latency-svc-nltjq
  Apr 23 15:09:36.997: INFO: Got endpoints: latency-svc-9jrnq [744.771354ms]
  Apr 23 15:09:37.038: INFO: Created: latency-svc-7n4ks
  Apr 23 15:09:37.045: INFO: Got endpoints: latency-svc-rb68k [750.981059ms]
  Apr 23 15:09:37.065: INFO: Created: latency-svc-4ppbp
  Apr 23 15:09:37.095: INFO: Got endpoints: latency-svc-wg6rq [746.116752ms]
  Apr 23 15:09:37.111: INFO: Created: latency-svc-2d42k
  Apr 23 15:09:37.144: INFO: Got endpoints: latency-svc-fkdwm [748.508187ms]
  Apr 23 15:09:37.163: INFO: Created: latency-svc-7rkd2
  Apr 23 15:09:37.198: INFO: Got endpoints: latency-svc-49ts7 [756.198491ms]
  Apr 23 15:09:37.224: INFO: Created: latency-svc-qss2r
  Apr 23 15:09:37.247: INFO: Got endpoints: latency-svc-kxxdp [748.029907ms]
  Apr 23 15:09:37.272: INFO: Created: latency-svc-xd4pt
  Apr 23 15:09:37.295: INFO: Got endpoints: latency-svc-cdwsm [747.647155ms]
  Apr 23 15:09:37.316: INFO: Created: latency-svc-bdt8h
  Apr 23 15:09:37.345: INFO: Got endpoints: latency-svc-vlvtc [748.815347ms]
  Apr 23 15:09:37.362: INFO: Created: latency-svc-8lqcv
  Apr 23 15:09:37.409: INFO: Got endpoints: latency-svc-xph2r [760.902689ms]
  Apr 23 15:09:37.450: INFO: Created: latency-svc-s7wvb
  Apr 23 15:09:37.462: INFO: Got endpoints: latency-svc-lmxlf [765.008265ms]
  Apr 23 15:09:37.487: INFO: Created: latency-svc-fkq95
  Apr 23 15:09:37.492: INFO: Got endpoints: latency-svc-cq597 [741.669784ms]
  Apr 23 15:09:37.512: INFO: Created: latency-svc-hpg4f
  Apr 23 15:09:37.556: INFO: Got endpoints: latency-svc-cmm9c [757.357459ms]
  Apr 23 15:09:37.576: INFO: Created: latency-svc-l7xj2
  Apr 23 15:09:37.593: INFO: Got endpoints: latency-svc-hldk8 [749.551368ms]
  Apr 23 15:09:37.642: INFO: Got endpoints: latency-svc-82gl9 [745.241978ms]
  Apr 23 15:09:37.700: INFO: Got endpoints: latency-svc-nltjq [754.114286ms]
  Apr 23 15:09:37.747: INFO: Got endpoints: latency-svc-7n4ks [750.06026ms]
  Apr 23 15:09:37.794: INFO: Got endpoints: latency-svc-4ppbp [748.835188ms]
  Apr 23 15:09:37.847: INFO: Got endpoints: latency-svc-2d42k [751.091773ms]
  Apr 23 15:09:37.896: INFO: Got endpoints: latency-svc-7rkd2 [751.994612ms]
  Apr 23 15:09:37.948: INFO: Got endpoints: latency-svc-qss2r [749.376071ms]
  Apr 23 15:09:37.996: INFO: Got endpoints: latency-svc-xd4pt [748.590554ms]
  Apr 23 15:09:38.046: INFO: Got endpoints: latency-svc-bdt8h [749.432549ms]
  Apr 23 15:09:38.094: INFO: Got endpoints: latency-svc-8lqcv [749.672081ms]
  Apr 23 15:09:38.143: INFO: Got endpoints: latency-svc-s7wvb [734.212498ms]
  Apr 23 15:09:38.196: INFO: Got endpoints: latency-svc-fkq95 [734.150706ms]
  Apr 23 15:09:38.246: INFO: Got endpoints: latency-svc-hpg4f [752.961475ms]
  Apr 23 15:09:38.302: INFO: Got endpoints: latency-svc-l7xj2 [745.266434ms]
  Apr 23 15:09:38.302: INFO: Latencies: [45.667018ms 71.502026ms 72.342573ms 102.877304ms 124.396394ms 124.439021ms 125.418976ms 125.781812ms 137.829477ms 144.395398ms 147.455613ms 153.331592ms 173.501751ms 175.785472ms 181.612167ms 185.908958ms 197.311594ms 220.61357ms 224.854697ms 228.743293ms 246.789526ms 269.377956ms 279.172093ms 281.507876ms 287.180526ms 291.445301ms 295.115992ms 296.296251ms 301.00734ms 307.152172ms 310.425124ms 311.843907ms 315.902131ms 317.140414ms 321.261897ms 327.043362ms 329.817641ms 330.065562ms 336.381525ms 343.160588ms 347.31789ms 348.955659ms 357.385527ms 366.814959ms 367.038667ms 394.521843ms 395.588941ms 400.875334ms 403.140174ms 413.636465ms 417.312651ms 440.736843ms 460.314404ms 502.676019ms 518.374115ms 563.11422ms 565.105881ms 566.276778ms 598.239137ms 643.240898ms 681.429101ms 699.334111ms 714.29182ms 722.091726ms 729.437523ms 732.853007ms 734.150706ms 734.212498ms 734.604717ms 735.784107ms 736.239209ms 737.322791ms 737.624587ms 738.316016ms 739.037864ms 739.165091ms 739.284797ms 740.136234ms 741.576265ms 741.669784ms 741.762549ms 741.788745ms 741.932855ms 742.207313ms 742.348708ms 743.67303ms 744.038685ms 744.667015ms 744.771354ms 744.873698ms 745.157144ms 745.241978ms 745.266434ms 745.331906ms 745.430594ms 745.699633ms 746.028467ms 746.049403ms 746.068362ms 746.116752ms 746.256109ms 746.364254ms 746.392202ms 746.76193ms 747.135821ms 747.174302ms 747.496268ms 747.563067ms 747.629802ms 747.647155ms 747.737372ms 747.863846ms 748.029907ms 748.2369ms 748.464627ms 748.508187ms 748.590554ms 748.641468ms 748.795185ms 748.815347ms 748.835188ms 748.857191ms 748.899233ms 748.900681ms 748.963113ms 748.986222ms 748.991771ms 749.031775ms 749.249021ms 749.273213ms 749.302953ms 749.319652ms 749.376071ms 749.432549ms 749.454558ms 749.551368ms 749.650262ms 749.672081ms 749.80633ms 750.06026ms 750.095083ms 750.215224ms 750.32367ms 750.364258ms 750.496937ms 750.565149ms 750.598649ms 750.701562ms 750.940702ms 750.981059ms 750.987143ms 751.016321ms 751.053136ms 751.091773ms 751.401265ms 751.474179ms 751.499669ms 751.579076ms 751.622134ms 751.741149ms 751.84494ms 751.994612ms 752.291161ms 752.45186ms 752.469696ms 752.658331ms 752.691255ms 752.698196ms 752.839378ms 752.961475ms 753.012585ms 753.384572ms 753.581876ms 754.109363ms 754.114286ms 754.126089ms 754.327203ms 754.551775ms 754.90658ms 755.034137ms 755.047696ms 755.760228ms 756.198491ms 756.420789ms 757.357459ms 757.380206ms 757.860667ms 758.02828ms 759.549775ms 759.581431ms 760.902689ms 761.916982ms 762.558014ms 764.633488ms 765.008265ms 765.649523ms 767.000806ms 768.263951ms 777.452917ms 787.520227ms]
  Apr 23 15:09:38.303: INFO: 50 %ile: 746.256109ms
  Apr 23 15:09:38.303: INFO: 90 %ile: 755.047696ms
  Apr 23 15:09:38.303: INFO: 99 %ile: 777.452917ms
  Apr 23 15:09:38.304: INFO: Total sample count: 200
  Apr 23 15:09:38.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-3661" for this suite. @ 04/23/23 15:09:38.323
• [10.846 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/23/23 15:09:38.35
  Apr 23 15:09:38.350: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:09:38.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:09:38.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:09:38.396
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/23/23 15:09:38.4
  STEP: Saw pod success @ 04/23/23 15:09:42.449
  Apr 23 15:09:42.457: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-50db2350-ce04-4967-90f2-6e67f01cbb81 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 15:09:42.495
  Apr 23 15:09:42.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1824" for this suite. @ 04/23/23 15:09:42.537
• [4.209 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/23/23 15:09:42.562
  Apr 23 15:09:42.562: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 15:09:42.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:09:42.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:09:42.64
  STEP: creating an Endpoint @ 04/23/23 15:09:42.67
  STEP: waiting for available Endpoint @ 04/23/23 15:09:42.688
  STEP: listing all Endpoints @ 04/23/23 15:09:42.69
  STEP: updating the Endpoint @ 04/23/23 15:09:42.701
  STEP: fetching the Endpoint @ 04/23/23 15:09:42.711
  STEP: patching the Endpoint @ 04/23/23 15:09:42.734
  STEP: fetching the Endpoint @ 04/23/23 15:09:42.748
  STEP: deleting the Endpoint by Collection @ 04/23/23 15:09:42.753
  STEP: waiting for Endpoint deletion @ 04/23/23 15:09:42.771
  STEP: fetching the Endpoint @ 04/23/23 15:09:42.774
  Apr 23 15:09:42.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-659" for this suite. @ 04/23/23 15:09:42.787
• [0.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/23/23 15:09:42.805
  Apr 23 15:09:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 15:09:42.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:09:42.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:09:42.844
  Apr 23 15:09:42.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8112" for this suite. @ 04/23/23 15:09:42.931
• [0.139 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/23/23 15:09:42.946
  Apr 23 15:09:42.946: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 15:09:42.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:09:42.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:09:42.984
  STEP: Setting up server cert @ 04/23/23 15:09:43.029
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 15:09:44.093
  STEP: Deploying the webhook pod @ 04/23/23 15:09:44.107
  STEP: Wait for the deployment to be ready @ 04/23/23 15:09:44.129
  Apr 23 15:09:44.143: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Apr 23 15:09:46.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 9, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 9, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 9, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 9, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/23/23 15:09:48.183
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 15:09:48.21
  Apr 23 15:09:49.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:09:50.211: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:09:51.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:09:52.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:09:53.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:09:54.212: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:09:55.211: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:09:56.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:09:56.217: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6054-crds.webhook.example.com via the AdmissionRegistration API @ 04/23/23 15:09:56.75
  STEP: Creating a custom resource while v1 is storage version @ 04/23/23 15:09:56.794
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/23/23 15:09:59.042
  STEP: Patching the custom resource while v2 is storage version @ 04/23/23 15:09:59.079
  Apr 23 15:09:59.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5007" for this suite. @ 04/23/23 15:09:59.792
  STEP: Destroying namespace "webhook-markers-3234" for this suite. @ 04/23/23 15:09:59.806
• [16.871 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/23/23 15:09:59.823
  Apr 23 15:09:59.823: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 15:09:59.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:09:59.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:09:59.857
  STEP: Creating a pod to test downward api env vars @ 04/23/23 15:09:59.86
  STEP: Saw pod success @ 04/23/23 15:10:03.898
  Apr 23 15:10:03.905: INFO: Trying to get logs from node soodi4ja4shi-3 pod downward-api-13a3b41f-0fb9-4bf4-8fca-7bdf86cdb6ff container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 15:10:03.922
  Apr 23 15:10:03.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7334" for this suite. @ 04/23/23 15:10:03.963
• [4.155 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/23/23 15:10:03.98
  Apr 23 15:10:03.980: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 15:10:03.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:10:04.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:10:04.016
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/23/23 15:10:04.021
  Apr 23 15:10:04.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4768 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 23 15:10:04.233: INFO: stderr: ""
  Apr 23 15:10:04.233: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/23/23 15:10:04.233
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/23/23 15:10:09.286
  Apr 23 15:10:09.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4768 get pod e2e-test-httpd-pod -o json'
  Apr 23 15:10:09.499: INFO: stderr: ""
  Apr 23 15:10:09.499: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-04-23T15:10:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4768\",\n        \"resourceVersion\": \"6000\",\n        \"uid\": \"5ede9384-1f6b-467d-8cdc-da7abdd05fde\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qf9lz\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"soodi4ja4shi-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qf9lz\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-23T15:10:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-23T15:10:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-23T15:10:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-23T15:10:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://80c3479faf5951d6d3b522d92d15a78752d98d87df8a3593ef67387a24bf4485\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-23T15:10:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.96\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.224\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.224\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-23T15:10:04Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/23/23 15:10:09.5
  Apr 23 15:10:09.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4768 replace -f -'
  Apr 23 15:10:10.387: INFO: stderr: ""
  Apr 23 15:10:10.387: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/23/23 15:10:10.387
  Apr 23 15:10:10.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4768 delete pods e2e-test-httpd-pod'
  Apr 23 15:10:12.767: INFO: stderr: ""
  Apr 23 15:10:12.768: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 23 15:10:12.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4768" for this suite. @ 04/23/23 15:10:12.78
• [8.813 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/23/23 15:10:12.793
  Apr 23 15:10:12.793: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename deployment @ 04/23/23 15:10:12.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:10:12.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:10:12.834
  Apr 23 15:10:12.839: INFO: Creating deployment "test-recreate-deployment"
  Apr 23 15:10:12.850: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 23 15:10:12.874: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Apr 23 15:10:14.892: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 23 15:10:14.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 10, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 10, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 10, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 10, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6c99bf8bf6\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:10:16.913: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 23 15:10:16.933: INFO: Updating deployment test-recreate-deployment
  Apr 23 15:10:16.933: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 23 15:10:17.175: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1104  5e006bcd-ecf7-4bd7-947a-aa28959984e9 6092 2 2023-04-23 15:10:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-23 15:10:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:10:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d5d7b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-23 15:10:17 +0000 UTC,LastTransitionTime:2023-04-23 15:10:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-04-23 15:10:17 +0000 UTC,LastTransitionTime:2023-04-23 15:10:12 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 23 15:10:17.183: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1104  428ae09f-ca13-4298-bcdc-9dd307a706b6 6090 1 2023-04-23 15:10:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5e006bcd-ecf7-4bd7-947a-aa28959984e9 0xc0054a5577 0xc0054a5578}] [] [{kube-controller-manager Update apps/v1 2023-04-23 15:10:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e006bcd-ecf7-4bd7-947a-aa28959984e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:10:17 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054a5618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 15:10:17.183: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 23 15:10:17.183: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1104  231617e1-7d01-45ef-a4af-7f3bfdbbf236 6080 2 2023-04-23 15:10:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5e006bcd-ecf7-4bd7-947a-aa28959984e9 0xc0054a5677 0xc0054a5678}] [] [{kube-controller-manager Update apps/v1 2023-04-23 15:10:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e006bcd-ecf7-4bd7-947a-aa28959984e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:10:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054a5728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 15:10:17.195: INFO: Pod "test-recreate-deployment-54757ffd6c-6bgmv" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-6bgmv test-recreate-deployment-54757ffd6c- deployment-1104  a1340f6b-bd86-46ce-b61f-e84d5962330f 6091 0 2023-04-23 15:10:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 428ae09f-ca13-4298-bcdc-9dd307a706b6 0xc0054a5b87 0xc0054a5b88}] [] [{kube-controller-manager Update v1 2023-04-23 15:10:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"428ae09f-ca13-4298-bcdc-9dd307a706b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 15:10:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gr2ml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gr2ml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:10:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 15:10:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 15:10:17.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1104" for this suite. @ 04/23/23 15:10:17.215
• [4.437 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/23/23 15:10:17.268
  Apr 23 15:10:17.268: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:10:17.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:10:17.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:10:17.317
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/23/23 15:10:17.322
  STEP: Saw pod success @ 04/23/23 15:10:21.367
  Apr 23 15:10:21.376: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-29da919f-13f9-439a-8e2e-2fe56c5f7235 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 15:10:21.394
  Apr 23 15:10:21.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4206" for this suite. @ 04/23/23 15:10:21.506
• [4.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/23/23 15:10:21.537
  Apr 23 15:10:21.538: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 15:10:21.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:10:21.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:10:21.576
  STEP: Creating resourceQuota "e2e-rq-status-kd69t" @ 04/23/23 15:10:21.587
  Apr 23 15:10:21.608: INFO: Resource quota "e2e-rq-status-kd69t" reports spec: hard cpu limit of 500m
  Apr 23 15:10:21.608: INFO: Resource quota "e2e-rq-status-kd69t" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-kd69t" /status @ 04/23/23 15:10:21.608
  STEP: Confirm /status for "e2e-rq-status-kd69t" resourceQuota via watch @ 04/23/23 15:10:21.629
  Apr 23 15:10:21.634: INFO: observed resourceQuota "e2e-rq-status-kd69t" in namespace "resourcequota-9533" with hard status: v1.ResourceList(nil)
  Apr 23 15:10:21.635: INFO: Found resourceQuota "e2e-rq-status-kd69t" in namespace "resourcequota-9533" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 23 15:10:21.635: INFO: ResourceQuota "e2e-rq-status-kd69t" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/23/23 15:10:21.642
  Apr 23 15:10:21.653: INFO: Resource quota "e2e-rq-status-kd69t" reports spec: hard cpu limit of 1
  Apr 23 15:10:21.653: INFO: Resource quota "e2e-rq-status-kd69t" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-kd69t" /status @ 04/23/23 15:10:21.654
  STEP: Confirm /status for "e2e-rq-status-kd69t" resourceQuota via watch @ 04/23/23 15:10:21.665
  Apr 23 15:10:21.671: INFO: observed resourceQuota "e2e-rq-status-kd69t" in namespace "resourcequota-9533" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 23 15:10:21.671: INFO: Found resourceQuota "e2e-rq-status-kd69t" in namespace "resourcequota-9533" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 23 15:10:21.672: INFO: ResourceQuota "e2e-rq-status-kd69t" /status was patched
  STEP: Get "e2e-rq-status-kd69t" /status @ 04/23/23 15:10:21.672
  Apr 23 15:10:21.685: INFO: Resourcequota "e2e-rq-status-kd69t" reports status: hard cpu of 1
  Apr 23 15:10:21.686: INFO: Resourcequota "e2e-rq-status-kd69t" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-kd69t" /status before checking Spec is unchanged @ 04/23/23 15:10:21.695
  Apr 23 15:10:21.710: INFO: Resourcequota "e2e-rq-status-kd69t" reports status: hard cpu of 2
  Apr 23 15:10:21.711: INFO: Resourcequota "e2e-rq-status-kd69t" reports status: hard memory of 2Gi
  Apr 23 15:10:21.715: INFO: Found resourceQuota "e2e-rq-status-kd69t" in namespace "resourcequota-9533" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Apr 23 15:15:21.733: INFO: ResourceQuota "e2e-rq-status-kd69t" Spec was unchanged and /status reset
  Apr 23 15:15:21.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9533" for this suite. @ 04/23/23 15:15:21.742
• [300.216 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/23/23 15:15:21.756
  Apr 23 15:15:21.756: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 15:15:21.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:15:21.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:15:21.801
  STEP: Creating a pod to test downward api env vars @ 04/23/23 15:15:21.806
  STEP: Saw pod success @ 04/23/23 15:15:25.867
  Apr 23 15:15:25.881: INFO: Trying to get logs from node soodi4ja4shi-3 pod downward-api-f50f77da-b0d6-4626-b2fa-9fe6e86ec2b8 container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 15:15:25.912
  Apr 23 15:15:25.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1301" for this suite. @ 04/23/23 15:15:25.961
• [4.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/23/23 15:15:25.988
  Apr 23 15:15:25.988: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 15:15:25.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:15:26.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:15:26.027
  STEP: Setting up server cert @ 04/23/23 15:15:26.072
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 15:15:26.866
  STEP: Deploying the webhook pod @ 04/23/23 15:15:26.882
  STEP: Wait for the deployment to be ready @ 04/23/23 15:15:26.91
  Apr 23 15:15:26.927: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Apr 23 15:15:28.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 15, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 15, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 15, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 15, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/23/23 15:15:30.967
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 15:15:31.001
  Apr 23 15:15:32.002: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 15:15:32.008: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3172-crds.webhook.example.com via the AdmissionRegistration API @ 04/23/23 15:15:32.526
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/23/23 15:15:32.563
  Apr 23 15:15:34.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5966" for this suite. @ 04/23/23 15:15:35.414
  STEP: Destroying namespace "webhook-markers-7869" for this suite. @ 04/23/23 15:15:35.425
• [9.448 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/23/23 15:15:35.44
  Apr 23 15:15:35.440: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 15:15:35.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:15:35.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:15:35.496
  Apr 23 15:15:35.535: INFO: created pod
  STEP: Saw pod success @ 04/23/23 15:15:39.563
  Apr 23 15:16:09.564: INFO: polling logs
  Apr 23 15:16:09.581: INFO: Pod logs: 
  I0423 15:15:36.550236       1 log.go:198] OK: Got token
  I0423 15:15:36.550379       1 log.go:198] validating with in-cluster discovery
  I0423 15:15:36.551678       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0423 15:15:36.551740       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9639:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682263535, NotBefore:1682262935, IssuedAt:1682262935, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9639", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a7a0cc0c-5b8a-490a-8dfa-7f10f64609b7"}}}
  I0423 15:15:36.580005       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0423 15:15:36.594929       1 log.go:198] OK: Validated signature on JWT
  I0423 15:15:36.595063       1 log.go:198] OK: Got valid claims from token!
  I0423 15:15:36.595106       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9639:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682263535, NotBefore:1682262935, IssuedAt:1682262935, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9639", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a7a0cc0c-5b8a-490a-8dfa-7f10f64609b7"}}}

  Apr 23 15:16:09.586: INFO: completed pod
  Apr 23 15:16:09.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9639" for this suite. @ 04/23/23 15:16:09.608
• [34.181 seconds]
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/23/23 15:16:09.623
  Apr 23 15:16:09.623: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename podtemplate @ 04/23/23 15:16:09.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:16:09.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:16:09.67
  Apr 23 15:16:09.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2842" for this suite. @ 04/23/23 15:16:09.75
• [0.141 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/23/23 15:16:09.765
  Apr 23 15:16:09.765: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 15:16:09.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:16:09.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:16:09.805
  STEP: creating a secret @ 04/23/23 15:16:09.81
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/23/23 15:16:09.82
  STEP: patching the secret @ 04/23/23 15:16:09.827
  STEP: deleting the secret using a LabelSelector @ 04/23/23 15:16:09.847
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/23/23 15:16:09.862
  Apr 23 15:16:09.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9624" for this suite. @ 04/23/23 15:16:09.886
• [0.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/23/23 15:16:09.908
  Apr 23 15:16:09.908: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 15:16:09.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:16:09.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:16:09.953
  STEP: Creating a pod to test env composition @ 04/23/23 15:16:09.959
  STEP: Saw pod success @ 04/23/23 15:16:14.018
  Apr 23 15:16:14.023: INFO: Trying to get logs from node soodi4ja4shi-3 pod var-expansion-2e5151de-95b3-46f4-b9b9-05374ab1af7f container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 15:16:14.035
  Apr 23 15:16:14.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4149" for this suite. @ 04/23/23 15:16:14.068
• [4.171 seconds]
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/23/23 15:16:14.081
  Apr 23 15:16:14.081: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename events @ 04/23/23 15:16:14.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:16:14.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:16:14.138
  STEP: creating a test event @ 04/23/23 15:16:14.142
  STEP: listing events in all namespaces @ 04/23/23 15:16:14.155
  STEP: listing events in test namespace @ 04/23/23 15:16:14.174
  STEP: listing events with field selection filtering on source @ 04/23/23 15:16:14.18
  STEP: listing events with field selection filtering on reportingController @ 04/23/23 15:16:14.186
  STEP: getting the test event @ 04/23/23 15:16:14.192
  STEP: patching the test event @ 04/23/23 15:16:14.197
  STEP: getting the test event @ 04/23/23 15:16:14.221
  STEP: updating the test event @ 04/23/23 15:16:14.228
  STEP: getting the test event @ 04/23/23 15:16:14.244
  STEP: deleting the test event @ 04/23/23 15:16:14.254
  STEP: listing events in all namespaces @ 04/23/23 15:16:14.27
  STEP: listing events in test namespace @ 04/23/23 15:16:14.297
  Apr 23 15:16:14.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4611" for this suite. @ 04/23/23 15:16:14.315
• [0.250 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/23/23 15:16:14.335
  Apr 23 15:16:14.336: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 15:16:14.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:16:14.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:16:14.388
  STEP: Creating a pod to test substitution in container's args @ 04/23/23 15:16:14.395
  STEP: Saw pod success @ 04/23/23 15:16:18.431
  Apr 23 15:16:18.437: INFO: Trying to get logs from node soodi4ja4shi-3 pod var-expansion-70185c5f-9a1c-401a-a40d-7a00a38479de container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 15:16:18.449
  Apr 23 15:16:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5099" for this suite. @ 04/23/23 15:16:18.484
• [4.165 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/23/23 15:16:18.502
  Apr 23 15:16:18.502: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:16:18.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:16:18.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:16:18.561
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/23/23 15:16:18.566
  STEP: Saw pod success @ 04/23/23 15:16:22.621
  Apr 23 15:16:22.626: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-dd77516b-da43-4295-9273-999882881632 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 15:16:22.638
  Apr 23 15:16:22.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7520" for this suite. @ 04/23/23 15:16:22.68
• [4.198 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/23/23 15:16:22.701
  Apr 23 15:16:22.701: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 15:16:22.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:16:22.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:16:22.765
  Apr 23 15:16:22.810: INFO: Got root ca configmap in namespace "svcaccounts-4269"
  Apr 23 15:16:22.828: INFO: Deleted root ca configmap in namespace "svcaccounts-4269"
  STEP: waiting for a new root ca configmap created @ 04/23/23 15:16:23.329
  Apr 23 15:16:23.334: INFO: Recreated root ca configmap in namespace "svcaccounts-4269"
  Apr 23 15:16:23.343: INFO: Updated root ca configmap in namespace "svcaccounts-4269"
  STEP: waiting for the root ca configmap reconciled @ 04/23/23 15:16:23.845
  Apr 23 15:16:23.850: INFO: Reconciled root ca configmap in namespace "svcaccounts-4269"
  Apr 23 15:16:23.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4269" for this suite. @ 04/23/23 15:16:23.858
• [1.168 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/23/23 15:16:23.871
  Apr 23 15:16:23.871: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 15:16:23.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:16:23.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:16:23.914
  STEP: Creating pod busybox-8773f425-f3f8-4203-bc7b-cda24995b311 in namespace container-probe-1395 @ 04/23/23 15:16:23.919
  Apr 23 15:16:25.950: INFO: Started pod busybox-8773f425-f3f8-4203-bc7b-cda24995b311 in namespace container-probe-1395
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 15:16:25.95
  Apr 23 15:16:25.954: INFO: Initial restart count of pod busybox-8773f425-f3f8-4203-bc7b-cda24995b311 is 0
  Apr 23 15:17:16.176: INFO: Restart count of pod container-probe-1395/busybox-8773f425-f3f8-4203-bc7b-cda24995b311 is now 1 (50.221878555s elapsed)
  Apr 23 15:17:16.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 15:17:16.186
  STEP: Destroying namespace "container-probe-1395" for this suite. @ 04/23/23 15:17:16.214
• [52.358 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/23/23 15:17:16.232
  Apr 23 15:17:16.232: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename containers @ 04/23/23 15:17:16.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:17:16.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:17:16.288
  STEP: Creating a pod to test override arguments @ 04/23/23 15:17:16.292
  STEP: Saw pod success @ 04/23/23 15:17:20.339
  Apr 23 15:17:20.344: INFO: Trying to get logs from node soodi4ja4shi-3 pod client-containers-4a3fd655-0cf6-496b-9fc0-ed6ec3cc6424 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 15:17:20.357
  Apr 23 15:17:20.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6569" for this suite. @ 04/23/23 15:17:20.399
• [4.179 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/23/23 15:17:20.413
  Apr 23 15:17:20.413: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-preemption @ 04/23/23 15:17:20.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:17:20.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:17:20.453
  Apr 23 15:17:20.492: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 23 15:18:20.541: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/23/23 15:18:20.548
  Apr 23 15:18:20.548: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/23/23 15:18:20.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:18:20.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:18:20.596
  Apr 23 15:18:20.633: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Apr 23 15:18:20.642: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 23 15:18:20.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 15:18:20.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-3394" for this suite. @ 04/23/23 15:18:20.819
  STEP: Destroying namespace "sched-preemption-1496" for this suite. @ 04/23/23 15:18:20.836
• [60.435 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/23/23 15:18:20.853
  Apr 23 15:18:20.853: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 15:18:20.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:18:20.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:18:20.895
  STEP: fetching the /apis discovery document @ 04/23/23 15:18:20.899
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/23/23 15:18:20.901
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/23/23 15:18:20.901
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/23/23 15:18:20.901
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/23/23 15:18:20.903
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/23/23 15:18:20.903
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/23/23 15:18:20.905
  Apr 23 15:18:20.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6873" for this suite. @ 04/23/23 15:18:20.914
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/23/23 15:18:20.933
  Apr 23 15:18:20.933: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 15:18:20.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:18:20.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:18:20.976
  STEP: Creating a test namespace @ 04/23/23 15:18:20.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:18:21.011
  STEP: Creating a service in the namespace @ 04/23/23 15:18:21.018
  STEP: Deleting the namespace @ 04/23/23 15:18:21.059
  STEP: Waiting for the namespace to be removed. @ 04/23/23 15:18:21.087
  STEP: Recreating the namespace @ 04/23/23 15:18:28.097
  STEP: Verifying there is no service in the namespace @ 04/23/23 15:18:28.132
  Apr 23 15:18:28.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6817" for this suite. @ 04/23/23 15:18:28.147
  STEP: Destroying namespace "nsdeletetest-6422" for this suite. @ 04/23/23 15:18:28.159
  Apr 23 15:18:28.166: INFO: Namespace nsdeletetest-6422 was already deleted
  STEP: Destroying namespace "nsdeletetest-4173" for this suite. @ 04/23/23 15:18:28.166
• [7.247 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/23/23 15:18:28.182
  Apr 23 15:18:28.182: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename ingress @ 04/23/23 15:18:28.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:18:28.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:18:28.216
  STEP: getting /apis @ 04/23/23 15:18:28.223
  STEP: getting /apis/networking.k8s.io @ 04/23/23 15:18:28.235
  STEP: getting /apis/networking.k8s.iov1 @ 04/23/23 15:18:28.238
  STEP: creating @ 04/23/23 15:18:28.239
  STEP: getting @ 04/23/23 15:18:28.275
  STEP: listing @ 04/23/23 15:18:28.283
  STEP: watching @ 04/23/23 15:18:28.29
  Apr 23 15:18:28.290: INFO: starting watch
  STEP: cluster-wide listing @ 04/23/23 15:18:28.292
  STEP: cluster-wide watching @ 04/23/23 15:18:28.31
  Apr 23 15:18:28.310: INFO: starting watch
  STEP: patching @ 04/23/23 15:18:28.313
  STEP: updating @ 04/23/23 15:18:28.34
  Apr 23 15:18:28.369: INFO: waiting for watch events with expected annotations
  Apr 23 15:18:28.369: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/23/23 15:18:28.37
  STEP: updating /status @ 04/23/23 15:18:28.387
  STEP: get /status @ 04/23/23 15:18:28.407
  STEP: deleting @ 04/23/23 15:18:28.418
  STEP: deleting a collection @ 04/23/23 15:18:28.445
  Apr 23 15:18:28.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9557" for this suite. @ 04/23/23 15:18:28.499
• [0.331 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/23/23 15:18:28.52
  Apr 23 15:18:28.520: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename subpath @ 04/23/23 15:18:28.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:18:28.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:18:28.566
  STEP: Setting up data @ 04/23/23 15:18:28.57
  STEP: Creating pod pod-subpath-test-downwardapi-8v5l @ 04/23/23 15:18:28.593
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 15:18:28.593
  STEP: Saw pod success @ 04/23/23 15:18:52.756
  Apr 23 15:18:52.765: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-subpath-test-downwardapi-8v5l container test-container-subpath-downwardapi-8v5l: <nil>
  STEP: delete the pod @ 04/23/23 15:18:52.815
  STEP: Deleting pod pod-subpath-test-downwardapi-8v5l @ 04/23/23 15:18:52.85
  Apr 23 15:18:52.850: INFO: Deleting pod "pod-subpath-test-downwardapi-8v5l" in namespace "subpath-8210"
  Apr 23 15:18:52.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8210" for this suite. @ 04/23/23 15:18:52.864
• [24.359 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/23/23 15:18:52.881
  Apr 23 15:18:52.881: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 15:18:52.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:18:52.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:18:52.923
  STEP: Creating pod test-webserver-d4ede6e8-0891-4923-9ac9-88fcffd20577 in namespace container-probe-8422 @ 04/23/23 15:18:52.929
  Apr 23 15:18:54.963: INFO: Started pod test-webserver-d4ede6e8-0891-4923-9ac9-88fcffd20577 in namespace container-probe-8422
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 15:18:54.963
  Apr 23 15:18:54.971: INFO: Initial restart count of pod test-webserver-d4ede6e8-0891-4923-9ac9-88fcffd20577 is 0
  Apr 23 15:22:56.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 15:22:56.183
  STEP: Destroying namespace "container-probe-8422" for this suite. @ 04/23/23 15:22:56.225
• [243.360 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/23/23 15:22:56.243
  Apr 23 15:22:56.243: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename disruption @ 04/23/23 15:22:56.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:22:56.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:22:56.293
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/23/23 15:22:56.298
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:22:56.309
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/23/23 15:22:58.343
  STEP: Waiting for all pods to be running @ 04/23/23 15:22:58.344
  Apr 23 15:22:58.358: INFO: pods: 0 < 3
  STEP: locating a running pod @ 04/23/23 15:23:00.368
  STEP: Updating the pdb to allow a pod to be evicted @ 04/23/23 15:23:00.385
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:23:00.399
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/23/23 15:23:02.414
  STEP: Waiting for all pods to be running @ 04/23/23 15:23:02.415
  STEP: Waiting for the pdb to observed all healthy pods @ 04/23/23 15:23:02.422
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/23/23 15:23:02.471
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:23:02.527
  STEP: Waiting for all pods to be running @ 04/23/23 15:23:04.544
  STEP: locating a running pod @ 04/23/23 15:23:04.554
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/23/23 15:23:04.573
  STEP: Waiting for the pdb to be deleted @ 04/23/23 15:23:04.586
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/23/23 15:23:04.596
  STEP: Waiting for all pods to be running @ 04/23/23 15:23:04.596
  Apr 23 15:23:04.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8759" for this suite. @ 04/23/23 15:23:04.668
• [8.494 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/23/23 15:23:04.743
  Apr 23 15:23:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename deployment @ 04/23/23 15:23:04.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:23:04.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:23:04.79
  STEP: creating a Deployment @ 04/23/23 15:23:04.802
  STEP: waiting for Deployment to be created @ 04/23/23 15:23:04.817
  STEP: waiting for all Replicas to be Ready @ 04/23/23 15:23:04.82
  Apr 23 15:23:04.822: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 15:23:04.822: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 15:23:04.845: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 15:23:04.845: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 15:23:04.888: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 15:23:04.888: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 15:23:05.003: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 15:23:05.004: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 15:23:06.178: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 23 15:23:06.178: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 23 15:23:06.808: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/23/23 15:23:06.808
  W0423 15:23:06.825917      15 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 23 15:23:06.830: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/23/23 15:23:06.83
  Apr 23 15:23:06.833: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0
  Apr 23 15:23:06.833: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0
  Apr 23 15:23:06.833: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0
  Apr 23 15:23:06.833: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0
  Apr 23 15:23:06.834: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0
  Apr 23 15:23:06.838: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0
  Apr 23 15:23:06.838: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0
  Apr 23 15:23:06.838: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 0
  Apr 23 15:23:06.839: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:06.839: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:06.840: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:06.840: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:06.840: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:06.840: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:06.849: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:06.849: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:06.889: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:06.889: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:06.918: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:06.919: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:06.945: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:06.945: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:07.822: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:07.822: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:07.921: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  STEP: listing Deployments @ 04/23/23 15:23:07.922
  Apr 23 15:23:07.930: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/23/23 15:23:07.93
  Apr 23 15:23:07.952: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/23/23 15:23:07.952
  Apr 23 15:23:07.979: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:07.992: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:08.052: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:08.108: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:08.141: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:09.710: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:09.872: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:10.010: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:10.063: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 15:23:11.283: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/23/23 15:23:11.365
  STEP: fetching the DeploymentStatus @ 04/23/23 15:23:11.392
  Apr 23 15:23:11.410: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:11.411: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:11.411: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:11.412: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:11.412: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 1
  Apr 23 15:23:11.413: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:11.414: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 3
  Apr 23 15:23:11.414: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 3
  Apr 23 15:23:11.415: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 2
  Apr 23 15:23:11.416: INFO: observed Deployment test-deployment in namespace deployment-2584 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/23/23 15:23:11.416
  Apr 23 15:23:11.443: INFO: observed event type MODIFIED
  Apr 23 15:23:11.444: INFO: observed event type MODIFIED
  Apr 23 15:23:11.445: INFO: observed event type MODIFIED
  Apr 23 15:23:11.445: INFO: observed event type MODIFIED
  Apr 23 15:23:11.445: INFO: observed event type MODIFIED
  Apr 23 15:23:11.446: INFO: observed event type MODIFIED
  Apr 23 15:23:11.446: INFO: observed event type MODIFIED
  Apr 23 15:23:11.447: INFO: observed event type MODIFIED
  Apr 23 15:23:11.447: INFO: observed event type MODIFIED
  Apr 23 15:23:11.447: INFO: observed event type MODIFIED
  Apr 23 15:23:11.447: INFO: observed event type MODIFIED
  Apr 23 15:23:11.458: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 23 15:23:11.481: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-2584  03b5dcbb-a02a-43f1-8c0e-28c036c53d14 8490 3 2023-04-23 15:23:04 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cd0ab648-c424-4435-a95e-312aa2f3d7d8 0xc001e9a8b7 0xc001e9a8b8}] [] [{kube-controller-manager Update apps/v1 2023-04-23 15:23:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0ab648-c424-4435-a95e-312aa2f3d7d8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:23:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e9a940 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 23 15:23:11.492: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-2584  d7c1dbcd-6f36-4b88-aa30-0e276742e4f4 8617 4 2023-04-23 15:23:06 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cd0ab648-c424-4435-a95e-312aa2f3d7d8 0xc001e9a9a7 0xc001e9a9a8}] [] [{kube-controller-manager Update apps/v1 2023-04-23 15:23:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0ab648-c424-4435-a95e-312aa2f3d7d8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:23:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e9aa30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 23 15:23:11.499: INFO: pod: "test-deployment-5b5dcbcd95-f2p7v":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-f2p7v test-deployment-5b5dcbcd95- deployment-2584  72c8e037-bd93-4e00-991a-85d8ec60f152 8612 0 2023-04-23 15:23:07 +0000 UTC 2023-04-23 15:23:12 +0000 UTC 0xc001e9adc0 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 d7c1dbcd-6f36-4b88-aa30-0e276742e4f4 0xc001e9adf7 0xc001e9adf8}] [] [{kube-controller-manager Update v1 2023-04-23 15:23:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7c1dbcd-6f36-4b88-aa30-0e276742e4f4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 15:23:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftw4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftw4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:10.233.65.138,StartTime:2023-04-23 15:23:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 15:23:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://77f7b9332752ed9bc0d5bc5a10de04ea2c04b99cc42b6132ad32b50e0380682b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.138,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 23 15:23:11.500: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-2584  35a25028-49cf-484d-a60c-dbd5de2e4165 8607 2 2023-04-23 15:23:07 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cd0ab648-c424-4435-a95e-312aa2f3d7d8 0xc001e9aa97 0xc001e9aa98}] [] [{kube-controller-manager Update apps/v1 2023-04-23 15:23:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd0ab648-c424-4435-a95e-312aa2f3d7d8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:23:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e9ab20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Apr 23 15:23:11.512: INFO: pod: "test-deployment-6fc78d85c6-dlxdk":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-dlxdk test-deployment-6fc78d85c6- deployment-2584  b1bd7d49-ab0b-46cb-9a4f-c69043b95247 8606 0 2023-04-23 15:23:09 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 35a25028-49cf-484d-a60c-dbd5de2e4165 0xc00190fba7 0xc00190fba8}] [] [{kube-controller-manager Update v1 2023-04-23 15:23:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35a25028-49cf-484d-a60c-dbd5de2e4165\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 15:23:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52vhf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52vhf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:10.233.64.9,StartTime:2023-04-23 15:23:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 15:23:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2621eb200eee51f8ba3aa65e02564606183e00f4d9e5a74b575a17bec7b94168,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.9,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 23 15:23:11.513: INFO: pod: "test-deployment-6fc78d85c6-sqnvb":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-sqnvb test-deployment-6fc78d85c6- deployment-2584  47ad5c0a-d8f4-4ad0-83f9-c8c122214a13 8536 0 2023-04-23 15:23:08 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 35a25028-49cf-484d-a60c-dbd5de2e4165 0xc00190fd97 0xc00190fd98}] [] [{kube-controller-manager Update v1 2023-04-23 15:23:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35a25028-49cf-484d-a60c-dbd5de2e4165\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 15:23:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcmjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcmjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:23:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.66.151,StartTime:2023-04-23 15:23:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 15:23:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e8d100c93d912ed8a04f1230f307da813da915652a6d00526a915bef0d0678f4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.151,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 23 15:23:11.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2584" for this suite. @ 04/23/23 15:23:11.525
• [6.810 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/23/23 15:23:11.572
  Apr 23 15:23:11.572: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename subpath @ 04/23/23 15:23:11.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:23:11.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:23:11.621
  STEP: Setting up data @ 04/23/23 15:23:11.625
  STEP: Creating pod pod-subpath-test-projected-sb5g @ 04/23/23 15:23:11.641
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 15:23:11.641
  STEP: Saw pod success @ 04/23/23 15:23:35.783
  Apr 23 15:23:35.789: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-subpath-test-projected-sb5g container test-container-subpath-projected-sb5g: <nil>
  STEP: delete the pod @ 04/23/23 15:23:35.823
  STEP: Deleting pod pod-subpath-test-projected-sb5g @ 04/23/23 15:23:35.852
  Apr 23 15:23:35.852: INFO: Deleting pod "pod-subpath-test-projected-sb5g" in namespace "subpath-7631"
  Apr 23 15:23:35.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7631" for this suite. @ 04/23/23 15:23:35.866
• [24.306 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 04/23/23 15:23:35.886
  Apr 23 15:23:35.886: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 15:23:35.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:23:35.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:23:35.928
  Apr 23 15:23:35.955: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  W0423 15:23:35.958899      15 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc001087cd0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0423 15:23:38.772211      15 warnings.go:70] unknown field "alpha"
  W0423 15:23:38.772289      15 warnings.go:70] unknown field "beta"
  W0423 15:23:38.772359      15 warnings.go:70] unknown field "delta"
  W0423 15:23:38.772370      15 warnings.go:70] unknown field "epsilon"
  W0423 15:23:38.772427      15 warnings.go:70] unknown field "gamma"
  Apr 23 15:23:38.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8852" for this suite. @ 04/23/23 15:23:38.84
• [2.967 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/23/23 15:23:38.855
  Apr 23 15:23:38.855: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename dns @ 04/23/23 15:23:38.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:23:38.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:23:38.897
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9404.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9404.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/23/23 15:23:38.901
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9404.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9404.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/23/23 15:23:38.901
  STEP: creating a pod to probe /etc/hosts @ 04/23/23 15:23:38.901
  STEP: submitting the pod to kubernetes @ 04/23/23 15:23:38.901
  STEP: retrieving the pod @ 04/23/23 15:23:42.948
  STEP: looking for the results for each expected name from probers @ 04/23/23 15:23:42.954
  Apr 23 15:23:42.984: INFO: DNS probes using dns-9404/dns-test-f7b0aa30-7013-4ea2-9ce0-baa58e5124a7 succeeded

  Apr 23 15:23:42.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 15:23:42.992
  STEP: Destroying namespace "dns-9404" for this suite. @ 04/23/23 15:23:43.02
• [4.193 seconds]
------------------------------
SS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/23/23 15:23:43.05
  Apr 23 15:23:43.051: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 15:23:43.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:23:43.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:23:43.142
  Apr 23 15:23:43.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9478" for this suite. @ 04/23/23 15:23:43.18
• [0.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/23/23 15:23:43.198
  Apr 23 15:23:43.198: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 15:23:43.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:23:43.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:23:43.25
  STEP: Creating secret with name secret-test-map-d43d85d6-6b06-4cc3-8a6b-8d4b72e44188 @ 04/23/23 15:23:43.258
  STEP: Creating a pod to test consume secrets @ 04/23/23 15:23:43.268
  STEP: Saw pod success @ 04/23/23 15:23:47.311
  Apr 23 15:23:47.318: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-secrets-d0b6532d-5562-4e59-be0f-a01486d72f9b container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 15:23:47.335
  Apr 23 15:23:47.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7080" for this suite. @ 04/23/23 15:23:47.378
• [4.199 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/23/23 15:23:47.4
  Apr 23 15:23:47.400: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 15:23:47.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:23:47.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:23:47.447
  STEP: Creating a ForbidConcurrent cronjob @ 04/23/23 15:23:47.452
  STEP: Ensuring a job is scheduled @ 04/23/23 15:23:47.466
  STEP: Ensuring exactly one is scheduled @ 04/23/23 15:24:01.477
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/23/23 15:24:01.482
  STEP: Ensuring no more jobs are scheduled @ 04/23/23 15:24:01.492
  STEP: Removing cronjob @ 04/23/23 15:29:01.507
  Apr 23 15:29:01.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6894" for this suite. @ 04/23/23 15:29:01.544
• [314.171 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/23/23 15:29:01.577
  Apr 23 15:29:01.577: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:29:01.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:29:01.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:29:01.631
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/23/23 15:29:01.64
  STEP: Saw pod success @ 04/23/23 15:29:05.679
  Apr 23 15:29:05.685: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-49b1ece1-ab9b-44b0-98aa-318d53f32d0c container test-container: <nil>
  STEP: delete the pod @ 04/23/23 15:29:05.72
  Apr 23 15:29:05.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3760" for this suite. @ 04/23/23 15:29:05.756
• [4.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/23/23 15:29:05.773
  Apr 23 15:29:05.773: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 15:29:05.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:29:05.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:29:05.816
  STEP: Creating a pod to test downward api env vars @ 04/23/23 15:29:05.821
  STEP: Saw pod success @ 04/23/23 15:29:09.865
  Apr 23 15:29:09.878: INFO: Trying to get logs from node soodi4ja4shi-3 pod downward-api-a4d75805-b1d0-407c-a82f-ce37e09c4a7e container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 15:29:09.894
  Apr 23 15:29:09.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4934" for this suite. @ 04/23/23 15:29:09.934
• [4.174 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/23/23 15:29:09.95
  Apr 23 15:29:09.950: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename gc @ 04/23/23 15:29:09.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:29:09.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:29:09.989
  Apr 23 15:29:10.053: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9e4da8fc-b73e-442a-8e54-7a3d82af5a1e", Controller:(*bool)(0xc00173c1ce), BlockOwnerDeletion:(*bool)(0xc00173c1cf)}}
  Apr 23 15:29:10.101: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c24a98ac-3614-45c8-9172-41d64a477904", Controller:(*bool)(0xc002b766fe), BlockOwnerDeletion:(*bool)(0xc002b766ff)}}
  Apr 23 15:29:10.112: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d68f90fd-8252-43ae-a8fb-9f373d62cd2d", Controller:(*bool)(0xc002b76926), BlockOwnerDeletion:(*bool)(0xc002b76927)}}
  Apr 23 15:29:15.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-572" for this suite. @ 04/23/23 15:29:15.14
• [5.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/23/23 15:29:15.157
  Apr 23 15:29:15.157: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 15:29:15.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:29:15.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:29:15.199
  STEP: creating a collection of services @ 04/23/23 15:29:15.204
  Apr 23 15:29:15.205: INFO: Creating e2e-svc-a-hzcbj
  Apr 23 15:29:15.229: INFO: Creating e2e-svc-b-v4zm2
  Apr 23 15:29:15.250: INFO: Creating e2e-svc-c-px9v2
  STEP: deleting service collection @ 04/23/23 15:29:15.32
  Apr 23 15:29:15.404: INFO: Collection of services has been deleted
  Apr 23 15:29:15.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9873" for this suite. @ 04/23/23 15:29:15.416
• [0.271 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 04/23/23 15:29:15.432
  Apr 23 15:29:15.432: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 15:29:15.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:29:15.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:29:15.485
  STEP: Creating service test in namespace statefulset-4344 @ 04/23/23 15:29:15.489
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/23/23 15:29:15.5
  STEP: Creating stateful set ss in namespace statefulset-4344 @ 04/23/23 15:29:15.512
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4344 @ 04/23/23 15:29:15.531
  Apr 23 15:29:15.539: INFO: Found 0 stateful pods, waiting for 1
  Apr 23 15:29:25.563: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/23/23 15:29:25.564
  Apr 23 15:29:25.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-4344 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 15:29:25.853: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 15:29:25.853: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 15:29:25.853: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 15:29:25.859: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 23 15:29:35.870: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 15:29:35.870: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:29:35.899: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999957s
  Apr 23 15:29:36.908: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993590688s
  Apr 23 15:29:37.919: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984749912s
  Apr 23 15:29:38.927: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973887511s
  Apr 23 15:29:39.934: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.966418173s
  Apr 23 15:29:40.941: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.958958739s
  Apr 23 15:29:41.949: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951861732s
  Apr 23 15:29:42.959: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.943442524s
  Apr 23 15:29:43.968: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.933277376s
  Apr 23 15:29:44.976: INFO: Verifying statefulset ss doesn't scale past 1 for another 925.337568ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4344 @ 04/23/23 15:29:45.976
  Apr 23 15:29:45.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-4344 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 15:29:46.251: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 15:29:46.251: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 15:29:46.251: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 15:29:46.259: INFO: Found 1 stateful pods, waiting for 3
  Apr 23 15:29:56.269: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:29:56.270: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:29:56.271: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/23/23 15:29:56.271
  STEP: Scale down will halt with unhealthy stateful pod @ 04/23/23 15:29:56.271
  Apr 23 15:29:56.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-4344 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 15:29:56.526: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 15:29:56.526: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 15:29:56.526: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 15:29:56.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-4344 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 15:29:56.778: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 15:29:56.778: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 15:29:56.778: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 15:29:56.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-4344 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 15:29:57.141: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 15:29:57.141: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 15:29:57.141: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 15:29:57.142: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:29:57.148: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Apr 23 15:30:07.162: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 15:30:07.162: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 15:30:07.162: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 15:30:07.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999237s
  Apr 23 15:30:08.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990283926s
  Apr 23 15:30:09.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980500994s
  Apr 23 15:30:10.220: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970757261s
  Apr 23 15:30:11.236: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.960060735s
  Apr 23 15:30:12.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.943726823s
  Apr 23 15:30:13.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.931791488s
  Apr 23 15:30:14.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.922856362s
  Apr 23 15:30:15.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.913969734s
  Apr 23 15:30:16.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 902.794797ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4344 @ 04/23/23 15:30:17.291
  Apr 23 15:30:17.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-4344 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 15:30:17.580: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 15:30:17.581: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 15:30:17.581: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 15:30:17.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-4344 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 15:30:17.858: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 15:30:17.858: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 15:30:17.858: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 15:30:17.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-4344 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 15:30:18.125: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 15:30:18.125: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 15:30:18.125: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 15:30:18.125: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/23/23 15:30:28.161
  Apr 23 15:30:28.162: INFO: Deleting all statefulset in ns statefulset-4344
  Apr 23 15:30:28.168: INFO: Scaling statefulset ss to 0
  Apr 23 15:30:28.198: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:30:28.203: INFO: Deleting statefulset ss
  Apr 23 15:30:28.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4344" for this suite. @ 04/23/23 15:30:28.236
• [72.824 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/23/23 15:30:28.261
  Apr 23 15:30:28.261: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 15:30:28.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:30:28.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:30:28.32
  STEP: creating secret secrets-5108/secret-test-49e45f02-8fe4-4145-8c66-3e9569fd604f @ 04/23/23 15:30:28.326
  STEP: Creating a pod to test consume secrets @ 04/23/23 15:30:28.338
  STEP: Saw pod success @ 04/23/23 15:30:32.386
  Apr 23 15:30:32.393: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-941bb087-95fb-4ed7-a051-4587cc831a7a container env-test: <nil>
  STEP: delete the pod @ 04/23/23 15:30:32.411
  Apr 23 15:30:32.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5108" for this suite. @ 04/23/23 15:30:32.446
• [4.198 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/23/23 15:30:32.461
  Apr 23 15:30:32.461: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:30:32.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:30:32.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:30:32.499
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/23/23 15:30:32.504
  STEP: Saw pod success @ 04/23/23 15:30:36.547
  Apr 23 15:30:36.554: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-244f804a-dc1f-4c77-b681-dcdbd3c92b62 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 15:30:36.57
  Apr 23 15:30:36.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1521" for this suite. @ 04/23/23 15:30:36.605
• [4.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/23/23 15:30:36.634
  Apr 23 15:30:36.635: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename job @ 04/23/23 15:30:36.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:30:36.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:30:36.666
  STEP: Creating a job @ 04/23/23 15:30:36.671
  STEP: Ensuring active pods == parallelism @ 04/23/23 15:30:36.68
  STEP: Orphaning one of the Job's Pods @ 04/23/23 15:30:40.692
  Apr 23 15:30:41.233: INFO: Successfully updated pod "adopt-release-dxlc9"
  STEP: Checking that the Job readopts the Pod @ 04/23/23 15:30:41.233
  STEP: Removing the labels from the Job's Pod @ 04/23/23 15:30:43.246
  Apr 23 15:30:43.768: INFO: Successfully updated pod "adopt-release-dxlc9"
  STEP: Checking that the Job releases the Pod @ 04/23/23 15:30:43.768
  Apr 23 15:30:45.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-574" for this suite. @ 04/23/23 15:30:45.788
• [9.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 04/23/23 15:30:45.81
  Apr 23 15:30:45.810: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 15:30:45.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:30:45.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:30:45.843
  STEP: Creating simple DaemonSet "daemon-set" @ 04/23/23 15:30:45.895
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 15:30:45.907
  Apr 23 15:30:45.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:30:45.924: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:30:46.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:30:46.945: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:30:47.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:30:47.943: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:30:48.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:30:48.947: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/23/23 15:30:48.98
  STEP: DeleteCollection of the DaemonSets @ 04/23/23 15:30:48.994
  STEP: Verify that ReplicaSets have been deleted @ 04/23/23 15:30:49.03
  Apr 23 15:30:49.100: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10393"},"items":null}

  Apr 23 15:30:49.109: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10394"},"items":[{"metadata":{"name":"daemon-set-6cflv","generateName":"daemon-set-","namespace":"daemonsets-362","uid":"76d3ec21-20a2-467c-9a11-e68e01013aa2","resourceVersion":"10389","creationTimestamp":"2023-04-23T15:30:45Z","deletionTimestamp":"2023-04-23T15:31:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"423a9d12-7e8e-4efa-be3b-95a5eb3b978f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-23T15:30:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"423a9d12-7e8e-4efa-be3b-95a5eb3b978f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-23T15:30:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-t7chk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-t7chk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"soodi4ja4shi-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["soodi4ja4shi-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:45Z"}],"hostIP":"192.168.121.241","podIP":"10.233.64.112","podIPs":[{"ip":"10.233.64.112"}],"startTime":"2023-04-23T15:30:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-23T15:30:47Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://ef88578cb3ae4e10b9b9590260cba27c4bab0f4bf64909c0eee9cbd844ce08e8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gm9pj","generateName":"daemon-set-","namespace":"daemonsets-362","uid":"5be36e6d-f506-4df2-a852-adbf88ddcb2e","resourceVersion":"10390","creationTimestamp":"2023-04-23T15:30:45Z","deletionTimestamp":"2023-04-23T15:31:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"423a9d12-7e8e-4efa-be3b-95a5eb3b978f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-23T15:30:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"423a9d12-7e8e-4efa-be3b-95a5eb3b978f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-23T15:30:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-frwbq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-frwbq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"soodi4ja4shi-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["soodi4ja4shi-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:46Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:45Z"}],"hostIP":"192.168.121.96","podIP":"10.233.66.165","podIPs":[{"ip":"10.233.66.165"}],"startTime":"2023-04-23T15:30:46Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-23T15:30:47Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://6d6bf8e7f83e12aa4050ebbfbde9c904422136d1a9ac68582f97e0d4e2d75fde","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pdw2q","generateName":"daemon-set-","namespace":"daemonsets-362","uid":"a575b365-460c-49c9-bdfe-1a57c2460c46","resourceVersion":"10392","creationTimestamp":"2023-04-23T15:30:45Z","deletionTimestamp":"2023-04-23T15:31:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"423a9d12-7e8e-4efa-be3b-95a5eb3b978f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-23T15:30:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"423a9d12-7e8e-4efa-be3b-95a5eb3b978f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-23T15:30:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hcxpr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hcxpr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"soodi4ja4shi-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["soodi4ja4shi-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T15:30:45Z"}],"hostIP":"192.168.121.106","podIP":"10.233.65.242","podIPs":[{"ip":"10.233.65.242"}],"startTime":"2023-04-23T15:30:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-23T15:30:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://8c79f498f8d5d2be86a8e9845a1117c6c048ba968ffd2025c740068cdf29d828","started":true}],"qosClass":"BestEffort"}}]}

  Apr 23 15:30:49.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-362" for this suite. @ 04/23/23 15:30:49.153
• [3.357 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/23/23 15:30:49.177
  Apr 23 15:30:49.177: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 15:30:49.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:30:49.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:30:49.249
  Apr 23 15:30:49.308: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 23 15:30:54.319: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 15:30:54.32
  STEP: Scaling up "test-rs" replicaset  @ 04/23/23 15:30:54.32
  Apr 23 15:30:54.351: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/23/23 15:30:54.351
  W0423 15:30:54.370975      15 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 23 15:30:54.376: INFO: observed ReplicaSet test-rs in namespace replicaset-3231 with ReadyReplicas 1, AvailableReplicas 1
  Apr 23 15:30:54.423: INFO: observed ReplicaSet test-rs in namespace replicaset-3231 with ReadyReplicas 1, AvailableReplicas 1
  Apr 23 15:30:54.462: INFO: observed ReplicaSet test-rs in namespace replicaset-3231 with ReadyReplicas 1, AvailableReplicas 1
  Apr 23 15:30:54.483: INFO: observed ReplicaSet test-rs in namespace replicaset-3231 with ReadyReplicas 1, AvailableReplicas 1
  Apr 23 15:30:55.641: INFO: observed ReplicaSet test-rs in namespace replicaset-3231 with ReadyReplicas 2, AvailableReplicas 2
  Apr 23 15:30:56.029: INFO: observed Replicaset test-rs in namespace replicaset-3231 with ReadyReplicas 3 found true
  Apr 23 15:30:56.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3231" for this suite. @ 04/23/23 15:30:56.04
• [6.874 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/23/23 15:30:56.054
  Apr 23 15:30:56.055: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 15:30:56.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:30:56.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:30:56.092
  Apr 23 15:30:58.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5459" for this suite. @ 04/23/23 15:30:58.152
• [2.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/23/23 15:30:58.165
  Apr 23 15:30:58.165: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 15:30:58.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:30:58.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:30:58.203
  STEP: creating service endpoint-test2 in namespace services-7010 @ 04/23/23 15:30:58.209
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7010 to expose endpoints map[] @ 04/23/23 15:30:58.228
  Apr 23 15:30:58.243: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Apr 23 15:30:59.268: INFO: successfully validated that service endpoint-test2 in namespace services-7010 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7010 @ 04/23/23 15:30:59.269
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7010 to expose endpoints map[pod1:[80]] @ 04/23/23 15:31:01.335
  Apr 23 15:31:01.377: INFO: successfully validated that service endpoint-test2 in namespace services-7010 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/23/23 15:31:01.377
  Apr 23 15:31:01.377: INFO: Creating new exec pod
  Apr 23 15:31:04.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodzkrn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 23 15:31:04.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 23 15:31:04.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 15:31:04.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodzkrn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.42.130 80'
  Apr 23 15:31:05.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.42.130 80\nConnection to 10.233.42.130 80 port [tcp/http] succeeded!\n"
  Apr 23 15:31:05.149: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-7010 @ 04/23/23 15:31:05.15
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7010 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/23/23 15:31:07.199
  Apr 23 15:31:07.245: INFO: successfully validated that service endpoint-test2 in namespace services-7010 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/23/23 15:31:07.245
  Apr 23 15:31:08.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodzkrn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 23 15:31:08.576: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 23 15:31:08.576: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 15:31:08.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodzkrn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.42.130 80'
  Apr 23 15:31:08.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.42.130 80\nConnection to 10.233.42.130 80 port [tcp/http] succeeded!\n"
  Apr 23 15:31:08.830: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7010 @ 04/23/23 15:31:08.83
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7010 to expose endpoints map[pod2:[80]] @ 04/23/23 15:31:08.905
  Apr 23 15:31:09.978: INFO: successfully validated that service endpoint-test2 in namespace services-7010 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/23/23 15:31:09.979
  Apr 23 15:31:10.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodzkrn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 23 15:31:11.246: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 23 15:31:11.247: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 15:31:11.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodzkrn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.42.130 80'
  Apr 23 15:31:11.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.42.130 80\nConnection to 10.233.42.130 80 port [tcp/http] succeeded!\n"
  Apr 23 15:31:11.455: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-7010 @ 04/23/23 15:31:11.456
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7010 to expose endpoints map[] @ 04/23/23 15:31:11.483
  Apr 23 15:31:12.512: INFO: successfully validated that service endpoint-test2 in namespace services-7010 exposes endpoints map[]
  Apr 23 15:31:12.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7010" for this suite. @ 04/23/23 15:31:12.566
• [14.418 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/23/23 15:31:12.588
  Apr 23 15:31:12.588: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 15:31:12.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:31:12.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:31:12.644
  Apr 23 15:31:12.648: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:31:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6574" for this suite. @ 04/23/23 15:31:13.32
• [0.748 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/23/23 15:31:13.347
  Apr 23 15:31:13.348: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:31:13.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:31:13.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:31:13.401
  STEP: Creating configMap with name projected-configmap-test-volume-map-453f535e-1546-4f76-a007-16ee659e566d @ 04/23/23 15:31:13.409
  STEP: Creating a pod to test consume configMaps @ 04/23/23 15:31:13.419
  STEP: Saw pod success @ 04/23/23 15:31:17.511
  Apr 23 15:31:17.518: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-configmaps-8c60eb5a-4a34-41e6-aebf-8f22c0a8f54f container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 15:31:17.531
  Apr 23 15:31:17.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5988" for this suite. @ 04/23/23 15:31:17.575
• [4.241 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/23/23 15:31:17.589
  Apr 23 15:31:17.589: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 15:31:17.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:31:17.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:31:17.629
  Apr 23 15:31:17.633: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/23/23 15:31:18.675
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/23/23 15:31:18.69
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/23/23 15:31:19.711
  Apr 23 15:31:19.731: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/23/23 15:31:19.734
  Apr 23 15:31:20.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7950" for this suite. @ 04/23/23 15:31:20.765
• [3.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/23/23 15:31:20.803
  Apr 23 15:31:20.803: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 15:31:20.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:31:20.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:31:20.844
  STEP: creating service in namespace services-473 @ 04/23/23 15:31:20.851
  STEP: creating service affinity-nodeport-transition in namespace services-473 @ 04/23/23 15:31:20.851
  STEP: creating replication controller affinity-nodeport-transition in namespace services-473 @ 04/23/23 15:31:20.883
  I0423 15:31:20.903594      15 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-473, replica count: 3
  I0423 15:31:23.954906      15 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 15:31:23.977: INFO: Creating new exec pod
  Apr 23 15:31:27.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-473 exec execpod-affinitybxmjl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 23 15:31:27.306: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 23 15:31:27.306: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 15:31:27.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-473 exec execpod-affinitybxmjl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.22.244 80'
  Apr 23 15:31:27.574: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.22.244 80\nConnection to 10.233.22.244 80 port [tcp/http] succeeded!\n"
  Apr 23 15:31:27.574: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 15:31:27.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-473 exec execpod-affinitybxmjl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.106 31546'
  Apr 23 15:31:27.795: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.106 31546\nConnection to 192.168.121.106 31546 port [tcp/*] succeeded!\n"
  Apr 23 15:31:27.795: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 15:31:27.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-473 exec execpod-affinitybxmjl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.241 31546'
  Apr 23 15:31:28.032: INFO: stderr: "+ nc -v -t -w 2 192.168.121.241 31546\n+ echo hostName\nConnection to 192.168.121.241 31546 port [tcp/*] succeeded!\n"
  Apr 23 15:31:28.032: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 15:31:28.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-473 exec execpod-affinitybxmjl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.241:31546/ ; done'
  Apr 23 15:31:28.471: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n"
  Apr 23 15:31:28.472: INFO: stdout: "\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-wc5m2\naffinity-nodeport-transition-wc5m2\naffinity-nodeport-transition-wc5m2\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-5dx46\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-5dx46\naffinity-nodeport-transition-5dx46\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-5dx46\naffinity-nodeport-transition-5dx46\naffinity-nodeport-transition-5dx46\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-5dx46"
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-wc5m2
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-wc5m2
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-wc5m2
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-5dx46
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-5dx46
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-5dx46
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-5dx46
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-5dx46
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-5dx46
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.472: INFO: Received response from host: affinity-nodeport-transition-5dx46
  Apr 23 15:31:28.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-473 exec execpod-affinitybxmjl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.241:31546/ ; done'
  Apr 23 15:31:28.979: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:31546/\n"
  Apr 23 15:31:28.979: INFO: stdout: "\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c\naffinity-nodeport-transition-tk48c"
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Received response from host: affinity-nodeport-transition-tk48c
  Apr 23 15:31:28.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 15:31:28.987: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-473, will wait for the garbage collector to delete the pods @ 04/23/23 15:31:29.013
  Apr 23 15:31:29.084: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.726401ms
  Apr 23 15:31:29.185: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.249519ms
  STEP: Destroying namespace "services-473" for this suite. @ 04/23/23 15:31:31.438
• [10.648 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/23/23 15:31:31.454
  Apr 23 15:31:31.454: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename gc @ 04/23/23 15:31:31.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:31:31.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:31:31.498
  STEP: create the deployment @ 04/23/23 15:31:31.504
  W0423 15:31:31.514557      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/23/23 15:31:31.514
  STEP: delete the deployment @ 04/23/23 15:31:32.03
  STEP: wait for all rs to be garbage collected @ 04/23/23 15:31:32.042
  STEP: expected 0 rs, got 1 rs @ 04/23/23 15:31:32.052
  STEP: expected 0 pods, got 2 pods @ 04/23/23 15:31:32.063
  STEP: Gathering metrics @ 04/23/23 15:31:32.584
  Apr 23 15:31:32.795: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 15:31:32.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4022" for this suite. @ 04/23/23 15:31:32.813
• [1.380 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/23/23 15:31:32.841
  Apr 23 15:31:32.841: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 15:31:32.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:31:32.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:31:32.884
  STEP: Creating a cronjob @ 04/23/23 15:31:32.889
  STEP: Ensuring more than one job is running at a time @ 04/23/23 15:31:32.9
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/23/23 15:33:00.922
  STEP: Removing cronjob @ 04/23/23 15:33:00.933
  Apr 23 15:33:00.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1466" for this suite. @ 04/23/23 15:33:00.955
• [88.128 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/23/23 15:33:00.972
  Apr 23 15:33:00.972: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:33:00.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:33:01.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:33:01.029
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 15:33:01.035
  STEP: Saw pod success @ 04/23/23 15:33:05.113
  Apr 23 15:33:05.119: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-c4821e6b-162b-4efe-81d8-539ef12970bd container client-container: <nil>
  STEP: delete the pod @ 04/23/23 15:33:05.147
  Apr 23 15:33:05.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1048" for this suite. @ 04/23/23 15:33:05.186
• [4.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/23/23 15:33:05.203
  Apr 23 15:33:05.203: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-watch @ 04/23/23 15:33:05.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:33:05.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:33:05.24
  Apr 23 15:33:05.244: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Creating first CR  @ 04/23/23 15:33:07.917
  Apr 23 15:33:07.928: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T15:33:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T15:33:07Z]] name:name1 resourceVersion:11339 uid:5bf3a4bf-4c88-4371-b679-7c0851b4e365] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 04/23/23 15:33:17.928
  Apr 23 15:33:17.940: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T15:33:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T15:33:17Z]] name:name2 resourceVersion:11370 uid:ecb16268-f36b-4070-a412-2de10999c0c5] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 04/23/23 15:33:27.943
  Apr 23 15:33:27.961: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T15:33:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T15:33:27Z]] name:name1 resourceVersion:11393 uid:5bf3a4bf-4c88-4371-b679-7c0851b4e365] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 04/23/23 15:33:37.961
  Apr 23 15:33:37.977: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T15:33:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T15:33:37Z]] name:name2 resourceVersion:11424 uid:ecb16268-f36b-4070-a412-2de10999c0c5] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 04/23/23 15:33:47.978
  Apr 23 15:33:47.993: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T15:33:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T15:33:27Z]] name:name1 resourceVersion:11449 uid:5bf3a4bf-4c88-4371-b679-7c0851b4e365] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 04/23/23 15:33:57.994
  Apr 23 15:33:58.010: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T15:33:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T15:33:37Z]] name:name2 resourceVersion:11471 uid:ecb16268-f36b-4070-a412-2de10999c0c5] num:map[num1:9223372036854775807 num2:1000000]]}
  Apr 23 15:34:08.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-2571" for this suite. @ 04/23/23 15:34:08.554
• [63.365 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/23/23 15:34:08.571
  Apr 23 15:34:08.571: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 15:34:08.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:34:08.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:34:08.612
  STEP: Creating pod test-grpc-b456e20d-c1c4-4bae-a22e-08fa63642298 in namespace container-probe-1402 @ 04/23/23 15:34:08.617
  Apr 23 15:34:10.652: INFO: Started pod test-grpc-b456e20d-c1c4-4bae-a22e-08fa63642298 in namespace container-probe-1402
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 15:34:10.652
  Apr 23 15:34:10.659: INFO: Initial restart count of pod test-grpc-b456e20d-c1c4-4bae-a22e-08fa63642298 is 0
  Apr 23 15:38:11.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 15:38:11.76
  STEP: Destroying namespace "container-probe-1402" for this suite. @ 04/23/23 15:38:11.781
• [243.230 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/23/23 15:38:11.804
  Apr 23 15:38:11.804: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 15:38:11.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:38:11.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:38:11.855
  Apr 23 15:38:13.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1640" for this suite. @ 04/23/23 15:38:13.921
• [2.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 04/23/23 15:38:13.945
  Apr 23 15:38:13.945: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename aggregator @ 04/23/23 15:38:13.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:38:13.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:38:13.975
  Apr 23 15:38:13.980: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Registering the sample API server. @ 04/23/23 15:38:13.982
  Apr 23 15:38:14.788: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 23 15:38:14.851: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  Apr 23 15:38:16.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:18.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:20.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:22.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:24.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:26.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:28.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:30.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:32.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:34.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:36.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:38.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:38:41.115: INFO: Waited 125.311812ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/23/23 15:38:41.202
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/23/23 15:38:41.208
  STEP: List APIServices @ 04/23/23 15:38:41.238
  Apr 23 15:38:41.257: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/23/23 15:38:41.257
  Apr 23 15:38:41.301: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/23/23 15:38:41.302
  Apr 23 15:38:41.323: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.April, 23, 15, 38, 40, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/23/23 15:38:41.323
  Apr 23 15:38:41.339: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-04-23 15:38:40 +0000 UTC Passed all checks passed}
  Apr 23 15:38:41.340: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 15:38:41.341: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/23/23 15:38:41.341
  Apr 23 15:38:41.385: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1458152988" @ 04/23/23 15:38:41.385
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/23/23 15:38:41.419
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/23/23 15:38:41.437
  STEP: Patch APIService Status @ 04/23/23 15:38:41.444
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/23/23 15:38:41.465
  Apr 23 15:38:41.494: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-04-23 15:38:40 +0000 UTC Passed all checks passed}
  Apr 23 15:38:41.494: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 15:38:41.494: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 23 15:38:41.495: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/23/23 15:38:41.495
  STEP: Confirm that the generated APIService has been deleted @ 04/23/23 15:38:41.509
  Apr 23 15:38:41.509: INFO: Requesting list of APIServices to confirm quantity
  Apr 23 15:38:41.519: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 23 15:38:41.519: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 23 15:38:41.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-751" for this suite. @ 04/23/23 15:38:41.716
• [27.783 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/23/23 15:38:41.731
  Apr 23 15:38:41.731: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 15:38:41.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:38:41.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:38:41.871
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 15:38:41.885
  STEP: Saw pod success @ 04/23/23 15:38:45.945
  Apr 23 15:38:45.953: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-92e229a4-5a6b-4a8b-859f-c857bc29dd44 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 15:38:45.981
  Apr 23 15:38:46.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1374" for this suite. @ 04/23/23 15:38:46.024
• [4.308 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/23/23 15:38:46.045
  Apr 23 15:38:46.045: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:38:46.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:38:46.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:38:46.083
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/23/23 15:38:46.089
  STEP: Saw pod success @ 04/23/23 15:38:50.135
  Apr 23 15:38:50.141: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-7a27842a-a43d-4118-8f16-e482f30189b7 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 15:38:50.152
  Apr 23 15:38:50.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3211" for this suite. @ 04/23/23 15:38:50.195
• [4.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/23/23 15:38:50.218
  Apr 23 15:38:50.218: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename disruption @ 04/23/23 15:38:50.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:38:50.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:38:50.252
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:38:50.264
  STEP: Updating PodDisruptionBudget status @ 04/23/23 15:38:52.276
  STEP: Waiting for all pods to be running @ 04/23/23 15:38:52.292
  Apr 23 15:38:52.298: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 04/23/23 15:38:54.311
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:38:54.329
  STEP: Patching PodDisruptionBudget status @ 04/23/23 15:38:54.364
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:38:54.39
  Apr 23 15:38:54.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9039" for this suite. @ 04/23/23 15:38:54.403
• [4.218 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/23/23 15:38:54.436
  Apr 23 15:38:54.436: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 15:38:54.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:38:54.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:38:54.471
  STEP: creating the pod @ 04/23/23 15:38:54.474
  STEP: submitting the pod to kubernetes @ 04/23/23 15:38:54.475
  STEP: verifying the pod is in kubernetes @ 04/23/23 15:38:56.504
  STEP: updating the pod @ 04/23/23 15:38:56.51
  Apr 23 15:38:57.034: INFO: Successfully updated pod "pod-update-d2b456d6-a9fd-4ad9-8ee4-50c835e63623"
  STEP: verifying the updated pod is in kubernetes @ 04/23/23 15:38:57.051
  Apr 23 15:38:57.062: INFO: Pod update OK
  Apr 23 15:38:57.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8691" for this suite. @ 04/23/23 15:38:57.08
• [2.655 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/23/23 15:38:57.093
  Apr 23 15:38:57.093: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 15:38:57.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:38:57.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:38:57.128
  STEP: Creating secret with name s-test-opt-del-3ae6b9fd-16d0-4210-b15a-eabe0c6dd780 @ 04/23/23 15:38:57.14
  STEP: Creating secret with name s-test-opt-upd-ea8719f0-245b-4657-bf53-a75329c5df2c @ 04/23/23 15:38:57.148
  STEP: Creating the pod @ 04/23/23 15:38:57.157
  STEP: Deleting secret s-test-opt-del-3ae6b9fd-16d0-4210-b15a-eabe0c6dd780 @ 04/23/23 15:39:01.25
  STEP: Updating secret s-test-opt-upd-ea8719f0-245b-4657-bf53-a75329c5df2c @ 04/23/23 15:39:01.268
  STEP: Creating secret with name s-test-opt-create-f5a86660-a371-4036-ac94-f923f8d029e1 @ 04/23/23 15:39:01.279
  STEP: waiting to observe update in volume @ 04/23/23 15:39:01.288
  Apr 23 15:40:16.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1777" for this suite. @ 04/23/23 15:40:16.038
• [78.959 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/23/23 15:40:16.053
  Apr 23 15:40:16.053: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:40:16.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:40:16.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:40:16.099
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 15:40:16.107
  STEP: Saw pod success @ 04/23/23 15:40:20.15
  Apr 23 15:40:20.157: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-7cc9139a-1645-4cb4-8c97-34dbf2068af8 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 15:40:20.169
  Apr 23 15:40:20.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5803" for this suite. @ 04/23/23 15:40:20.214
• [4.171 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/23/23 15:40:20.227
  Apr 23 15:40:20.227: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename deployment @ 04/23/23 15:40:20.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:40:20.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:40:20.27
  STEP: creating a Deployment @ 04/23/23 15:40:20.285
  Apr 23 15:40:20.286: INFO: Creating simple deployment test-deployment-54rjj
  Apr 23 15:40:20.312: INFO: new replicaset for deployment "test-deployment-54rjj" is yet to be created
  STEP: Getting /status @ 04/23/23 15:40:22.34
  Apr 23 15:40:22.354: INFO: Deployment test-deployment-54rjj has Conditions: [{Available True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54rjj-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/23/23 15:40:22.354
  Apr 23 15:40:22.376: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 40, 21, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 40, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 40, 20, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-54rjj-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/23/23 15:40:22.376
  Apr 23 15:40:22.381: INFO: Observed &Deployment event: ADDED
  Apr 23 15:40:22.381: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54rjj-5994cf9475"}
  Apr 23 15:40:22.381: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.382: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54rjj-5994cf9475"}
  Apr 23 15:40:22.382: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 23 15:40:22.382: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.382: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 23 15:40:22.382: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-54rjj-5994cf9475" is progressing.}
  Apr 23 15:40:22.382: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.383: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 23 15:40:22.383: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54rjj-5994cf9475" has successfully progressed.}
  Apr 23 15:40:22.383: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.383: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 23 15:40:22.383: INFO: Observed Deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54rjj-5994cf9475" has successfully progressed.}
  Apr 23 15:40:22.383: INFO: Found Deployment test-deployment-54rjj in namespace deployment-2733 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 15:40:22.383: INFO: Deployment test-deployment-54rjj has an updated status
  STEP: patching the Statefulset Status @ 04/23/23 15:40:22.383
  Apr 23 15:40:22.383: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 23 15:40:22.402: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/23/23 15:40:22.402
  Apr 23 15:40:22.406: INFO: Observed &Deployment event: ADDED
  Apr 23 15:40:22.406: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54rjj-5994cf9475"}
  Apr 23 15:40:22.406: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.406: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-54rjj-5994cf9475"}
  Apr 23 15:40:22.406: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 23 15:40:22.407: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.407: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 23 15:40:22.407: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:20 +0000 UTC 2023-04-23 15:40:20 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-54rjj-5994cf9475" is progressing.}
  Apr 23 15:40:22.407: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.407: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 23 15:40:22.407: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54rjj-5994cf9475" has successfully progressed.}
  Apr 23 15:40:22.408: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.408: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 23 15:40:22.408: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 15:40:21 +0000 UTC 2023-04-23 15:40:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-54rjj-5994cf9475" has successfully progressed.}
  Apr 23 15:40:22.408: INFO: Observed deployment test-deployment-54rjj in namespace deployment-2733 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 15:40:22.408: INFO: Observed &Deployment event: MODIFIED
  Apr 23 15:40:22.408: INFO: Found deployment test-deployment-54rjj in namespace deployment-2733 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 23 15:40:22.408: INFO: Deployment test-deployment-54rjj has a patched status
  Apr 23 15:40:22.423: INFO: Deployment "test-deployment-54rjj":
  &Deployment{ObjectMeta:{test-deployment-54rjj  deployment-2733  269dbcb9-80fd-4f93-9baf-a593fec75829 12671 1 2023-04-23 15:40:20 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-23 15:40:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-23 15:40:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-23 15:40:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e9a688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-54rjj-5994cf9475",LastUpdateTime:2023-04-23 15:40:22 +0000 UTC,LastTransitionTime:2023-04-23 15:40:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 23 15:40:22.431: INFO: New ReplicaSet "test-deployment-54rjj-5994cf9475" of Deployment "test-deployment-54rjj":
  &ReplicaSet{ObjectMeta:{test-deployment-54rjj-5994cf9475  deployment-2733  e9e93710-972f-4c6f-8c6f-38c4fd3e4db3 12660 1 2023-04-23 15:40:20 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-54rjj 269dbcb9-80fd-4f93-9baf-a593fec75829 0xc001e9aa70 0xc001e9aa71}] [] [{kube-controller-manager Update apps/v1 2023-04-23 15:40:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"269dbcb9-80fd-4f93-9baf-a593fec75829\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:40:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e9ab18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 15:40:22.437: INFO: Pod "test-deployment-54rjj-5994cf9475-8rb5b" is available:
  &Pod{ObjectMeta:{test-deployment-54rjj-5994cf9475-8rb5b test-deployment-54rjj-5994cf9475- deployment-2733  5ecdf615-df93-493d-8078-791537e07ea1 12659 0 2023-04-23 15:40:20 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-54rjj-5994cf9475 e9e93710-972f-4c6f-8c6f-38c4fd3e4db3 0xc001e9aeb0 0xc001e9aeb1}] [] [{kube-controller-manager Update v1 2023-04-23 15:40:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9e93710-972f-4c6f-8c6f-38c4fd3e4db3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 15:40:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmcmg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmcmg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:40:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:40:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:40:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:40:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.66.118,StartTime:2023-04-23 15:40:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 15:40:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://308f00feb49c40c9d92caff995e97d19b5b36edcc873f0eb7e7ea2727c1254ae,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.118,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 15:40:22.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2733" for this suite. @ 04/23/23 15:40:22.446
• [2.229 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 04/23/23 15:40:22.457
  Apr 23 15:40:22.457: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 15:40:22.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:40:22.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:40:22.505
  STEP: Creating service test in namespace statefulset-584 @ 04/23/23 15:40:22.509
  STEP: Looking for a node to schedule stateful set and pod @ 04/23/23 15:40:22.526
  STEP: Creating pod with conflicting port in namespace statefulset-584 @ 04/23/23 15:40:22.559
  STEP: Waiting until pod test-pod will start running in namespace statefulset-584 @ 04/23/23 15:40:22.577
  STEP: Creating statefulset with conflicting port in namespace statefulset-584 @ 04/23/23 15:40:24.607
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-584 @ 04/23/23 15:40:24.619
  Apr 23 15:40:24.666: INFO: Observed stateful pod in namespace: statefulset-584, name: ss-0, uid: 8f45500a-d6ba-45df-b71a-a1bf5cd02657, status phase: Pending. Waiting for statefulset controller to delete.
  Apr 23 15:40:24.698: INFO: Observed stateful pod in namespace: statefulset-584, name: ss-0, uid: 8f45500a-d6ba-45df-b71a-a1bf5cd02657, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 23 15:40:24.727: INFO: Observed stateful pod in namespace: statefulset-584, name: ss-0, uid: 8f45500a-d6ba-45df-b71a-a1bf5cd02657, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 23 15:40:24.739: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-584
  STEP: Removing pod with conflicting port in namespace statefulset-584 @ 04/23/23 15:40:24.739
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-584 and will be in running state @ 04/23/23 15:40:24.777
  Apr 23 15:40:40.860: INFO: Deleting all statefulset in ns statefulset-584
  Apr 23 15:40:40.868: INFO: Scaling statefulset ss to 0
  Apr 23 15:40:50.915: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:40:50.920: INFO: Deleting statefulset ss
  Apr 23 15:40:50.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-584" for this suite. @ 04/23/23 15:40:50.979
• [28.544 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/23/23 15:40:51.006
  Apr 23 15:40:51.006: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/23/23 15:40:51.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:40:51.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:40:51.059
  STEP: mirroring a new custom Endpoint @ 04/23/23 15:40:51.098
  Apr 23 15:40:51.121: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 04/23/23 15:40:53.128
  Apr 23 15:40:53.147: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 04/23/23 15:40:55.159
  Apr 23 15:40:55.179: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Apr 23 15:40:57.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-832" for this suite. @ 04/23/23 15:40:57.196
• [6.202 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 04/23/23 15:40:57.21
  Apr 23 15:40:57.210: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 15:40:57.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:40:57.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:40:57.248
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/23/23 15:40:57.293
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 15:40:57.306
  Apr 23 15:40:57.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:40:57.337: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:40:58.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:40:58.372: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:40:59.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:40:59.353: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/23/23 15:40:59.358
  Apr 23 15:40:59.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:40:59.392: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/23/23 15:40:59.392
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 15:41:00.416
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4089, will wait for the garbage collector to delete the pods @ 04/23/23 15:41:00.416
  Apr 23 15:41:00.486: INFO: Deleting DaemonSet.extensions daemon-set took: 11.968939ms
  Apr 23 15:41:00.586: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.790087ms
  Apr 23 15:41:03.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:41:03.495: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 15:41:03.502: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12984"},"items":null}

  Apr 23 15:41:03.510: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12984"},"items":null}

  Apr 23 15:41:03.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4089" for this suite. @ 04/23/23 15:41:03.546
• [6.348 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/23/23 15:41:03.559
  Apr 23 15:41:03.559: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-preemption @ 04/23/23 15:41:03.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:41:03.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:41:03.601
  Apr 23 15:41:03.631: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 23 15:42:03.689: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/23/23 15:42:03.698
  Apr 23 15:42:03.758: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 23 15:42:03.770: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 23 15:42:03.813: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 23 15:42:03.824: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 23 15:42:03.863: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 23 15:42:03.880: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/23/23 15:42:03.88
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/23/23 15:42:07.947
  Apr 23 15:42:12.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-4521" for this suite. @ 04/23/23 15:42:12.153
• [68.614 seconds]
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/23/23 15:42:12.173
  Apr 23 15:42:12.173: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 15:42:12.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:42:12.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:42:12.218
  STEP: Creating configMap with name cm-test-opt-del-52d37164-48e6-40b4-8062-abdaf0284ae8 @ 04/23/23 15:42:12.232
  STEP: Creating configMap with name cm-test-opt-upd-47036633-dd5d-4d64-aedc-1bb18cbdaf3f @ 04/23/23 15:42:12.244
  STEP: Creating the pod @ 04/23/23 15:42:12.253
  STEP: Deleting configmap cm-test-opt-del-52d37164-48e6-40b4-8062-abdaf0284ae8 @ 04/23/23 15:42:14.394
  STEP: Updating configmap cm-test-opt-upd-47036633-dd5d-4d64-aedc-1bb18cbdaf3f @ 04/23/23 15:42:14.407
  STEP: Creating configMap with name cm-test-opt-create-398910ff-4079-4795-ab69-9b3be94884b1 @ 04/23/23 15:42:14.419
  STEP: waiting to observe update in volume @ 04/23/23 15:42:14.432
  Apr 23 15:43:27.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2191" for this suite. @ 04/23/23 15:43:27.331
• [75.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/23/23 15:43:27.369
  Apr 23 15:43:27.370: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename events @ 04/23/23 15:43:27.373
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:43:27.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:43:27.432
  STEP: Create set of events @ 04/23/23 15:43:27.438
  STEP: get a list of Events with a label in the current namespace @ 04/23/23 15:43:27.469
  STEP: delete a list of events @ 04/23/23 15:43:27.478
  Apr 23 15:43:27.478: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/23/23 15:43:27.524
  Apr 23 15:43:27.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3982" for this suite. @ 04/23/23 15:43:27.545
• [0.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/23/23 15:43:27.576
  Apr 23 15:43:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 15:43:27.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:43:27.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:43:27.614
  STEP: Creating configMap with name configmap-test-upd-0992f059-5e49-47e9-b9b0-952c24017904 @ 04/23/23 15:43:27.63
  STEP: Creating the pod @ 04/23/23 15:43:27.641
  STEP: Updating configmap configmap-test-upd-0992f059-5e49-47e9-b9b0-952c24017904 @ 04/23/23 15:43:29.7
  STEP: waiting to observe update in volume @ 04/23/23 15:43:29.709
  Apr 23 15:44:56.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6331" for this suite. @ 04/23/23 15:44:56.56
• [89.001 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/23/23 15:44:56.58
  Apr 23 15:44:56.580: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:44:56.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:44:56.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:44:56.619
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 15:44:56.626
  STEP: Saw pod success @ 04/23/23 15:45:00.687
  Apr 23 15:45:00.699: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-820ae937-45b2-41dc-9e5a-37c8555c3565 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 15:45:00.715
  Apr 23 15:45:00.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5895" for this suite. @ 04/23/23 15:45:00.758
• [4.192 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/23/23 15:45:00.777
  Apr 23 15:45:00.777: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:45:00.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:45:00.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:45:00.829
  STEP: Creating the pod @ 04/23/23 15:45:00.836
  Apr 23 15:45:03.414: INFO: Successfully updated pod "annotationupdatefc3dda09-f659-419a-9645-b57903e95bce"
  Apr 23 15:45:07.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8022" for this suite. @ 04/23/23 15:45:07.478
• [6.719 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/23/23 15:45:07.498
  Apr 23 15:45:07.498: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename gc @ 04/23/23 15:45:07.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:45:07.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:45:07.544
  STEP: create the rc1 @ 04/23/23 15:45:07.561
  STEP: create the rc2 @ 04/23/23 15:45:07.57
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/23/23 15:45:13.966
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/23/23 15:45:22.339
  STEP: wait for the rc to be deleted @ 04/23/23 15:45:22.522
  Apr 23 15:45:28.444: INFO: 93 pods remaining
  Apr 23 15:45:28.444: INFO: 69 pods has nil DeletionTimestamp
  Apr 23 15:45:28.444: INFO: 
  Apr 23 15:45:32.618: INFO: 81 pods remaining
  Apr 23 15:45:32.618: INFO: 50 pods has nil DeletionTimestamp
  Apr 23 15:45:32.618: INFO: 
  STEP: Gathering metrics @ 04/23/23 15:45:37.59
  Apr 23 15:45:37.809: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 15:45:37.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j9h6" in namespace "gc-5341"
  Apr 23 15:45:37.872: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s2kv" in namespace "gc-5341"
  Apr 23 15:45:37.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s99p" in namespace "gc-5341"
  Apr 23 15:45:37.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-2sbb7" in namespace "gc-5341"
  Apr 23 15:45:38.087: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fthp" in namespace "gc-5341"
  Apr 23 15:45:38.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-4g72f" in namespace "gc-5341"
  Apr 23 15:45:38.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nft4" in namespace "gc-5341"
  Apr 23 15:45:38.454: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p8ft" in namespace "gc-5341"
  Apr 23 15:45:38.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-54q4q" in namespace "gc-5341"
  Apr 23 15:45:39.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cflv" in namespace "gc-5341"
  Apr 23 15:45:39.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-64l6k" in namespace "gc-5341"
  Apr 23 15:45:39.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-6626r" in namespace "gc-5341"
  Apr 23 15:45:39.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-69jhs" in namespace "gc-5341"
  Apr 23 15:45:39.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ghfj" in namespace "gc-5341"
  Apr 23 15:45:39.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h8t7" in namespace "gc-5341"
  Apr 23 15:45:39.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k44k" in namespace "gc-5341"
  Apr 23 15:45:39.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nkkj" in namespace "gc-5341"
  Apr 23 15:45:39.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-6t5d6" in namespace "gc-5341"
  Apr 23 15:45:39.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-76gx9" in namespace "gc-5341"
  Apr 23 15:45:40.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rsck" in namespace "gc-5341"
  Apr 23 15:45:40.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-84qjx" in namespace "gc-5341"
  Apr 23 15:45:40.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-84rvm" in namespace "gc-5341"
  Apr 23 15:45:40.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-88nv5" in namespace "gc-5341"
  Apr 23 15:45:40.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-8p2sf" in namespace "gc-5341"
  Apr 23 15:45:40.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rst7" in namespace "gc-5341"
  Apr 23 15:45:41.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vk2c" in namespace "gc-5341"
  Apr 23 15:45:41.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fm4l" in namespace "gc-5341"
  Apr 23 15:45:41.247: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ng7p" in namespace "gc-5341"
  Apr 23 15:45:41.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nzdm" in namespace "gc-5341"
  Apr 23 15:45:41.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qpql" in namespace "gc-5341"
  Apr 23 15:45:41.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdzpl" in namespace "gc-5341"
  Apr 23 15:45:41.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn2mp" in namespace "gc-5341"
  Apr 23 15:45:41.889: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz78z" in namespace "gc-5341"
  Apr 23 15:45:41.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2bnl" in namespace "gc-5341"
  Apr 23 15:45:42.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5hcg" in namespace "gc-5341"
  Apr 23 15:45:42.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7fhb" in namespace "gc-5341"
  Apr 23 15:45:42.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmm5f" in namespace "gc-5341"
  Apr 23 15:45:42.396: INFO: Deleting pod "simpletest-rc-to-be-deleted-d44b4" in namespace "gc-5341"
  Apr 23 15:45:42.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7snx" in namespace "gc-5341"
  Apr 23 15:45:42.801: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgk58" in namespace "gc-5341"
  Apr 23 15:45:43.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-djnjx" in namespace "gc-5341"
  Apr 23 15:45:43.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-dk54p" in namespace "gc-5341"
  Apr 23 15:45:43.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-drsbc" in namespace "gc-5341"
  Apr 23 15:45:43.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwz94" in namespace "gc-5341"
  Apr 23 15:45:43.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6rf8" in namespace "gc-5341"
  Apr 23 15:45:43.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8nmx" in namespace "gc-5341"
  Apr 23 15:45:43.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9k9v" in namespace "gc-5341"
  Apr 23 15:45:43.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbrk4" in namespace "gc-5341"
  Apr 23 15:45:43.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg88h" in namespace "gc-5341"
  Apr 23 15:45:43.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj687" in namespace "gc-5341"
  Apr 23 15:45:43.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5341" for this suite. @ 04/23/23 15:45:43.938
• [36.494 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/23/23 15:45:43.996
  Apr 23 15:45:43.996: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:45:43.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:45:44.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:45:44.058
  STEP: Creating configMap with name projected-configmap-test-volume-map-d3da2eea-70fb-4689-be75-85b5db72d6c4 @ 04/23/23 15:45:44.063
  STEP: Creating a pod to test consume configMaps @ 04/23/23 15:45:44.079
  STEP: Saw pod success @ 04/23/23 15:45:48.17
  Apr 23 15:45:48.211: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-configmaps-31739c33-82b9-47ac-889b-02b6754a2cd3 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 15:45:48.273
  Apr 23 15:45:48.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4724" for this suite. @ 04/23/23 15:45:48.383
• [4.479 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/23/23 15:45:48.48
  Apr 23 15:45:48.480: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 15:45:48.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:45:48.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:45:48.59
  STEP: Creating the pod @ 04/23/23 15:45:48.604
  Apr 23 15:45:51.512: INFO: Successfully updated pod "annotationupdate8d82ac4e-aefb-4ed2-9ac7-47388483f217"
  Apr 23 15:45:53.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1639" for this suite. @ 04/23/23 15:45:53.619
• [5.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/23/23 15:45:53.697
  Apr 23 15:45:53.697: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename events @ 04/23/23 15:45:53.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:45:53.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:45:53.805
  STEP: Create set of events @ 04/23/23 15:45:53.81
  Apr 23 15:45:53.828: INFO: created test-event-1
  Apr 23 15:45:53.847: INFO: created test-event-2
  Apr 23 15:45:53.890: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/23/23 15:45:53.89
  STEP: delete collection of events @ 04/23/23 15:45:53.914
  Apr 23 15:45:53.915: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/23/23 15:45:54.038
  Apr 23 15:45:54.038: INFO: requesting list of events to confirm quantity
  Apr 23 15:45:54.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8160" for this suite. @ 04/23/23 15:45:54.078
• [0.425 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/23/23 15:45:54.123
  Apr 23 15:45:54.123: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 15:45:54.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:45:54.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:45:54.374
  STEP: creating the pod @ 04/23/23 15:45:54.381
  Apr 23 15:45:54.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2807 create -f -'
  Apr 23 15:45:55.612: INFO: stderr: ""
  Apr 23 15:45:55.613: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/23/23 15:45:57.67
  Apr 23 15:45:57.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2807 label pods pause testing-label=testing-label-value'
  Apr 23 15:45:57.853: INFO: stderr: ""
  Apr 23 15:45:57.853: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/23/23 15:45:57.853
  Apr 23 15:45:57.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2807 get pod pause -L testing-label'
  Apr 23 15:45:58.050: INFO: stderr: ""
  Apr 23 15:45:58.050: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/23/23 15:45:58.05
  Apr 23 15:45:58.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2807 label pods pause testing-label-'
  Apr 23 15:45:58.247: INFO: stderr: ""
  Apr 23 15:45:58.247: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/23/23 15:45:58.248
  Apr 23 15:45:58.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2807 get pod pause -L testing-label'
  Apr 23 15:45:58.445: INFO: stderr: ""
  Apr 23 15:45:58.445: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 04/23/23 15:45:58.446
  Apr 23 15:45:58.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2807 delete --grace-period=0 --force -f -'
  Apr 23 15:45:58.986: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 15:45:58.987: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 23 15:45:58.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2807 get rc,svc -l name=pause --no-headers'
  Apr 23 15:45:59.355: INFO: stderr: "No resources found in kubectl-2807 namespace.\n"
  Apr 23 15:45:59.355: INFO: stdout: ""
  Apr 23 15:45:59.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2807 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 23 15:45:59.665: INFO: stderr: ""
  Apr 23 15:45:59.665: INFO: stdout: ""
  Apr 23 15:45:59.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2807" for this suite. @ 04/23/23 15:45:59.775
• [5.688 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/23/23 15:45:59.812
  Apr 23 15:45:59.812: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:45:59.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:45:59.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:45:59.908
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 15:45:59.912
  STEP: Saw pod success @ 04/23/23 15:46:04.012
  Apr 23 15:46:04.022: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-5fdad457-3f29-40d0-aff7-0bbad3b67a22 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 15:46:04.039
  Apr 23 15:46:04.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1773" for this suite. @ 04/23/23 15:46:04.093
• [4.304 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/23/23 15:46:04.123
  Apr 23 15:46:04.123: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 15:46:04.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:46:04.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:46:04.212
  Apr 23 15:46:26.383: INFO: Container started at 2023-04-23 15:46:05 +0000 UTC, pod became ready at 2023-04-23 15:46:24 +0000 UTC
  Apr 23 15:46:26.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3168" for this suite. @ 04/23/23 15:46:26.396
• [22.287 seconds]
------------------------------
S
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/23/23 15:46:26.411
  Apr 23 15:46:26.411: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename limitrange @ 04/23/23 15:46:26.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:46:26.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:46:26.452
  STEP: Creating a LimitRange @ 04/23/23 15:46:26.457
  STEP: Setting up watch @ 04/23/23 15:46:26.457
  STEP: Submitting a LimitRange @ 04/23/23 15:46:26.563
  STEP: Verifying LimitRange creation was observed @ 04/23/23 15:46:26.58
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/23/23 15:46:26.586
  Apr 23 15:46:26.596: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 23 15:46:26.596: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/23/23 15:46:26.597
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/23/23 15:46:26.614
  Apr 23 15:46:26.730: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 23 15:46:26.730: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/23/23 15:46:26.73
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/23/23 15:46:26.76
  Apr 23 15:46:26.771: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 23 15:46:26.771: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/23/23 15:46:26.771
  STEP: Failing to create a Pod with more than max resources @ 04/23/23 15:46:26.776
  STEP: Updating a LimitRange @ 04/23/23 15:46:26.78
  STEP: Verifying LimitRange updating is effective @ 04/23/23 15:46:26.79
  STEP: Creating a Pod with less than former min resources @ 04/23/23 15:46:28.798
  STEP: Failing to create a Pod with more than max resources @ 04/23/23 15:46:28.808
  STEP: Deleting a LimitRange @ 04/23/23 15:46:28.813
  STEP: Verifying the LimitRange was deleted @ 04/23/23 15:46:28.828
  Apr 23 15:46:33.835: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/23/23 15:46:33.836
  Apr 23 15:46:33.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5978" for this suite. @ 04/23/23 15:46:33.866
• [7.478 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/23/23 15:46:33.895
  Apr 23 15:46:33.895: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pod-network-test @ 04/23/23 15:46:33.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:46:33.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:46:33.935
  STEP: Performing setup for networking test in namespace pod-network-test-4282 @ 04/23/23 15:46:33.939
  STEP: creating a selector @ 04/23/23 15:46:33.939
  STEP: Creating the service pods in kubernetes @ 04/23/23 15:46:33.939
  Apr 23 15:46:33.939: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/23/23 15:46:56.199
  Apr 23 15:46:58.237: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 23 15:46:58.237: INFO: Breadth first check of 10.233.64.187 on host 192.168.121.241...
  Apr 23 15:46:58.249: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.214:9080/dial?request=hostname&protocol=http&host=10.233.64.187&port=8083&tries=1'] Namespace:pod-network-test-4282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:46:58.249: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:46:58.252: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:46:58.252: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.214%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.187%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 15:46:58.439: INFO: Waiting for responses: map[]
  Apr 23 15:46:58.439: INFO: reached 10.233.64.187 after 0/1 tries
  Apr 23 15:46:58.439: INFO: Breadth first check of 10.233.65.221 on host 192.168.121.106...
  Apr 23 15:46:58.450: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.214:9080/dial?request=hostname&protocol=http&host=10.233.65.221&port=8083&tries=1'] Namespace:pod-network-test-4282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:46:58.451: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:46:58.453: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:46:58.454: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.214%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.221%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 15:46:58.575: INFO: Waiting for responses: map[]
  Apr 23 15:46:58.576: INFO: reached 10.233.65.221 after 0/1 tries
  Apr 23 15:46:58.576: INFO: Breadth first check of 10.233.66.192 on host 192.168.121.96...
  Apr 23 15:46:58.585: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.214:9080/dial?request=hostname&protocol=http&host=10.233.66.192&port=8083&tries=1'] Namespace:pod-network-test-4282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:46:58.585: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:46:58.587: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:46:58.587: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.214%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.192%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 15:46:58.711: INFO: Waiting for responses: map[]
  Apr 23 15:46:58.712: INFO: reached 10.233.66.192 after 0/1 tries
  Apr 23 15:46:58.712: INFO: Going to retry 0 out of 3 pods....
  Apr 23 15:46:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4282" for this suite. @ 04/23/23 15:46:58.723
• [24.854 seconds]
------------------------------
SSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/23/23 15:46:58.751
  Apr 23 15:46:58.751: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 15:46:58.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:46:58.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:46:58.806
  STEP: Creating projection with secret that has name secret-emptykey-test-9f8f992e-75af-4d43-840a-0771a0fc01b2 @ 04/23/23 15:46:58.81
  Apr 23 15:46:58.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6185" for this suite. @ 04/23/23 15:46:58.827
• [0.091 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/23/23 15:46:58.846
  Apr 23 15:46:58.846: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename subpath @ 04/23/23 15:46:58.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:46:58.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:46:58.899
  STEP: Setting up data @ 04/23/23 15:46:58.906
  STEP: Creating pod pod-subpath-test-configmap-rjx9 @ 04/23/23 15:46:58.928
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 15:46:58.928
  STEP: Saw pod success @ 04/23/23 15:47:23.084
  Apr 23 15:47:23.091: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-subpath-test-configmap-rjx9 container test-container-subpath-configmap-rjx9: <nil>
  STEP: delete the pod @ 04/23/23 15:47:23.112
  STEP: Deleting pod pod-subpath-test-configmap-rjx9 @ 04/23/23 15:47:23.144
  Apr 23 15:47:23.144: INFO: Deleting pod "pod-subpath-test-configmap-rjx9" in namespace "subpath-4664"
  Apr 23 15:47:23.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4664" for this suite. @ 04/23/23 15:47:23.165
• [24.335 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/23/23 15:47:23.193
  Apr 23 15:47:23.193: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/23/23 15:47:23.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:47:23.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:47:23.248
  STEP: Creating 50 configmaps @ 04/23/23 15:47:23.259
  STEP: Creating RC which spawns configmap-volume pods @ 04/23/23 15:47:23.853
  Apr 23 15:47:23.885: INFO: Pod name wrapped-volume-race-dc0b433d-bfcc-4922-91ba-dc1d185d6089: Found 0 pods out of 5
  Apr 23 15:47:28.904: INFO: Pod name wrapped-volume-race-dc0b433d-bfcc-4922-91ba-dc1d185d6089: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/23/23 15:47:28.905
  STEP: Creating RC which spawns configmap-volume pods @ 04/23/23 15:47:28.999
  Apr 23 15:47:29.044: INFO: Pod name wrapped-volume-race-86d998ec-1a3a-4d29-bfd6-0baaa2567100: Found 0 pods out of 5
  Apr 23 15:47:34.076: INFO: Pod name wrapped-volume-race-86d998ec-1a3a-4d29-bfd6-0baaa2567100: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/23/23 15:47:34.077
  STEP: Creating RC which spawns configmap-volume pods @ 04/23/23 15:47:36.155
  Apr 23 15:47:36.190: INFO: Pod name wrapped-volume-race-5dbc125f-d070-4b20-aa25-5d41173d9958: Found 0 pods out of 5
  Apr 23 15:47:41.257: INFO: Pod name wrapped-volume-race-5dbc125f-d070-4b20-aa25-5d41173d9958: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/23/23 15:47:41.257
  Apr 23 15:47:43.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-5dbc125f-d070-4b20-aa25-5d41173d9958 in namespace emptydir-wrapper-6095, will wait for the garbage collector to delete the pods @ 04/23/23 15:47:43.33
  Apr 23 15:47:43.409: INFO: Deleting ReplicationController wrapped-volume-race-5dbc125f-d070-4b20-aa25-5d41173d9958 took: 19.726874ms
  Apr 23 15:47:43.610: INFO: Terminating ReplicationController wrapped-volume-race-5dbc125f-d070-4b20-aa25-5d41173d9958 pods took: 200.944437ms
  STEP: deleting ReplicationController wrapped-volume-race-86d998ec-1a3a-4d29-bfd6-0baaa2567100 in namespace emptydir-wrapper-6095, will wait for the garbage collector to delete the pods @ 04/23/23 15:47:47.312
  Apr 23 15:47:47.401: INFO: Deleting ReplicationController wrapped-volume-race-86d998ec-1a3a-4d29-bfd6-0baaa2567100 took: 17.040244ms
  Apr 23 15:47:47.501: INFO: Terminating ReplicationController wrapped-volume-race-86d998ec-1a3a-4d29-bfd6-0baaa2567100 pods took: 100.540718ms
  STEP: deleting ReplicationController wrapped-volume-race-dc0b433d-bfcc-4922-91ba-dc1d185d6089 in namespace emptydir-wrapper-6095, will wait for the garbage collector to delete the pods @ 04/23/23 15:47:50.203
  Apr 23 15:47:50.283: INFO: Deleting ReplicationController wrapped-volume-race-dc0b433d-bfcc-4922-91ba-dc1d185d6089 took: 19.981362ms
  Apr 23 15:47:50.484: INFO: Terminating ReplicationController wrapped-volume-race-dc0b433d-bfcc-4922-91ba-dc1d185d6089 pods took: 201.072796ms
  STEP: Cleaning up the configMaps @ 04/23/23 15:47:52.587
  STEP: Destroying namespace "emptydir-wrapper-6095" for this suite. @ 04/23/23 15:47:53.372
• [30.204 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/23/23 15:47:53.4
  Apr 23 15:47:53.400: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename job @ 04/23/23 15:47:53.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:47:53.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:47:53.477
  STEP: Creating a job @ 04/23/23 15:47:53.488
  STEP: Ensuring active pods == parallelism @ 04/23/23 15:47:53.504
  STEP: delete a job @ 04/23/23 15:47:55.515
  STEP: deleting Job.batch foo in namespace job-5980, will wait for the garbage collector to delete the pods @ 04/23/23 15:47:55.515
  Apr 23 15:47:55.588: INFO: Deleting Job.batch foo took: 15.167151ms
  Apr 23 15:47:55.689: INFO: Terminating Job.batch foo pods took: 101.177788ms
  STEP: Ensuring job was deleted @ 04/23/23 15:48:27.791
  Apr 23 15:48:27.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5980" for this suite. @ 04/23/23 15:48:27.818
• [34.435 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/23/23 15:48:27.84
  Apr 23 15:48:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 15:48:27.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:48:27.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:48:27.891
  STEP: Creating a pod to test downward api env vars @ 04/23/23 15:48:27.896
  STEP: Saw pod success @ 04/23/23 15:48:31.941
  Apr 23 15:48:31.949: INFO: Trying to get logs from node soodi4ja4shi-3 pod downward-api-f4ae7ed3-c6c8-4c13-83c9-2a911e434b98 container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 15:48:31.965
  Apr 23 15:48:31.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6315" for this suite. @ 04/23/23 15:48:32.004
• [4.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/23/23 15:48:32.031
  Apr 23 15:48:32.031: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 15:48:32.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:48:32.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:48:32.083
  STEP: getting /apis @ 04/23/23 15:48:32.088
  STEP: getting /apis/node.k8s.io @ 04/23/23 15:48:32.098
  STEP: getting /apis/node.k8s.io/v1 @ 04/23/23 15:48:32.1
  STEP: creating @ 04/23/23 15:48:32.103
  STEP: watching @ 04/23/23 15:48:32.142
  Apr 23 15:48:32.142: INFO: starting watch
  STEP: getting @ 04/23/23 15:48:32.163
  STEP: listing @ 04/23/23 15:48:32.169
  STEP: patching @ 04/23/23 15:48:32.178
  STEP: updating @ 04/23/23 15:48:32.193
  Apr 23 15:48:32.206: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/23/23 15:48:32.207
  STEP: deleting a collection @ 04/23/23 15:48:32.238
  Apr 23 15:48:32.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8314" for this suite. @ 04/23/23 15:48:32.297
• [0.295 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/23/23 15:48:32.327
  Apr 23 15:48:32.328: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename subpath @ 04/23/23 15:48:32.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:48:32.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:48:32.374
  STEP: Setting up data @ 04/23/23 15:48:32.381
  STEP: Creating pod pod-subpath-test-secret-t5lp @ 04/23/23 15:48:32.404
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 15:48:32.404
  STEP: Saw pod success @ 04/23/23 15:48:56.575
  Apr 23 15:48:56.585: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-subpath-test-secret-t5lp container test-container-subpath-secret-t5lp: <nil>
  STEP: delete the pod @ 04/23/23 15:48:56.614
  STEP: Deleting pod pod-subpath-test-secret-t5lp @ 04/23/23 15:48:56.659
  Apr 23 15:48:56.660: INFO: Deleting pod "pod-subpath-test-secret-t5lp" in namespace "subpath-7547"
  Apr 23 15:48:56.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7547" for this suite. @ 04/23/23 15:48:56.683
• [24.374 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 04/23/23 15:48:56.702
  Apr 23 15:48:56.702: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 15:48:56.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:48:56.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:48:56.757
  Apr 23 15:48:56.832: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 15:48:56.86
  Apr 23 15:48:56.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:48:56.885: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:48:57.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:48:57.914: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:48:58.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:48:58.910: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:48:59.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:48:59.902: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/23/23 15:48:59.934
  STEP: Check that daemon pods images are updated. @ 04/23/23 15:48:59.966
  Apr 23 15:48:59.979: INFO: Wrong image for pod: daemon-set-49f2s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:48:59.979: INFO: Wrong image for pod: daemon-set-5vsfx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:48:59.979: INFO: Wrong image for pod: daemon-set-mwxhz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:01.010: INFO: Wrong image for pod: daemon-set-49f2s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:01.011: INFO: Wrong image for pod: daemon-set-5vsfx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:01.011: INFO: Pod daemon-set-kkjkc is not available
  Apr 23 15:49:02.009: INFO: Wrong image for pod: daemon-set-49f2s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:02.009: INFO: Wrong image for pod: daemon-set-5vsfx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:02.009: INFO: Pod daemon-set-kkjkc is not available
  Apr 23 15:49:03.011: INFO: Wrong image for pod: daemon-set-5vsfx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:04.007: INFO: Wrong image for pod: daemon-set-5vsfx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:05.011: INFO: Wrong image for pod: daemon-set-5vsfx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:05.011: INFO: Pod daemon-set-mtjgb is not available
  Apr 23 15:49:06.012: INFO: Wrong image for pod: daemon-set-5vsfx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 15:49:06.012: INFO: Pod daemon-set-mtjgb is not available
  Apr 23 15:49:08.010: INFO: Pod daemon-set-xbskw is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/23/23 15:49:08.022
  Apr 23 15:49:08.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:49:08.039: INFO: Node soodi4ja4shi-2 is running 0 daemon pod, expected 1
  Apr 23 15:49:09.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:49:09.059: INFO: Node soodi4ja4shi-2 is running 0 daemon pod, expected 1
  Apr 23 15:49:10.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:49:10.069: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 15:49:10.107
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4004, will wait for the garbage collector to delete the pods @ 04/23/23 15:49:10.107
  Apr 23 15:49:10.179: INFO: Deleting DaemonSet.extensions daemon-set took: 14.546212ms
  Apr 23 15:49:10.280: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.03642ms
  Apr 23 15:49:12.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:49:12.790: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 15:49:12.798: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17384"},"items":null}

  Apr 23 15:49:12.806: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17384"},"items":null}

  Apr 23 15:49:12.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4004" for this suite. @ 04/23/23 15:49:12.855
• [16.175 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 04/23/23 15:49:12.882
  Apr 23 15:49:12.882: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 15:49:12.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:49:12.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:49:12.944
  STEP: Creating service test in namespace statefulset-8735 @ 04/23/23 15:49:12.951
  STEP: Creating stateful set ss in namespace statefulset-8735 @ 04/23/23 15:49:12.963
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8735 @ 04/23/23 15:49:12.978
  Apr 23 15:49:12.986: INFO: Found 0 stateful pods, waiting for 1
  Apr 23 15:49:22.996: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/23/23 15:49:22.996
  Apr 23 15:49:23.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8735 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 15:49:23.543: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 15:49:23.543: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 15:49:23.543: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 15:49:23.551: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 23 15:49:33.561: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 15:49:33.561: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:49:33.600: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 23 15:49:33.600: INFO: ss-0  soodi4ja4shi-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:13 +0000 UTC  }]
  Apr 23 15:49:33.601: INFO: 
  Apr 23 15:49:33.601: INFO: StatefulSet ss has not reached scale 3, at 1
  Apr 23 15:49:34.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98990576s
  Apr 23 15:49:35.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974021785s
  Apr 23 15:49:36.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962461791s
  Apr 23 15:49:37.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.952004624s
  Apr 23 15:49:38.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.943446985s
  Apr 23 15:49:39.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.93452725s
  Apr 23 15:49:40.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.925421481s
  Apr 23 15:49:41.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916377132s
  Apr 23 15:49:42.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.235091ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8735 @ 04/23/23 15:49:43.722
  Apr 23 15:49:43.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8735 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 15:49:43.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 15:49:43.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 15:49:43.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 15:49:43.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8735 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 15:49:44.264: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 23 15:49:44.265: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 15:49:44.265: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 15:49:44.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8735 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 15:49:44.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 23 15:49:44.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 15:49:44.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 15:49:44.844: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:49:44.844: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:49:44.844: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/23/23 15:49:44.844
  Apr 23 15:49:44.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8735 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 15:49:45.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 15:49:45.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 15:49:45.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 15:49:45.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8735 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 15:49:45.571: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 15:49:45.572: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 15:49:45.572: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 15:49:45.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8735 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 15:49:45.859: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 15:49:45.859: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 15:49:45.859: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 15:49:45.859: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:49:45.866: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Apr 23 15:49:55.882: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 15:49:55.882: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 15:49:55.882: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 15:49:55.906: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 23 15:49:55.907: INFO: ss-0  soodi4ja4shi-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:13 +0000 UTC  }]
  Apr 23 15:49:55.907: INFO: ss-1  soodi4ja4shi-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:33 +0000 UTC  }]
  Apr 23 15:49:55.907: INFO: ss-2  soodi4ja4shi-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:33 +0000 UTC  }]
  Apr 23 15:49:55.908: INFO: 
  Apr 23 15:49:55.908: INFO: StatefulSet ss has not reached scale 0, at 3
  Apr 23 15:49:56.921: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Apr 23 15:49:56.922: INFO: ss-0  soodi4ja4shi-3  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:13 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:45 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:45 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 15:49:13 +0000 UTC  }]
  Apr 23 15:49:56.922: INFO: 
  Apr 23 15:49:56.922: INFO: StatefulSet ss has not reached scale 0, at 1
  Apr 23 15:49:57.930: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.977344452s
  Apr 23 15:49:58.939: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.969108844s
  Apr 23 15:49:59.947: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.960060284s
  Apr 23 15:50:00.957: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.952510076s
  Apr 23 15:50:01.964: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.942112753s
  Apr 23 15:50:02.971: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.934475909s
  Apr 23 15:50:03.977: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.928757994s
  Apr 23 15:50:04.984: INFO: Verifying statefulset ss doesn't scale past 0 for another 922.694383ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8735 @ 04/23/23 15:50:05.985
  Apr 23 15:50:05.994: INFO: Scaling statefulset ss to 0
  Apr 23 15:50:06.031: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:50:06.037: INFO: Deleting all statefulset in ns statefulset-8735
  Apr 23 15:50:06.046: INFO: Scaling statefulset ss to 0
  Apr 23 15:50:06.083: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:50:06.093: INFO: Deleting statefulset ss
  Apr 23 15:50:06.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8735" for this suite. @ 04/23/23 15:50:06.142
• [53.276 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/23/23 15:50:06.161
  Apr 23 15:50:06.161: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:50:06.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:50:06.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:50:06.228
  STEP: Creating projection with secret that has name projected-secret-test-ced1f882-0e67-4cb1-84e2-41e6df5eef8e @ 04/23/23 15:50:06.234
  STEP: Creating a pod to test consume secrets @ 04/23/23 15:50:06.248
  STEP: Saw pod success @ 04/23/23 15:50:10.332
  Apr 23 15:50:10.339: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-secrets-81200c1d-887c-4d61-b4f6-b41b285c6100 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 15:50:10.361
  Apr 23 15:50:10.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7107" for this suite. @ 04/23/23 15:50:10.412
• [4.274 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/23/23 15:50:10.437
  Apr 23 15:50:10.438: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 15:50:10.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:50:10.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:50:10.5
  STEP: Creating ServiceAccount "e2e-sa-4rgk9"  @ 04/23/23 15:50:10.507
  Apr 23 15:50:10.517: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-4rgk9"  @ 04/23/23 15:50:10.517
  Apr 23 15:50:10.536: INFO: AutomountServiceAccountToken: true
  Apr 23 15:50:10.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8639" for this suite. @ 04/23/23 15:50:10.551
• [0.139 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/23/23 15:50:10.577
  Apr 23 15:50:10.577: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 15:50:10.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:50:10.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:50:10.844
  Apr 23 15:50:10.852: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/23/23 15:50:13.995
  Apr 23 15:50:13.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 create -f -'
  Apr 23 15:50:15.678: INFO: stderr: ""
  Apr 23 15:50:15.678: INFO: stdout: "e2e-test-crd-publish-openapi-5757-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 23 15:50:15.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 delete e2e-test-crd-publish-openapi-5757-crds test-foo'
  Apr 23 15:50:15.907: INFO: stderr: ""
  Apr 23 15:50:15.907: INFO: stdout: "e2e-test-crd-publish-openapi-5757-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 23 15:50:15.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 apply -f -'
  Apr 23 15:50:17.167: INFO: stderr: ""
  Apr 23 15:50:17.167: INFO: stdout: "e2e-test-crd-publish-openapi-5757-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 23 15:50:17.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 delete e2e-test-crd-publish-openapi-5757-crds test-foo'
  Apr 23 15:50:17.462: INFO: stderr: ""
  Apr 23 15:50:17.462: INFO: stdout: "e2e-test-crd-publish-openapi-5757-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/23/23 15:50:17.463
  Apr 23 15:50:17.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 create -f -'
  Apr 23 15:50:18.093: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/23/23 15:50:18.094
  Apr 23 15:50:18.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 create -f -'
  Apr 23 15:50:18.671: INFO: rc: 1
  Apr 23 15:50:18.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 apply -f -'
  Apr 23 15:50:19.454: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/23/23 15:50:19.454
  Apr 23 15:50:19.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 create -f -'
  Apr 23 15:50:20.017: INFO: rc: 1
  Apr 23 15:50:20.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 --namespace=crd-publish-openapi-7159 apply -f -'
  Apr 23 15:50:20.449: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/23/23 15:50:20.451
  Apr 23 15:50:20.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 explain e2e-test-crd-publish-openapi-5757-crds'
  Apr 23 15:50:21.023: INFO: stderr: ""
  Apr 23 15:50:21.024: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-5757-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/23/23 15:50:21.024
  Apr 23 15:50:21.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 explain e2e-test-crd-publish-openapi-5757-crds.metadata'
  Apr 23 15:50:21.764: INFO: stderr: ""
  Apr 23 15:50:21.764: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-5757-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 23 15:50:21.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 explain e2e-test-crd-publish-openapi-5757-crds.spec'
  Apr 23 15:50:22.198: INFO: stderr: ""
  Apr 23 15:50:22.198: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-5757-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 23 15:50:22.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 explain e2e-test-crd-publish-openapi-5757-crds.spec.bars'
  Apr 23 15:50:22.706: INFO: stderr: ""
  Apr 23 15:50:22.706: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-5757-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/23/23 15:50:22.706
  Apr 23 15:50:22.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-7159 explain e2e-test-crd-publish-openapi-5757-crds.spec.bars2'
  Apr 23 15:50:23.524: INFO: rc: 1
  Apr 23 15:50:25.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7159" for this suite. @ 04/23/23 15:50:25.565
• [15.002 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/23/23 15:50:25.585
  Apr 23 15:50:25.586: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:50:25.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:50:25.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:50:25.64
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/23/23 15:50:25.645
  STEP: Saw pod success @ 04/23/23 15:50:29.685
  Apr 23 15:50:29.690: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-700d8923-772b-4f75-80fd-94722339e541 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 15:50:29.705
  Apr 23 15:50:29.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7143" for this suite. @ 04/23/23 15:50:29.746
• [4.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/23/23 15:50:29.764
  Apr 23 15:50:29.764: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename gc @ 04/23/23 15:50:29.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:50:29.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:50:29.804
  STEP: create the rc @ 04/23/23 15:50:29.818
  W0423 15:50:29.834049      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/23/23 15:50:35.927
  STEP: wait for the rc to be deleted @ 04/23/23 15:50:36.184
  Apr 23 15:50:38.427: INFO: 91 pods remaining
  Apr 23 15:50:38.428: INFO: 79 pods has nil DeletionTimestamp
  Apr 23 15:50:38.428: INFO: 
  Apr 23 15:50:39.859: INFO: 75 pods remaining
  Apr 23 15:50:39.860: INFO: 63 pods has nil DeletionTimestamp
  Apr 23 15:50:39.860: INFO: 
  Apr 23 15:50:40.704: INFO: 69 pods remaining
  Apr 23 15:50:40.704: INFO: 52 pods has nil DeletionTimestamp
  Apr 23 15:50:40.704: INFO: 
  Apr 23 15:50:42.487: INFO: 53 pods remaining
  Apr 23 15:50:42.488: INFO: 26 pods has nil DeletionTimestamp
  Apr 23 15:50:42.488: INFO: 
  Apr 23 15:50:44.065: INFO: 46 pods remaining
  Apr 23 15:50:44.065: INFO: 5 pods has nil DeletionTimestamp
  Apr 23 15:50:44.066: INFO: 
  Apr 23 15:50:44.380: INFO: 40 pods remaining
  Apr 23 15:50:44.380: INFO: 0 pods has nil DeletionTimestamp
  Apr 23 15:50:44.380: INFO: 
  Apr 23 15:50:45.404: INFO: 35 pods remaining
  Apr 23 15:50:45.404: INFO: 0 pods has nil DeletionTimestamp
  Apr 23 15:50:45.404: INFO: 
  Apr 23 15:50:47.381: INFO: 25 pods remaining
  Apr 23 15:50:47.381: INFO: 0 pods has nil DeletionTimestamp
  Apr 23 15:50:47.381: INFO: 
  Apr 23 15:50:48.239: INFO: 17 pods remaining
  Apr 23 15:50:48.239: INFO: 0 pods has nil DeletionTimestamp
  Apr 23 15:50:48.240: INFO: 
  Apr 23 15:50:49.559: INFO: 6 pods remaining
  Apr 23 15:50:49.676: INFO: 0 pods has nil DeletionTimestamp
  Apr 23 15:50:49.676: INFO: 
  Apr 23 15:50:50.560: INFO: 0 pods remaining
  Apr 23 15:50:50.560: INFO: 0 pods has nil DeletionTimestamp
  Apr 23 15:50:50.560: INFO: 
  Apr 23 15:50:51.382: INFO: 0 pods remaining
  Apr 23 15:50:51.382: INFO: 0 pods has nil DeletionTimestamp
  Apr 23 15:50:51.382: INFO: 
  STEP: Gathering metrics @ 04/23/23 15:50:52.227
  Apr 23 15:50:52.734: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 15:50:52.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1165" for this suite. @ 04/23/23 15:50:52.755
• [23.017 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/23/23 15:50:52.785
  Apr 23 15:50:52.785: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 15:50:52.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:50:52.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:50:52.936
  STEP: Setting up server cert @ 04/23/23 15:50:53.229
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 15:50:54.919
  STEP: Deploying the webhook pod @ 04/23/23 15:50:55.106
  STEP: Wait for the deployment to be ready @ 04/23/23 15:50:55.179
  Apr 23 15:50:55.231: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Apr 23 15:50:57.283: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:50:59.294: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:51:01.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:51:03.294: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 50, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/23/23 15:51:05.296
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 15:51:05.353
  Apr 23 15:51:06.354: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/23/23 15:51:07.051
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 15:51:07.374
  STEP: Deleting the collection of validation webhooks @ 04/23/23 15:51:07.522
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 15:51:07.853
  Apr 23 15:51:07.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5031" for this suite. @ 04/23/23 15:51:08.096
  STEP: Destroying namespace "webhook-markers-8365" for this suite. @ 04/23/23 15:51:08.143
• [15.404 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/23/23 15:51:08.191
  Apr 23 15:51:08.191: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 15:51:08.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:51:08.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:51:08.378
  STEP: create the container @ 04/23/23 15:51:08.386
  W0423 15:51:08.442739      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/23/23 15:51:08.443
  STEP: get the container status @ 04/23/23 15:51:12.603
  STEP: the container should be terminated @ 04/23/23 15:51:12.619
  STEP: the termination message should be set @ 04/23/23 15:51:12.619
  Apr 23 15:51:12.619: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/23/23 15:51:12.619
  Apr 23 15:51:12.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6936" for this suite. @ 04/23/23 15:51:12.741
• [4.572 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/23/23 15:51:12.769
  Apr 23 15:51:12.769: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename dns @ 04/23/23 15:51:12.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:51:12.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:51:12.868
  STEP: Creating a test headless service @ 04/23/23 15:51:12.874
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5228.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5228.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5228.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5228.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5228.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 179.51.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.51.179_udp@PTR;check="$$(dig +tcp +noall +answer +search 179.51.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.51.179_tcp@PTR;sleep 1; done
   @ 04/23/23 15:51:12.99
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5228.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5228.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5228.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5228.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5228.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5228.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 179.51.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.51.179_udp@PTR;check="$$(dig +tcp +noall +answer +search 179.51.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.51.179_tcp@PTR;sleep 1; done
   @ 04/23/23 15:51:12.99
  STEP: creating a pod to probe DNS @ 04/23/23 15:51:12.99
  STEP: submitting the pod to kubernetes @ 04/23/23 15:51:12.99
  STEP: retrieving the pod @ 04/23/23 15:51:17.245
  STEP: looking for the results for each expected name from probers @ 04/23/23 15:51:17.283
  Apr 23 15:51:17.323: INFO: Unable to read wheezy_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:17.356: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:17.382: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:17.448: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:17.616: INFO: Unable to read jessie_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:17.658: INFO: Unable to read jessie_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:17.705: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:17.797: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:17.979: INFO: Lookups using dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d failed for: [wheezy_udp@dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_udp@dns-test-service.dns-5228.svc.cluster.local jessie_tcp@dns-test-service.dns-5228.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local]

  Apr 23 15:51:22.994: INFO: Unable to read wheezy_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:23.006: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:23.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:23.030: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:23.089: INFO: Unable to read jessie_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:23.100: INFO: Unable to read jessie_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:23.111: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:23.121: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:23.168: INFO: Lookups using dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d failed for: [wheezy_udp@dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_udp@dns-test-service.dns-5228.svc.cluster.local jessie_tcp@dns-test-service.dns-5228.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local]

  Apr 23 15:51:27.991: INFO: Unable to read wheezy_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:27.999: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:28.005: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:28.011: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:28.050: INFO: Unable to read jessie_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:28.057: INFO: Unable to read jessie_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:28.065: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:28.074: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:28.107: INFO: Lookups using dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d failed for: [wheezy_udp@dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_udp@dns-test-service.dns-5228.svc.cluster.local jessie_tcp@dns-test-service.dns-5228.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local]

  Apr 23 15:51:32.991: INFO: Unable to read wheezy_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:33.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:33.008: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:33.018: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:33.063: INFO: Unable to read jessie_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:33.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:33.084: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:33.093: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:33.129: INFO: Lookups using dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d failed for: [wheezy_udp@dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_udp@dns-test-service.dns-5228.svc.cluster.local jessie_tcp@dns-test-service.dns-5228.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local]

  Apr 23 15:51:37.990: INFO: Unable to read wheezy_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:37.999: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:38.007: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:38.015: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:38.082: INFO: Unable to read jessie_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:38.091: INFO: Unable to read jessie_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:38.102: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:38.112: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:38.162: INFO: Lookups using dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d failed for: [wheezy_udp@dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_udp@dns-test-service.dns-5228.svc.cluster.local jessie_tcp@dns-test-service.dns-5228.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local]

  Apr 23 15:51:42.995: INFO: Unable to read wheezy_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:43.015: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:43.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:43.036: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:43.086: INFO: Unable to read jessie_udp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:43.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:43.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:43.112: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local from pod dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d: the server could not find the requested resource (get pods dns-test-602f10de-ca8f-4505-93da-d36358a7150d)
  Apr 23 15:51:43.154: INFO: Lookups using dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d failed for: [wheezy_udp@dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@dns-test-service.dns-5228.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_udp@dns-test-service.dns-5228.svc.cluster.local jessie_tcp@dns-test-service.dns-5228.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5228.svc.cluster.local]

  Apr 23 15:51:48.117: INFO: DNS probes using dns-5228/dns-test-602f10de-ca8f-4505-93da-d36358a7150d succeeded

  Apr 23 15:51:48.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 15:51:48.135
  STEP: deleting the test service @ 04/23/23 15:51:48.184
  STEP: deleting the test headless service @ 04/23/23 15:51:48.256
  STEP: Destroying namespace "dns-5228" for this suite. @ 04/23/23 15:51:48.289
• [35.541 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/23/23 15:51:48.313
  Apr 23 15:51:48.313: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename server-version @ 04/23/23 15:51:48.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:51:48.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:51:48.445
  STEP: Request ServerVersion @ 04/23/23 15:51:48.453
  STEP: Confirm major version @ 04/23/23 15:51:48.455
  Apr 23 15:51:48.455: INFO: Major version: 1
  STEP: Confirm minor version @ 04/23/23 15:51:48.455
  Apr 23 15:51:48.455: INFO: cleanMinorVersion: 27
  Apr 23 15:51:48.455: INFO: Minor version: 27
  Apr 23 15:51:48.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-832" for this suite. @ 04/23/23 15:51:48.467
• [0.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/23/23 15:51:48.489
  Apr 23 15:51:48.489: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename disruption @ 04/23/23 15:51:48.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:51:48.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:51:48.556
  STEP: Creating a kubernetes client @ 04/23/23 15:51:48.566
  Apr 23 15:51:48.566: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename disruption-2 @ 04/23/23 15:51:48.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:51:48.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:51:48.613
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:51:48.632
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:51:50.72
  STEP: Waiting for the pdb to be processed @ 04/23/23 15:51:52.758
  STEP: listing a collection of PDBs across all namespaces @ 04/23/23 15:51:54.784
  STEP: listing a collection of PDBs in namespace disruption-9683 @ 04/23/23 15:51:54.791
  STEP: deleting a collection of PDBs @ 04/23/23 15:51:54.82
  STEP: Waiting for the PDB collection to be deleted @ 04/23/23 15:51:54.888
  Apr 23 15:51:54.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 15:51:54.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-2857" for this suite. @ 04/23/23 15:51:54.917
  STEP: Destroying namespace "disruption-9683" for this suite. @ 04/23/23 15:51:54.935
• [6.460 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/23/23 15:51:54.952
  Apr 23 15:51:54.952: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename deployment @ 04/23/23 15:51:54.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:51:54.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:51:54.995
  Apr 23 15:51:55.018: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Apr 23 15:52:00.029: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 15:52:00.03
  Apr 23 15:52:00.030: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Apr 23 15:52:02.042: INFO: Creating deployment "test-rollover-deployment"
  Apr 23 15:52:02.061: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Apr 23 15:52:04.091: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 23 15:52:04.109: INFO: Ensure that both replica sets have 1 created replica
  Apr 23 15:52:04.125: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 23 15:52:04.143: INFO: Updating deployment test-rollover-deployment
  Apr 23 15:52:04.143: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Apr 23 15:52:06.163: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Apr 23 15:52:06.177: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 23 15:52:06.191: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 15:52:06.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:52:08.206: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 15:52:08.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:52:10.206: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 15:52:10.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:52:12.209: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 15:52:12.210: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:52:14.211: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 15:52:14.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:52:16.213: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 15:52:16.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 23 15:52:18.207: INFO: 
  Apr 23 15:52:18.207: INFO: Ensure that both old replica sets have no replicas
  Apr 23 15:52:18.226: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4336  971fdd4e-8db6-4b1e-9ab9-93583d8be85b 19620 2 2023-04-23 15:52:02 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-23 15:52:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060bac18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-23 15:52:02 +0000 UTC,LastTransitionTime:2023-04-23 15:52:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-04-23 15:52:16 +0000 UTC,LastTransitionTime:2023-04-23 15:52:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 23 15:52:18.233: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-4336  3911f5e2-7703-46f0-bcb3-2b7b479f6964 19609 2 2023-04-23 15:52:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 971fdd4e-8db6-4b1e-9ab9-93583d8be85b 0xc006053137 0xc006053138}] [] [{kube-controller-manager Update apps/v1 2023-04-23 15:52:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"971fdd4e-8db6-4b1e-9ab9-93583d8be85b\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:52:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060531e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 15:52:18.233: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 23 15:52:18.233: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4336  cfcf956b-e8d0-4a58-b234-ea4a1d4aa144 19618 2 2023-04-23 15:51:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 971fdd4e-8db6-4b1e-9ab9-93583d8be85b 0xc006053007 0xc006053008}] [] [{e2e.test Update apps/v1 2023-04-23 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"971fdd4e-8db6-4b1e-9ab9-93583d8be85b\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:52:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0060530c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 15:52:18.234: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-4336  6fd4be08-b925-4d73-8463-ead4fb403d5f 19563 2 2023-04-23 15:52:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 971fdd4e-8db6-4b1e-9ab9-93583d8be85b 0xc006053257 0xc006053258}] [] [{kube-controller-manager Update apps/v1 2023-04-23 15:52:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"971fdd4e-8db6-4b1e-9ab9-93583d8be85b\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:52:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006053308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 15:52:18.242: INFO: Pod "test-rollover-deployment-57777854c9-4hw2q" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-4hw2q test-rollover-deployment-57777854c9- deployment-4336  81c128c4-21b2-46e5-8863-fb8810ec3fd0 19584 0 2023-04-23 15:52:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 3911f5e2-7703-46f0-bcb3-2b7b479f6964 0xc006053847 0xc006053848}] [] [{kube-controller-manager Update v1 2023-04-23 15:52:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3911f5e2-7703-46f0-bcb3-2b7b479f6964\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 15:52:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j2b4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j2b4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:52:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:52:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:52:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:52:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.66.160,StartTime:2023-04-23 15:52:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 15:52:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://ca9e9e5f5d3419b141d2195ebbc8d7ffc31f98c858926f18c562662c1ad2095b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.160,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 15:52:18.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4336" for this suite. @ 04/23/23 15:52:18.252
• [23.314 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/23/23 15:52:18.27
  Apr 23 15:52:18.270: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 15:52:18.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:18.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:18.325
  Apr 23 15:52:18.364: INFO: created pod pod-service-account-defaultsa
  Apr 23 15:52:18.365: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Apr 23 15:52:18.377: INFO: created pod pod-service-account-mountsa
  Apr 23 15:52:18.377: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 23 15:52:18.394: INFO: created pod pod-service-account-nomountsa
  Apr 23 15:52:18.394: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 23 15:52:18.414: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 23 15:52:18.415: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Apr 23 15:52:18.438: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 23 15:52:18.438: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Apr 23 15:52:18.448: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 23 15:52:18.448: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 23 15:52:18.473: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 23 15:52:18.473: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 23 15:52:18.511: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 23 15:52:18.511: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Apr 23 15:52:18.532: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 23 15:52:18.532: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 23 15:52:18.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3639" for this suite. @ 04/23/23 15:52:18.655
• [0.773 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/23/23 15:52:19.212
  Apr 23 15:52:19.234: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 15:52:19.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:19.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:20.21
  STEP: Setting up server cert @ 04/23/23 15:52:20.389
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 15:52:23.252
  STEP: Deploying the webhook pod @ 04/23/23 15:52:23.274
  STEP: Wait for the deployment to be ready @ 04/23/23 15:52:23.295
  Apr 23 15:52:23.307: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Apr 23 15:52:25.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/23/23 15:52:27.366
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 15:52:27.432
  Apr 23 15:52:28.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/23/23 15:52:28.442
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 15:52:28.479
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/23/23 15:52:28.497
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 15:52:28.522
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/23/23 15:52:28.551
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 15:52:28.568
  Apr 23 15:52:28.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4074" for this suite. @ 04/23/23 15:52:28.73
  STEP: Destroying namespace "webhook-markers-7282" for this suite. @ 04/23/23 15:52:28.751
• [9.556 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/23/23 15:52:28.795
  Apr 23 15:52:28.795: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 15:52:28.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:28.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:28.867
  STEP: Read namespace status @ 04/23/23 15:52:28.875
  Apr 23 15:52:28.880: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/23/23 15:52:28.88
  Apr 23 15:52:28.890: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/23/23 15:52:28.891
  Apr 23 15:52:28.908: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 23 15:52:28.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7229" for this suite. @ 04/23/23 15:52:28.916
• [0.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/23/23 15:52:28.932
  Apr 23 15:52:28.932: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 15:52:28.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:28.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:28.961
  STEP: creating a Namespace @ 04/23/23 15:52:28.966
  STEP: patching the Namespace @ 04/23/23 15:52:28.996
  STEP: get the Namespace and ensuring it has the label @ 04/23/23 15:52:29.007
  Apr 23 15:52:29.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5039" for this suite. @ 04/23/23 15:52:29.023
  STEP: Destroying namespace "nspatchtest-9b9e17be-5c51-45e3-a938-dfc0fbd34971-1671" for this suite. @ 04/23/23 15:52:29.038
• [0.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/23/23 15:52:29.059
  Apr 23 15:52:29.059: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename containers @ 04/23/23 15:52:29.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:29.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:29.096
  STEP: Creating a pod to test override command @ 04/23/23 15:52:29.101
  STEP: Saw pod success @ 04/23/23 15:52:33.15
  Apr 23 15:52:33.159: INFO: Trying to get logs from node soodi4ja4shi-3 pod client-containers-25059b5b-ab28-478d-ad8d-4a1e574a5af9 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 15:52:33.198
  Apr 23 15:52:33.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2364" for this suite. @ 04/23/23 15:52:33.266
• [4.223 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/23/23 15:52:33.283
  Apr 23 15:52:33.283: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 15:52:33.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:33.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:33.32
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-5690 @ 04/23/23 15:52:33.324
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/23/23 15:52:33.36
  STEP: creating service externalsvc in namespace services-5690 @ 04/23/23 15:52:33.36
  STEP: creating replication controller externalsvc in namespace services-5690 @ 04/23/23 15:52:33.432
  I0423 15:52:33.487869      15 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5690, replica count: 2
  I0423 15:52:36.540429      15 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/23/23 15:52:36.547
  Apr 23 15:52:36.619: INFO: Creating new exec pod
  Apr 23 15:52:38.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-5690 exec execpodkqckz -- /bin/sh -x -c nslookup nodeport-service.services-5690.svc.cluster.local'
  Apr 23 15:52:39.288: INFO: stderr: "+ nslookup nodeport-service.services-5690.svc.cluster.local\n"
  Apr 23 15:52:39.288: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-5690.svc.cluster.local\tcanonical name = externalsvc.services-5690.svc.cluster.local.\nName:\texternalsvc.services-5690.svc.cluster.local\nAddress: 10.233.50.157\n\n"
  Apr 23 15:52:39.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5690, will wait for the garbage collector to delete the pods @ 04/23/23 15:52:39.297
  Apr 23 15:52:39.372: INFO: Deleting ReplicationController externalsvc took: 15.142822ms
  Apr 23 15:52:39.473: INFO: Terminating ReplicationController externalsvc pods took: 100.657854ms
  Apr 23 15:52:42.026: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-5690" for this suite. @ 04/23/23 15:52:42.053
• [8.781 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 04/23/23 15:52:42.066
  Apr 23 15:52:42.066: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 15:52:42.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:42.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:42.117
  Apr 23 15:52:42.122: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:52:44.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9985" for this suite. @ 04/23/23 15:52:44.93
• [2.873 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/23/23 15:52:44.951
  Apr 23 15:52:44.952: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 15:52:44.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:44.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:44.991
  STEP: Setting up server cert @ 04/23/23 15:52:45.045
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 15:52:46.635
  STEP: Deploying the webhook pod @ 04/23/23 15:52:46.703
  STEP: Wait for the deployment to be ready @ 04/23/23 15:52:46.761
  Apr 23 15:52:46.806: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Apr 23 15:52:48.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 15, 52, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 15, 52, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/23/23 15:52:50.838
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 15:52:50.861
  Apr 23 15:52:51.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/23/23 15:52:51.868
  STEP: create a pod that should be updated by the webhook @ 04/23/23 15:52:51.896
  Apr 23 15:52:51.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2872" for this suite. @ 04/23/23 15:52:52.049
  STEP: Destroying namespace "webhook-markers-554" for this suite. @ 04/23/23 15:52:52.066
• [7.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/23/23 15:52:52.084
  Apr 23 15:52:52.084: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename watch @ 04/23/23 15:52:52.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:52:52.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:52:52.127
  STEP: creating a watch on configmaps with a certain label @ 04/23/23 15:52:52.132
  STEP: creating a new configmap @ 04/23/23 15:52:52.133
  STEP: modifying the configmap once @ 04/23/23 15:52:52.144
  STEP: changing the label value of the configmap @ 04/23/23 15:52:52.158
  STEP: Expecting to observe a delete notification for the watched object @ 04/23/23 15:52:52.17
  Apr 23 15:52:52.170: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9313  c79e4f50-89b7-4de8-b94e-bca5b09fade8 20146 0 2023-04-23 15:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 15:52:52.171: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9313  c79e4f50-89b7-4de8-b94e-bca5b09fade8 20147 0 2023-04-23 15:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 15:52:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 15:52:52.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9313  c79e4f50-89b7-4de8-b94e-bca5b09fade8 20148 0 2023-04-23 15:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 15:52:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/23/23 15:52:52.171
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/23/23 15:52:52.183
  STEP: changing the label value of the configmap back @ 04/23/23 15:53:02.183
  STEP: modifying the configmap a third time @ 04/23/23 15:53:02.199
  STEP: deleting the configmap @ 04/23/23 15:53:02.212
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/23/23 15:53:02.221
  Apr 23 15:53:02.221: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9313  c79e4f50-89b7-4de8-b94e-bca5b09fade8 20187 0 2023-04-23 15:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 15:53:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 15:53:02.221: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9313  c79e4f50-89b7-4de8-b94e-bca5b09fade8 20188 0 2023-04-23 15:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 15:53:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 15:53:02.221: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9313  c79e4f50-89b7-4de8-b94e-bca5b09fade8 20189 0 2023-04-23 15:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 15:53:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 15:53:02.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9313" for this suite. @ 04/23/23 15:53:02.23
• [10.156 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/23/23 15:53:02.242
  Apr 23 15:53:02.242: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 15:53:02.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:53:02.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:53:02.277
  Apr 23 15:53:02.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-8489 version'
  Apr 23 15:53:02.415: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 23 15:53:02.415: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 23 15:53:02.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8489" for this suite. @ 04/23/23 15:53:02.427
• [0.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 04/23/23 15:53:02.451
  Apr 23 15:53:02.451: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 15:53:02.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:53:02.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:53:02.502
  STEP: Creating service test in namespace statefulset-8743 @ 04/23/23 15:53:02.51
  STEP: Creating a new StatefulSet @ 04/23/23 15:53:02.526
  Apr 23 15:53:02.567: INFO: Found 0 stateful pods, waiting for 3
  Apr 23 15:53:12.582: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:53:12.582: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:53:12.582: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/23/23 15:53:12.622
  Apr 23 15:53:12.657: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/23/23 15:53:12.658
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/23/23 15:53:22.754
  STEP: Performing a canary update @ 04/23/23 15:53:22.755
  Apr 23 15:53:22.797: INFO: Updating stateful set ss2
  Apr 23 15:53:22.827: INFO: Waiting for Pod statefulset-8743/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/23/23 15:53:32.848
  Apr 23 15:53:32.964: INFO: Found 2 stateful pods, waiting for 3
  Apr 23 15:53:42.975: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:53:42.975: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:53:42.975: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/23/23 15:53:42.987
  Apr 23 15:53:43.018: INFO: Updating stateful set ss2
  Apr 23 15:53:43.063: INFO: Waiting for Pod statefulset-8743/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 23 15:53:53.106: INFO: Updating stateful set ss2
  Apr 23 15:53:53.128: INFO: Waiting for StatefulSet statefulset-8743/ss2 to complete update
  Apr 23 15:53:53.129: INFO: Waiting for Pod statefulset-8743/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 23 15:54:03.152: INFO: Waiting for StatefulSet statefulset-8743/ss2 to complete update
  Apr 23 15:54:13.156: INFO: Deleting all statefulset in ns statefulset-8743
  Apr 23 15:54:13.167: INFO: Scaling statefulset ss2 to 0
  Apr 23 15:54:23.228: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:54:23.243: INFO: Deleting statefulset ss2
  Apr 23 15:54:23.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8743" for this suite. @ 04/23/23 15:54:23.286
• [80.858 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/23/23 15:54:23.309
  Apr 23 15:54:23.309: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename watch @ 04/23/23 15:54:23.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:54:23.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:54:23.373
  STEP: getting a starting resourceVersion @ 04/23/23 15:54:23.379
  STEP: starting a background goroutine to produce watch events @ 04/23/23 15:54:23.389
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/23/23 15:54:23.39
  Apr 23 15:54:26.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-813" for this suite. @ 04/23/23 15:54:26.172
• [2.916 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/23/23 15:54:26.227
  Apr 23 15:54:26.228: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 15:54:26.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:54:26.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:54:26.27
  STEP: Creating a pod to test service account token:  @ 04/23/23 15:54:26.278
  STEP: Saw pod success @ 04/23/23 15:54:30.323
  Apr 23 15:54:30.328: INFO: Trying to get logs from node soodi4ja4shi-3 pod test-pod-8b53425b-8176-4a3c-9170-54924d65fa2c container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 15:54:30.359
  Apr 23 15:54:30.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1281" for this suite. @ 04/23/23 15:54:30.41
• [4.193 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/23/23 15:54:30.424
  Apr 23 15:54:30.424: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:54:30.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:54:30.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:54:30.466
  STEP: Creating Pod @ 04/23/23 15:54:30.47
  STEP: Reading file content from the nginx-container @ 04/23/23 15:54:32.507
  Apr 23 15:54:32.508: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1310 PodName:pod-sharedvolume-c4c45f99-7c03-44ba-811e-637f646cc6c3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:54:32.508: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:54:32.510: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:54:32.510: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-1310/pods/pod-sharedvolume-c4c45f99-7c03-44ba-811e-637f646cc6c3/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Apr 23 15:54:32.599: INFO: Exec stderr: ""
  Apr 23 15:54:32.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1310" for this suite. @ 04/23/23 15:54:32.612
• [2.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/23/23 15:54:32.629
  Apr 23 15:54:32.629: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:54:32.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:54:32.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:54:32.664
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 15:54:32.67
  STEP: Saw pod success @ 04/23/23 15:54:36.724
  Apr 23 15:54:36.731: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-c4152572-f918-46b3-8259-d8a8bec9339e container client-container: <nil>
  STEP: delete the pod @ 04/23/23 15:54:36.747
  Apr 23 15:54:36.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4326" for this suite. @ 04/23/23 15:54:36.793
• [4.175 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/23/23 15:54:36.806
  Apr 23 15:54:36.806: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename podtemplate @ 04/23/23 15:54:36.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:54:36.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:54:36.841
  STEP: Create set of pod templates @ 04/23/23 15:54:36.844
  Apr 23 15:54:36.854: INFO: created test-podtemplate-1
  Apr 23 15:54:36.864: INFO: created test-podtemplate-2
  Apr 23 15:54:36.874: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/23/23 15:54:36.875
  STEP: delete collection of pod templates @ 04/23/23 15:54:36.88
  Apr 23 15:54:36.881: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/23/23 15:54:36.907
  Apr 23 15:54:36.907: INFO: requesting list of pod templates to confirm quantity
  Apr 23 15:54:36.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4659" for this suite. @ 04/23/23 15:54:36.917
• [0.121 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/23/23 15:54:36.928
  Apr 23 15:54:36.928: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pod-network-test @ 04/23/23 15:54:36.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:54:36.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:54:36.962
  STEP: Performing setup for networking test in namespace pod-network-test-3178 @ 04/23/23 15:54:36.966
  STEP: creating a selector @ 04/23/23 15:54:36.966
  STEP: Creating the service pods in kubernetes @ 04/23/23 15:54:36.966
  Apr 23 15:54:36.966: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/23/23 15:54:59.239
  Apr 23 15:55:01.326: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 23 15:55:01.328: INFO: Going to poll 10.233.64.112 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 15:55:01.337: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3178 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:55:01.337: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:55:01.341: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:55:01.342: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3178/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.112+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 15:55:02.567: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 23 15:55:02.567: INFO: Going to poll 10.233.65.92 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 15:55:02.580: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3178 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:55:02.580: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:55:02.583: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:55:02.583: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3178/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 15:55:03.802: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 23 15:55:03.803: INFO: Going to poll 10.233.66.99 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 15:55:03.811: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.99 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3178 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:55:03.811: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:55:03.813: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:55:03.813: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3178/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.99+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 15:55:04.957: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 23 15:55:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3178" for this suite. @ 04/23/23 15:55:04.968
• [28.056 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/23/23 15:55:04.985
  Apr 23 15:55:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:55:04.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:55:05.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:55:05.031
  STEP: Creating secret with name projected-secret-test-3c9cf0bf-4ea8-4cc5-a6da-ad316fea05b7 @ 04/23/23 15:55:05.035
  STEP: Creating a pod to test consume secrets @ 04/23/23 15:55:05.044
  STEP: Saw pod success @ 04/23/23 15:55:09.101
  Apr 23 15:55:09.107: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-secrets-fd8bf35b-b381-4c57-bfc3-dac0873e8352 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 15:55:09.121
  Apr 23 15:55:09.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2111" for this suite. @ 04/23/23 15:55:09.171
• [4.204 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/23/23 15:55:09.19
  Apr 23 15:55:09.190: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 15:55:09.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:55:09.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:55:09.241
  STEP: Creating a pod to test downward api env vars @ 04/23/23 15:55:09.248
  STEP: Saw pod success @ 04/23/23 15:55:13.309
  Apr 23 15:55:13.316: INFO: Trying to get logs from node soodi4ja4shi-3 pod downward-api-71781940-9596-4fda-a63a-4d60206a0857 container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 15:55:13.339
  Apr 23 15:55:13.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8784" for this suite. @ 04/23/23 15:55:13.38
• [4.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/23/23 15:55:13.403
  Apr 23 15:55:13.403: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename hostport @ 04/23/23 15:55:13.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:55:13.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:55:13.464
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/23/23 15:55:13.475
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.106 on the node which pod1 resides and expect scheduled @ 04/23/23 15:55:15.565
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.106 but use UDP protocol on the node which pod2 resides @ 04/23/23 15:55:27.644
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/23/23 15:55:31.703
  Apr 23 15:55:31.703: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.106 http://127.0.0.1:54323/hostname] Namespace:hostport-5398 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:55:31.704: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:55:31.705: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:55:31.705: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5398/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.106+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.106, port: 54323 @ 04/23/23 15:55:31.857
  Apr 23 15:55:31.858: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.106:54323/hostname] Namespace:hostport-5398 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:55:31.858: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:55:31.860: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:55:31.860: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5398/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.106%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.106, port: 54323 UDP @ 04/23/23 15:55:31.976
  Apr 23 15:55:31.976: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.106 54323] Namespace:hostport-5398 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 15:55:31.977: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 15:55:31.979: INFO: ExecWithOptions: Clientset creation
  Apr 23 15:55:31.979: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5398/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.106+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Apr 23 15:55:37.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-5398" for this suite. @ 04/23/23 15:55:37.082
• [23.698 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/23/23 15:55:37.108
  Apr 23 15:55:37.108: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 15:55:37.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:55:37.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:55:37.148
  Apr 23 15:55:37.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1480" for this suite. @ 04/23/23 15:55:37.168
• [0.074 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/23/23 15:55:37.182
  Apr 23 15:55:37.182: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:55:37.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:55:37.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:55:37.231
  STEP: Creating projection with secret that has name projected-secret-test-176f90f2-9ba8-42e2-bd56-4dbc51637c9a @ 04/23/23 15:55:37.243
  STEP: Creating a pod to test consume secrets @ 04/23/23 15:55:37.252
  STEP: Saw pod success @ 04/23/23 15:55:41.292
  Apr 23 15:55:41.297: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-secrets-260b67db-0792-40d2-ab85-0d2e2a8ab79a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 15:55:41.311
  Apr 23 15:55:41.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2957" for this suite. @ 04/23/23 15:55:41.355
• [4.187 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/23/23 15:55:41.372
  Apr 23 15:55:41.372: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 15:55:41.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:55:41.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:55:41.422
  STEP: creating the pod @ 04/23/23 15:55:41.429
  STEP: submitting the pod to kubernetes @ 04/23/23 15:55:41.429
  STEP: verifying QOS class is set on the pod @ 04/23/23 15:55:41.498
  Apr 23 15:55:41.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1547" for this suite. @ 04/23/23 15:55:41.56
• [0.200 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/23/23 15:55:41.572
  Apr 23 15:55:41.572: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 15:55:41.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:55:41.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:55:41.715
  STEP: creating all guestbook components @ 04/23/23 15:55:41.718
  Apr 23 15:55:41.718: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 23 15:55:41.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 create -f -'
  Apr 23 15:55:42.567: INFO: stderr: ""
  Apr 23 15:55:42.567: INFO: stdout: "service/agnhost-replica created\n"
  Apr 23 15:55:42.567: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 23 15:55:42.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 create -f -'
  Apr 23 15:55:43.387: INFO: stderr: ""
  Apr 23 15:55:43.387: INFO: stdout: "service/agnhost-primary created\n"
  Apr 23 15:55:43.387: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 23 15:55:43.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 create -f -'
  Apr 23 15:55:44.145: INFO: stderr: ""
  Apr 23 15:55:44.145: INFO: stdout: "service/frontend created\n"
  Apr 23 15:55:44.145: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 23 15:55:44.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 create -f -'
  Apr 23 15:55:45.711: INFO: stderr: ""
  Apr 23 15:55:45.711: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 23 15:55:45.711: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 23 15:55:45.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 create -f -'
  Apr 23 15:55:46.323: INFO: stderr: ""
  Apr 23 15:55:46.323: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 23 15:55:46.323: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 23 15:55:46.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 create -f -'
  Apr 23 15:55:47.334: INFO: stderr: ""
  Apr 23 15:55:47.335: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/23/23 15:55:47.335
  Apr 23 15:55:47.335: INFO: Waiting for all frontend pods to be Running.
  Apr 23 15:55:52.386: INFO: Waiting for frontend to serve content.
  Apr 23 15:55:52.416: INFO: Trying to add a new entry to the guestbook.
  Apr 23 15:55:52.439: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/23/23 15:55:52.461
  Apr 23 15:55:52.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 delete --grace-period=0 --force -f -'
  Apr 23 15:55:52.684: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 15:55:52.684: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 15:55:52.685
  Apr 23 15:55:52.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 delete --grace-period=0 --force -f -'
  Apr 23 15:55:52.905: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 15:55:52.905: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 15:55:52.905
  Apr 23 15:55:52.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 delete --grace-period=0 --force -f -'
  Apr 23 15:55:53.108: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 15:55:53.108: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 15:55:53.108
  Apr 23 15:55:53.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 delete --grace-period=0 --force -f -'
  Apr 23 15:55:53.349: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 15:55:53.349: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 15:55:53.35
  Apr 23 15:55:53.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 delete --grace-period=0 --force -f -'
  Apr 23 15:55:53.679: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 15:55:53.679: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 15:55:53.679
  Apr 23 15:55:53.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4932 delete --grace-period=0 --force -f -'
  Apr 23 15:55:53.995: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 15:55:53.995: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 23 15:55:53.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4932" for this suite. @ 04/23/23 15:55:54.027
• [12.468 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 04/23/23 15:55:54.04
  Apr 23 15:55:54.040: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 15:55:54.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:55:54.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:55:54.133
  STEP: Creating service test in namespace statefulset-5075 @ 04/23/23 15:55:54.137
  Apr 23 15:55:54.209: INFO: Found 0 stateful pods, waiting for 1
  Apr 23 15:56:04.219: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/23/23 15:56:04.232
  W0423 15:56:04.252486      15 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 23 15:56:04.263: INFO: Found 1 stateful pods, waiting for 2
  Apr 23 15:56:14.273: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 15:56:14.273: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/23/23 15:56:14.284
  STEP: Delete all of the StatefulSets @ 04/23/23 15:56:14.29
  STEP: Verify that StatefulSets have been deleted @ 04/23/23 15:56:14.303
  Apr 23 15:56:14.310: INFO: Deleting all statefulset in ns statefulset-5075
  Apr 23 15:56:14.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5075" for this suite. @ 04/23/23 15:56:14.331
• [20.305 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/23/23 15:56:14.35
  Apr 23 15:56:14.350: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 15:56:14.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:14.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:14.436
  STEP: Creating configMap with name configmap-test-upd-d0fa6bd2-87c0-4aba-8615-2939699e36c4 @ 04/23/23 15:56:14.471
  STEP: Creating the pod @ 04/23/23 15:56:14.481
  STEP: Waiting for pod with text data @ 04/23/23 15:56:16.51
  STEP: Waiting for pod with binary data @ 04/23/23 15:56:16.528
  Apr 23 15:56:16.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7043" for this suite. @ 04/23/23 15:56:16.546
• [2.205 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/23/23 15:56:16.555
  Apr 23 15:56:16.555: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 15:56:16.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:16.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:16.603
  STEP: create the container @ 04/23/23 15:56:16.607
  W0423 15:56:16.622445      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/23/23 15:56:16.622
  STEP: get the container status @ 04/23/23 15:56:19.699
  STEP: the container should be terminated @ 04/23/23 15:56:19.704
  STEP: the termination message should be set @ 04/23/23 15:56:19.704
  Apr 23 15:56:19.704: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/23/23 15:56:19.704
  Apr 23 15:56:19.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1302" for this suite. @ 04/23/23 15:56:19.747
• [3.202 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/23/23 15:56:19.765
  Apr 23 15:56:19.765: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename conformance-tests @ 04/23/23 15:56:19.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:19.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:19.806
  STEP: Getting node addresses @ 04/23/23 15:56:19.809
  Apr 23 15:56:19.810: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 23 15:56:19.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-8446" for this suite. @ 04/23/23 15:56:19.831
• [0.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/23/23 15:56:19.855
  Apr 23 15:56:19.855: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 15:56:19.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:19.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:19.892
  Apr 23 15:56:19.899: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/23/23 15:56:22.717
  Apr 23 15:56:22.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1360 --namespace=crd-publish-openapi-1360 create -f -'
  Apr 23 15:56:24.354: INFO: stderr: ""
  Apr 23 15:56:24.354: INFO: stdout: "e2e-test-crd-publish-openapi-5104-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 23 15:56:24.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1360 --namespace=crd-publish-openapi-1360 delete e2e-test-crd-publish-openapi-5104-crds test-cr'
  Apr 23 15:56:24.519: INFO: stderr: ""
  Apr 23 15:56:24.519: INFO: stdout: "e2e-test-crd-publish-openapi-5104-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 23 15:56:24.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1360 --namespace=crd-publish-openapi-1360 apply -f -'
  Apr 23 15:56:25.956: INFO: stderr: ""
  Apr 23 15:56:25.956: INFO: stdout: "e2e-test-crd-publish-openapi-5104-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 23 15:56:25.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1360 --namespace=crd-publish-openapi-1360 delete e2e-test-crd-publish-openapi-5104-crds test-cr'
  Apr 23 15:56:26.095: INFO: stderr: ""
  Apr 23 15:56:26.095: INFO: stdout: "e2e-test-crd-publish-openapi-5104-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/23/23 15:56:26.095
  Apr 23 15:56:26.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1360 explain e2e-test-crd-publish-openapi-5104-crds'
  Apr 23 15:56:26.530: INFO: stderr: ""
  Apr 23 15:56:26.530: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-5104-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Apr 23 15:56:28.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1360" for this suite. @ 04/23/23 15:56:28.457
• [8.625 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/23/23 15:56:28.489
  Apr 23 15:56:28.490: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 15:56:28.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:28.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:28.604
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 15:56:28.61
  STEP: Saw pod success @ 04/23/23 15:56:32.716
  Apr 23 15:56:32.721: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-67af2e88-2b6c-4293-a15b-7df3ed87f2a7 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 15:56:32.749
  Apr 23 15:56:32.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5465" for this suite. @ 04/23/23 15:56:32.801
• [4.327 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/23/23 15:56:32.818
  Apr 23 15:56:32.818: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:56:32.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:32.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:32.86
  STEP: Creating configMap with name projected-configmap-test-volume-b860f8b0-b480-43a4-babd-1a240c121c29 @ 04/23/23 15:56:32.864
  STEP: Creating a pod to test consume configMaps @ 04/23/23 15:56:32.872
  STEP: Saw pod success @ 04/23/23 15:56:36.909
  Apr 23 15:56:36.918: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-configmaps-c3077cba-e25f-4174-b80f-2b5d0c9d2e60 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 15:56:37.033
  Apr 23 15:56:37.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1088" for this suite. @ 04/23/23 15:56:37.081
• [4.275 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/23/23 15:56:37.1
  Apr 23 15:56:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 15:56:37.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:37.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:37.14
  STEP: Creating a ResourceQuota with terminating scope @ 04/23/23 15:56:37.145
  STEP: Ensuring ResourceQuota status is calculated @ 04/23/23 15:56:37.153
  STEP: Creating a ResourceQuota with not terminating scope @ 04/23/23 15:56:39.162
  STEP: Ensuring ResourceQuota status is calculated @ 04/23/23 15:56:39.179
  STEP: Creating a long running pod @ 04/23/23 15:56:41.2
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/23/23 15:56:41.229
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/23/23 15:56:43.237
  STEP: Deleting the pod @ 04/23/23 15:56:45.245
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 15:56:45.273
  STEP: Creating a terminating pod @ 04/23/23 15:56:47.281
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/23/23 15:56:47.303
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/23/23 15:56:49.31
  STEP: Deleting the pod @ 04/23/23 15:56:51.32
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 15:56:51.351
  Apr 23 15:56:53.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7951" for this suite. @ 04/23/23 15:56:53.37
• [16.281 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 04/23/23 15:56:53.382
  Apr 23 15:56:53.382: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 15:56:53.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:53.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:53.5
  STEP: Creating simple DaemonSet "daemon-set" @ 04/23/23 15:56:53.583
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 15:56:53.604
  Apr 23 15:56:53.622: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:56:53.622: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:56:54.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:56:54.763: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:56:55.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 15:56:55.640: INFO: Node soodi4ja4shi-3 is running 0 daemon pod, expected 1
  Apr 23 15:56:56.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 15:56:56.639: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 04/23/23 15:56:56.643
  Apr 23 15:56:56.653: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/23/23 15:56:56.653
  Apr 23 15:56:56.671: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/23/23 15:56:56.672
  Apr 23 15:56:56.678: INFO: Observed &DaemonSet event: ADDED
  Apr 23 15:56:56.679: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.679: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.680: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.680: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.681: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.682: INFO: Found daemon set daemon-set in namespace daemonsets-761 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 23 15:56:56.682: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/23/23 15:56:56.683
  STEP: watching for the daemon set status to be patched @ 04/23/23 15:56:56.704
  Apr 23 15:56:56.715: INFO: Observed &DaemonSet event: ADDED
  Apr 23 15:56:56.716: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.716: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.717: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.718: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.719: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.720: INFO: Observed daemon set daemon-set in namespace daemonsets-761 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 23 15:56:56.720: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 15:56:56.721: INFO: Found daemon set daemon-set in namespace daemonsets-761 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 23 15:56:56.721: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 15:56:56.741
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-761, will wait for the garbage collector to delete the pods @ 04/23/23 15:56:56.742
  Apr 23 15:56:56.813: INFO: Deleting DaemonSet.extensions daemon-set took: 13.731481ms
  Apr 23 15:56:56.914: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.71336ms
  Apr 23 15:56:58.921: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:56:58.921: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 15:56:58.929: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22125"},"items":null}

  Apr 23 15:56:58.937: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22125"},"items":null}

  Apr 23 15:56:58.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-761" for this suite. @ 04/23/23 15:56:58.972
• [5.605 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/23/23 15:56:58.987
  Apr 23 15:56:58.987: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 15:56:58.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:56:59.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:56:59.023
  STEP: Creating projection with secret that has name projected-secret-test-map-0ffe1581-5012-4502-b112-7f00b3f30c1b @ 04/23/23 15:56:59.033
  STEP: Creating a pod to test consume secrets @ 04/23/23 15:56:59.044
  STEP: Saw pod success @ 04/23/23 15:57:03.088
  Apr 23 15:57:03.094: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-secrets-2a475ab1-d2a5-452c-a974-58728818a3f4 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 15:57:03.119
  Apr 23 15:57:03.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8733" for this suite. @ 04/23/23 15:57:03.167
• [4.200 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 04/23/23 15:57:03.192
  Apr 23 15:57:03.192: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 15:57:03.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:57:03.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:57:03.228
  STEP: Creating service test in namespace statefulset-1854 @ 04/23/23 15:57:03.237
  STEP: Creating statefulset ss in namespace statefulset-1854 @ 04/23/23 15:57:03.261
  Apr 23 15:57:03.280: INFO: Found 0 stateful pods, waiting for 1
  Apr 23 15:57:13.291: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/23/23 15:57:13.308
  STEP: Getting /status @ 04/23/23 15:57:13.328
  Apr 23 15:57:13.338: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/23/23 15:57:13.339
  Apr 23 15:57:13.360: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/23/23 15:57:13.361
  Apr 23 15:57:13.367: INFO: Observed &StatefulSet event: ADDED
  Apr 23 15:57:13.367: INFO: Found Statefulset ss in namespace statefulset-1854 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 15:57:13.367: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/23/23 15:57:13.367
  Apr 23 15:57:13.367: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 23 15:57:13.380: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/23/23 15:57:13.381
  Apr 23 15:57:13.385: INFO: Observed &StatefulSet event: ADDED
  Apr 23 15:57:13.385: INFO: Deleting all statefulset in ns statefulset-1854
  Apr 23 15:57:13.408: INFO: Scaling statefulset ss to 0
  Apr 23 15:57:23.475: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 15:57:23.481: INFO: Deleting statefulset ss
  Apr 23 15:57:23.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1854" for this suite. @ 04/23/23 15:57:23.53
• [20.358 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/23/23 15:57:23.56
  Apr 23 15:57:23.560: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 15:57:23.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:57:23.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:57:23.619
  STEP: Creating pod test-grpc-89e828cb-aff8-41e4-a941-ca3077764774 in namespace container-probe-3886 @ 04/23/23 15:57:23.624
  Apr 23 15:57:25.659: INFO: Started pod test-grpc-89e828cb-aff8-41e4-a941-ca3077764774 in namespace container-probe-3886
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 15:57:25.659
  Apr 23 15:57:25.665: INFO: Initial restart count of pod test-grpc-89e828cb-aff8-41e4-a941-ca3077764774 is 0
  Apr 23 15:58:31.954: INFO: Restart count of pod container-probe-3886/test-grpc-89e828cb-aff8-41e4-a941-ca3077764774 is now 1 (1m6.288430649s elapsed)
  Apr 23 15:58:31.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 15:58:31.967
  STEP: Destroying namespace "container-probe-3886" for this suite. @ 04/23/23 15:58:32
• [68.455 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/23/23 15:58:32.028
  Apr 23 15:58:32.029: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename deployment @ 04/23/23 15:58:32.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:58:32.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:58:32.077
  Apr 23 15:58:32.102: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Apr 23 15:58:37.117: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 15:58:37.117
  Apr 23 15:58:37.117: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/23/23 15:58:37.144
  Apr 23 15:58:37.168: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4330  74bb2781-cd96-45c9-b1a0-18a267c29554 22513 1 2023-04-23 15:58:37 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-23 15:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0006699e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 23 15:58:37.179: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Apr 23 15:58:37.179: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Apr 23 15:58:37.180: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4330  de355b1f-7aad-4feb-be65-b1c6f4916be8 22514 1 2023-04-23 15:58:32 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 74bb2781-cd96-45c9-b1a0-18a267c29554 0xc005d65b87 0xc005d65b88}] [] [{e2e.test Update apps/v1 2023-04-23 15:58:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 15:58:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-23 15:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"74bb2781-cd96-45c9-b1a0-18a267c29554\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005d65c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 15:58:37.192: INFO: Pod "test-cleanup-controller-vt2tp" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-vt2tp test-cleanup-controller- deployment-4330  1a3b64db-4120-46b3-a112-4572a978dcfe 22505 0 2023-04-23 15:58:32 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller de355b1f-7aad-4feb-be65-b1c6f4916be8 0xc000669d47 0xc000669d48}] [] [{kube-controller-manager Update v1 2023-04-23 15:58:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de355b1f-7aad-4feb-be65-b1c6f4916be8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 15:58:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rz692,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rz692,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:58:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:58:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:58:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 15:58:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.66.199,StartTime:2023-04-23 15:58:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 15:58:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5adcbe053a30d2104eb1c767b7478c43f2246c86fb798ad78a3df86d19afc352,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.199,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 15:58:37.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4330" for this suite. @ 04/23/23 15:58:37.216
• [5.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/23/23 15:58:37.253
  Apr 23 15:58:37.253: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 15:58:37.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:58:37.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:58:37.321
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/23/23 15:58:37.338
  STEP: Saw pod success @ 04/23/23 15:58:41.399
  Apr 23 15:58:41.407: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-f5be12c8-0032-46c0-a504-32e7488ac29e container test-container: <nil>
  STEP: delete the pod @ 04/23/23 15:58:41.443
  Apr 23 15:58:41.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9103" for this suite. @ 04/23/23 15:58:41.562
• [4.321 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/23/23 15:58:41.575
  Apr 23 15:58:41.576: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename gc @ 04/23/23 15:58:41.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:58:41.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:58:41.648
  STEP: create the rc @ 04/23/23 15:58:41.682
  W0423 15:58:41.703586      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/23/23 15:58:48.401
  STEP: wait for the rc to be deleted @ 04/23/23 15:58:48.931
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/23/23 15:58:54.859
  STEP: Gathering metrics @ 04/23/23 15:59:24.91
  Apr 23 15:59:25.163: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 15:59:25.175: INFO: Deleting pod "simpletest.rc-24kzg" in namespace "gc-5947"
  Apr 23 15:59:25.215: INFO: Deleting pod "simpletest.rc-25v2h" in namespace "gc-5947"
  Apr 23 15:59:25.276: INFO: Deleting pod "simpletest.rc-286m8" in namespace "gc-5947"
  Apr 23 15:59:25.370: INFO: Deleting pod "simpletest.rc-2dvtz" in namespace "gc-5947"
  Apr 23 15:59:25.448: INFO: Deleting pod "simpletest.rc-2j96j" in namespace "gc-5947"
  Apr 23 15:59:25.716: INFO: Deleting pod "simpletest.rc-2zk6x" in namespace "gc-5947"
  Apr 23 15:59:25.870: INFO: Deleting pod "simpletest.rc-45s7j" in namespace "gc-5947"
  Apr 23 15:59:25.971: INFO: Deleting pod "simpletest.rc-479ns" in namespace "gc-5947"
  Apr 23 15:59:26.066: INFO: Deleting pod "simpletest.rc-4cx4l" in namespace "gc-5947"
  Apr 23 15:59:26.157: INFO: Deleting pod "simpletest.rc-4fvzv" in namespace "gc-5947"
  Apr 23 15:59:26.231: INFO: Deleting pod "simpletest.rc-4m6r6" in namespace "gc-5947"
  Apr 23 15:59:26.339: INFO: Deleting pod "simpletest.rc-55xxz" in namespace "gc-5947"
  Apr 23 15:59:26.570: INFO: Deleting pod "simpletest.rc-5h2lc" in namespace "gc-5947"
  Apr 23 15:59:26.941: INFO: Deleting pod "simpletest.rc-5lpbc" in namespace "gc-5947"
  Apr 23 15:59:27.144: INFO: Deleting pod "simpletest.rc-67hkc" in namespace "gc-5947"
  Apr 23 15:59:27.297: INFO: Deleting pod "simpletest.rc-6fk6z" in namespace "gc-5947"
  Apr 23 15:59:27.404: INFO: Deleting pod "simpletest.rc-6j58x" in namespace "gc-5947"
  Apr 23 15:59:27.475: INFO: Deleting pod "simpletest.rc-6xtrm" in namespace "gc-5947"
  Apr 23 15:59:27.699: INFO: Deleting pod "simpletest.rc-7hjz2" in namespace "gc-5947"
  Apr 23 15:59:27.850: INFO: Deleting pod "simpletest.rc-7rwlg" in namespace "gc-5947"
  Apr 23 15:59:27.921: INFO: Deleting pod "simpletest.rc-7v7wr" in namespace "gc-5947"
  Apr 23 15:59:27.990: INFO: Deleting pod "simpletest.rc-7whfp" in namespace "gc-5947"
  Apr 23 15:59:28.077: INFO: Deleting pod "simpletest.rc-95g7p" in namespace "gc-5947"
  Apr 23 15:59:28.156: INFO: Deleting pod "simpletest.rc-9dbq2" in namespace "gc-5947"
  Apr 23 15:59:28.309: INFO: Deleting pod "simpletest.rc-b5b6l" in namespace "gc-5947"
  Apr 23 15:59:28.552: INFO: Deleting pod "simpletest.rc-b8j6t" in namespace "gc-5947"
  Apr 23 15:59:28.719: INFO: Deleting pod "simpletest.rc-bfjzh" in namespace "gc-5947"
  Apr 23 15:59:28.886: INFO: Deleting pod "simpletest.rc-bw87m" in namespace "gc-5947"
  Apr 23 15:59:28.990: INFO: Deleting pod "simpletest.rc-c2m2z" in namespace "gc-5947"
  Apr 23 15:59:29.127: INFO: Deleting pod "simpletest.rc-c7ch2" in namespace "gc-5947"
  Apr 23 15:59:29.265: INFO: Deleting pod "simpletest.rc-c9ct7" in namespace "gc-5947"
  Apr 23 15:59:29.725: INFO: Deleting pod "simpletest.rc-chkxw" in namespace "gc-5947"
  Apr 23 15:59:29.794: INFO: Deleting pod "simpletest.rc-dkmvv" in namespace "gc-5947"
  Apr 23 15:59:29.892: INFO: Deleting pod "simpletest.rc-dmwdr" in namespace "gc-5947"
  Apr 23 15:59:29.972: INFO: Deleting pod "simpletest.rc-dz6l9" in namespace "gc-5947"
  Apr 23 15:59:30.132: INFO: Deleting pod "simpletest.rc-fds55" in namespace "gc-5947"
  Apr 23 15:59:30.220: INFO: Deleting pod "simpletest.rc-ffnkq" in namespace "gc-5947"
  Apr 23 15:59:30.301: INFO: Deleting pod "simpletest.rc-fzbd8" in namespace "gc-5947"
  Apr 23 15:59:30.453: INFO: Deleting pod "simpletest.rc-g6lzj" in namespace "gc-5947"
  Apr 23 15:59:30.616: INFO: Deleting pod "simpletest.rc-gknzw" in namespace "gc-5947"
  Apr 23 15:59:30.947: INFO: Deleting pod "simpletest.rc-gkzk6" in namespace "gc-5947"
  Apr 23 15:59:31.424: INFO: Deleting pod "simpletest.rc-gv9xr" in namespace "gc-5947"
  Apr 23 15:59:31.605: INFO: Deleting pod "simpletest.rc-gwgkd" in namespace "gc-5947"
  Apr 23 15:59:31.887: INFO: Deleting pod "simpletest.rc-gxjgw" in namespace "gc-5947"
  Apr 23 15:59:32.095: INFO: Deleting pod "simpletest.rc-h7svd" in namespace "gc-5947"
  Apr 23 15:59:32.188: INFO: Deleting pod "simpletest.rc-h84wh" in namespace "gc-5947"
  Apr 23 15:59:32.245: INFO: Deleting pod "simpletest.rc-hr6lm" in namespace "gc-5947"
  Apr 23 15:59:32.369: INFO: Deleting pod "simpletest.rc-hxq6b" in namespace "gc-5947"
  Apr 23 15:59:32.548: INFO: Deleting pod "simpletest.rc-hzs2w" in namespace "gc-5947"
  Apr 23 15:59:32.615: INFO: Deleting pod "simpletest.rc-j4fdc" in namespace "gc-5947"
  Apr 23 15:59:32.684: INFO: Deleting pod "simpletest.rc-kc7ct" in namespace "gc-5947"
  Apr 23 15:59:32.796: INFO: Deleting pod "simpletest.rc-kcs4n" in namespace "gc-5947"
  Apr 23 15:59:32.887: INFO: Deleting pod "simpletest.rc-kdjmm" in namespace "gc-5947"
  Apr 23 15:59:33.000: INFO: Deleting pod "simpletest.rc-kqw72" in namespace "gc-5947"
  Apr 23 15:59:33.160: INFO: Deleting pod "simpletest.rc-l2g2q" in namespace "gc-5947"
  Apr 23 15:59:33.387: INFO: Deleting pod "simpletest.rc-l4ndj" in namespace "gc-5947"
  Apr 23 15:59:33.804: INFO: Deleting pod "simpletest.rc-m5nrf" in namespace "gc-5947"
  Apr 23 15:59:33.998: INFO: Deleting pod "simpletest.rc-m88pl" in namespace "gc-5947"
  Apr 23 15:59:34.194: INFO: Deleting pod "simpletest.rc-m8xkg" in namespace "gc-5947"
  Apr 23 15:59:34.403: INFO: Deleting pod "simpletest.rc-m925k" in namespace "gc-5947"
  Apr 23 15:59:34.691: INFO: Deleting pod "simpletest.rc-n26qw" in namespace "gc-5947"
  Apr 23 15:59:35.095: INFO: Deleting pod "simpletest.rc-ncrz5" in namespace "gc-5947"
  Apr 23 15:59:35.629: INFO: Deleting pod "simpletest.rc-ngx92" in namespace "gc-5947"
  Apr 23 15:59:35.961: INFO: Deleting pod "simpletest.rc-nqbhm" in namespace "gc-5947"
  Apr 23 15:59:36.086: INFO: Deleting pod "simpletest.rc-ntzsn" in namespace "gc-5947"
  Apr 23 15:59:36.184: INFO: Deleting pod "simpletest.rc-p5gdb" in namespace "gc-5947"
  Apr 23 15:59:36.369: INFO: Deleting pod "simpletest.rc-pbm59" in namespace "gc-5947"
  Apr 23 15:59:36.528: INFO: Deleting pod "simpletest.rc-qn6rf" in namespace "gc-5947"
  Apr 23 15:59:36.594: INFO: Deleting pod "simpletest.rc-qvjhg" in namespace "gc-5947"
  Apr 23 15:59:36.693: INFO: Deleting pod "simpletest.rc-r68sf" in namespace "gc-5947"
  Apr 23 15:59:36.785: INFO: Deleting pod "simpletest.rc-rqdlx" in namespace "gc-5947"
  Apr 23 15:59:36.856: INFO: Deleting pod "simpletest.rc-rszqs" in namespace "gc-5947"
  Apr 23 15:59:36.939: INFO: Deleting pod "simpletest.rc-rx677" in namespace "gc-5947"
  Apr 23 15:59:37.029: INFO: Deleting pod "simpletest.rc-s9r76" in namespace "gc-5947"
  Apr 23 15:59:37.079: INFO: Deleting pod "simpletest.rc-smlfc" in namespace "gc-5947"
  Apr 23 15:59:37.130: INFO: Deleting pod "simpletest.rc-sw7pf" in namespace "gc-5947"
  Apr 23 15:59:37.179: INFO: Deleting pod "simpletest.rc-szqq6" in namespace "gc-5947"
  Apr 23 15:59:37.221: INFO: Deleting pod "simpletest.rc-t7llb" in namespace "gc-5947"
  Apr 23 15:59:37.387: INFO: Deleting pod "simpletest.rc-tfr64" in namespace "gc-5947"
  Apr 23 15:59:37.782: INFO: Deleting pod "simpletest.rc-tkwrn" in namespace "gc-5947"
  Apr 23 15:59:37.942: INFO: Deleting pod "simpletest.rc-v2cxk" in namespace "gc-5947"
  Apr 23 15:59:38.173: INFO: Deleting pod "simpletest.rc-v6trc" in namespace "gc-5947"
  Apr 23 15:59:38.432: INFO: Deleting pod "simpletest.rc-vcrml" in namespace "gc-5947"
  Apr 23 15:59:38.968: INFO: Deleting pod "simpletest.rc-vvrfj" in namespace "gc-5947"
  Apr 23 15:59:39.468: INFO: Deleting pod "simpletest.rc-vzvjh" in namespace "gc-5947"
  Apr 23 15:59:39.551: INFO: Deleting pod "simpletest.rc-w2v6l" in namespace "gc-5947"
  Apr 23 15:59:39.716: INFO: Deleting pod "simpletest.rc-wdzmm" in namespace "gc-5947"
  Apr 23 15:59:39.835: INFO: Deleting pod "simpletest.rc-wmm6m" in namespace "gc-5947"
  Apr 23 15:59:40.041: INFO: Deleting pod "simpletest.rc-wq82f" in namespace "gc-5947"
  Apr 23 15:59:40.145: INFO: Deleting pod "simpletest.rc-wqxft" in namespace "gc-5947"
  Apr 23 15:59:40.518: INFO: Deleting pod "simpletest.rc-wrfpt" in namespace "gc-5947"
  Apr 23 15:59:40.705: INFO: Deleting pod "simpletest.rc-wwd2d" in namespace "gc-5947"
  Apr 23 15:59:40.780: INFO: Deleting pod "simpletest.rc-wwdnf" in namespace "gc-5947"
  Apr 23 15:59:40.920: INFO: Deleting pod "simpletest.rc-x5ffd" in namespace "gc-5947"
  Apr 23 15:59:41.010: INFO: Deleting pod "simpletest.rc-xltrl" in namespace "gc-5947"
  Apr 23 15:59:41.108: INFO: Deleting pod "simpletest.rc-z7m88" in namespace "gc-5947"
  Apr 23 15:59:41.261: INFO: Deleting pod "simpletest.rc-z7qht" in namespace "gc-5947"
  Apr 23 15:59:41.678: INFO: Deleting pod "simpletest.rc-zlxg5" in namespace "gc-5947"
  Apr 23 15:59:41.910: INFO: Deleting pod "simpletest.rc-zvln9" in namespace "gc-5947"
  Apr 23 15:59:42.091: INFO: Deleting pod "simpletest.rc-zxfw4" in namespace "gc-5947"
  Apr 23 15:59:42.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5947" for this suite. @ 04/23/23 15:59:42.418
• [60.954 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/23/23 15:59:42.531
  Apr 23 15:59:42.531: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/23/23 15:59:42.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:59:42.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:59:42.903
  STEP: getting /apis @ 04/23/23 15:59:42.911
  STEP: getting /apis/storage.k8s.io @ 04/23/23 15:59:42.922
  STEP: getting /apis/storage.k8s.io/v1 @ 04/23/23 15:59:42.924
  STEP: creating @ 04/23/23 15:59:42.927
  STEP: watching @ 04/23/23 15:59:43.367
  Apr 23 15:59:43.367: INFO: starting watch
  STEP: getting @ 04/23/23 15:59:43.427
  STEP: listing in namespace @ 04/23/23 15:59:43.442
  STEP: listing across namespaces @ 04/23/23 15:59:43.454
  STEP: patching @ 04/23/23 15:59:43.465
  STEP: updating @ 04/23/23 15:59:43.476
  Apr 23 15:59:43.522: INFO: waiting for watch events with expected annotations in namespace
  Apr 23 15:59:43.522: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/23/23 15:59:43.524
  STEP: deleting a collection @ 04/23/23 15:59:43.598
  Apr 23 15:59:43.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-5374" for this suite. @ 04/23/23 15:59:43.696
• [1.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 04/23/23 15:59:43.743
  Apr 23 15:59:43.743: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 15:59:43.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 15:59:43.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 15:59:43.779
  Apr 23 15:59:43.832: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/23/23 15:59:43.843
  Apr 23 15:59:43.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:43.849: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/23/23 15:59:43.85
  Apr 23 15:59:43.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:43.922: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:44.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:44.956: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:45.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:45.951: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:47.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:47.100: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:47.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:47.950: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:48.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:48.985: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:49.936: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:49.936: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:51.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:51.005: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:51.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:51.943: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:52.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:52.930: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:53.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:53.944: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:54.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 15:59:54.938: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/23/23 15:59:54.946
  Apr 23 15:59:54.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 15:59:54.991: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Apr 23 15:59:56.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:56.056: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/23/23 15:59:56.056
  Apr 23 15:59:56.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:56.096: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:57.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:57.107: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:58.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:58.120: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 15:59:59.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 15:59:59.111: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 16:00:00.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 16:00:00.112: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 16:00:01.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 16:00:01.104: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 16:00:02.121: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 16:00:02.122: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 16:00:03.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 16:00:03.115: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 16:00:04.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 16:00:04.105: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 16:00:04.117
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6350, will wait for the garbage collector to delete the pods @ 04/23/23 16:00:04.117
  Apr 23 16:00:04.201: INFO: Deleting DaemonSet.extensions daemon-set took: 27.585688ms
  Apr 23 16:00:04.302: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.047034ms
  Apr 23 16:00:06.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 16:00:06.413: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 16:00:06.418: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24849"},"items":null}

  Apr 23 16:00:06.423: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24849"},"items":null}

  Apr 23 16:00:06.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6350" for this suite. @ 04/23/23 16:00:06.496
• [22.780 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/23/23 16:00:06.531
  Apr 23 16:00:06.532: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename job @ 04/23/23 16:00:06.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:00:06.656
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:00:06.683
  STEP: Creating a suspended job @ 04/23/23 16:00:06.732
  STEP: Patching the Job @ 04/23/23 16:00:06.751
  STEP: Watching for Job to be patched @ 04/23/23 16:00:06.824
  Apr 23 16:00:06.829: INFO: Event ADDED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 23 16:00:06.829: INFO: Event MODIFIED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 23 16:00:06.829: INFO: Event MODIFIED found for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/23/23 16:00:06.829
  STEP: Watching for Job to be updated @ 04/23/23 16:00:06.869
  Apr 23 16:00:06.873: INFO: Event MODIFIED found for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 16:00:06.873: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/23/23 16:00:06.873
  Apr 23 16:00:06.905: INFO: Job: e2e-6hh7z as labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z]
  STEP: Waiting for job to complete @ 04/23/23 16:00:06.905
  STEP: Delete a job collection with a labelselector @ 04/23/23 16:00:16.936
  STEP: Watching for Job to be deleted @ 04/23/23 16:00:16.967
  Apr 23 16:00:16.975: INFO: Event MODIFIED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 16:00:16.976: INFO: Event MODIFIED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 16:00:16.976: INFO: Event MODIFIED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 16:00:16.977: INFO: Event MODIFIED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 16:00:16.977: INFO: Event MODIFIED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 16:00:16.977: INFO: Event MODIFIED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 16:00:16.978: INFO: Event MODIFIED observed for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 16:00:16.979: INFO: Event DELETED found for Job e2e-6hh7z in namespace job-177 with labels: map[e2e-6hh7z:patched e2e-job-label:e2e-6hh7z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/23/23 16:00:16.979
  Apr 23 16:00:16.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-177" for this suite. @ 04/23/23 16:00:17.007
• [10.524 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/23/23 16:00:17.059
  Apr 23 16:00:17.059: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:00:17.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:00:17.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:00:17.165
  STEP: validating cluster-info @ 04/23/23 16:00:17.182
  Apr 23 16:00:17.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-6034 cluster-info'
  Apr 23 16:00:17.468: INFO: stderr: ""
  Apr 23 16:00:17.470: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 23 16:00:17.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6034" for this suite. @ 04/23/23 16:00:17.522
• [0.541 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/23/23 16:00:17.6
  Apr 23 16:00:17.600: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:00:17.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:00:17.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:00:17.657
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-4292 @ 04/23/23 16:00:17.666
  STEP: changing the ExternalName service to type=ClusterIP @ 04/23/23 16:00:17.676
  STEP: creating replication controller externalname-service in namespace services-4292 @ 04/23/23 16:00:17.711
  I0423 16:00:17.727745      15 runners.go:194] Created replication controller with name: externalname-service, namespace: services-4292, replica count: 2
  I0423 16:00:20.780094      15 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:00:20.780: INFO: Creating new exec pod
  Apr 23 16:00:25.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-4292 exec execpodstmk6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 23 16:00:26.161: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 23 16:00:26.161: INFO: stdout: "externalname-service-p2gsc"
  Apr 23 16:00:26.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-4292 exec execpodstmk6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.234 80'
  Apr 23 16:00:26.443: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.234 80\nConnection to 10.233.4.234 80 port [tcp/http] succeeded!\n"
  Apr 23 16:00:26.443: INFO: stdout: ""
  Apr 23 16:00:27.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-4292 exec execpodstmk6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.234 80'
  Apr 23 16:00:27.838: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.234 80\nConnection to 10.233.4.234 80 port [tcp/http] succeeded!\n"
  Apr 23 16:00:27.838: INFO: stdout: ""
  Apr 23 16:00:28.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-4292 exec execpodstmk6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.4.234 80'
  Apr 23 16:00:28.934: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.4.234 80\nConnection to 10.233.4.234 80 port [tcp/http] succeeded!\n"
  Apr 23 16:00:28.935: INFO: stdout: "externalname-service-p2gsc"
  Apr 23 16:00:28.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:00:28.949: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-4292" for this suite. @ 04/23/23 16:00:29.041
• [11.463 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/23/23 16:00:29.064
  Apr 23 16:00:29.064: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename controllerrevisions @ 04/23/23 16:00:29.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:00:29.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:00:29.151
  STEP: Creating DaemonSet "e2e-n75fj-daemon-set" @ 04/23/23 16:00:29.234
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 16:00:29.275
  Apr 23 16:00:29.364: INFO: Number of nodes with available pods controlled by daemonset e2e-n75fj-daemon-set: 0
  Apr 23 16:00:29.364: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 16:00:30.418: INFO: Number of nodes with available pods controlled by daemonset e2e-n75fj-daemon-set: 0
  Apr 23 16:00:30.418: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 16:00:31.387: INFO: Number of nodes with available pods controlled by daemonset e2e-n75fj-daemon-set: 1
  Apr 23 16:00:31.387: INFO: Node soodi4ja4shi-1 is running 0 daemon pod, expected 1
  Apr 23 16:00:32.385: INFO: Number of nodes with available pods controlled by daemonset e2e-n75fj-daemon-set: 2
  Apr 23 16:00:32.385: INFO: Node soodi4ja4shi-3 is running 0 daemon pod, expected 1
  Apr 23 16:00:33.386: INFO: Number of nodes with available pods controlled by daemonset e2e-n75fj-daemon-set: 3
  Apr 23 16:00:33.386: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-n75fj-daemon-set
  STEP: Confirm DaemonSet "e2e-n75fj-daemon-set" successfully created with "daemonset-name=e2e-n75fj-daemon-set" label @ 04/23/23 16:00:33.402
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-n75fj-daemon-set" @ 04/23/23 16:00:33.416
  Apr 23 16:00:33.424: INFO: Located ControllerRevision: "e2e-n75fj-daemon-set-59b85c79ff"
  STEP: Patching ControllerRevision "e2e-n75fj-daemon-set-59b85c79ff" @ 04/23/23 16:00:33.435
  Apr 23 16:00:33.448: INFO: e2e-n75fj-daemon-set-59b85c79ff has been patched
  STEP: Create a new ControllerRevision @ 04/23/23 16:00:33.448
  Apr 23 16:00:33.523: INFO: Created ControllerRevision: e2e-n75fj-daemon-set-5cbcbbb947
  STEP: Confirm that there are two ControllerRevisions @ 04/23/23 16:00:33.524
  Apr 23 16:00:33.524: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 16:00:33.531: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-n75fj-daemon-set-59b85c79ff" @ 04/23/23 16:00:33.532
  STEP: Confirm that there is only one ControllerRevision @ 04/23/23 16:00:33.562
  Apr 23 16:00:33.562: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 16:00:33.571: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-n75fj-daemon-set-5cbcbbb947" @ 04/23/23 16:00:33.579
  Apr 23 16:00:33.620: INFO: e2e-n75fj-daemon-set-5cbcbbb947 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/23/23 16:00:33.621
  W0423 16:00:33.638511      15 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/23/23 16:00:33.639
  Apr 23 16:00:33.639: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 16:00:34.645: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 16:00:34.786: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-n75fj-daemon-set-5cbcbbb947=updated" @ 04/23/23 16:00:34.786
  STEP: Confirm that there is only one ControllerRevision @ 04/23/23 16:00:34.882
  Apr 23 16:00:34.883: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 16:00:34.903: INFO: Found 1 ControllerRevisions
  Apr 23 16:00:34.938: INFO: ControllerRevision "e2e-n75fj-daemon-set-86d5d8f794" has revision 3
  STEP: Deleting DaemonSet "e2e-n75fj-daemon-set" @ 04/23/23 16:00:34.953
  STEP: deleting DaemonSet.extensions e2e-n75fj-daemon-set in namespace controllerrevisions-5202, will wait for the garbage collector to delete the pods @ 04/23/23 16:00:34.954
  Apr 23 16:00:35.044: INFO: Deleting DaemonSet.extensions e2e-n75fj-daemon-set took: 22.903717ms
  Apr 23 16:00:35.145: INFO: Terminating DaemonSet.extensions e2e-n75fj-daemon-set pods took: 101.219411ms
  Apr 23 16:00:36.452: INFO: Number of nodes with available pods controlled by daemonset e2e-n75fj-daemon-set: 0
  Apr 23 16:00:36.452: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-n75fj-daemon-set
  Apr 23 16:00:36.459: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25211"},"items":null}

  Apr 23 16:00:36.468: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25211"},"items":null}

  Apr 23 16:00:36.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-5202" for this suite. @ 04/23/23 16:00:36.562
• [7.525 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/23/23 16:00:36.594
  Apr 23 16:00:36.594: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename security-context-test @ 04/23/23 16:00:36.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:00:36.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:00:36.666
  Apr 23 16:00:40.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3822" for this suite. @ 04/23/23 16:00:40.746
• [4.174 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/23/23 16:00:40.774
  Apr 23 16:00:40.774: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:00:40.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:00:40.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:00:40.826
  STEP: Setting up server cert @ 04/23/23 16:00:40.902
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:00:42.065
  STEP: Deploying the webhook pod @ 04/23/23 16:00:42.086
  STEP: Wait for the deployment to be ready @ 04/23/23 16:00:42.128
  Apr 23 16:00:42.201: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Apr 23 16:00:44.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 0, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 0, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 0, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 0, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/23/23 16:00:46.228
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:00:46.255
  Apr 23 16:00:47.256: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/23/23 16:00:47.263
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/23/23 16:00:47.263
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/23/23 16:00:47.293
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/23/23 16:00:48.313
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/23/23 16:00:48.314
  STEP: Having no error when timeout is longer than webhook latency @ 04/23/23 16:00:49.369
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/23/23 16:00:49.369
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/23/23 16:00:54.471
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/23/23 16:00:54.471
  Apr 23 16:00:59.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8032" for this suite. @ 04/23/23 16:00:59.74
  STEP: Destroying namespace "webhook-markers-6914" for this suite. @ 04/23/23 16:00:59.772
• [19.015 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/23/23 16:00:59.79
  Apr 23 16:00:59.790: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:00:59.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:00:59.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:00:59.839
  STEP: starting the proxy server @ 04/23/23 16:00:59.855
  Apr 23 16:00:59.864: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-1265 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/23/23 16:01:00.019
  Apr 23 16:01:00.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1265" for this suite. @ 04/23/23 16:01:00.061
• [0.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/23/23 16:01:00.077
  Apr 23 16:01:00.077: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 16:01:00.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:01:00.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:01:00.109
  STEP: creating the pod with failed condition @ 04/23/23 16:01:00.113
  STEP: updating the pod @ 04/23/23 16:03:00.135
  Apr 23 16:03:00.658: INFO: Successfully updated pod "var-expansion-0e8ab09a-e69f-45f2-beed-54ed79d58b84"
  STEP: waiting for pod running @ 04/23/23 16:03:00.661
  STEP: deleting the pod gracefully @ 04/23/23 16:03:02.692
  Apr 23 16:03:02.692: INFO: Deleting pod "var-expansion-0e8ab09a-e69f-45f2-beed-54ed79d58b84" in namespace "var-expansion-3593"
  Apr 23 16:03:02.705: INFO: Wait up to 5m0s for pod "var-expansion-0e8ab09a-e69f-45f2-beed-54ed79d58b84" to be fully deleted
  Apr 23 16:03:34.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3593" for this suite. @ 04/23/23 16:03:34.905
• [154.850 seconds]
------------------------------
S
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/23/23 16:03:34.93
  Apr 23 16:03:34.930: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename events @ 04/23/23 16:03:34.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:03:34.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:03:34.987
  STEP: creating a test event @ 04/23/23 16:03:34.996
  STEP: listing all events in all namespaces @ 04/23/23 16:03:35.015
  STEP: patching the test event @ 04/23/23 16:03:35.027
  STEP: fetching the test event @ 04/23/23 16:03:35.046
  STEP: updating the test event @ 04/23/23 16:03:35.055
  STEP: getting the test event @ 04/23/23 16:03:35.08
  STEP: deleting the test event @ 04/23/23 16:03:35.1
  STEP: listing all events in all namespaces @ 04/23/23 16:03:35.129
  Apr 23 16:03:35.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6948" for this suite. @ 04/23/23 16:03:35.157
• [0.247 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/23/23 16:03:35.177
  Apr 23 16:03:35.177: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 16:03:35.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:03:35.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:03:35.265
  Apr 23 16:03:37.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4573" for this suite. @ 04/23/23 16:03:37.368
• [2.207 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/23/23 16:03:37.386
  Apr 23 16:03:37.386: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:03:37.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:03:37.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:03:37.431
  STEP: creating pod @ 04/23/23 16:03:37.437
  Apr 23 16:03:39.563: INFO: Pod pod-hostip-1705bfa1-d47f-42e2-bbe3-91c27beeba7d has hostIP: 192.168.121.96
  Apr 23 16:03:39.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4196" for this suite. @ 04/23/23 16:03:39.575
• [2.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/23/23 16:03:39.602
  Apr 23 16:03:39.602: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-pred @ 04/23/23 16:03:39.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:03:39.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:03:39.651
  Apr 23 16:03:39.657: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 23 16:03:39.689: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 16:03:39.703: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-1 before test
  Apr 23 16:03:39.719: INFO: cilium-8bln9 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.720: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:03:39.720: INFO: cilium-node-init-7rprd from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.721: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:03:39.721: INFO: coredns-5d78c9869d-rztdc from kube-system started at 2023-04-23 15:02:07 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.722: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 16:03:39.723: INFO: kube-addon-manager-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:58:03 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.724: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 23 16:03:39.725: INFO: kube-apiserver-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.730: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 23 16:03:39.731: INFO: kube-controller-manager-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.732: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 23 16:03:39.733: INFO: kube-proxy-lgs9b from kube-system started at 2023-04-23 14:55:18 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.733: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:03:39.734: INFO: kube-scheduler-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.735: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 23 16:03:39.735: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-gg8kg from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:03:39.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:03:39.736: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 16:03:39.737: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-2 before test
  Apr 23 16:03:39.764: INFO: cilium-node-init-s5qwm from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.764: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:03:39.765: INFO: cilium-q8j55 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.765: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:03:39.765: INFO: coredns-5d78c9869d-jk5pb from kube-system started at 2023-04-23 14:59:51 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.766: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 16:03:39.766: INFO: kube-addon-manager-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:58:02 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.767: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 23 16:03:39.767: INFO: kube-apiserver-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.768: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 23 16:03:39.768: INFO: kube-controller-manager-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.769: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 23 16:03:39.770: INFO: kube-proxy-7j684 from kube-system started at 2023-04-23 14:55:59 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.771: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:03:39.772: INFO: kube-scheduler-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.773: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 23 16:03:39.773: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-z9sfh from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:03:39.774: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:03:39.775: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 16:03:39.775: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-3 before test
  Apr 23 16:03:39.792: INFO: cilium-hqn2w from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.793: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:03:39.793: INFO: cilium-node-init-hq966 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.794: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:03:39.794: INFO: cilium-operator-85fcfcb8b4-lvsvw from kube-system started at 2023-04-23 14:58:17 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.795: INFO: 	Container cilium-operator ready: true, restart count 0
  Apr 23 16:03:39.795: INFO: kube-proxy-qhxf2 from kube-system started at 2023-04-23 14:56:32 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.796: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:03:39.797: INFO: busybox-readonly-fsf9f2c0fa-1fca-48b7-a364-d1efb359b029 from kubelet-test-4573 started at 2023-04-23 16:03:35 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.797: INFO: 	Container busybox-readonly-fsf9f2c0fa-1fca-48b7-a364-d1efb359b029 ready: true, restart count 0
  Apr 23 16:03:39.798: INFO: pod-hostip-1705bfa1-d47f-42e2-bbe3-91c27beeba7d from pods-4196 started at 2023-04-23 16:03:37 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.798: INFO: 	Container test ready: true, restart count 0
  Apr 23 16:03:39.799: INFO: sonobuoy from sonobuoy started at 2023-04-23 14:59:49 +0000 UTC (1 container statuses recorded)
  Apr 23 16:03:39.800: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 23 16:03:39.801: INFO: sonobuoy-e2e-job-e20a28f544554453 from sonobuoy started at 2023-04-23 15:00:04 +0000 UTC (2 container statuses recorded)
  Apr 23 16:03:39.801: INFO: 	Container e2e ready: true, restart count 0
  Apr 23 16:03:39.802: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:03:39.802: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-pxgxt from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:03:39.802: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:03:39.802: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/23/23 16:03:39.802
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/23/23 16:03:41.861
  STEP: Trying to apply a random label on the found node. @ 04/23/23 16:03:41.901
  STEP: verifying the node has the label kubernetes.io/e2e-cdf32f60-c908-4b8b-8b79-761899063bb1 95 @ 04/23/23 16:03:41.932
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/23/23 16:03:41.956
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.96 on the node which pod4 resides and expect not scheduled @ 04/23/23 16:03:43.983
  STEP: removing the label kubernetes.io/e2e-cdf32f60-c908-4b8b-8b79-761899063bb1 off the node soodi4ja4shi-3 @ 04/23/23 16:08:43.998
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-cdf32f60-c908-4b8b-8b79-761899063bb1 @ 04/23/23 16:08:44.032
  Apr 23 16:08:44.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3349" for this suite. @ 04/23/23 16:08:44.066
• [304.482 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/23/23 16:08:44.087
  Apr 23 16:08:44.087: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:08:44.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:08:44.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:08:44.139
  STEP: creating the pod @ 04/23/23 16:08:44.145
  STEP: setting up watch @ 04/23/23 16:08:44.146
  STEP: submitting the pod to kubernetes @ 04/23/23 16:08:44.26
  STEP: verifying the pod is in kubernetes @ 04/23/23 16:08:44.296
  STEP: verifying pod creation was observed @ 04/23/23 16:08:44.325
  STEP: deleting the pod gracefully @ 04/23/23 16:08:48.386
  STEP: verifying pod deletion was observed @ 04/23/23 16:08:48.41
  Apr 23 16:08:50.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-712" for this suite. @ 04/23/23 16:08:50.084
• [6.011 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/23/23 16:08:50.102
  Apr 23 16:08:50.102: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:08:50.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:08:50.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:08:50.18
  STEP: creating service multi-endpoint-test in namespace services-2891 @ 04/23/23 16:08:50.197
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2891 to expose endpoints map[] @ 04/23/23 16:08:50.224
  Apr 23 16:08:50.239: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Apr 23 16:08:51.265: INFO: successfully validated that service multi-endpoint-test in namespace services-2891 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2891 @ 04/23/23 16:08:51.266
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2891 to expose endpoints map[pod1:[100]] @ 04/23/23 16:08:55.346
  Apr 23 16:08:55.377: INFO: successfully validated that service multi-endpoint-test in namespace services-2891 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-2891 @ 04/23/23 16:08:55.377
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2891 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/23/23 16:08:57.424
  Apr 23 16:08:57.456: INFO: successfully validated that service multi-endpoint-test in namespace services-2891 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/23/23 16:08:57.456
  Apr 23 16:08:57.456: INFO: Creating new exec pod
  Apr 23 16:09:00.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-2891 exec execpodndtf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Apr 23 16:09:00.985: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 23 16:09:00.985: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:09:00.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-2891 exec execpodndtf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.18.196 80'
  Apr 23 16:09:01.456: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.18.196 80\nConnection to 10.233.18.196 80 port [tcp/http] succeeded!\n"
  Apr 23 16:09:01.456: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:09:01.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-2891 exec execpodndtf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Apr 23 16:09:01.892: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 23 16:09:01.893: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:09:01.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-2891 exec execpodndtf5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.18.196 81'
  Apr 23 16:09:02.281: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.18.196 81\nConnection to 10.233.18.196 81 port [tcp/*] succeeded!\n"
  Apr 23 16:09:02.281: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2891 @ 04/23/23 16:09:02.281
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2891 to expose endpoints map[pod2:[101]] @ 04/23/23 16:09:02.346
  Apr 23 16:09:02.504: INFO: successfully validated that service multi-endpoint-test in namespace services-2891 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-2891 @ 04/23/23 16:09:02.504
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2891 to expose endpoints map[] @ 04/23/23 16:09:02.751
  Apr 23 16:09:02.857: INFO: successfully validated that service multi-endpoint-test in namespace services-2891 exposes endpoints map[]
  Apr 23 16:09:02.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2891" for this suite. @ 04/23/23 16:09:03.005
• [12.936 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/23/23 16:09:03.038
  Apr 23 16:09:03.038: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:09:03.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:09:03.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:09:03.116
  STEP: Setting up server cert @ 04/23/23 16:09:03.188
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:09:04.998
  STEP: Deploying the webhook pod @ 04/23/23 16:09:05.016
  STEP: Wait for the deployment to be ready @ 04/23/23 16:09:05.04
  Apr 23 16:09:05.061: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Apr 23 16:09:07.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 9, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 9, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 9, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 9, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/23/23 16:09:09.104
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:09:09.143
  Apr 23 16:09:10.144: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/23/23 16:09:10.151
  STEP: create a namespace for the webhook @ 04/23/23 16:09:10.187
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/23/23 16:09:10.238
  Apr 23 16:09:10.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3277" for this suite. @ 04/23/23 16:09:10.437
  STEP: Destroying namespace "webhook-markers-6379" for this suite. @ 04/23/23 16:09:10.466
  STEP: Destroying namespace "fail-closed-namespace-2821" for this suite. @ 04/23/23 16:09:10.488
• [7.473 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/23/23 16:09:10.514
  Apr 23 16:09:10.514: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename watch @ 04/23/23 16:09:10.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:09:10.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:09:10.563
  STEP: creating a watch on configmaps with label A @ 04/23/23 16:09:10.576
  STEP: creating a watch on configmaps with label B @ 04/23/23 16:09:10.58
  STEP: creating a watch on configmaps with label A or B @ 04/23/23 16:09:10.582
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/23/23 16:09:10.585
  Apr 23 16:09:10.669: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9682  7c814bc6-6a81-455a-9ca8-6c3078e59f82 26835 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:09:10.672: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9682  7c814bc6-6a81-455a-9ca8-6c3078e59f82 26835 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/23/23 16:09:10.673
  Apr 23 16:09:10.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9682  7c814bc6-6a81-455a-9ca8-6c3078e59f82 26836 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:09:10.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9682  7c814bc6-6a81-455a-9ca8-6c3078e59f82 26836 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/23/23 16:09:10.744
  Apr 23 16:09:10.783: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9682  7c814bc6-6a81-455a-9ca8-6c3078e59f82 26837 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:09:10.784: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9682  7c814bc6-6a81-455a-9ca8-6c3078e59f82 26837 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/23/23 16:09:10.785
  Apr 23 16:09:10.810: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9682  7c814bc6-6a81-455a-9ca8-6c3078e59f82 26838 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:09:10.811: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9682  7c814bc6-6a81-455a-9ca8-6c3078e59f82 26838 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/23/23 16:09:10.811
  Apr 23 16:09:10.826: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9682  824861e3-f1ae-4d76-8bcc-7ddd93301b1e 26839 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:09:10.826: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9682  824861e3-f1ae-4d76-8bcc-7ddd93301b1e 26839 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/23/23 16:09:20.828
  Apr 23 16:09:20.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9682  824861e3-f1ae-4d76-8bcc-7ddd93301b1e 26880 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:09:20.849: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9682  824861e3-f1ae-4d76-8bcc-7ddd93301b1e 26880 0 2023-04-23 16:09:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-23 16:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:09:30.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9682" for this suite. @ 04/23/23 16:09:30.874
• [20.398 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/23/23 16:09:30.914
  Apr 23 16:09:30.914: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 16:09:30.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:09:30.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:09:30.977
  STEP: creating a ReplicationController @ 04/23/23 16:09:30.993
  STEP: waiting for RC to be added @ 04/23/23 16:09:31.009
  STEP: waiting for available Replicas @ 04/23/23 16:09:31.01
  STEP: patching ReplicationController @ 04/23/23 16:09:33.777
  STEP: waiting for RC to be modified @ 04/23/23 16:09:33.797
  STEP: patching ReplicationController status @ 04/23/23 16:09:33.798
  STEP: waiting for RC to be modified @ 04/23/23 16:09:33.821
  STEP: waiting for available Replicas @ 04/23/23 16:09:33.821
  STEP: fetching ReplicationController status @ 04/23/23 16:09:33.835
  STEP: patching ReplicationController scale @ 04/23/23 16:09:33.848
  STEP: waiting for RC to be modified @ 04/23/23 16:09:33.867
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/23/23 16:09:33.868
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/23/23 16:09:36.303
  STEP: updating ReplicationController status @ 04/23/23 16:09:36.323
  STEP: waiting for RC to be modified @ 04/23/23 16:09:36.432
  STEP: listing all ReplicationControllers @ 04/23/23 16:09:36.433
  STEP: checking that ReplicationController has expected values @ 04/23/23 16:09:36.449
  STEP: deleting ReplicationControllers by collection @ 04/23/23 16:09:36.45
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/23/23 16:09:36.519
  Apr 23 16:09:36.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 16:09:36.904719      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-4060" for this suite. @ 04/23/23 16:09:36.921
• [6.020 seconds]
------------------------------
SSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/23/23 16:09:36.936
  Apr 23 16:09:36.936: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename containers @ 04/23/23 16:09:36.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:09:36.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:09:37.003
  STEP: Creating a pod to test override all @ 04/23/23 16:09:37.015
  E0423 16:09:37.905131      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:38.904835      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:39.909328      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:40.906004      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:41.906108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:42.906758      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:09:43.104
  Apr 23 16:09:43.130: INFO: Trying to get logs from node soodi4ja4shi-3 pod client-containers-835e07ec-f260-470f-9b0c-5ccacb34bf22 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:09:43.203
  Apr 23 16:09:43.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8039" for this suite. @ 04/23/23 16:09:43.361
• [6.467 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/23/23 16:09:43.41
  Apr 23 16:09:43.410: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:09:43.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:09:43.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:09:43.479
  STEP: Creating configMap with name projected-configmap-test-volume-1605fe66-8d54-405a-bf5f-2a0a74e829b2 @ 04/23/23 16:09:43.489
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:09:43.505
  E0423 16:09:43.907297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:44.908059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:45.908211      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:46.908541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:09:47.579
  Apr 23 16:09:47.587: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-configmaps-a7f26754-e53a-40b5-bd9a-3c0005bb4786 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:09:47.607
  Apr 23 16:09:47.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8923" for this suite. @ 04/23/23 16:09:47.668
• [4.280 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/23/23 16:09:47.699
  Apr 23 16:09:47.700: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:09:47.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:09:47.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:09:47.753
  STEP: Creating secret with name secret-test-19c14d57-82a5-4b6f-870d-5af8224e28f3 @ 04/23/23 16:09:47.759
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:09:47.777
  E0423 16:09:47.908730      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:48.909549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:49.910305      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:50.911211      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:09:51.858
  Apr 23 16:09:51.878: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-secrets-29161cf7-c564-46d0-812f-a0f3055b33a1 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:09:51.9
  E0423 16:09:51.911168      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:09:51.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2374" for this suite. @ 04/23/23 16:09:51.981
• [4.307 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/23/23 16:09:52.01
  Apr 23 16:09:52.010: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 16:09:52.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:09:52.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:09:52.065
  STEP: set up a multi version CRD @ 04/23/23 16:09:52.071
  Apr 23 16:09:52.072: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:09:52.912300      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:53.913058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:54.912841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:55.913230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:56.913429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:57.914586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 04/23/23 16:09:58.209
  STEP: check the new version name is served @ 04/23/23 16:09:58.244
  E0423 16:09:58.915521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:09:59.915513      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 04/23/23 16:10:00.766
  E0423 16:10:00.915811      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:01.916663      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/23/23 16:10:02.007
  E0423 16:10:02.916923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:03.917539      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:04.922951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:05.923755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:10:06.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9532" for this suite. @ 04/23/23 16:10:06.802
• [14.874 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/23/23 16:10:06.887
  Apr 23 16:10:06.888: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename deployment @ 04/23/23 16:10:06.89
  E0423 16:10:06.924098      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:10:06.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:10:06.967
  Apr 23 16:10:06.975: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 23 16:10:07.002: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0423 16:10:07.924315      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:08.924890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:09.925063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:10.925255      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:11.925420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:10:12.014: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 16:10:12.014
  Apr 23 16:10:12.014: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 23 16:10:12.033: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 23 16:10:12.054: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0423 16:10:12.959781      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:13.959946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:10:14.084: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 23 16:10:14.093: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 23 16:10:14.133: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6122  e8a4f737-bb88-454c-95d6-a674ff948ccf 27230 1 2023-04-23 16:10:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-23 16:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004db9cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-23 16:10:12 +0000 UTC,LastTransitionTime:2023-04-23 16:10:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-04-23 16:10:13 +0000 UTC,LastTransitionTime:2023-04-23 16:10:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 23 16:10:14.146: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-6122  95438437-9735-440e-9886-bc986d9d7a5d 27219 1 2023-04-23 16:10:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e8a4f737-bb88-454c-95d6-a674ff948ccf 0xc003eb0657 0xc003eb0658}] [] [{kube-controller-manager Update apps/v1 2023-04-23 16:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8a4f737-bb88-454c-95d6-a674ff948ccf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:10:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb07d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 16:10:14.147: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 23 16:10:14.147: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6122  2ac37fae-8ea4-48da-b963-a0c05e93dc31 27229 2 2023-04-23 16:10:06 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e8a4f737-bb88-454c-95d6-a674ff948ccf 0xc003eb01b7 0xc003eb01b8}] [] [{e2e.test Update apps/v1 2023-04-23 16:10:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:10:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8a4f737-bb88-454c-95d6-a674ff948ccf\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:10:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003eb0548 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 16:10:14.159: INFO: Pod "test-rolling-update-deployment-656d657cd8-fh447" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-fh447 test-rolling-update-deployment-656d657cd8- deployment-6122  630685c9-fe99-447f-b660-4f5419526144 27218 0 2023-04-23 16:10:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 95438437-9735-440e-9886-bc986d9d7a5d 0xc003eb1567 0xc003eb1568}] [] [{kube-controller-manager Update v1 2023-04-23 16:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95438437-9735-440e-9886-bc986d9d7a5d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mc9nj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mc9nj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.66.197,StartTime:2023-04-23 16:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:10:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://6bb056c8fb50769d3f5a91fefa4d0a05cb0b9782464b60bc8d9b63f82119eb45,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.197,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:10:14.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6122" for this suite. @ 04/23/23 16:10:14.177
• [7.310 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/23/23 16:10:14.203
  Apr 23 16:10:14.203: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:10:14.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:10:14.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:10:14.259
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:10:14.265
  E0423 16:10:14.959969      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:15.960015      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:16.960397      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:17.961529      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:10:18.327
  Apr 23 16:10:18.335: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-6e74345f-b9c0-4650-807e-ecff4c6af8b0 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:10:18.37
  Apr 23 16:10:18.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2832" for this suite. @ 04/23/23 16:10:18.429
• [4.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 04/23/23 16:10:18.45
  Apr 23 16:10:18.450: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 16:10:18.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:10:18.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:10:18.5
  Apr 23 16:10:18.509: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:10:18.962566      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:19.963589      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:20.963766      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0423 16:10:21.361837      15 warnings.go:70] unknown field "alpha"
  W0423 16:10:21.361906      15 warnings.go:70] unknown field "beta"
  W0423 16:10:21.361928      15 warnings.go:70] unknown field "delta"
  W0423 16:10:21.361949      15 warnings.go:70] unknown field "epsilon"
  W0423 16:10:21.361967      15 warnings.go:70] unknown field "gamma"
  Apr 23 16:10:21.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9988" for this suite. @ 04/23/23 16:10:21.503
• [3.095 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/23/23 16:10:21.55
  Apr 23 16:10:21.550: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 16:10:21.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:10:21.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:10:21.599
  E0423 16:10:21.963886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:22.964012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:23.964295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:24.964562      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:10:25.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1825" for this suite. @ 04/23/23 16:10:25.734
• [4.214 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/23/23 16:10:25.77
  Apr 23 16:10:25.771: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:10:25.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:10:25.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:10:25.844
  STEP: validating api versions @ 04/23/23 16:10:25.865
  Apr 23 16:10:25.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4558 api-versions'
  E0423 16:10:25.966022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:10:26.193: INFO: stderr: ""
  Apr 23 16:10:26.193: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Apr 23 16:10:26.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4558" for this suite. @ 04/23/23 16:10:26.204
• [0.447 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/23/23 16:10:26.22
  Apr 23 16:10:26.220: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:10:26.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:10:26.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:10:26.263
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-36b47e7d-14f5-4abf-9251-f8fea3868adb @ 04/23/23 16:10:26.281
  STEP: Creating the pod @ 04/23/23 16:10:26.3
  E0423 16:10:26.970760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:27.970783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-36b47e7d-14f5-4abf-9251-f8fea3868adb @ 04/23/23 16:10:28.375
  STEP: waiting to observe update in volume @ 04/23/23 16:10:28.391
  E0423 16:10:28.971794      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:29.971955      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:30.972053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:31.972361      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:32.972606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:33.972880      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:34.973911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:35.974064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:36.975082      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:37.975489      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:38.976102      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:39.976238      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:40.976497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:41.976615      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:42.977620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:43.977590      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:44.977694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:45.978142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:46.978782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:47.978955      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:48.980096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:49.980180      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:50.980387      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:51.980685      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:52.981412      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:53.981714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:54.982944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:55.983198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:56.983836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:57.984101      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:58.984545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:10:59.984785      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:00.985481      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:01.986590      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:02.988460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:03.988213      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:04.989100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:05.989079      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:06.989834      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:07.990725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:08.990995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:09.991862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:10.992656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:11.992700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:12.993688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:13.993810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:14.994065      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:15.995063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:16.995340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:17.996089      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:18.996281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:19.997345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:20.997679      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:21.997990      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:22.998097      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:23.998950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:24.999967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:26.000011      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:27.000747      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:28.000858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:29.001009      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:30.002058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:31.002167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:32.002339      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:33.002582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:34.002792      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:35.003056      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:36.004171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:37.004547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:38.005637      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:39.005642      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:40.006158      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:41.006138      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:11:41.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6869" for this suite. @ 04/23/23 16:11:41.817
• [75.617 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/23/23 16:11:41.841
  Apr 23 16:11:41.841: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:11:41.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:11:41.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:11:41.906
  STEP: Creating configMap with name projected-configmap-test-volume-c5299dd0-1db8-4246-8e1e-48e710ba177d @ 04/23/23 16:11:41.915
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:11:41.94
  E0423 16:11:42.006485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:43.006611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:44.007571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:45.007743      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:11:46.001
  E0423 16:11:46.008962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:11:46.010: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-configmaps-ca9b97ce-b7d7-4740-9adc-b01948aba643 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:11:46.032
  Apr 23 16:11:46.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9096" for this suite. @ 04/23/23 16:11:46.087
• [4.271 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/23/23 16:11:46.118
  Apr 23 16:11:46.119: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:11:46.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:11:46.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:11:46.178
  Apr 23 16:11:46.187: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: creating the pod @ 04/23/23 16:11:46.189
  STEP: submitting the pod to kubernetes @ 04/23/23 16:11:46.189
  E0423 16:11:47.079016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:48.067834      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:49.068242      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:50.068827      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:11:50.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5291" for this suite. @ 04/23/23 16:11:50.402
• [4.298 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/23/23 16:11:50.418
  Apr 23 16:11:50.418: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 16:11:50.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:11:50.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:11:50.463
  STEP: Creating a cronjob @ 04/23/23 16:11:50.47
  STEP: creating @ 04/23/23 16:11:50.472
  STEP: getting @ 04/23/23 16:11:50.488
  STEP: listing @ 04/23/23 16:11:50.495
  STEP: watching @ 04/23/23 16:11:50.503
  Apr 23 16:11:50.504: INFO: starting watch
  STEP: cluster-wide listing @ 04/23/23 16:11:50.506
  STEP: cluster-wide watching @ 04/23/23 16:11:50.512
  Apr 23 16:11:50.512: INFO: starting watch
  STEP: patching @ 04/23/23 16:11:50.515
  STEP: updating @ 04/23/23 16:11:50.531
  Apr 23 16:11:50.552: INFO: waiting for watch events with expected annotations
  Apr 23 16:11:50.553: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/23/23 16:11:50.553
  STEP: updating /status @ 04/23/23 16:11:50.622
  STEP: get /status @ 04/23/23 16:11:50.765
  STEP: deleting @ 04/23/23 16:11:50.834
  STEP: deleting a collection @ 04/23/23 16:11:50.895
  Apr 23 16:11:50.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6707" for this suite. @ 04/23/23 16:11:50.938
• [0.539 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/23/23 16:11:50.963
  Apr 23 16:11:50.963: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/23/23 16:11:50.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:11:51.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:11:51.01
  Apr 23 16:11:51.015: INFO: Waiting up to 1m0s for all nodes to be ready
  E0423 16:11:51.069746      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:52.069759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:53.070970      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:54.070986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:55.071641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:56.071779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:57.072371      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:58.075044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:11:59.076047      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:00.076264      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:01.076384      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:02.076863      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:03.077645      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:04.077788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:05.078548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:06.078915      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:07.079426      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:08.079549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:09.079853      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:10.080346      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:11.080700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:12.081206      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:13.081594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:14.081866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:15.082061      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:16.083035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:17.083412      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:18.084046      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:19.084443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:20.085321      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:21.086085      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:22.086220      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:23.086684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:24.087326      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:25.087542      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:26.091628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:27.092577      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:28.093249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:29.094080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:30.094460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:31.095280      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:32.095945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:33.096029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:34.096257      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:35.096854      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:36.097700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:37.098322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:38.098321      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:39.099327      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:40.100022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:41.100901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:42.101221      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:43.101949      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:44.102132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:45.103220      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:46.103635      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:47.104418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:48.104505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:49.105137      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:50.105508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:12:51.070: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 16:12:51.082: INFO: Starting informer...
  STEP: Starting pods... @ 04/23/23 16:12:51.082
  E0423 16:12:51.105594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:12:51.344: INFO: Pod1 is running on soodi4ja4shi-3. Tainting Node
  E0423 16:12:52.106506      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:53.106413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:54.107206      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:55.107569      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:12:55.613: INFO: Pod2 is running on soodi4ja4shi-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/23/23 16:12:55.613
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/23/23 16:12:55.644
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/23/23 16:12:55.656
  E0423 16:12:56.107871      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:57.108100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:58.109015      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:12:59.109572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:00.109752      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:01.110908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:02.111074      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:13:02.501: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0423 16:13:03.112077      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:04.112445      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:05.112584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:06.113283      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:07.113468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:08.114363      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:09.116717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:10.116678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:11.116344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:12.117927      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:13.118959      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:14.118935      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:15.119020      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:16.119800      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:17.119912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:18.121027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:19.121761      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:20.122206      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:21.122872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:22.123014      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:13:22.247: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 23 16:13:22.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/23/23 16:13:22.289
  STEP: Destroying namespace "taint-multiple-pods-985" for this suite. @ 04/23/23 16:13:22.305
• [91.354 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/23/23 16:13:22.322
  Apr 23 16:13:22.323: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 16:13:22.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:13:22.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:13:22.378
  STEP: Waiting for pod completion @ 04/23/23 16:13:22.407
  E0423 16:13:23.124075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:24.125055      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:25.125832      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:26.126853      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:13:26.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1705" for this suite. @ 04/23/23 16:13:26.484
• [4.177 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/23/23 16:13:26.503
  Apr 23 16:13:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:13:26.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:13:26.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:13:26.576
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/23/23 16:13:26.587
  Apr 23 16:13:26.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-3748 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 23 16:13:26.871: INFO: stderr: ""
  Apr 23 16:13:26.871: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/23/23 16:13:26.871
  Apr 23 16:13:26.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-3748 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  E0423 16:13:27.127822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:13:27.128: INFO: stderr: ""
  Apr 23 16:13:27.128: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/23/23 16:13:27.128
  Apr 23 16:13:27.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-3748 delete pods e2e-test-httpd-pod'
  E0423 16:13:28.127884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:29.128135      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:30.128863      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:31.129001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:32.128986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:33.130011      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:34.130217      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:35.130436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:36.130764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:37.130894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:38.131432      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:39.131608      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:40.131842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:41.131980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:42.132483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:43.132585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:44.132931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:45.133044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:46.133145      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:47.133314      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:48.134136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:49.134417      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:50.135077      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:51.135138      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:52.135866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:53.136183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:54.136225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:55.136659      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:56.136992      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:57.137636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:58.138323      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:13:59.138853      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:00.138971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:00.791: INFO: stderr: ""
  Apr 23 16:14:00.791: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 23 16:14:00.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3748" for this suite. @ 04/23/23 16:14:00.809
• [34.326 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/23/23 16:14:00.838
  Apr 23 16:14:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:14:00.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:00.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:00.901
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4725 @ 04/23/23 16:14:00.908
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/23/23 16:14:00.94
  STEP: creating service externalsvc in namespace services-4725 @ 04/23/23 16:14:00.941
  STEP: creating replication controller externalsvc in namespace services-4725 @ 04/23/23 16:14:00.995
  I0423 16:14:01.017016      15 runners.go:194] Created replication controller with name: externalsvc, namespace: services-4725, replica count: 2
  E0423 16:14:01.139496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:02.139974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:03.159142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:14:04.072209      15 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/23/23 16:14:04.084
  Apr 23 16:14:04.115: INFO: Creating new exec pod
  E0423 16:14:04.144148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:05.143755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:06.144113      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:07.144509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:08.144485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:08.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-4725 exec execpodfwg49 -- /bin/sh -x -c nslookup clusterip-service.services-4725.svc.cluster.local'
  Apr 23 16:14:08.559: INFO: stderr: "+ nslookup clusterip-service.services-4725.svc.cluster.local\n"
  Apr 23 16:14:08.559: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-4725.svc.cluster.local\tcanonical name = externalsvc.services-4725.svc.cluster.local.\nName:\texternalsvc.services-4725.svc.cluster.local\nAddress: 10.233.45.221\n\n"
  Apr 23 16:14:08.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-4725, will wait for the garbage collector to delete the pods @ 04/23/23 16:14:08.568
  Apr 23 16:14:08.641: INFO: Deleting ReplicationController externalsvc took: 14.92307ms
  Apr 23 16:14:08.742: INFO: Terminating ReplicationController externalsvc pods took: 101.280545ms
  E0423 16:14:09.145138      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:10.146186      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:11.146334      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:11.298: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-4725" for this suite. @ 04/23/23 16:14:11.336
• [10.534 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/23/23 16:14:11.375
  Apr 23 16:14:11.376: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:14:11.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:11.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:11.42
  STEP: Creating secret with name secret-test-07fa42ea-c30d-4b7c-bf85-218ded9b0e1a @ 04/23/23 16:14:11.433
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:14:11.449
  E0423 16:14:12.146714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:13.146894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:14.147084      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:15.147172      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:16.148037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:17.148289      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:14:17.537
  Apr 23 16:14:17.570: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-secrets-c782c6f5-6bdf-4a4c-995f-86f7b3fd2014 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:14:17.65
  Apr 23 16:14:17.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9054" for this suite. @ 04/23/23 16:14:17.779
• [6.437 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/23/23 16:14:17.814
  Apr 23 16:14:17.814: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:14:17.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:17.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:17.9
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7010 @ 04/23/23 16:14:17.907
  STEP: changing the ExternalName service to type=NodePort @ 04/23/23 16:14:17.925
  STEP: creating replication controller externalname-service in namespace services-7010 @ 04/23/23 16:14:18.025
  I0423 16:14:18.038629      15 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7010, replica count: 2
  E0423 16:14:18.149126      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:19.149074      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:20.149272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:14:21.090440      15 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:14:21.090: INFO: Creating new exec pod
  E0423 16:14:21.150364      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:22.150547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:23.152287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:24.153793      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:25.153661      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:26.154436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:26.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodrh27v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 23 16:14:26.545: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 23 16:14:26.545: INFO: stdout: "externalname-service-xzt2j"
  Apr 23 16:14:26.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodrh27v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.239 80'
  E0423 16:14:27.157472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:27.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.239 80\nConnection to 10.233.16.239 80 port [tcp/http] succeeded!\n"
  Apr 23 16:14:27.338: INFO: stdout: ""
  E0423 16:14:28.155733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:28.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodrh27v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.239 80'
  Apr 23 16:14:28.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.239 80\nConnection to 10.233.16.239 80 port [tcp/http] succeeded!\n"
  Apr 23 16:14:28.695: INFO: stdout: "externalname-service-4jhbd"
  Apr 23 16:14:28.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodrh27v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.241 31134'
  E0423 16:14:29.156620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:29.431: INFO: stderr: "+ nc -v -t -w 2 192.168.121.241 31134\n+ echo hostName\nConnection to 192.168.121.241 31134 port [tcp/*] succeeded!\n"
  Apr 23 16:14:29.431: INFO: stdout: "externalname-service-4jhbd"
  Apr 23 16:14:29.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-7010 exec execpodrh27v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.96 31134'
  Apr 23 16:14:29.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.96 31134\nConnection to 192.168.121.96 31134 port [tcp/*] succeeded!\n"
  Apr 23 16:14:29.978: INFO: stdout: "externalname-service-xzt2j"
  Apr 23 16:14:29.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:14:29.992: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-7010" for this suite. @ 04/23/23 16:14:30.087
• [12.299 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/23/23 16:14:30.121
  Apr 23 16:14:30.121: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename subjectreview @ 04/23/23 16:14:30.126
  E0423 16:14:30.159060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:30.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:30.171
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-2360" @ 04/23/23 16:14:30.182
  Apr 23 16:14:30.196: INFO: saUsername: "system:serviceaccount:subjectreview-2360:e2e"
  Apr 23 16:14:30.196: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-2360"}
  Apr 23 16:14:30.196: INFO: saUID: "4687d3f8-7d05-428c-acb9-4933af52c9b6"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-2360:e2e" @ 04/23/23 16:14:30.196
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-2360:e2e" @ 04/23/23 16:14:30.197
  Apr 23 16:14:30.202: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-2360:e2e" api 'list' configmaps in "subjectreview-2360" namespace @ 04/23/23 16:14:30.203
  Apr 23 16:14:30.206: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-2360:e2e" @ 04/23/23 16:14:30.207
  Apr 23 16:14:30.212: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Apr 23 16:14:30.213: INFO: LocalSubjectAccessReview has been verified
  Apr 23 16:14:30.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-2360" for this suite. @ 04/23/23 16:14:30.225
• [0.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/23/23 16:14:30.254
  Apr 23 16:14:30.254: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 16:14:30.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:30.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:30.314
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/23/23 16:14:30.321
  E0423 16:14:31.191316      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:32.161139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:33.161722      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:34.162382      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:14:34.376
  Apr 23 16:14:34.397: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-046eb90c-cc5f-4ca3-b977-930735acb58a container test-container: <nil>
  STEP: delete the pod @ 04/23/23 16:14:34.412
  Apr 23 16:14:34.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-668" for this suite. @ 04/23/23 16:14:34.463
• [4.224 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/23/23 16:14:34.488
  Apr 23 16:14:34.488: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:14:34.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:34.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:34.543
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:14:34.554
  E0423 16:14:35.167123      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:36.163936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:37.164733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:38.165515      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:14:38.647
  Apr 23 16:14:38.668: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-7333c368-069d-4b04-8f77-702faea13cb6 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:14:39.135
  E0423 16:14:39.166815      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:39.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1332" for this suite. @ 04/23/23 16:14:39.236
• [4.817 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/23/23 16:14:39.311
  Apr 23 16:14:39.312: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:14:39.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:39.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:39.438
  STEP: Counting existing ResourceQuota @ 04/23/23 16:14:39.446
  E0423 16:14:40.167070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:41.168078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:42.169190      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:43.170172      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:44.170820      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 16:14:44.455
  STEP: Ensuring resource quota status is calculated @ 04/23/23 16:14:44.466
  E0423 16:14:45.171240      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:46.171883      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 04/23/23 16:14:46.476
  STEP: Creating a NodePort Service @ 04/23/23 16:14:46.531
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/23/23 16:14:46.588
  E0423 16:14:47.172431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status captures service creation @ 04/23/23 16:14:47.179
  E0423 16:14:48.172847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:49.173592      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 04/23/23 16:14:49.186
  STEP: Ensuring resource quota status released usage @ 04/23/23 16:14:49.296
  E0423 16:14:50.173843      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:51.174181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:14:51.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2007" for this suite. @ 04/23/23 16:14:51.324
• [12.033 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/23/23 16:14:51.345
  Apr 23 16:14:51.345: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 16:14:51.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:51.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:51.408
  STEP: Creating namespace "e2e-ns-v66zk" @ 04/23/23 16:14:51.415
  Apr 23 16:14:51.456: INFO: Namespace "e2e-ns-v66zk-4952" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-v66zk-4952" @ 04/23/23 16:14:51.456
  Apr 23 16:14:51.476: INFO: Namespace "e2e-ns-v66zk-4952" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-v66zk-4952" @ 04/23/23 16:14:51.476
  Apr 23 16:14:51.504: INFO: Namespace "e2e-ns-v66zk-4952" has []v1.FinalizerName{"kubernetes"}
  Apr 23 16:14:51.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8582" for this suite. @ 04/23/23 16:14:51.518
  STEP: Destroying namespace "e2e-ns-v66zk-4952" for this suite. @ 04/23/23 16:14:51.55
• [0.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/23/23 16:14:51.59
  Apr 23 16:14:51.590: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:14:51.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:51.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:51.655
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:14:51.663
  E0423 16:14:52.174465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:53.174796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:54.175486      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:55.176270      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:56.176543      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:57.177519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:14:57.765
  Apr 23 16:14:57.780: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-7fa3c845-7061-4212-b74d-66deb5c41a6f container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:14:57.817
  Apr 23 16:14:57.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5681" for this suite. @ 04/23/23 16:14:57.96
• [6.396 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/23/23 16:14:57.992
  Apr 23 16:14:57.992: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename security-context @ 04/23/23 16:14:57.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:14:58.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:14:58.062
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/23/23 16:14:58.073
  E0423 16:14:58.178382      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:14:59.179075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:00.180120      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:01.180414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:15:02.14
  Apr 23 16:15:02.151: INFO: Trying to get logs from node soodi4ja4shi-3 pod security-context-58fb42c3-60be-4016-b492-30f447da5b2c container test-container: <nil>
  STEP: delete the pod @ 04/23/23 16:15:02.17
  E0423 16:15:02.181531      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:02.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-6207" for this suite. @ 04/23/23 16:15:02.224
• [4.252 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/23/23 16:15:02.253
  Apr 23 16:15:02.253: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:15:02.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:02.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:02.334
  STEP: Creating secret with name secret-test-463de162-006d-4a1c-af17-101318d77deb @ 04/23/23 16:15:02.342
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:15:02.356
  E0423 16:15:03.247497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:04.200460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:05.201320      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:06.201549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:15:06.428
  Apr 23 16:15:06.435: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-secrets-65ffe564-3e1b-444a-8c1d-5ec6b8f281c4 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:15:06.451
  Apr 23 16:15:06.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2072" for this suite. @ 04/23/23 16:15:06.502
• [4.270 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/23/23 16:15:06.524
  Apr 23 16:15:06.524: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:15:06.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:06.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:06.591
  STEP: Creating the pod @ 04/23/23 16:15:06.598
  E0423 16:15:07.201740      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:08.305247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:09.203874      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:10.203992      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:11.204400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:11.839: INFO: Successfully updated pod "labelsupdate6c895b43-bf13-4269-814b-b71e14b75eaf"
  E0423 16:15:12.205428      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:13.205907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:13.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6233" for this suite. @ 04/23/23 16:15:13.94
• [7.441 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/23/23 16:15:13.966
  Apr 23 16:15:13.966: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-webhook @ 04/23/23 16:15:13.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:14.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:14.045
  STEP: Setting up server cert @ 04/23/23 16:15:14.056
  E0423 16:15:14.207277      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:15.208075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/23/23 16:15:15.612
  STEP: Deploying the custom resource conversion webhook pod @ 04/23/23 16:15:15.633
  STEP: Wait for the deployment to be ready @ 04/23/23 16:15:15.669
  Apr 23 16:15:15.686: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0423 16:15:16.208575      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:17.210388      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:17.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 15, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 15, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 15, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:15:18.211549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:19.211919      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 16:15:19.769
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:15:19.795
  E0423 16:15:20.212079      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:20.796: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 23 16:15:20.807: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:15:21.212427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:22.212765      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:23.213027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/23/23 16:15:23.803
  STEP: v2 custom resource should be converted @ 04/23/23 16:15:23.835
  Apr 23 16:15:23.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 16:15:24.214517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-2934" for this suite. @ 04/23/23 16:15:24.598
• [10.675 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/23/23 16:15:24.65
  Apr 23 16:15:24.651: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename job @ 04/23/23 16:15:24.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:24.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:24.706
  STEP: Creating a job @ 04/23/23 16:15:24.712
  STEP: Ensuring job reaches completions @ 04/23/23 16:15:24.747
  E0423 16:15:25.214734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:26.215008      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:27.215714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:28.216508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:29.216584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:30.217051      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:31.218380      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:32.219232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:33.219500      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:34.223636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:35.229022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:36.224373      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:37.224621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:38.224759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:38.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-237" for this suite. @ 04/23/23 16:15:38.781
• [14.158 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/23/23 16:15:38.81
  Apr 23 16:15:38.810: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:15:38.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:38.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:38.914
  STEP: Creating configMap with name configmap-test-volume-map-4ff8faa1-1430-4d58-9426-ad20050e826f @ 04/23/23 16:15:38.921
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:15:38.936
  E0423 16:15:39.225072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:40.229927      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:41.230967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:42.231537      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:43.232036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:44.233009      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:15:45.016
  Apr 23 16:15:45.024: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-b4820a29-bd6a-4569-b3d8-52854b0e9000 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:15:45.154
  Apr 23 16:15:45.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 16:15:45.232990      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "configmap-6265" for this suite. @ 04/23/23 16:15:45.233
• [6.466 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/23/23 16:15:45.279
  Apr 23 16:15:45.279: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-pred @ 04/23/23 16:15:45.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:45.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:45.364
  Apr 23 16:15:45.370: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 23 16:15:45.393: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 16:15:45.400: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-1 before test
  Apr 23 16:15:45.564: INFO: cilium-8bln9 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: cilium-node-init-7rprd from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: coredns-5d78c9869d-rztdc from kube-system started at 2023-04-23 15:02:07 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: kube-addon-manager-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:58:03 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: kube-apiserver-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: kube-controller-manager-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: kube-proxy-lgs9b from kube-system started at 2023-04-23 14:55:18 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: kube-scheduler-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-gg8kg from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:15:45.565: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 16:15:45.565: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-2 before test
  Apr 23 16:15:45.728: INFO: cilium-node-init-s5qwm from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: cilium-q8j55 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: coredns-5d78c9869d-jk5pb from kube-system started at 2023-04-23 14:59:51 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: kube-addon-manager-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:58:02 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: kube-apiserver-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: kube-controller-manager-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: kube-proxy-7j684 from kube-system started at 2023-04-23 14:55:59 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: kube-scheduler-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-z9sfh from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:15:45.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 16:15:45.728: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-3 before test
  Apr 23 16:15:45.830: INFO: cilium-hqn2w from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.831: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:15:45.831: INFO: cilium-node-init-hq966 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.831: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:15:45.831: INFO: cilium-operator-85fcfcb8b4-lvsvw from kube-system started at 2023-04-23 14:58:17 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.831: INFO: 	Container cilium-operator ready: true, restart count 0
  Apr 23 16:15:45.831: INFO: kube-proxy-qhxf2 from kube-system started at 2023-04-23 14:56:32 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.832: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:15:45.832: INFO: sonobuoy from sonobuoy started at 2023-04-23 14:59:49 +0000 UTC (1 container statuses recorded)
  Apr 23 16:15:45.832: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 23 16:15:45.832: INFO: sonobuoy-e2e-job-e20a28f544554453 from sonobuoy started at 2023-04-23 15:00:04 +0000 UTC (2 container statuses recorded)
  Apr 23 16:15:45.832: INFO: 	Container e2e ready: true, restart count 0
  Apr 23 16:15:45.833: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:15:45.833: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-pxgxt from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:15:45.834: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:15:45.834: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/23/23 16:15:45.834
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17589c82af8770e0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 04/23/23 16:15:45.959
  E0423 16:15:46.233950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:46.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8558" for this suite. @ 04/23/23 16:15:46.961
• [1.705 seconds]
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/23/23 16:15:46.983
  Apr 23 16:15:46.984: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 16:15:46.989
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:47.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:47.058
  E0423 16:15:47.233916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:48.234312      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:49.234844      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:50.234890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:51.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:15:51.147: INFO: Deleting pod "var-expansion-b3e3dd1d-0ebf-4c45-8bca-526faf2d877c" in namespace "var-expansion-3575"
  Apr 23 16:15:51.187: INFO: Wait up to 5m0s for pod "var-expansion-b3e3dd1d-0ebf-4c45-8bca-526faf2d877c" to be fully deleted
  E0423 16:15:51.235568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:52.235950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:53.236610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3575" for this suite. @ 04/23/23 16:15:53.239
• [6.273 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/23/23 16:15:53.256
  Apr 23 16:15:53.256: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename gc @ 04/23/23 16:15:53.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:53.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:53.304
  STEP: create the deployment @ 04/23/23 16:15:53.311
  W0423 16:15:53.327324      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/23/23 16:15:53.327
  STEP: delete the deployment @ 04/23/23 16:15:53.862
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/23/23 16:15:53.883
  E0423 16:15:54.237635      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/23/23 16:15:54.506
  Apr 23 16:15:55.049: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 16:15:55.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6552" for this suite. @ 04/23/23 16:15:55.071
• [1.853 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/23/23 16:15:55.116
  Apr 23 16:15:55.117: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:15:55.123
  E0423 16:15:55.238315      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:55.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:55.271
  STEP: Creating a ResourceQuota @ 04/23/23 16:15:55.277
  STEP: Getting a ResourceQuota @ 04/23/23 16:15:55.313
  STEP: Updating a ResourceQuota @ 04/23/23 16:15:55.333
  STEP: Verifying a ResourceQuota was modified @ 04/23/23 16:15:55.36
  STEP: Deleting a ResourceQuota @ 04/23/23 16:15:55.379
  STEP: Verifying the deleted ResourceQuota @ 04/23/23 16:15:55.399
  Apr 23 16:15:55.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4102" for this suite. @ 04/23/23 16:15:55.421
• [0.326 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/23/23 16:15:55.446
  Apr 23 16:15:55.446: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename proxy @ 04/23/23 16:15:55.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:15:55.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:15:55.509
  Apr 23 16:15:55.516: INFO: Creating pod...
  E0423 16:15:56.239779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:57.249858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:58.250748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:15:59.250405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:15:59.632: INFO: Creating service...
  Apr 23 16:15:59.816: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/pods/agnhost/proxy?method=DELETE
  Apr 23 16:15:59.853: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 23 16:15:59.858: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/pods/agnhost/proxy?method=OPTIONS
  Apr 23 16:15:59.869: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 23 16:15:59.869: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/pods/agnhost/proxy?method=PATCH
  Apr 23 16:15:59.879: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 23 16:15:59.879: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/pods/agnhost/proxy?method=POST
  Apr 23 16:15:59.891: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 23 16:15:59.892: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/pods/agnhost/proxy?method=PUT
  Apr 23 16:15:59.900: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 23 16:15:59.900: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 23 16:15:59.911: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 23 16:15:59.911: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 23 16:15:59.922: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 23 16:15:59.923: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 23 16:15:59.958: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 23 16:15:59.959: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/services/e2e-proxy-test-service/proxy?method=POST
  Apr 23 16:15:59.980: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 23 16:15:59.980: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 23 16:15:59.996: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 23 16:15:59.997: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/pods/agnhost/proxy?method=GET
  Apr 23 16:16:00.009: INFO: http.Client request:GET StatusCode:301
  Apr 23 16:16:00.010: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/services/e2e-proxy-test-service/proxy?method=GET
  Apr 23 16:16:00.040: INFO: http.Client request:GET StatusCode:301
  Apr 23 16:16:00.040: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/pods/agnhost/proxy?method=HEAD
  Apr 23 16:16:00.047: INFO: http.Client request:HEAD StatusCode:301
  Apr 23 16:16:00.047: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2383/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 23 16:16:00.063: INFO: http.Client request:HEAD StatusCode:301
  Apr 23 16:16:00.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2383" for this suite. @ 04/23/23 16:16:00.073
• [4.646 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/23/23 16:16:00.093
  Apr 23 16:16:00.093: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 16:16:00.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:16:00.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:16:00.141
  STEP: Creating pod busybox-89b2b247-08d2-42cd-b79d-0b778ee16067 in namespace container-probe-4079 @ 04/23/23 16:16:00.151
  E0423 16:16:00.251246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:01.328215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:02.267789      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:03.268725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:16:04.219: INFO: Started pod busybox-89b2b247-08d2-42cd-b79d-0b778ee16067 in namespace container-probe-4079
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 16:16:04.219
  Apr 23 16:16:04.226: INFO: Initial restart count of pod busybox-89b2b247-08d2-42cd-b79d-0b778ee16067 is 0
  E0423 16:16:04.269199      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:05.269912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:06.270187      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:07.271483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:08.271235      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:09.271287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:10.271521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:11.272448      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:12.273286      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:13.274405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:14.274594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:15.275006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:16.275030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:17.275225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:18.275576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:19.276015      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:20.276368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:21.277616      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:22.277897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:23.278372      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:24.278515      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:25.279173      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:26.281581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:27.279703      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:28.279825      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:29.279861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:30.280891      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:31.281567      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:32.281787      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:33.282914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:34.283349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:35.283436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:36.283819      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:37.283920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:38.284179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:39.284637      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:40.285215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:41.285956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:42.287134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:43.287887      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:44.288455      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:45.288496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:46.289493      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:47.290975      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:48.291159      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:49.291232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:50.291427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:51.291936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:52.292262      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:53.293261      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:54.293904      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:55.294314      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:56.294853      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:57.294885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:58.295444      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:16:59.295614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:00.296323      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:01.296743      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:02.297437      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:03.297614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:04.298274      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:05.298738      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:06.299245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:07.299579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:08.300658      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:09.300669      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:10.301939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:11.301955      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:12.302091      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:13.302263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:14.302851      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:15.303281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:16.303999      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:17.305070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:18.305404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:19.305830      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:20.305983      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:21.306104      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:22.306882      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:23.306856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:24.307108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:25.308220      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:26.308939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:27.309492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:28.309825      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:29.310290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:30.311002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:31.311493      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:32.312042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:33.311619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:34.311886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:35.312064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:36.312386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:37.313454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:38.313251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:39.313824      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:40.314138      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:41.314534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:42.314740      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:43.314886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:44.314987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:45.315873      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:46.316178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:47.316697      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:48.316810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:49.318548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:50.318775      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:51.318958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:52.320144      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:53.320540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:54.320866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:55.321139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:56.321896      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:57.322457      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:58.323531      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:17:59.323766      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:00.324027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:01.324085      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:02.324336      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:03.325544      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:04.325467      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:05.325754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:06.326842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:07.326946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:08.327126      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:09.328234      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:10.328435      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:11.329534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:12.329836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:13.330831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:14.331441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:15.331883      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:16.333681      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:17.333736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:18.334771      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:19.334304      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:20.335203      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:21.335527      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:22.335702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:23.336646      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:24.336829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:25.336949      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:26.337733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:27.338027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:28.338620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:29.339344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:30.339511      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:31.340252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:32.340641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:33.340862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:34.341040      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:35.341455      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:36.341839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:37.342428      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:38.342622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:39.343675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:40.343917      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:41.344626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:42.345194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:43.345633      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:44.346036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:45.346717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:46.346794      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:47.347719      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:48.347865      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:49.348061      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:50.348311      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:51.349167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:52.349437      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:53.350061      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:54.350405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:55.350814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:56.351225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:57.351355      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:58.352306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:18:59.352704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:00.353613      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:01.354814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:02.355604      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:03.356154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:04.357522      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:05.357274      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:06.357753      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:07.357813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:08.358324      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:09.358407      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:10.358568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:11.358780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:12.359101      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:13.359241      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:14.359260      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:15.360009      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:16.360946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:17.361091      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:18.361257      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:19.361730      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:20.361893      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:21.362575      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:22.362996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:23.363138      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:24.363361      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:25.364128      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:26.364392      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:27.364497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:28.364921      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:29.365079      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:30.365787      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:31.365894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:32.366120      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:33.366818      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:34.366928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:35.367139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:36.367303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:37.367485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:38.367661      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:39.368640      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:40.368380      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:41.369064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:42.369197      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:43.369423      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:44.370313      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:45.370911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:46.371632      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:47.371700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:48.372709      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:49.373493      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:50.373950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:51.374467      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:52.374600      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:53.375587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:54.375821      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:55.376002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:56.376136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:57.376403      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:58.377225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:19:59.377444      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:00.377641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:01.377825      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:02.378145      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:03.378274      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:04.378950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:05.379060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:20:05.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:20:05.731
  STEP: Destroying namespace "container-probe-4079" for this suite. @ 04/23/23 16:20:05.753
• [245.684 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/23/23 16:20:05.783
  Apr 23 16:20:05.783: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:20:05.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:05.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:05.855
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:20:05.863
  E0423 16:20:06.380750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:07.380514      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:08.381464      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:09.381490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:20:09.929
  Apr 23 16:20:09.940: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-a587b665-d325-46ae-9ebb-3a274ba1a651 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:20:09.985
  Apr 23 16:20:10.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5917" for this suite. @ 04/23/23 16:20:10.036
• [4.275 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/23/23 16:20:10.065
  Apr 23 16:20:10.065: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:20:10.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:10.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:10.117
  STEP: Creating a ResourceQuota with best effort scope @ 04/23/23 16:20:10.124
  STEP: Ensuring ResourceQuota status is calculated @ 04/23/23 16:20:10.14
  E0423 16:20:10.381979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:11.382619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 04/23/23 16:20:12.15
  STEP: Ensuring ResourceQuota status is calculated @ 04/23/23 16:20:12.161
  E0423 16:20:12.382782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:13.383281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 04/23/23 16:20:14.17
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/23/23 16:20:14.194
  E0423 16:20:14.383985      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:15.384611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/23/23 16:20:16.205
  E0423 16:20:16.387052      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:17.387112      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/23/23 16:20:18.218
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 16:20:18.247
  E0423 16:20:18.388350      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:19.389254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 04/23/23 16:20:20.254
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/23/23 16:20:20.277
  E0423 16:20:20.389454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:21.389707      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/23/23 16:20:22.287
  E0423 16:20:22.389891      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:23.390533      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/23/23 16:20:24.297
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 16:20:24.316
  E0423 16:20:24.391572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:25.391617      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:20:26.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9085" for this suite. @ 04/23/23 16:20:26.336
• [16.288 seconds]
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/23/23 16:20:26.354
  Apr 23 16:20:26.354: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sysctl @ 04/23/23 16:20:26.36
  E0423 16:20:26.391474      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:26.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:26.404
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/23/23 16:20:26.412
  STEP: Watching for error events or started pod @ 04/23/23 16:20:26.435
  E0423 16:20:27.391658      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:28.392035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 04/23/23 16:20:28.442
  E0423 16:20:29.392064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:30.393051      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 04/23/23 16:20:30.468
  STEP: Getting logs from the pod @ 04/23/23 16:20:30.468
  STEP: Checking that the sysctl is actually updated @ 04/23/23 16:20:30.483
  Apr 23 16:20:30.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-8468" for this suite. @ 04/23/23 16:20:30.493
• [4.157 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/23/23 16:20:30.514
  Apr 23 16:20:30.514: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 16:20:30.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:30.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:30.569
  Apr 23 16:20:30.574: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:20:31.393561      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:32.393914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:33.393957      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:20:34.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8944" for this suite. @ 04/23/23 16:20:34.277
• [3.778 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/23/23 16:20:34.297
  Apr 23 16:20:34.298: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename watch @ 04/23/23 16:20:34.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:34.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:34.351
  STEP: creating a watch on configmaps @ 04/23/23 16:20:34.36
  STEP: creating a new configmap @ 04/23/23 16:20:34.364
  STEP: modifying the configmap once @ 04/23/23 16:20:34.375
  STEP: closing the watch once it receives two notifications @ 04/23/23 16:20:34.392
  Apr 23 16:20:34.392: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7331  cb7c4bcf-a318-4326-a264-c73bb48a14da 29905 0 2023-04-23 16:20:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-23 16:20:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0423 16:20:34.394545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:20:34.394: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7331  cb7c4bcf-a318-4326-a264-c73bb48a14da 29906 0 2023-04-23 16:20:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-23 16:20:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/23/23 16:20:34.395
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/23/23 16:20:34.416
  STEP: deleting the configmap @ 04/23/23 16:20:34.42
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/23/23 16:20:34.441
  Apr 23 16:20:34.442: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7331  cb7c4bcf-a318-4326-a264-c73bb48a14da 29907 0 2023-04-23 16:20:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-23 16:20:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:20:34.442: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7331  cb7c4bcf-a318-4326-a264-c73bb48a14da 29908 0 2023-04-23 16:20:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-23 16:20:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:20:34.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7331" for this suite. @ 04/23/23 16:20:34.454
• [0.172 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/23/23 16:20:34.471
  Apr 23 16:20:34.471: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:20:34.473
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:34.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:34.506
  Apr 23 16:20:34.512: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: creating the pod @ 04/23/23 16:20:34.514
  STEP: submitting the pod to kubernetes @ 04/23/23 16:20:34.514
  E0423 16:20:35.395163      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:36.402595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:20:36.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7577" for this suite. @ 04/23/23 16:20:36.6
• [2.143 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/23/23 16:20:36.615
  Apr 23 16:20:36.615: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:20:36.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:36.662
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:36.67
  STEP: Creating configMap with name configmap-test-volume-map-43158ce0-2ac5-4c99-8e99-b6dd09e0dcaa @ 04/23/23 16:20:36.677
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:20:36.689
  E0423 16:20:37.402768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:38.402960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:39.404188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:40.405176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:20:40.752
  Apr 23 16:20:40.760: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-d841090c-9a15-4d4d-8df1-6267a4eed01f container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:20:40.783
  Apr 23 16:20:40.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3284" for this suite. @ 04/23/23 16:20:40.83
• [4.242 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/23/23 16:20:40.86
  Apr 23 16:20:40.861: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:20:40.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:40.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:40.911
  STEP: Creating configMap with name configmap-projected-all-test-volume-3e0e99f5-61c7-482b-badb-05fa63031c72 @ 04/23/23 16:20:40.919
  STEP: Creating secret with name secret-projected-all-test-volume-e529127e-d404-4f15-bd90-b7210572330c @ 04/23/23 16:20:40.928
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/23/23 16:20:40.939
  E0423 16:20:41.406467      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:42.406967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:43.418350      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:44.414891      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:45.415558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:46.416202      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:20:47.002
  Apr 23 16:20:47.013: INFO: Trying to get logs from node soodi4ja4shi-3 pod projected-volume-a5a6ee77-50d0-4165-8189-d58666b235d5 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:20:47.133
  Apr 23 16:20:47.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9723" for this suite. @ 04/23/23 16:20:47.176
• [6.332 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/23/23 16:20:47.198
  Apr 23 16:20:47.198: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 16:20:47.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:47.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:47.241
  STEP: Deleting RuntimeClass runtimeclass-7535-delete-me @ 04/23/23 16:20:47.254
  STEP: Waiting for the RuntimeClass to disappear @ 04/23/23 16:20:47.274
  Apr 23 16:20:47.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7535" for this suite. @ 04/23/23 16:20:47.323
• [0.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS  E0423 16:20:47.417691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/23/23 16:20:47.425
  Apr 23 16:20:47.425: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sysctl @ 04/23/23 16:20:47.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:47.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:47.489
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/23/23 16:20:47.501
  Apr 23 16:20:47.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9927" for this suite. @ 04/23/23 16:20:47.524
• [0.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/23/23 16:20:47.553
  Apr 23 16:20:47.553: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:20:47.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:47.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:47.624
  STEP: Creating configMap that has name configmap-test-emptyKey-f377b48c-3c9d-4ccf-bf91-6badf570b9bc @ 04/23/23 16:20:47.629
  Apr 23 16:20:47.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1865" for this suite. @ 04/23/23 16:20:47.646
• [0.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/23/23 16:20:47.664
  Apr 23 16:20:47.664: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:20:47.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:47.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:47.71
  STEP: Creating secret with name secret-test-b7a3520d-0e1e-4230-a0c9-11521219546f @ 04/23/23 16:20:47.763
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:20:47.775
  E0423 16:20:48.423759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:49.424932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:50.424796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:51.425842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:20:51.827
  Apr 23 16:20:51.838: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-secrets-d737e958-de26-4d73-868b-66f030ffdcd5 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:20:51.857
  Apr 23 16:20:51.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6406" for this suite. @ 04/23/23 16:20:51.903
  STEP: Destroying namespace "secret-namespace-1124" for this suite. @ 04/23/23 16:20:51.915
• [4.271 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/23/23 16:20:51.939
  Apr 23 16:20:51.939: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/23/23 16:20:51.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:20:51.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:20:51.98
  STEP: Setting up the test @ 04/23/23 16:20:51.986
  STEP: Creating hostNetwork=false pod @ 04/23/23 16:20:51.987
  E0423 16:20:52.426520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:53.427753      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:54.428651      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:55.429841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 04/23/23 16:20:56.068
  E0423 16:20:56.430047      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:20:57.430970      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 04/23/23 16:20:58.107
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/23/23 16:20:58.107
  Apr 23 16:20:58.107: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:20:58.107: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:20:58.110: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:20:58.110: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 23 16:20:58.227: INFO: Exec stderr: ""
  Apr 23 16:20:58.229: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:20:58.229: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:20:58.232: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:20:58.232: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 23 16:20:58.355: INFO: Exec stderr: ""
  Apr 23 16:20:58.355: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:20:58.355: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:20:58.359: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:20:58.359: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0423 16:20:58.431898      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:20:58.466: INFO: Exec stderr: ""
  Apr 23 16:20:58.466: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:20:58.466: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:20:58.468: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:20:58.468: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 23 16:20:58.634: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/23/23 16:20:58.634
  Apr 23 16:20:58.634: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:20:58.634: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:20:58.637: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:20:58.637: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 23 16:20:59.290: INFO: Exec stderr: ""
  Apr 23 16:20:59.290: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:20:59.290: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:20:59.294: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:20:59.294: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 23 16:20:59.428: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/23/23 16:20:59.428
  Apr 23 16:20:59.429: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:20:59.429: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:20:59.431: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:20:59.431: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  E0423 16:20:59.432246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:20:59.962: INFO: Exec stderr: ""
  Apr 23 16:20:59.963: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:20:59.963: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:20:59.966: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:20:59.967: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 23 16:21:00.090: INFO: Exec stderr: ""
  Apr 23 16:21:00.090: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:21:00.090: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:21:00.095: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:21:00.097: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 23 16:21:00.246: INFO: Exec stderr: ""
  Apr 23 16:21:00.246: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2017 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:21:00.247: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:21:00.249: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:21:00.250: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2017/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 23 16:21:00.358: INFO: Exec stderr: ""
  Apr 23 16:21:00.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2017" for this suite. @ 04/23/23 16:21:00.37
• [8.462 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/23/23 16:21:00.403
  Apr 23 16:21:00.403: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-pred @ 04/23/23 16:21:00.406
  E0423 16:21:00.432873      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:00.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:00.459
  Apr 23 16:21:00.465: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 23 16:21:00.484: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 16:21:00.492: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-1 before test
  Apr 23 16:21:00.512: INFO: cilium-8bln9 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.512: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:21:00.512: INFO: cilium-node-init-7rprd from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.512: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:21:00.512: INFO: coredns-5d78c9869d-rztdc from kube-system started at 2023-04-23 15:02:07 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.512: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 16:21:00.512: INFO: kube-addon-manager-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:58:03 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.513: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 23 16:21:00.513: INFO: kube-apiserver-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.513: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 23 16:21:00.513: INFO: kube-controller-manager-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.513: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 23 16:21:00.513: INFO: kube-proxy-lgs9b from kube-system started at 2023-04-23 14:55:18 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.513: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:21:00.513: INFO: kube-scheduler-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.513: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 23 16:21:00.513: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-gg8kg from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:21:00.513: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:21:00.513: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 16:21:00.513: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-2 before test
  Apr 23 16:21:00.532: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-2017 started at 2023-04-23 16:20:56 +0000 UTC (2 container statuses recorded)
  Apr 23 16:21:00.532: INFO: 	Container busybox-1 ready: true, restart count 0
  Apr 23 16:21:00.532: INFO: 	Container busybox-2 ready: true, restart count 0
  Apr 23 16:21:00.532: INFO: cilium-node-init-s5qwm from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.533: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:21:00.533: INFO: cilium-q8j55 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.533: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:21:00.533: INFO: coredns-5d78c9869d-jk5pb from kube-system started at 2023-04-23 14:59:51 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.533: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 16:21:00.534: INFO: kube-addon-manager-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:58:02 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.534: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 23 16:21:00.534: INFO: kube-apiserver-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.534: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 23 16:21:00.534: INFO: kube-controller-manager-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.534: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 23 16:21:00.535: INFO: kube-proxy-7j684 from kube-system started at 2023-04-23 14:55:59 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.535: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:21:00.535: INFO: kube-scheduler-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.535: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 23 16:21:00.535: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-z9sfh from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:21:00.536: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:21:00.536: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 16:21:00.536: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-3 before test
  Apr 23 16:21:00.558: INFO: test-pod from e2e-kubelet-etc-hosts-2017 started at 2023-04-23 16:20:52 +0000 UTC (3 container statuses recorded)
  Apr 23 16:21:00.558: INFO: 	Container busybox-1 ready: true, restart count 0
  Apr 23 16:21:00.558: INFO: 	Container busybox-2 ready: true, restart count 0
  Apr 23 16:21:00.558: INFO: 	Container busybox-3 ready: true, restart count 0
  Apr 23 16:21:00.558: INFO: cilium-hqn2w from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.558: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 16:21:00.558: INFO: cilium-node-init-hq966 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.558: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 16:21:00.558: INFO: cilium-operator-85fcfcb8b4-lvsvw from kube-system started at 2023-04-23 14:58:17 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.558: INFO: 	Container cilium-operator ready: true, restart count 0
  Apr 23 16:21:00.558: INFO: kube-proxy-qhxf2 from kube-system started at 2023-04-23 14:56:32 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.558: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 16:21:00.559: INFO: pod-logs-websocket-02b06a3f-146f-4226-a500-fc01afc3f4e9 from pods-7577 started at 2023-04-23 16:20:34 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.559: INFO: 	Container main ready: true, restart count 0
  Apr 23 16:21:00.559: INFO: sonobuoy from sonobuoy started at 2023-04-23 14:59:49 +0000 UTC (1 container statuses recorded)
  Apr 23 16:21:00.559: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 23 16:21:00.559: INFO: sonobuoy-e2e-job-e20a28f544554453 from sonobuoy started at 2023-04-23 15:00:04 +0000 UTC (2 container statuses recorded)
  Apr 23 16:21:00.559: INFO: 	Container e2e ready: true, restart count 0
  Apr 23 16:21:00.559: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:21:00.559: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-pxgxt from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 16:21:00.559: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 16:21:00.559: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/23/23 16:21:00.56
  E0423 16:21:01.433740      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:02.434099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:03.434162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:04.434282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/23/23 16:21:04.621
  STEP: Trying to apply a random label on the found node. @ 04/23/23 16:21:04.651
  STEP: verifying the node has the label kubernetes.io/e2e-d2d97c4b-7f9e-4715-86a8-8b6a5a72151c 42 @ 04/23/23 16:21:04.713
  STEP: Trying to relaunch the pod, now with labels. @ 04/23/23 16:21:04.8
  E0423 16:21:05.435533      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:06.435268      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:07.436665      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:08.437670      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-d2d97c4b-7f9e-4715-86a8-8b6a5a72151c off the node soodi4ja4shi-1 @ 04/23/23 16:21:08.92
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-d2d97c4b-7f9e-4715-86a8-8b6a5a72151c @ 04/23/23 16:21:08.956
  Apr 23 16:21:08.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4209" for this suite. @ 04/23/23 16:21:08.998
• [8.619 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/23/23 16:21:09.026
  Apr 23 16:21:09.026: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename proxy @ 04/23/23 16:21:09.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:09.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:09.096
  STEP: starting an echo server on multiple ports @ 04/23/23 16:21:09.137
  STEP: creating replication controller proxy-service-wcxqc in namespace proxy-2900 @ 04/23/23 16:21:09.138
  I0423 16:21:09.161287      15 runners.go:194] Created replication controller with name: proxy-service-wcxqc, namespace: proxy-2900, replica count: 1
  E0423 16:21:09.437205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:21:10.214866      15 runners.go:194] proxy-service-wcxqc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:21:10.437265      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:21:11.215891      15 runners.go:194] proxy-service-wcxqc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:21:11.437481      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:21:12.216753      15 runners.go:194] proxy-service-wcxqc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0423 16:21:12.438472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:21:13.217140      15 runners.go:194] proxy-service-wcxqc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:21:13.240: INFO: setup took 4.135849676s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/23/23 16:21:13.24
  Apr 23 16:21:13.307: INFO: (0) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 64.728576ms)
  Apr 23 16:21:13.310: INFO: (0) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 69.011322ms)
  Apr 23 16:21:13.324: INFO: (0) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 82.887195ms)
  Apr 23 16:21:13.324: INFO: (0) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 82.749007ms)
  Apr 23 16:21:13.328: INFO: (0) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 87.440796ms)
  Apr 23 16:21:13.328: INFO: (0) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 85.895252ms)
  Apr 23 16:21:13.328: INFO: (0) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 87.347512ms)
  Apr 23 16:21:13.332: INFO: (0) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 89.73492ms)
  Apr 23 16:21:13.332: INFO: (0) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 91.4992ms)
  Apr 23 16:21:13.333: INFO: (0) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 91.147517ms)
  Apr 23 16:21:13.350: INFO: (0) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 107.693557ms)
  Apr 23 16:21:13.350: INFO: (0) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 108.233865ms)
  Apr 23 16:21:13.351: INFO: (0) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 109.311503ms)
  Apr 23 16:21:13.355: INFO: (0) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 113.11379ms)
  Apr 23 16:21:13.355: INFO: (0) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 112.843747ms)
  Apr 23 16:21:13.356: INFO: (0) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 115.536655ms)
  Apr 23 16:21:13.396: INFO: (1) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 38.617141ms)
  Apr 23 16:21:13.396: INFO: (1) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 38.403884ms)
  Apr 23 16:21:13.396: INFO: (1) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 38.278305ms)
  Apr 23 16:21:13.396: INFO: (1) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 38.750921ms)
  Apr 23 16:21:13.398: INFO: (1) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 40.206827ms)
  Apr 23 16:21:13.398: INFO: (1) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 40.191536ms)
  Apr 23 16:21:13.398: INFO: (1) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 40.096039ms)
  Apr 23 16:21:13.409: INFO: (1) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 51.115926ms)
  Apr 23 16:21:13.409: INFO: (1) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 53.116555ms)
  Apr 23 16:21:13.413: INFO: (1) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 55.312514ms)
  Apr 23 16:21:13.414: INFO: (1) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 56.437084ms)
  Apr 23 16:21:13.414: INFO: (1) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 56.445344ms)
  Apr 23 16:21:13.414: INFO: (1) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 56.911665ms)
  Apr 23 16:21:13.416: INFO: (1) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 58.347202ms)
  Apr 23 16:21:13.419: INFO: (1) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 61.114707ms)
  Apr 23 16:21:13.430: INFO: (1) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 72.145262ms)
  E0423 16:21:13.439155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:21:13.446: INFO: (2) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 14.843891ms)
  Apr 23 16:21:13.449: INFO: (2) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 15.863279ms)
  Apr 23 16:21:13.451: INFO: (2) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 16.746876ms)
  Apr 23 16:21:13.456: INFO: (2) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 25.227846ms)
  Apr 23 16:21:13.458: INFO: (2) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 23.700641ms)
  Apr 23 16:21:13.464: INFO: (2) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 33.314546ms)
  Apr 23 16:21:13.465: INFO: (2) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 32.707951ms)
  Apr 23 16:21:13.468: INFO: (2) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 37.005761ms)
  Apr 23 16:21:13.468: INFO: (2) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 37.416452ms)
  Apr 23 16:21:13.468: INFO: (2) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 35.998284ms)
  Apr 23 16:21:13.477: INFO: (2) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 45.229926ms)
  Apr 23 16:21:13.480: INFO: (2) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 46.085378ms)
  Apr 23 16:21:13.480: INFO: (2) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 48.526651ms)
  Apr 23 16:21:13.483: INFO: (2) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 51.879259ms)
  Apr 23 16:21:13.492: INFO: (2) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 59.557936ms)
  Apr 23 16:21:13.493: INFO: (2) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 59.703616ms)
  Apr 23 16:21:13.513: INFO: (3) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 19.20131ms)
  Apr 23 16:21:13.513: INFO: (3) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 20.10874ms)
  Apr 23 16:21:13.515: INFO: (3) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 22.646952ms)
  Apr 23 16:21:13.516: INFO: (3) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 23.194289ms)
  Apr 23 16:21:13.517: INFO: (3) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 22.892252ms)
  Apr 23 16:21:13.531: INFO: (3) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 37.089219ms)
  Apr 23 16:21:13.531: INFO: (3) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 37.928342ms)
  Apr 23 16:21:13.531: INFO: (3) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 38.064278ms)
  Apr 23 16:21:13.533: INFO: (3) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 38.762916ms)
  Apr 23 16:21:13.533: INFO: (3) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 39.342451ms)
  Apr 23 16:21:13.534: INFO: (3) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 40.624245ms)
  Apr 23 16:21:13.536: INFO: (3) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 42.680252ms)
  Apr 23 16:21:13.546: INFO: (3) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 52.256299ms)
  Apr 23 16:21:13.561: INFO: (3) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 67.604859ms)
  Apr 23 16:21:13.561: INFO: (3) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 67.337626ms)
  Apr 23 16:21:13.562: INFO: (3) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 68.583789ms)
  Apr 23 16:21:13.572: INFO: (4) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 10.469212ms)
  Apr 23 16:21:13.584: INFO: (4) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 21.131754ms)
  Apr 23 16:21:13.586: INFO: (4) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 23.692334ms)
  Apr 23 16:21:13.588: INFO: (4) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 25.027668ms)
  Apr 23 16:21:13.596: INFO: (4) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 33.513363ms)
  Apr 23 16:21:13.597: INFO: (4) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 34.169199ms)
  Apr 23 16:21:13.597: INFO: (4) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 33.618744ms)
  Apr 23 16:21:13.597: INFO: (4) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 34.019373ms)
  Apr 23 16:21:13.599: INFO: (4) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 36.318802ms)
  Apr 23 16:21:13.609: INFO: (4) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 47.184491ms)
  Apr 23 16:21:13.619: INFO: (4) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 56.683136ms)
  Apr 23 16:21:13.624: INFO: (4) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 61.902414ms)
  Apr 23 16:21:13.625: INFO: (4) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 62.500707ms)
  Apr 23 16:21:13.625: INFO: (4) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 62.758833ms)
  Apr 23 16:21:13.625: INFO: (4) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 63.082521ms)
  Apr 23 16:21:13.626: INFO: (4) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 62.785293ms)
  Apr 23 16:21:13.635: INFO: (5) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 8.631563ms)
  Apr 23 16:21:13.647: INFO: (5) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 18.669668ms)
  Apr 23 16:21:13.647: INFO: (5) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 20.243358ms)
  Apr 23 16:21:13.647: INFO: (5) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 19.492574ms)
  Apr 23 16:21:13.649: INFO: (5) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 23.540644ms)
  Apr 23 16:21:13.655: INFO: (5) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 27.420185ms)
  Apr 23 16:21:13.656: INFO: (5) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 29.129654ms)
  Apr 23 16:21:13.656: INFO: (5) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 28.14382ms)
  Apr 23 16:21:13.656: INFO: (5) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 28.959215ms)
  Apr 23 16:21:13.663: INFO: (5) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 37.210907ms)
  Apr 23 16:21:13.664: INFO: (5) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 36.103097ms)
  Apr 23 16:21:13.666: INFO: (5) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 38.937443ms)
  Apr 23 16:21:13.667: INFO: (5) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 39.339944ms)
  Apr 23 16:21:13.675: INFO: (5) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 48.069597ms)
  Apr 23 16:21:13.675: INFO: (5) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 48.595287ms)
  Apr 23 16:21:13.676: INFO: (5) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 47.911777ms)
  Apr 23 16:21:13.696: INFO: (6) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 20.697981ms)
  Apr 23 16:21:13.701: INFO: (6) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 24.384468ms)
  Apr 23 16:21:13.701: INFO: (6) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 24.47527ms)
  Apr 23 16:21:13.701: INFO: (6) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 25.080817ms)
  Apr 23 16:21:13.701: INFO: (6) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 24.223217ms)
  Apr 23 16:21:13.701: INFO: (6) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 23.533333ms)
  Apr 23 16:21:13.701: INFO: (6) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 25.039391ms)
  Apr 23 16:21:13.701: INFO: (6) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 22.735984ms)
  Apr 23 16:21:13.701: INFO: (6) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 23.869368ms)
  Apr 23 16:21:13.710: INFO: (6) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 32.205466ms)
  Apr 23 16:21:13.748: INFO: (6) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 71.286112ms)
  Apr 23 16:21:13.748: INFO: (6) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 70.956407ms)
  Apr 23 16:21:13.748: INFO: (6) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 71.764267ms)
  Apr 23 16:21:13.750: INFO: (6) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 71.913332ms)
  Apr 23 16:21:13.750: INFO: (6) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 71.709809ms)
  Apr 23 16:21:13.750: INFO: (6) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 72.202722ms)
  Apr 23 16:21:13.774: INFO: (7) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 22.915544ms)
  Apr 23 16:21:13.776: INFO: (7) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 25.201386ms)
  Apr 23 16:21:13.779: INFO: (7) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 29.317501ms)
  Apr 23 16:21:13.786: INFO: (7) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 34.062608ms)
  Apr 23 16:21:13.787: INFO: (7) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 34.922414ms)
  Apr 23 16:21:13.790: INFO: (7) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 38.97889ms)
  Apr 23 16:21:13.792: INFO: (7) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 40.30371ms)
  Apr 23 16:21:13.792: INFO: (7) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 42.044342ms)
  Apr 23 16:21:13.792: INFO: (7) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 39.786877ms)
  Apr 23 16:21:13.792: INFO: (7) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 42.285385ms)
  Apr 23 16:21:13.792: INFO: (7) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 39.325856ms)
  Apr 23 16:21:13.792: INFO: (7) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 41.050979ms)
  Apr 23 16:21:13.801: INFO: (7) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 49.94428ms)
  Apr 23 16:21:13.802: INFO: (7) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 49.86582ms)
  Apr 23 16:21:13.822: INFO: (7) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 69.869901ms)
  Apr 23 16:21:13.822: INFO: (7) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 69.466153ms)
  Apr 23 16:21:13.852: INFO: (8) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 24.974939ms)
  Apr 23 16:21:13.857: INFO: (8) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 29.912574ms)
  Apr 23 16:21:13.858: INFO: (8) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 29.880121ms)
  Apr 23 16:21:13.861: INFO: (8) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 35.784515ms)
  Apr 23 16:21:13.864: INFO: (8) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 35.884482ms)
  Apr 23 16:21:13.871: INFO: (8) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 42.212952ms)
  Apr 23 16:21:13.872: INFO: (8) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 44.084852ms)
  Apr 23 16:21:13.872: INFO: (8) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 46.752307ms)
  Apr 23 16:21:13.874: INFO: (8) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 45.439095ms)
  Apr 23 16:21:13.874: INFO: (8) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 49.920366ms)
  Apr 23 16:21:13.885: INFO: (8) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 59.435334ms)
  Apr 23 16:21:13.886: INFO: (8) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 58.217042ms)
  Apr 23 16:21:13.886: INFO: (8) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 61.213434ms)
  Apr 23 16:21:13.892: INFO: (8) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 66.81541ms)
  Apr 23 16:21:13.893: INFO: (8) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 66.610422ms)
  Apr 23 16:21:13.893: INFO: (8) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 65.030628ms)
  Apr 23 16:21:13.932: INFO: (9) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 38.264791ms)
  Apr 23 16:21:13.933: INFO: (9) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 38.11775ms)
  Apr 23 16:21:13.935: INFO: (9) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 40.576331ms)
  Apr 23 16:21:13.936: INFO: (9) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 42.437738ms)
  Apr 23 16:21:13.937: INFO: (9) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 42.463557ms)
  Apr 23 16:21:13.937: INFO: (9) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 42.950482ms)
  Apr 23 16:21:13.937: INFO: (9) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 43.847349ms)
  Apr 23 16:21:13.946: INFO: (9) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 51.61791ms)
  Apr 23 16:21:13.947: INFO: (9) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 51.796738ms)
  Apr 23 16:21:13.947: INFO: (9) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 54.042857ms)
  Apr 23 16:21:13.954: INFO: (9) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 59.427363ms)
  Apr 23 16:21:13.954: INFO: (9) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 60.739186ms)
  Apr 23 16:21:13.958: INFO: (9) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 63.440663ms)
  Apr 23 16:21:13.958: INFO: (9) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 63.678172ms)
  Apr 23 16:21:13.961: INFO: (9) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 66.67896ms)
  Apr 23 16:21:13.962: INFO: (9) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 67.483701ms)
  Apr 23 16:21:13.976: INFO: (10) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 13.425029ms)
  Apr 23 16:21:13.983: INFO: (10) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 20.221089ms)
  Apr 23 16:21:13.983: INFO: (10) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 19.998545ms)
  Apr 23 16:21:13.983: INFO: (10) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 19.108964ms)
  Apr 23 16:21:13.990: INFO: (10) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 24.962749ms)
  Apr 23 16:21:13.990: INFO: (10) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 25.995226ms)
  Apr 23 16:21:13.990: INFO: (10) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 25.718491ms)
  Apr 23 16:21:13.990: INFO: (10) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 26.511254ms)
  Apr 23 16:21:13.991: INFO: (10) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 27.801685ms)
  Apr 23 16:21:13.991: INFO: (10) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 26.51442ms)
  Apr 23 16:21:13.991: INFO: (10) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 28.532601ms)
  Apr 23 16:21:13.992: INFO: (10) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 28.437449ms)
  Apr 23 16:21:13.993: INFO: (10) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 28.686386ms)
  Apr 23 16:21:13.998: INFO: (10) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 35.853561ms)
  Apr 23 16:21:14.006: INFO: (10) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 42.188626ms)
  Apr 23 16:21:14.018: INFO: (10) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 55.044627ms)
  Apr 23 16:21:14.039: INFO: (11) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 20.76355ms)
  Apr 23 16:21:14.040: INFO: (11) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 19.125047ms)
  Apr 23 16:21:14.041: INFO: (11) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 22.306702ms)
  Apr 23 16:21:14.044: INFO: (11) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 24.360387ms)
  Apr 23 16:21:14.047: INFO: (11) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 27.417492ms)
  Apr 23 16:21:14.047: INFO: (11) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 27.265584ms)
  Apr 23 16:21:14.048: INFO: (11) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 28.813154ms)
  Apr 23 16:21:14.064: INFO: (11) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 43.45897ms)
  Apr 23 16:21:14.064: INFO: (11) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 44.147821ms)
  Apr 23 16:21:14.065: INFO: (11) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 44.83492ms)
  Apr 23 16:21:14.066: INFO: (11) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 47.397134ms)
  Apr 23 16:21:14.070: INFO: (11) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 50.683193ms)
  Apr 23 16:21:14.070: INFO: (11) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 50.398583ms)
  Apr 23 16:21:14.070: INFO: (11) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 49.274565ms)
  Apr 23 16:21:14.070: INFO: (11) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 50.896252ms)
  Apr 23 16:21:14.078: INFO: (11) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 58.471582ms)
  Apr 23 16:21:14.102: INFO: (12) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 22.896353ms)
  Apr 23 16:21:14.112: INFO: (12) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 32.600235ms)
  Apr 23 16:21:14.113: INFO: (12) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 34.056981ms)
  Apr 23 16:21:14.114: INFO: (12) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 34.349163ms)
  Apr 23 16:21:14.128: INFO: (12) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 48.511474ms)
  Apr 23 16:21:14.138: INFO: (12) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 58.26484ms)
  Apr 23 16:21:14.142: INFO: (12) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 62.560212ms)
  Apr 23 16:21:14.143: INFO: (12) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 64.054618ms)
  Apr 23 16:21:14.144: INFO: (12) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 64.186215ms)
  Apr 23 16:21:14.148: INFO: (12) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 67.841449ms)
  Apr 23 16:21:14.148: INFO: (12) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 69.148702ms)
  Apr 23 16:21:14.148: INFO: (12) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 68.228439ms)
  Apr 23 16:21:14.151: INFO: (12) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 71.112643ms)
  Apr 23 16:21:14.157: INFO: (12) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 77.369978ms)
  Apr 23 16:21:14.166: INFO: (12) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 87.110417ms)
  Apr 23 16:21:14.168: INFO: (12) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 88.912536ms)
  Apr 23 16:21:14.189: INFO: (13) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 18.466594ms)
  Apr 23 16:21:14.200: INFO: (13) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 29.0135ms)
  Apr 23 16:21:14.205: INFO: (13) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 34.318744ms)
  Apr 23 16:21:14.205: INFO: (13) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 36.271224ms)
  Apr 23 16:21:14.205: INFO: (13) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 36.944156ms)
  Apr 23 16:21:14.205: INFO: (13) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 36.186433ms)
  Apr 23 16:21:14.207: INFO: (13) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 38.16264ms)
  Apr 23 16:21:14.207: INFO: (13) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 37.141219ms)
  Apr 23 16:21:14.207: INFO: (13) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 38.275954ms)
  Apr 23 16:21:14.207: INFO: (13) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 37.745844ms)
  Apr 23 16:21:14.207: INFO: (13) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 36.146213ms)
  Apr 23 16:21:14.207: INFO: (13) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 36.250141ms)
  Apr 23 16:21:14.207: INFO: (13) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 38.300257ms)
  Apr 23 16:21:14.212: INFO: (13) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 42.864246ms)
  Apr 23 16:21:14.221: INFO: (13) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 51.785622ms)
  Apr 23 16:21:14.227: INFO: (13) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 58.12787ms)
  Apr 23 16:21:14.291: INFO: (14) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 63.181184ms)
  Apr 23 16:21:14.302: INFO: (14) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 74.891966ms)
  Apr 23 16:21:14.326: INFO: (14) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 97.762022ms)
  Apr 23 16:21:14.326: INFO: (14) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 98.927924ms)
  Apr 23 16:21:14.341: INFO: (14) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 113.441439ms)
  Apr 23 16:21:14.356: INFO: (14) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 128.39289ms)
  Apr 23 16:21:14.356: INFO: (14) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 128.605426ms)
  Apr 23 16:21:14.357: INFO: (14) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 128.812822ms)
  Apr 23 16:21:14.367: INFO: (14) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 139.486635ms)
  Apr 23 16:21:14.369: INFO: (14) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 140.879719ms)
  Apr 23 16:21:14.384: INFO: (14) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 156.874664ms)
  Apr 23 16:21:14.390: INFO: (14) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 162.61645ms)
  Apr 23 16:21:14.409: INFO: (14) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 181.868781ms)
  Apr 23 16:21:14.409: INFO: (14) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 181.570528ms)
  Apr 23 16:21:14.410: INFO: (14) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 182.128062ms)
  Apr 23 16:21:14.410: INFO: (14) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 182.621794ms)
  E0423 16:21:14.439943      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:21:14.463: INFO: (15) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 52.164124ms)
  Apr 23 16:21:14.474: INFO: (15) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 62.80966ms)
  Apr 23 16:21:14.475: INFO: (15) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 64.274339ms)
  Apr 23 16:21:14.476: INFO: (15) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 63.564381ms)
  Apr 23 16:21:14.476: INFO: (15) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 64.207557ms)
  Apr 23 16:21:14.476: INFO: (15) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 64.143234ms)
  Apr 23 16:21:14.476: INFO: (15) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 64.793271ms)
  Apr 23 16:21:14.476: INFO: (15) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 64.007902ms)
  Apr 23 16:21:14.478: INFO: (15) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 66.432336ms)
  Apr 23 16:21:14.478: INFO: (15) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 66.089287ms)
  Apr 23 16:21:14.489: INFO: (15) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 77.991527ms)
  Apr 23 16:21:14.496: INFO: (15) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 84.136485ms)
  Apr 23 16:21:14.496: INFO: (15) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 84.825334ms)
  Apr 23 16:21:14.498: INFO: (15) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 87.49124ms)
  Apr 23 16:21:14.499: INFO: (15) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 87.529049ms)
  Apr 23 16:21:14.500: INFO: (15) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 87.822885ms)
  Apr 23 16:21:14.514: INFO: (16) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 13.090085ms)
  Apr 23 16:21:14.521: INFO: (16) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 21.062006ms)
  Apr 23 16:21:14.536: INFO: (16) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 35.389544ms)
  Apr 23 16:21:14.539: INFO: (16) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 36.896802ms)
  Apr 23 16:21:14.539: INFO: (16) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 36.37618ms)
  Apr 23 16:21:14.540: INFO: (16) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 37.860463ms)
  Apr 23 16:21:14.541: INFO: (16) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 38.681649ms)
  Apr 23 16:21:14.543: INFO: (16) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 40.573214ms)
  Apr 23 16:21:14.543: INFO: (16) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 40.484432ms)
  Apr 23 16:21:14.543: INFO: (16) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 40.918004ms)
  Apr 23 16:21:14.543: INFO: (16) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 41.592107ms)
  Apr 23 16:21:14.543: INFO: (16) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 41.379055ms)
  Apr 23 16:21:14.567: INFO: (16) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 66.094997ms)
  Apr 23 16:21:14.568: INFO: (16) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 66.239507ms)
  Apr 23 16:21:14.569: INFO: (16) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 67.933452ms)
  Apr 23 16:21:14.575: INFO: (16) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 73.594922ms)
  Apr 23 16:21:14.597: INFO: (17) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 21.690721ms)
  Apr 23 16:21:14.597: INFO: (17) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 21.460167ms)
  Apr 23 16:21:14.613: INFO: (17) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 35.812269ms)
  Apr 23 16:21:14.613: INFO: (17) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 35.945769ms)
  Apr 23 16:21:14.614: INFO: (17) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 37.193386ms)
  Apr 23 16:21:14.618: INFO: (17) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 41.344797ms)
  Apr 23 16:21:14.619: INFO: (17) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 42.592757ms)
  Apr 23 16:21:14.630: INFO: (17) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 52.52894ms)
  Apr 23 16:21:14.636: INFO: (17) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 59.092386ms)
  Apr 23 16:21:14.638: INFO: (17) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 60.838123ms)
  Apr 23 16:21:14.642: INFO: (17) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 65.387459ms)
  Apr 23 16:21:14.647: INFO: (17) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 70.605285ms)
  Apr 23 16:21:14.647: INFO: (17) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 70.607421ms)
  Apr 23 16:21:14.683: INFO: (17) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 106.210022ms)
  Apr 23 16:21:14.690: INFO: (17) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 112.604879ms)
  Apr 23 16:21:14.767: INFO: (17) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 189.462862ms)
  Apr 23 16:21:14.817: INFO: (18) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 49.768973ms)
  Apr 23 16:21:14.820: INFO: (18) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 51.801005ms)
  Apr 23 16:21:14.855: INFO: (18) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 85.571584ms)
  Apr 23 16:21:14.855: INFO: (18) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 85.168769ms)
  Apr 23 16:21:14.876: INFO: (18) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 106.584898ms)
  Apr 23 16:21:14.876: INFO: (18) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 108.035881ms)
  Apr 23 16:21:14.877: INFO: (18) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 106.422175ms)
  Apr 23 16:21:14.921: INFO: (18) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 152.5724ms)
  Apr 23 16:21:14.925: INFO: (18) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 156.309386ms)
  Apr 23 16:21:14.926: INFO: (18) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 159.543186ms)
  Apr 23 16:21:14.938: INFO: (18) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 170.658943ms)
  Apr 23 16:21:14.952: INFO: (18) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 182.358788ms)
  Apr 23 16:21:14.959: INFO: (18) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 191.863508ms)
  Apr 23 16:21:15.003: INFO: (18) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 234.370493ms)
  Apr 23 16:21:15.006: INFO: (18) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 238.605505ms)
  Apr 23 16:21:15.007: INFO: (18) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 239.439269ms)
  Apr 23 16:21:15.062: INFO: (19) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">test<... (200; 54.939204ms)
  Apr 23 16:21:15.073: INFO: (19) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:443/proxy/tlsrewritem... (200; 66.521336ms)
  Apr 23 16:21:15.074: INFO: (19) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 64.478602ms)
  Apr 23 16:21:15.074: INFO: (19) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:460/proxy/: tls baz (200; 65.022645ms)
  Apr 23 16:21:15.075: INFO: (19) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8/proxy/rewriteme">test</a> (200; 66.546163ms)
  Apr 23 16:21:15.081: INFO: (19) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 73.161276ms)
  Apr 23 16:21:15.083: INFO: (19) /api/v1/namespaces/proxy-2900/pods/https:proxy-service-wcxqc-5x5w8:462/proxy/: tls qux (200; 73.28311ms)
  Apr 23 16:21:15.083: INFO: (19) /api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/: <a href="/api/v1/namespaces/proxy-2900/pods/http:proxy-service-wcxqc-5x5w8:1080/proxy/rewriteme">... (200; 74.345086ms)
  Apr 23 16:21:15.087: INFO: (19) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:160/proxy/: foo (200; 77.961368ms)
  Apr 23 16:21:15.087: INFO: (19) /api/v1/namespaces/proxy-2900/pods/proxy-service-wcxqc-5x5w8:162/proxy/: bar (200; 78.001469ms)
  Apr 23 16:21:15.088: INFO: (19) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname2/proxy/: bar (200; 78.105411ms)
  Apr 23 16:21:15.098: INFO: (19) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname2/proxy/: bar (200; 88.734565ms)
  Apr 23 16:21:15.099: INFO: (19) /api/v1/namespaces/proxy-2900/services/http:proxy-service-wcxqc:portname1/proxy/: foo (200; 90.254644ms)
  Apr 23 16:21:15.101: INFO: (19) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname2/proxy/: tls qux (200; 92.808428ms)
  Apr 23 16:21:15.102: INFO: (19) /api/v1/namespaces/proxy-2900/services/https:proxy-service-wcxqc:tlsportname1/proxy/: tls baz (200; 92.613326ms)
  Apr 23 16:21:15.102: INFO: (19) /api/v1/namespaces/proxy-2900/services/proxy-service-wcxqc:portname1/proxy/: foo (200; 92.614043ms)
  Apr 23 16:21:15.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-wcxqc in namespace proxy-2900, will wait for the garbage collector to delete the pods @ 04/23/23 16:21:15.117
  Apr 23 16:21:15.258: INFO: Deleting ReplicationController proxy-service-wcxqc took: 63.173391ms
  Apr 23 16:21:15.359: INFO: Terminating ReplicationController proxy-service-wcxqc pods took: 100.573139ms
  E0423 16:21:15.440403      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:16.441434      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:17.442306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:18.442896      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-2900" for this suite. @ 04/23/23 16:21:18.462
• [9.475 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/23/23 16:21:18.503
  Apr 23 16:21:18.504: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename deployment @ 04/23/23 16:21:18.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:18.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:18.577
  Apr 23 16:21:18.584: INFO: Creating simple deployment test-new-deployment
  Apr 23 16:21:18.735: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0423 16:21:19.443642      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:20.446986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:21:20.772: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 21, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 21, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 21, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 21, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-67bd4bf6dc\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:21:21.447634      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:22.448418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 04/23/23 16:21:22.822
  STEP: updating a scale subresource @ 04/23/23 16:21:22.83
  STEP: verifying the deployment Spec.Replicas was modified @ 04/23/23 16:21:22.848
  STEP: Patch a scale subresource @ 04/23/23 16:21:22.861
  Apr 23 16:21:22.994: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-33  388c5ba6-61bf-4802-9963-f8e7a6b46b82 30363 3 2023-04-23 16:21:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-23 16:21:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003936848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-23 16:21:22 +0000 UTC,LastTransitionTime:2023-04-23 16:21:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-04-23 16:21:22 +0000 UTC,LastTransitionTime:2023-04-23 16:21:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 23 16:21:23.008: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-33  156f62bf-6ab5-48cb-b7e4-b3511f9b1b06 30368 2 2023-04-23 16:21:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 388c5ba6-61bf-4802-9963-f8e7a6b46b82 0xc003936c77 0xc003936c78}] [] [{kube-controller-manager Update apps/v1 2023-04-23 16:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"388c5ba6-61bf-4802-9963-f8e7a6b46b82\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:21:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003936d08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 16:21:23.021: INFO: Pod "test-new-deployment-67bd4bf6dc-2jwbl" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-2jwbl test-new-deployment-67bd4bf6dc- deployment-33  e44ff1f8-c5e0-4a60-88a4-d98aa55a670f 30365 0 2023-04-23 16:21:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 156f62bf-6ab5-48cb-b7e4-b3511f9b1b06 0xc00470d737 0xc00470d738}] [] [{kube-controller-manager Update v1 2023-04-23 16:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"156f62bf-6ab5-48cb-b7e4-b3511f9b1b06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbs7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbs7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:21:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:21:23.023: INFO: Pod "test-new-deployment-67bd4bf6dc-whf7l" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-whf7l test-new-deployment-67bd4bf6dc- deployment-33  5d0b2deb-0c5d-4612-9d0d-534dbb27d8f3 30357 0 2023-04-23 16:21:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 156f62bf-6ab5-48cb-b7e4-b3511f9b1b06 0xc00470d8a0 0xc00470d8a1}] [] [{kube-controller-manager Update v1 2023-04-23 16:21:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"156f62bf-6ab5-48cb-b7e4-b3511f9b1b06\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:21:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cphzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cphzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:21:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:21:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:21:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:21:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.66.170,StartTime:2023-04-23 16:21:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:21:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://cc936342eba5d05718f0b6d7a1073fac1d79bd0f02ed188f3a431b9de0b2e8d9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.170,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:21:23.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-33" for this suite. @ 04/23/23 16:21:23.055
• [4.596 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/23/23 16:21:23.131
  Apr 23 16:21:23.131: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename podtemplate @ 04/23/23 16:21:23.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:23.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:23.274
  STEP: Create a pod template @ 04/23/23 16:21:23.281
  STEP: Replace a pod template @ 04/23/23 16:21:23.291
  Apr 23 16:21:23.309: INFO: Found updated podtemplate annotation: "true"

  Apr 23 16:21:23.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1504" for this suite. @ 04/23/23 16:21:23.32
• [0.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/23/23 16:21:23.367
  Apr 23 16:21:23.368: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:21:23.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:23.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:23.408
  STEP: Counting existing ResourceQuota @ 04/23/23 16:21:23.415
  E0423 16:21:23.449251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:24.450268      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:25.451202      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:26.452299      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:27.452871      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 16:21:28.427
  STEP: Ensuring resource quota status is calculated @ 04/23/23 16:21:28.447
  E0423 16:21:28.453721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:29.454530      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:30.455412      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 04/23/23 16:21:30.457
  STEP: Ensuring resource quota status captures replication controller creation @ 04/23/23 16:21:30.483
  E0423 16:21:31.455692      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:32.455968      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 04/23/23 16:21:32.491
  STEP: Ensuring resource quota status released usage @ 04/23/23 16:21:32.507
  E0423 16:21:33.457094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:34.457618      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:21:34.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4078" for this suite. @ 04/23/23 16:21:34.529
• [11.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/23/23 16:21:34.545
  Apr 23 16:21:34.545: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:21:34.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:34.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:34.586
  STEP: Setting up server cert @ 04/23/23 16:21:34.897
  E0423 16:21:35.458380      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:36.458597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:21:36.529
  STEP: Deploying the webhook pod @ 04/23/23 16:21:36.545
  STEP: Wait for the deployment to be ready @ 04/23/23 16:21:36.573
  Apr 23 16:21:36.608: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 16:21:37.458954      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:38.461039      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:21:38.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 21, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 21, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 21, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 21, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:21:39.461596      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:40.461708      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 16:21:40.794
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:21:40.82
  E0423 16:21:41.462195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:21:41.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/23/23 16:21:41.83
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/23/23 16:21:41.833
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/23/23 16:21:41.833
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/23/23 16:21:41.834
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/23/23 16:21:41.836
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/23/23 16:21:41.836
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/23/23 16:21:41.839
  Apr 23 16:21:41.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6374" for this suite. @ 04/23/23 16:21:42.016
  STEP: Destroying namespace "webhook-markers-19" for this suite. @ 04/23/23 16:21:42.029
• [7.527 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/23/23 16:21:42.074
  Apr 23 16:21:42.074: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:21:42.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:42.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:42.121
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:21:42.145
  E0423 16:21:42.462374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:43.462931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:44.463838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:45.465464      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:21:46.21
  Apr 23 16:21:46.216: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-99cf4a4c-b00f-4cf9-b151-6ba56dccb859 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:21:46.242
  Apr 23 16:21:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6977" for this suite. @ 04/23/23 16:21:46.305
• [4.245 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/23/23 16:21:46.32
  Apr 23 16:21:46.320: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:21:46.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:46.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:46.366
  STEP: Creating configMap with name configmap-test-volume-bca4453c-04c8-49e9-b4a7-2f16242d6557 @ 04/23/23 16:21:46.374
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:21:46.385
  E0423 16:21:46.465815      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:47.483423      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:48.483918      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:49.484121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:21:50.431
  Apr 23 16:21:50.441: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-3ab77b56-cdfb-454b-9b50-4422531b1b8c container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:21:50.458
  E0423 16:21:50.487322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:21:50.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6536" for this suite. @ 04/23/23 16:21:50.509
• [4.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/23/23 16:21:50.534
  Apr 23 16:21:50.534: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:21:50.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:50.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:50.579
  STEP: fetching services @ 04/23/23 16:21:50.585
  Apr 23 16:21:50.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1415" for this suite. @ 04/23/23 16:21:50.613
• [0.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/23/23 16:21:50.757
  Apr 23 16:21:50.757: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:21:50.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:50.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:50.836
  STEP: creating the pod @ 04/23/23 16:21:50.841
  STEP: submitting the pod to kubernetes @ 04/23/23 16:21:50.842
  W0423 16:21:50.878215      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0423 16:21:51.485679      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:52.492937      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:53.493508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:54.494081      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/23/23 16:21:54.923
  STEP: updating the pod @ 04/23/23 16:21:54.928
  Apr 23 16:21:55.450: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b8a4a764-33c6-4ccb-93fc-9af8c14dd056"
  E0423 16:21:55.495053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:56.495472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:57.495359      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:21:58.496218      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:21:59.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9861" for this suite. @ 04/23/23 16:21:59.484
  E0423 16:21:59.497886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [8.754 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/23/23 16:21:59.519
  Apr 23 16:21:59.519: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename limitrange @ 04/23/23 16:21:59.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:59.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:59.578
  STEP: Creating LimitRange "e2e-limitrange-gncbg" in namespace "limitrange-1836" @ 04/23/23 16:21:59.586
  STEP: Creating another limitRange in another namespace @ 04/23/23 16:21:59.6
  Apr 23 16:21:59.632: INFO: Namespace "e2e-limitrange-gncbg-6934" created
  Apr 23 16:21:59.632: INFO: Creating LimitRange "e2e-limitrange-gncbg" in namespace "e2e-limitrange-gncbg-6934"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-gncbg" @ 04/23/23 16:21:59.649
  Apr 23 16:21:59.660: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-gncbg" in "limitrange-1836" namespace @ 04/23/23 16:21:59.661
  Apr 23 16:21:59.680: INFO: LimitRange "e2e-limitrange-gncbg" has been patched
  STEP: Delete LimitRange "e2e-limitrange-gncbg" by Collection with labelSelector: "e2e-limitrange-gncbg=patched" @ 04/23/23 16:21:59.681
  STEP: Confirm that the limitRange "e2e-limitrange-gncbg" has been deleted @ 04/23/23 16:21:59.706
  Apr 23 16:21:59.707: INFO: Requesting list of LimitRange to confirm quantity
  Apr 23 16:21:59.715: INFO: Found 0 LimitRange with label "e2e-limitrange-gncbg=patched"
  Apr 23 16:21:59.715: INFO: LimitRange "e2e-limitrange-gncbg" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-gncbg" @ 04/23/23 16:21:59.716
  Apr 23 16:21:59.728: INFO: Found 1 limitRange
  Apr 23 16:21:59.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-1836" for this suite. @ 04/23/23 16:21:59.744
  STEP: Destroying namespace "e2e-limitrange-gncbg-6934" for this suite. @ 04/23/23 16:21:59.764
• [0.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 04/23/23 16:21:59.801
  Apr 23 16:21:59.802: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 16:21:59.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:21:59.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:21:59.846
  STEP: Creating service test in namespace statefulset-8384 @ 04/23/23 16:21:59.85
  STEP: Creating a new StatefulSet @ 04/23/23 16:21:59.866
  Apr 23 16:21:59.893: INFO: Found 0 stateful pods, waiting for 3
  E0423 16:22:00.498813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:01.499768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:02.501208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:03.500677      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:04.501880      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:05.501805      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:06.501979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:07.502560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:08.502928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:09.503223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:22:09.911: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 16:22:09.911: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 16:22:09.912: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 16:22:09.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8384 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0423 16:22:10.504231      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:22:10.693: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 16:22:10.693: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 16:22:10.693: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0423 16:22:11.504638      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:12.505436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:13.505603      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:14.505991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:15.506237      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:16.506439      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:17.506721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:18.506903      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:19.507058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:20.507428      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/23/23 16:22:20.83
  Apr 23 16:22:20.862: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/23/23 16:22:20.862
  E0423 16:22:21.508638      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:22.508783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:23.509016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:24.509497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:25.510041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:26.510593      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:27.510858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:28.511123      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:29.511317      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:30.511572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 04/23/23 16:22:30.917
  Apr 23 16:22:30.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8384 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 16:22:31.288: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 16:22:31.288: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 16:22:31.288: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0423 16:22:31.512431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:32.513344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:33.514306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:34.514683      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:35.514921      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:36.515088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:37.516548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:38.516754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:39.517614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:40.518349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:22:41.334: INFO: Waiting for StatefulSet statefulset-8384/ss2 to complete update
  E0423 16:22:41.518914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:42.520313      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:43.520444      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:44.520610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:45.520804      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:46.520960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:47.521055      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:48.521223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:49.521454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:50.522306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 04/23/23 16:22:51.348
  Apr 23 16:22:51.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8384 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0423 16:22:51.522410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:22:51.719: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 16:22:51.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 16:22:51.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0423 16:22:52.523452      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:53.523674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:54.523934      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:55.524074      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:56.524122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:57.524327      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:58.524552      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:22:59.525441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:00.525584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:01.547413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:23:01.787: INFO: Updating stateful set ss2
  E0423 16:23:02.544259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:03.545181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:04.545426      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:05.545958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:06.546255      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:07.547149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:08.547636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:09.547937      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:10.548040      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:11.548316      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 04/23/23 16:23:11.843
  Apr 23 16:23:11.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=statefulset-8384 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 16:23:12.242: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 16:23:12.242: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 16:23:12.242: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0423 16:23:12.549407      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:13.549714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:14.550198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:15.550113      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:16.550416      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:17.551263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:18.551845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:19.552034      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:20.553134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:21.552546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:23:22.299: INFO: Waiting for StatefulSet statefulset-8384/ss2 to complete update
  Apr 23 16:23:22.300: INFO: Waiting for Pod statefulset-8384/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
  E0423 16:23:22.552979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:23.553045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:24.555069      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:25.555271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:26.555290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:27.556341      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:28.557299      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:29.557993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:30.558070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:31.558230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:23:32.320: INFO: Deleting all statefulset in ns statefulset-8384
  Apr 23 16:23:32.330: INFO: Scaling statefulset ss2 to 0
  E0423 16:23:32.559134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:33.559568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:34.559631      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:35.559679      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:36.561072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:37.562046      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:38.562111      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:39.562756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:40.562898      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:41.562978      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:23:42.381: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 16:23:42.395: INFO: Deleting statefulset ss2
  Apr 23 16:23:42.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8384" for this suite. @ 04/23/23 16:23:42.441
• [102.660 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/23/23 16:23:42.469
  Apr 23 16:23:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename dns @ 04/23/23 16:23:42.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:23:42.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:23:42.51
  STEP: Creating a test headless service @ 04/23/23 16:23:42.516
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8502 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8502;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8502 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8502;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8502.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8502.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8502.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8502.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8502.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8502.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8502.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8502.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8502.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8502.svc;check="$$(dig +notcp +noall +answer +search 82.12.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.12.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.12.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.12.82_tcp@PTR;sleep 1; done
   @ 04/23/23 16:23:42.561
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8502 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8502;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8502 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8502;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8502.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8502.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8502.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8502.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8502.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8502.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8502.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8502.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8502.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8502.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8502.svc;check="$$(dig +notcp +noall +answer +search 82.12.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.12.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.12.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.12.82_tcp@PTR;sleep 1; done
   @ 04/23/23 16:23:42.561
  STEP: creating a pod to probe DNS @ 04/23/23 16:23:42.562
  STEP: submitting the pod to kubernetes @ 04/23/23 16:23:42.562
  E0423 16:23:42.563570      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:43.563996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:44.566037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:45.566140      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:46.571820      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 16:23:46.701
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:23:46.795
  Apr 23 16:23:46.826: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.841: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.851: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.860: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.873: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.884: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.894: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.910: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.968: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.979: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:46.994: INFO: Unable to read jessie_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:47.004: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:47.018: INFO: Unable to read jessie_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:47.029: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:47.037: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:47.045: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:47.076: INFO: Lookups using dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8502 wheezy_tcp@dns-test-service.dns-8502 wheezy_udp@dns-test-service.dns-8502.svc wheezy_tcp@dns-test-service.dns-8502.svc wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8502 jessie_tcp@dns-test-service.dns-8502 jessie_udp@dns-test-service.dns-8502.svc jessie_tcp@dns-test-service.dns-8502.svc jessie_udp@_http._tcp.dns-test-service.dns-8502.svc jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc]

  E0423 16:23:47.572235      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:48.572400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:49.572272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:50.572494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:51.575514      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:23:52.088: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.101: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.110: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.120: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.130: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.142: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.152: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.163: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.231: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.241: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.251: INFO: Unable to read jessie_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.263: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.274: INFO: Unable to read jessie_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.289: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.329: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:52.369: INFO: Lookups using dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8502 wheezy_tcp@dns-test-service.dns-8502 wheezy_udp@dns-test-service.dns-8502.svc wheezy_tcp@dns-test-service.dns-8502.svc wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8502 jessie_tcp@dns-test-service.dns-8502 jessie_udp@dns-test-service.dns-8502.svc jessie_tcp@dns-test-service.dns-8502.svc jessie_udp@_http._tcp.dns-test-service.dns-8502.svc jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc]

  E0423 16:23:52.575903      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:53.578622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:54.578453      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:55.578958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:56.579263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:23:57.090: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.100: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.111: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.123: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.134: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.155: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.168: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.178: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.243: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.250: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.258: INFO: Unable to read jessie_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.267: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.277: INFO: Unable to read jessie_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.290: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.315: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.327: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:23:57.379: INFO: Lookups using dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8502 wheezy_tcp@dns-test-service.dns-8502 wheezy_udp@dns-test-service.dns-8502.svc wheezy_tcp@dns-test-service.dns-8502.svc wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8502 jessie_tcp@dns-test-service.dns-8502 jessie_udp@dns-test-service.dns-8502.svc jessie_tcp@dns-test-service.dns-8502.svc jessie_udp@_http._tcp.dns-test-service.dns-8502.svc jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc]

  E0423 16:23:57.579341      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:58.579617      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:23:59.580100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:00.581032      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:01.581016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:02.086: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.100: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.116: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.130: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.147: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.158: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.169: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.181: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.260: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.271: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.280: INFO: Unable to read jessie_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.297: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.321: INFO: Unable to read jessie_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.331: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.343: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.364: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:02.413: INFO: Lookups using dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8502 wheezy_tcp@dns-test-service.dns-8502 wheezy_udp@dns-test-service.dns-8502.svc wheezy_tcp@dns-test-service.dns-8502.svc wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8502 jessie_tcp@dns-test-service.dns-8502 jessie_udp@dns-test-service.dns-8502.svc jessie_tcp@dns-test-service.dns-8502.svc jessie_udp@_http._tcp.dns-test-service.dns-8502.svc jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc]

  E0423 16:24:02.581913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:03.583154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:04.584062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:05.584657      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:06.585317      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:07.091: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.104: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.111: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.120: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.128: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.135: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.147: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.165: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.212: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.227: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.238: INFO: Unable to read jessie_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.244: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.251: INFO: Unable to read jessie_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.257: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.264: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.274: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:07.322: INFO: Lookups using dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8502 wheezy_tcp@dns-test-service.dns-8502 wheezy_udp@dns-test-service.dns-8502.svc wheezy_tcp@dns-test-service.dns-8502.svc wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8502 jessie_tcp@dns-test-service.dns-8502 jessie_udp@dns-test-service.dns-8502.svc jessie_tcp@dns-test-service.dns-8502.svc jessie_udp@_http._tcp.dns-test-service.dns-8502.svc jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc]

  E0423 16:24:07.585887      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:08.585817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:09.585991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:10.586139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:11.586120      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:12.086: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.094: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.103: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.111: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.120: INFO: Unable to read wheezy_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.129: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.138: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.148: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.186: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.202: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.210: INFO: Unable to read jessie_udp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.220: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502 from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.230: INFO: Unable to read jessie_udp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.246: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.255: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc from pod dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95: the server could not find the requested resource (get pods dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95)
  Apr 23 16:24:12.325: INFO: Lookups using dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8502 wheezy_tcp@dns-test-service.dns-8502 wheezy_udp@dns-test-service.dns-8502.svc wheezy_tcp@dns-test-service.dns-8502.svc wheezy_udp@_http._tcp.dns-test-service.dns-8502.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8502.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8502 jessie_tcp@dns-test-service.dns-8502 jessie_udp@dns-test-service.dns-8502.svc jessie_tcp@dns-test-service.dns-8502.svc jessie_udp@_http._tcp.dns-test-service.dns-8502.svc jessie_tcp@_http._tcp.dns-test-service.dns-8502.svc]

  E0423 16:24:12.586411      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:13.587292      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:14.588058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:15.588053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:16.591120      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:17.361: INFO: DNS probes using dns-8502/dns-test-d612c1e3-d642-474e-b656-ef3daab5ad95 succeeded

  Apr 23 16:24:17.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:24:17.372
  STEP: deleting the test service @ 04/23/23 16:24:17.495
  E0423 16:24:17.591475      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the test headless service @ 04/23/23 16:24:17.766
  STEP: Destroying namespace "dns-8502" for this suite. @ 04/23/23 16:24:17.822
• [35.396 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/23/23 16:24:17.866
  Apr 23 16:24:17.866: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:24:17.887
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:24:17.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:24:17.96
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/23/23 16:24:17.969
  Apr 23 16:24:17.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2208 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Apr 23 16:24:18.568: INFO: stderr: ""
  Apr 23 16:24:18.568: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/23/23 16:24:18.568
  E0423 16:24:18.592241      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:18.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2208 delete pods e2e-test-httpd-pod'
  E0423 16:24:19.603463      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:20.596037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:21.596361      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:22.597420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:22.665: INFO: stderr: ""
  Apr 23 16:24:22.666: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 23 16:24:22.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2208" for this suite. @ 04/23/23 16:24:22.754
• [4.991 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/23/23 16:24:22.876
  Apr 23 16:24:22.876: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:24:22.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:24:22.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:24:22.951
  STEP: creating service nodeport-test with type=NodePort in namespace services-8604 @ 04/23/23 16:24:22.958
  STEP: creating replication controller nodeport-test in namespace services-8604 @ 04/23/23 16:24:23.003
  I0423 16:24:23.031751      15 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-8604, replica count: 2
  E0423 16:24:23.598162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:24.598271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:25.599319      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:24:26.082777      15 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:24:26.082: INFO: Creating new exec pod
  E0423 16:24:26.599822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:27.600902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:28.601049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:29.601687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:30.602151      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:31.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8604 exec execpodr5rc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 23 16:24:31.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 23 16:24:31.553: INFO: stdout: "nodeport-test-7mcbn"
  Apr 23 16:24:31.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8604 exec execpodr5rc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.83 80'
  E0423 16:24:31.602368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:31.952: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.83 80\nConnection to 10.233.25.83 80 port [tcp/http] succeeded!\n"
  Apr 23 16:24:31.952: INFO: stdout: "nodeport-test-d8nlb"
  Apr 23 16:24:31.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8604 exec execpodr5rc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.241 31829'
  Apr 23 16:24:32.320: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.241 31829\nConnection to 192.168.121.241 31829 port [tcp/*] succeeded!\n"
  Apr 23 16:24:32.321: INFO: stdout: "nodeport-test-d8nlb"
  Apr 23 16:24:32.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8604 exec execpodr5rc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.96 31829'
  E0423 16:24:32.603550      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:32.874: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.96 31829\nConnection to 192.168.121.96 31829 port [tcp/*] succeeded!\n"
  Apr 23 16:24:32.874: INFO: stdout: "nodeport-test-d8nlb"
  Apr 23 16:24:32.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8604" for this suite. @ 04/23/23 16:24:32.884
• [10.023 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/23/23 16:24:32.902
  Apr 23 16:24:32.902: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename dns @ 04/23/23 16:24:32.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:24:32.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:24:32.944
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/23/23 16:24:32.948
  Apr 23 16:24:32.963: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7  0406b7cb-d5f8-4b4d-abe8-f58111bf100d 31611 0 2023-04-23 16:24:32 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-23 16:24:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6h2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6h2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0423 16:24:33.603796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:34.604359      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:35.604482      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:36.605229      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/23/23 16:24:36.988
  Apr 23 16:24:36.989: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:24:36.989: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:24:36.991: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:24:36.991: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-7/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 04/23/23 16:24:37.331
  Apr 23 16:24:37.331: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:24:37.331: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:24:37.335: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:24:37.336: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-7/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 16:24:37.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:24:37.520: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-7" for this suite. @ 04/23/23 16:24:37.542
• [4.661 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/23/23 16:24:37.568
  Apr 23 16:24:37.568: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 16:24:37.57
  E0423 16:24:37.605637      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:24:37.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:24:37.618
  STEP: creating a ServiceAccount @ 04/23/23 16:24:37.623
  STEP: watching for the ServiceAccount to be added @ 04/23/23 16:24:37.645
  STEP: patching the ServiceAccount @ 04/23/23 16:24:37.653
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/23/23 16:24:37.668
  STEP: deleting the ServiceAccount @ 04/23/23 16:24:37.687
  Apr 23 16:24:37.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7714" for this suite. @ 04/23/23 16:24:37.735
• [0.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/23/23 16:24:37.767
  Apr 23 16:24:37.767: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename discovery @ 04/23/23 16:24:37.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:24:37.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:24:37.811
  STEP: Setting up server cert @ 04/23/23 16:24:37.819
  E0423 16:24:38.608195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:39.607257      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:24:39.720: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 23 16:24:39.722: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 23 16:24:39.723: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 23 16:24:39.723: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 23 16:24:39.724: INFO: Checking APIGroup: apps
  Apr 23 16:24:39.731: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 23 16:24:39.731: INFO: Versions found [{apps/v1 v1}]
  Apr 23 16:24:39.731: INFO: apps/v1 matches apps/v1
  Apr 23 16:24:39.731: INFO: Checking APIGroup: events.k8s.io
  Apr 23 16:24:39.734: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 23 16:24:39.734: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 23 16:24:39.735: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 23 16:24:39.735: INFO: Checking APIGroup: authentication.k8s.io
  Apr 23 16:24:39.738: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 23 16:24:39.738: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 23 16:24:39.738: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 23 16:24:39.738: INFO: Checking APIGroup: authorization.k8s.io
  Apr 23 16:24:39.740: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 23 16:24:39.741: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 23 16:24:39.741: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 23 16:24:39.741: INFO: Checking APIGroup: autoscaling
  Apr 23 16:24:39.746: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 23 16:24:39.746: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 23 16:24:39.746: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 23 16:24:39.746: INFO: Checking APIGroup: batch
  Apr 23 16:24:39.748: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 23 16:24:39.748: INFO: Versions found [{batch/v1 v1}]
  Apr 23 16:24:39.749: INFO: batch/v1 matches batch/v1
  Apr 23 16:24:39.749: INFO: Checking APIGroup: certificates.k8s.io
  Apr 23 16:24:39.751: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 23 16:24:39.751: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 23 16:24:39.751: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 23 16:24:39.751: INFO: Checking APIGroup: networking.k8s.io
  Apr 23 16:24:39.755: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 23 16:24:39.755: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 23 16:24:39.755: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 23 16:24:39.755: INFO: Checking APIGroup: policy
  Apr 23 16:24:39.757: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 23 16:24:39.757: INFO: Versions found [{policy/v1 v1}]
  Apr 23 16:24:39.757: INFO: policy/v1 matches policy/v1
  Apr 23 16:24:39.757: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 23 16:24:39.759: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 23 16:24:39.760: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 23 16:24:39.760: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 23 16:24:39.760: INFO: Checking APIGroup: storage.k8s.io
  Apr 23 16:24:39.762: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 23 16:24:39.762: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 23 16:24:39.763: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 23 16:24:39.763: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 23 16:24:39.765: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 23 16:24:39.766: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 23 16:24:39.766: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 23 16:24:39.766: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 23 16:24:39.768: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 23 16:24:39.769: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 23 16:24:39.769: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 23 16:24:39.769: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 23 16:24:39.771: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 23 16:24:39.771: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 23 16:24:39.771: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 23 16:24:39.771: INFO: Checking APIGroup: coordination.k8s.io
  Apr 23 16:24:39.773: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 23 16:24:39.774: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 23 16:24:39.774: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 23 16:24:39.774: INFO: Checking APIGroup: node.k8s.io
  Apr 23 16:24:39.779: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 23 16:24:39.779: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 23 16:24:39.780: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 23 16:24:39.780: INFO: Checking APIGroup: discovery.k8s.io
  Apr 23 16:24:39.783: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 23 16:24:39.783: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 23 16:24:39.783: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 23 16:24:39.784: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 23 16:24:39.786: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 23 16:24:39.787: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 23 16:24:39.787: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 23 16:24:39.787: INFO: Checking APIGroup: cilium.io
  Apr 23 16:24:39.790: INFO: PreferredVersion.GroupVersion: cilium.io/v2
  Apr 23 16:24:39.790: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
  Apr 23 16:24:39.791: INFO: cilium.io/v2 matches cilium.io/v2
  Apr 23 16:24:39.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-2185" for this suite. @ 04/23/23 16:24:39.808
• [2.062 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/23/23 16:24:39.83
  Apr 23 16:24:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:24:39.832
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:24:39.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:24:39.89
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:24:39.9
  E0423 16:24:40.608221      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:41.609153      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:42.610100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:43.610704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:44.611150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:45.611226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:46.611456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:47.612157      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:24:47.977
  Apr 23 16:24:47.982: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-5aa70a9f-232c-48a5-adfe-a12f12a1c09d container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:24:48.013
  Apr 23 16:24:48.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2181" for this suite. @ 04/23/23 16:24:48.053
• [8.237 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/23/23 16:24:48.069
  Apr 23 16:24:48.069: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 16:24:48.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:24:48.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:24:48.118
  STEP: Creating a ReplaceConcurrent cronjob @ 04/23/23 16:24:48.122
  STEP: Ensuring a job is scheduled @ 04/23/23 16:24:48.138
  E0423 16:24:48.612359      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:49.612509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:50.613754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:51.614363      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:52.615149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:53.616034      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:54.616280      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:55.616414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:56.616782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:57.618375      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:58.618755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:24:59.618947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/23/23 16:25:00.146
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/23/23 16:25:00.155
  STEP: Ensuring the job is replaced with a new one @ 04/23/23 16:25:00.164
  E0423 16:25:00.619819      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:01.619953      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:02.620980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:03.621179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:04.621414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:05.621647      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:06.622333      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:07.622326      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:08.622412      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:09.622986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:10.623146      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:11.623963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:12.624470      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:13.624534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:14.625462      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:15.625818      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:16.626008      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:17.626301      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:18.627246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:19.628342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:20.629382      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:21.630437      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:22.630803      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:23.631158      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:24.631798      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:25.632191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:26.632898      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:27.632946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:28.633846      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:29.634592      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:30.634780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:31.634986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:32.635910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:33.636068      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:34.637009      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:35.637196      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:36.637477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:37.637910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:38.638865      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:39.639167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:40.640051      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:41.640170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:42.641125      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:43.641352      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:44.641504      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:45.642188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:46.642354      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:47.643389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:48.644201      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:49.644497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:50.645007      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:51.645431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:52.645879      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:53.645856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:54.647246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:55.648016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:56.649026      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:57.649041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:58.649951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:25:59.650814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/23/23 16:26:00.173
  Apr 23 16:26:00.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1765" for this suite. @ 04/23/23 16:26:00.205
• [72.154 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/23/23 16:26:00.225
  Apr 23 16:26:00.225: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 16:26:00.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:26:00.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:26:00.312
  STEP: Creating pod liveness-b617d54b-872b-4226-be80-b7b491024f47 in namespace container-probe-6940 @ 04/23/23 16:26:00.326
  E0423 16:26:00.665619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:01.679270      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:02.680772      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:03.681134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:26:04.367: INFO: Started pod liveness-b617d54b-872b-4226-be80-b7b491024f47 in namespace container-probe-6940
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 16:26:04.367
  Apr 23 16:26:04.377: INFO: Initial restart count of pod liveness-b617d54b-872b-4226-be80-b7b491024f47 is 0
  E0423 16:26:04.682355      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:05.682107      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:06.683028      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:07.683896      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:08.684787      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:09.684845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:10.685288      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:11.685691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:12.686446      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:13.686943      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:14.687130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:15.688042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:16.688339      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:17.688779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:18.689764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:19.689923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:20.690858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:21.691004      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:22.691084      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:23.691318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:24.692250      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:25.692255      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:26.693178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:27.693425      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:28.693546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:29.693695      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:30.694997      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:31.695186      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:32.695602      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:33.696385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:34.696959      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:35.697862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:36.698558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:37.699578      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:38.699886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:39.700144      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:40.700871      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:41.701072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:42.701646      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:43.701790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:44.702803      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:45.703032      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:46.703796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:47.704274      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:48.704674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:49.704893      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:50.705157      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:51.705920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:52.705953      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:53.706368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:54.706750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:55.707470      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:56.707638      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:57.708355      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:58.708916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:26:59.709373      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:00.709719      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:01.710137      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:02.710512      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:03.711002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:04.711346      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:05.712386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:06.712460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:07.713324      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:08.714226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:09.715350      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:10.725257      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:11.719959      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:12.719374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:13.721163      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:14.721151      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:15.721254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:16.721739      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:17.722075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:18.722268      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:19.722439      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:20.723039      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:21.724243      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:22.724423      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:23.725509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:24.725598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:25.726374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:26.726864      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:27.727272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:28.728176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:29.728667      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:30.728692      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:31.729352      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:32.729669      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:33.729852      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:34.730023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:35.730410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:36.732189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:37.731501      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:38.731451      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:39.731459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:40.731696      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:41.732030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:42.732861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:43.733006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:44.733420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:45.733506      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:46.734228      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:47.735536      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:48.735496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:49.736113      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:50.737079      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:51.737848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:52.738067      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:53.737859      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:54.738028      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:55.739024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:56.738975      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:57.740290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:58.740142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:27:59.740534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:00.741358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:01.742897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:02.743149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:03.743358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:04.743528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:05.743595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:06.744906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:07.745745      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:08.745487      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:09.746485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:10.746999      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:11.747757      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:12.747847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:13.748090      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:14.748660      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:15.752786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:16.749513      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:17.750136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:18.750346      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:19.750427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:20.750714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:21.750984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:22.751109      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:23.752020      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:24.752275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:25.753318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:26.752768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:27.753195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:28.753505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:29.753611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:30.753847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:31.754172      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:32.754780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:33.755091      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:34.755606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:35.755859      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:36.756130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:37.756284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:38.756754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:39.756948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:40.757344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:41.757437      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:42.758202      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:43.758356      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:44.758761      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:45.759200      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:46.759295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:47.760340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:48.760493      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:49.761509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:50.761890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:51.763099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:52.763530      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:53.764021      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:54.764598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:55.765085      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:56.764946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:57.765170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:58.766112      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:28:59.766419      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:00.768020      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:01.768948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:02.769201      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:03.770229      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:04.771850      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:05.771358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:06.773115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:07.772194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:08.772601      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:09.773628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:10.783020      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:11.783672      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:12.784392      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:13.785802      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:14.785815      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:15.786047      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:16.786228      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:17.786473      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:18.787167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:19.787768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:20.788875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:21.789914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:22.790855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:23.791053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:24.792033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:25.793087      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:26.798939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:27.798966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:28.799107      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:29.800112      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:30.800441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:31.800561      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:32.800694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:33.801697      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:34.802385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:35.803173      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:36.804025      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:37.804878      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:38.805548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:39.806656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:40.810952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:41.811449      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:42.812598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:43.813623      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:44.814189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:45.814493      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:46.815869      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:47.816436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:48.817178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:49.817589      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:50.817782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:51.818314      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:52.819264      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:53.818992      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:54.819174      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:55.819295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:56.819492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:57.819678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:58.820018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:29:59.820228      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:00.820589      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:01.820838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:02.821410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:03.821875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:04.822195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:05.822955      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:30:05.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:30:05.924
  STEP: Destroying namespace "container-probe-6940" for this suite. @ 04/23/23 16:30:05.974
• [245.766 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/23/23 16:30:05.992
  Apr 23 16:30:05.992: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:30:05.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:30:06.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:30:06.065
  STEP: Setting up server cert @ 04/23/23 16:30:06.141
  E0423 16:30:06.829042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:30:06.84
  STEP: Deploying the webhook pod @ 04/23/23 16:30:06.901
  STEP: Wait for the deployment to be ready @ 04/23/23 16:30:07.02
  Apr 23 16:30:07.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 16:30:07.829749      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:08.829976      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:30:09.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 30, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 30, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 30, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 30, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:30:09.830149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:10.830611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:30:11.159: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 30, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 30, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 30, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 30, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:30:11.831275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:12.831584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 16:30:13.161
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:30:13.201
  E0423 16:30:13.832147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:30:14.202: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 16:30:14.214: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/23/23 16:30:14.749
  STEP: Creating a custom resource that should be denied by the webhook @ 04/23/23 16:30:14.807
  E0423 16:30:14.832481      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:15.832504      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:16.832907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/23/23 16:30:16.964
  STEP: Updating the custom resource with disallowed data should be denied @ 04/23/23 16:30:16.979
  STEP: Deleting the custom resource should be denied @ 04/23/23 16:30:17.002
  STEP: Remove the offending key and value from the custom resource data @ 04/23/23 16:30:17.024
  STEP: Deleting the updated custom resource should be successful @ 04/23/23 16:30:17.057
  Apr 23 16:30:17.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 16:30:17.833213      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8584" for this suite. @ 04/23/23 16:30:17.9
  STEP: Destroying namespace "webhook-markers-1171" for this suite. @ 04/23/23 16:30:17.917
• [11.941 seconds]
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/23/23 16:30:17.933
  Apr 23 16:30:17.934: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 16:30:17.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:30:18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:30:18.011
  STEP: Creating a suspended cronjob @ 04/23/23 16:30:18.02
  STEP: Ensuring no jobs are scheduled @ 04/23/23 16:30:18.032
  E0423 16:30:18.833133      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:19.833262      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:20.833501      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:21.834377      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:22.834927      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:23.835455      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:24.835780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:25.836598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:26.837001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:27.837497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:28.837817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:29.838571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:30.838534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:31.839289      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:32.839322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:33.839686      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:34.840339      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:35.840811      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:36.841150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:37.842202      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:38.842287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:39.842358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:40.842744      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:41.843017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:42.844041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:43.844206      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:44.845252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:45.845656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:46.845696      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:47.845900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:48.846027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:49.846215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:50.846221      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:51.847022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:52.847709      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:53.848233      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:54.848344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:55.848925      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:56.849204      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:57.849542      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:58.849775      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:30:59.850441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:00.850842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:01.851688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:02.852497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:03.853484      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:04.853266      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:05.853899      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:06.854184      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:07.854177      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:08.854474      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:09.854835      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:10.855254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:11.855445      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:12.856280      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:13.856593      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:14.856860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:15.857051      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:16.857956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:17.859073      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:18.859263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:19.859471      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:20.859998      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:21.860264      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:22.859981      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:23.860249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:24.860630      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:25.860795      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:26.860758      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:27.860819      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:28.861198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:29.861497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:30.861634      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:31.862022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:32.862980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:33.863086      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:34.863195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:35.863414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:36.863580      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:37.865790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:38.866227      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:39.866790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:40.867686      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:41.868381      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:42.869174      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:43.869964      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:44.870686      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:45.870977      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:46.871974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:47.872560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:48.873142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:49.873240      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:50.873687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:51.874117      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:52.874968      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:53.876277      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:54.876521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:55.877485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:56.877576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:57.878563      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:58.878884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:31:59.880076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:00.880241      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:01.880404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:02.880782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:03.881065      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:04.881552      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:05.881916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:06.882610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:07.883439      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:08.883583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:09.883711      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:10.883794      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:11.884024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:12.884073      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:13.884807      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:14.884907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:15.885080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:16.885365      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:17.885430      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:18.886545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:19.886900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:20.887040      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:21.887179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:22.888017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:23.888459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:24.888713      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:25.888873      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:26.889083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:27.889993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:28.890214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:29.890609      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:30.890970      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:31.892002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:32.893049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:33.893442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:34.893691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:35.893920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:36.894706      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:37.894837      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:38.894986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:39.895154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:40.895426      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:41.895398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:42.895582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:43.895843      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:44.895935      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:45.896220      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:46.896409      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:47.896863      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:48.897126      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:49.898194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:50.898822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:51.899088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:52.899696      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:53.900263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:54.900452      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:55.901094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:56.901179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:57.901886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:58.902085      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:32:59.902614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:00.903684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:01.904245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:02.905081      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:03.906556      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:04.907014      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:05.907759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:06.908360      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:07.909532      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:08.909478      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:09.909566      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:10.909821      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:11.910435      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:12.910877      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:13.911072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:14.911256      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:15.911490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:16.911599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:17.911945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:18.912783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:19.913128      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:20.913518      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:21.913738      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:22.914836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:23.915019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:24.915059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:25.915209      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:26.915974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:27.916198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:28.916245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:29.916967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:30.918156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:31.918307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:32.918360      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:33.919035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:34.919167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:35.919496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:36.919746      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:37.920578      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:38.920507      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:39.921498      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:40.921404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:41.923438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:42.923245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:43.923058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:44.923309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:45.923422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:46.923945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:47.925013      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:48.925363      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:49.925477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:50.925603      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:51.926851      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:52.927009      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:53.927335      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:54.928024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:55.929672      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:56.929638      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:57.929812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:58.930609      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:33:59.931190      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:00.932035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:01.932956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:02.933606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:03.934587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:04.934905      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:05.935318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:06.936071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:07.936573      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:08.937055      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:09.937560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:10.937902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:11.938064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:12.938116      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:13.939042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:14.939064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:15.939940      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:16.940311      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:17.941157      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:18.942450      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:19.942765      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:20.943297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:21.943475      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:22.944118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:23.944302      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:24.944478      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:25.944558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:26.944842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:27.945012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:28.945129      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:29.945502      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:30.945910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:31.945882      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:32.946875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:33.947039      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:34.947272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:35.947568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:36.948294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:37.948550      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:38.948755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:39.949153      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:40.949577      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:41.950031      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:42.950081      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:43.950208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:44.950413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:45.950759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:46.951155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:47.951121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:48.951294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:49.951968      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:50.952316      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:51.952232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:52.952912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:53.953261      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:54.953461      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:55.953418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:56.953932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:57.954815      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:58.955348      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:34:59.956774      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:00.956423      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:01.956712      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:02.957911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:03.958003      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:04.958599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:05.959012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:06.959319      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:07.959349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:08.959631      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:09.959586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:10.959945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:11.960220      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:12.960439      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:13.960732      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:14.960839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:15.960969      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:16.961235      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:17.961737      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/23/23 16:35:18.051
  STEP: Removing cronjob @ 04/23/23 16:35:18.062
  Apr 23 16:35:18.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8842" for this suite. @ 04/23/23 16:35:18.106
• [300.192 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/23/23 16:35:18.133
  Apr 23 16:35:18.133: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-preemption @ 04/23/23 16:35:18.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:35:18.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:35:18.225
  Apr 23 16:35:18.261: INFO: Waiting up to 1m0s for all nodes to be ready
  E0423 16:35:18.962083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:19.962282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:20.962475      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:21.962891      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:22.962737      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:23.962982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:24.963986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:25.964143      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:26.964589      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:27.964741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:28.965126      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:29.965569      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:30.965740      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:31.966014      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:32.966748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:33.966828      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:34.966971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:35.967142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:36.967275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:37.968176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:38.968318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:39.969420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:40.969521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:41.969776      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:42.970842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:43.971010      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:44.972018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:45.972176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:46.972649      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:47.972960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:48.973530      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:49.975385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:50.974966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:51.976215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:52.977195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:53.977468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:54.977691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:55.978135      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:56.978894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:57.979708      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:58.980411      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:35:59.980780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:00.980890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:01.981122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:02.981217      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:03.982249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:04.982315      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:05.982875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:06.983054      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:07.983365      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:08.983955      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:09.984835      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:10.985191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:11.985463      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:12.985576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:13.985663      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:14.986018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:15.986205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:16.986848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:17.987156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:36:18.326: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/23/23 16:36:18.335
  Apr 23 16:36:18.395: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 23 16:36:18.408: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 23 16:36:18.449: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 23 16:36:18.471: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 23 16:36:18.543: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 23 16:36:18.585: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/23/23 16:36:18.585
  E0423 16:36:18.987980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:19.988139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:21.025890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:21.995770      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:22.996700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:23.996911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:24.997385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/23/23 16:36:25.046
  E0423 16:36:25.997490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:26.998076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:27.998750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:28.999004      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:36:29.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-55" for this suite. @ 04/23/23 16:36:29.384
• [71.282 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/23/23 16:36:29.419
  Apr 23 16:36:29.419: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename tables @ 04/23/23 16:36:29.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:36:29.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:36:29.491
  Apr 23 16:36:29.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-5180" for this suite. @ 04/23/23 16:36:29.535
• [0.217 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/23/23 16:36:29.637
  Apr 23 16:36:29.637: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 16:36:29.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:36:29.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:36:29.684
  Apr 23 16:36:29.692: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:36:29.999984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:31.000407      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:32.005658      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/23/23 16:36:32.877
  Apr 23 16:36:32.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-5543 --namespace=crd-publish-openapi-5543 create -f -'
  E0423 16:36:33.005782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:34.005950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:35.006540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:36:35.289: INFO: stderr: ""
  Apr 23 16:36:35.289: INFO: stdout: "e2e-test-crd-publish-openapi-5275-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 23 16:36:35.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-5543 --namespace=crd-publish-openapi-5543 delete e2e-test-crd-publish-openapi-5275-crds test-cr'
  Apr 23 16:36:35.588: INFO: stderr: ""
  Apr 23 16:36:35.588: INFO: stdout: "e2e-test-crd-publish-openapi-5275-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 23 16:36:35.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-5543 --namespace=crd-publish-openapi-5543 apply -f -'
  E0423 16:36:36.006683      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:37.006996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:38.007650      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:36:38.631: INFO: stderr: ""
  Apr 23 16:36:38.631: INFO: stdout: "e2e-test-crd-publish-openapi-5275-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 23 16:36:38.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-5543 --namespace=crd-publish-openapi-5543 delete e2e-test-crd-publish-openapi-5275-crds test-cr'
  Apr 23 16:36:38.902: INFO: stderr: ""
  Apr 23 16:36:38.902: INFO: stdout: "e2e-test-crd-publish-openapi-5275-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/23/23 16:36:38.902
  Apr 23 16:36:38.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-5543 explain e2e-test-crd-publish-openapi-5275-crds'
  E0423 16:36:39.008278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:36:39.690: INFO: stderr: ""
  Apr 23 16:36:39.690: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-5275-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0423 16:36:40.009231      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:41.009263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:42.009607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:36:42.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5543" for this suite. @ 04/23/23 16:36:42.046
• [12.422 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 04/23/23 16:36:42.065
  Apr 23 16:36:42.065: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 16:36:42.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:36:42.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:36:42.108
  Apr 23 16:36:42.114: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:36:43.010397      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:44.010691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0423 16:36:44.841269      15 warnings.go:70] unknown field "alpha"
  W0423 16:36:44.841593      15 warnings.go:70] unknown field "beta"
  W0423 16:36:44.841760      15 warnings.go:70] unknown field "delta"
  W0423 16:36:44.841935      15 warnings.go:70] unknown field "epsilon"
  W0423 16:36:44.842102      15 warnings.go:70] unknown field "gamma"
  Apr 23 16:36:44.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6226" for this suite. @ 04/23/23 16:36:44.918
• [2.879 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/23/23 16:36:44.945
  Apr 23 16:36:44.945: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/23/23 16:36:44.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:36:45.004
  E0423 16:36:45.011151      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:36:45.011
  STEP: create the container to handle the HTTPGet hook request. @ 04/23/23 16:36:45.03
  E0423 16:36:46.011961      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:47.012174      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:48.012744      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:49.013074      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/23/23 16:36:49.091
  E0423 16:36:50.013932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:51.014314      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:52.014294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:53.015183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/23/23 16:36:53.135
  STEP: delete the pod with lifecycle hook @ 04/23/23 16:36:53.175
  E0423 16:36:54.016063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:55.016441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:56.016669      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:57.016772      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:36:57.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7577" for this suite. @ 04/23/23 16:36:57.255
• [12.328 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/23/23 16:36:57.275
  Apr 23 16:36:57.275: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:36:57.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:36:57.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:36:57.322
  STEP: Creating projection with secret that has name projected-secret-test-29f2dc89-e9c8-438b-b249-1185572e39bd @ 04/23/23 16:36:57.332
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:36:57.357
  E0423 16:36:58.017585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:36:59.018287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:00.018358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:01.018691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:02.019732      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:03.020349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:37:03.413
  Apr 23 16:37:03.421: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-secrets-3df210f3-4ba7-4724-81e3-9bd48ce8eba9 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:37:03.437
  Apr 23 16:37:03.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3966" for this suite. @ 04/23/23 16:37:03.481
• [6.221 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/23/23 16:37:03.496
  Apr 23 16:37:03.496: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:37:03.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:03.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:03.55
  STEP: Setting up server cert @ 04/23/23 16:37:03.639
  E0423 16:37:04.021071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:37:04.611
  STEP: Deploying the webhook pod @ 04/23/23 16:37:04.631
  STEP: Wait for the deployment to be ready @ 04/23/23 16:37:04.655
  Apr 23 16:37:04.672: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0423 16:37:05.022256      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:06.022494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 16:37:06.756
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:37:06.787
  E0423 16:37:07.023067      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:07.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/23/23 16:37:07.799
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/23/23 16:37:07.853
  STEP: Creating a configMap that should not be mutated @ 04/23/23 16:37:07.868
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/23/23 16:37:07.894
  STEP: Creating a configMap that should be mutated @ 04/23/23 16:37:07.91
  E0423 16:37:08.024168      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:08.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1352" for this suite. @ 04/23/23 16:37:08.203
  STEP: Destroying namespace "webhook-markers-1765" for this suite. @ 04/23/23 16:37:08.225
• [4.744 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/23/23 16:37:08.244
  Apr 23 16:37:08.244: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:37:08.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:08.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:08.303
  STEP: Creating configMap with name configmap-test-volume-e918c441-aaa4-4770-9566-c62821a7c889 @ 04/23/23 16:37:08.307
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:37:08.315
  E0423 16:37:09.030973      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:10.031386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:11.032291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:12.032874      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:37:12.37
  Apr 23 16:37:12.377: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-a9350429-6ed3-4f31-b793-85182216183b container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:37:12.393
  Apr 23 16:37:12.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3839" for this suite. @ 04/23/23 16:37:12.434
• [4.204 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/23/23 16:37:12.452
  Apr 23 16:37:12.452: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/23/23 16:37:12.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:12.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:12.493
  STEP: create the container to handle the HTTPGet hook request. @ 04/23/23 16:37:12.509
  E0423 16:37:13.033365      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:14.033658      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:15.033396      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:16.034048      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/23/23 16:37:16.561
  E0423 16:37:17.033971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:18.034913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:19.036277      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:20.036703      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/23/23 16:37:20.612
  E0423 16:37:21.036756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:22.037355      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/23/23 16:37:22.666
  Apr 23 16:37:22.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2356" for this suite. @ 04/23/23 16:37:22.821
• [10.383 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/23/23 16:37:22.846
  Apr 23 16:37:22.847: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 16:37:22.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:22.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:22.898
  STEP: Creating replication controller my-hostname-basic-43f1d8cf-4301-4374-a854-24a9ccd9f920 @ 04/23/23 16:37:22.905
  Apr 23 16:37:22.951: INFO: Pod name my-hostname-basic-43f1d8cf-4301-4374-a854-24a9ccd9f920: Found 0 pods out of 1
  E0423 16:37:23.037724      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:24.038580      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:25.038560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:26.038994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:27.039123      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:27.960: INFO: Pod name my-hostname-basic-43f1d8cf-4301-4374-a854-24a9ccd9f920: Found 1 pods out of 1
  Apr 23 16:37:27.960: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-43f1d8cf-4301-4374-a854-24a9ccd9f920" are running
  Apr 23 16:37:27.967: INFO: Pod "my-hostname-basic-43f1d8cf-4301-4374-a854-24a9ccd9f920-ssgr9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 16:37:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 16:37:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 16:37:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 16:37:22 +0000 UTC Reason: Message:}])
  Apr 23 16:37:27.967: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/23/23 16:37:27.967
  Apr 23 16:37:27.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4987" for this suite. @ 04/23/23 16:37:28.004
• [5.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/23/23 16:37:28.029
  Apr 23 16:37:28.029: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 16:37:28.032
  E0423 16:37:28.039715      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:28.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:28.07
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/23/23 16:37:28.074
  E0423 16:37:29.040176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:30.040599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 04/23/23 16:37:30.112
  STEP: Then the orphan pod is adopted @ 04/23/23 16:37:30.124
  E0423 16:37:31.040605      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 04/23/23 16:37:31.138
  Apr 23 16:37:31.145: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/23/23 16:37:31.174
  E0423 16:37:32.040856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:32.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5238" for this suite. @ 04/23/23 16:37:32.21
• [4.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/23/23 16:37:32.241
  Apr 23 16:37:32.241: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename job @ 04/23/23 16:37:32.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:32.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:32.305
  STEP: Creating Indexed job @ 04/23/23 16:37:32.313
  STEP: Ensuring job reaches completions @ 04/23/23 16:37:32.33
  E0423 16:37:33.055067      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:34.052232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:35.052129      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:36.052304      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:37.052447      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:38.052920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:39.053075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:40.053479      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:41.053624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:42.054263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:43.054537      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:44.054993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 04/23/23 16:37:44.337
  Apr 23 16:37:44.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5426" for this suite. @ 04/23/23 16:37:44.362
• [12.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/23/23 16:37:44.386
  Apr 23 16:37:44.386: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:37:44.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:44.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:44.44
  STEP: Create set of pods @ 04/23/23 16:37:44.45
  Apr 23 16:37:44.474: INFO: created test-pod-1
  Apr 23 16:37:44.492: INFO: created test-pod-2
  Apr 23 16:37:44.511: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/23/23 16:37:44.511
  E0423 16:37:45.055784      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:46.070934      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:47.064183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:48.065104      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 04/23/23 16:37:48.727
  Apr 23 16:37:48.741: INFO: Pod quantity 3 is different from expected quantity 0
  E0423 16:37:49.065749      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:49.756: INFO: Pod quantity 3 is different from expected quantity 0
  E0423 16:37:50.066707      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:50.775: INFO: Pod quantity 3 is different from expected quantity 0
  E0423 16:37:51.068458      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:51.750: INFO: Pod quantity 2 is different from expected quantity 0
  E0423 16:37:52.068735      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:52.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9439" for this suite. @ 04/23/23 16:37:52.763
• [8.397 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/23/23 16:37:52.821
  Apr 23 16:37:52.821: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 16:37:52.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:52.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:52.876
  Apr 23 16:37:52.884: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:37:53.069545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:54.069991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:55.071082      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:56.071373      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:57.072043      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:58.072975      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:37:59.073718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:37:59.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2526" for this suite. @ 04/23/23 16:37:59.853
• [7.052 seconds]
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/23/23 16:37:59.873
  Apr 23 16:37:59.873: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:37:59.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:59.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:59.922
  STEP: Creating configMap configmap-9483/configmap-test-b6cbbf25-bede-4f45-ab49-1222cb950d02 @ 04/23/23 16:37:59.927
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:37:59.939
  E0423 16:38:00.074246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:01.170028      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:02.083634      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:03.083926      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:38:03.993
  Apr 23 16:38:03.999: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-e964b66a-d9c6-44a8-8b09-79d4d92e464f container env-test: <nil>
  STEP: delete the pod @ 04/23/23 16:38:04.014
  Apr 23 16:38:04.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9483" for this suite. @ 04/23/23 16:38:04.061
• [4.206 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/23/23 16:38:04.082
  Apr 23 16:38:04.082: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:38:04.084075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 16:38:04.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:04.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:04.121
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/23/23 16:38:04.126
  E0423 16:38:05.084736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:06.084974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:07.085050      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:08.086274      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:38:08.168
  Apr 23 16:38:08.176: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-522ef05c-d446-4dbd-bc94-157f46a5f631 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 16:38:08.201
  Apr 23 16:38:08.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6455" for this suite. @ 04/23/23 16:38:08.246
• [4.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/23/23 16:38:08.272
  Apr 23 16:38:08.272: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 16:38:08.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:08.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:08.345
  STEP: apply creating a deployment @ 04/23/23 16:38:08.352
  Apr 23 16:38:08.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2793" for this suite. @ 04/23/23 16:38:08.4
• [0.163 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/23/23 16:38:08.435
  Apr 23 16:38:08.435: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:38:08.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:08.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:08.486
  STEP: Creating secret with name secret-test-538b6147-8ad8-4b33-b1ea-aad0a685db90 @ 04/23/23 16:38:08.492
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:38:08.501
  E0423 16:38:09.086529      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:10.088348      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:11.088044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:12.088820      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:38:12.541
  Apr 23 16:38:12.552: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-secrets-d259e7a3-ef7c-43b2-a947-e6ea46969ee7 container secret-env-test: <nil>
  STEP: delete the pod @ 04/23/23 16:38:12.568
  Apr 23 16:38:12.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5483" for this suite. @ 04/23/23 16:38:12.618
• [4.197 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/23/23 16:38:12.635
  Apr 23 16:38:12.635: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-preemption @ 04/23/23 16:38:12.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:12.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:12.685
  Apr 23 16:38:12.748: INFO: Waiting up to 1m0s for all nodes to be ready
  E0423 16:38:13.089553      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:14.090524      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:15.091591      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:16.091688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:17.092089      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:18.092349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:19.093154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:20.093529      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:21.094611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:22.094862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:23.095957      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:24.096566      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:25.097493      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:26.097619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:27.098140      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:28.098191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:29.098946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:30.100018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:31.101099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:32.102124      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:33.102579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:34.102901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:35.103597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:36.103855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:37.104355      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:38.105189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:39.105295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:40.105526      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:41.106045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:42.106608      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:43.107261      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:44.107420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:45.107827      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:46.108876      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:47.109468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:48.109739      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:49.109752      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:50.110059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:51.110979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:52.111074      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:53.112038      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:54.112987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:55.113559      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:56.114102      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:57.114282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:58.114429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:38:59.114609      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:00.115400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:01.115937      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:02.116320      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:03.116723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:04.117372      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:05.118328      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:06.118817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:07.119702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:08.119906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:09.120746      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:10.121016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:11.122059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:12.122231      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:39:12.839: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/23/23 16:39:12.851
  Apr 23 16:39:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/23/23 16:39:12.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:12.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:12.938
  STEP: Finding an available node @ 04/23/23 16:39:12.945
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/23/23 16:39:12.946
  E0423 16:39:13.123390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:14.124073      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:15.124551      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:16.124813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/23/23 16:39:17.047
  Apr 23 16:39:17.072: INFO: found a healthy node: soodi4ja4shi-3
  E0423 16:39:17.125731      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:18.126265      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:19.126414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:20.127012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:21.127782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:22.128426      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:23.129041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:24.129230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:25.129760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:26.130324      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:27.130621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:28.131785      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:29.132031      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:39:29.268: INFO: pods created so far: [1 1 1]
  Apr 23 16:39:29.268: INFO: length of pods created so far: 3
  E0423 16:39:30.132112      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:31.132244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:39:31.295: INFO: pods created so far: [2 2 1]
  E0423 16:39:32.132358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:33.169043      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:34.169643      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:35.170122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:36.170605      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:37.171181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:38.172384      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:39:38.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:39:38.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6832" for this suite. @ 04/23/23 16:39:38.532
  STEP: Destroying namespace "sched-preemption-2035" for this suite. @ 04/23/23 16:39:38.555
• [85.944 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/23/23 16:39:38.586
  Apr 23 16:39:38.586: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 16:39:38.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:38.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:38.665
  STEP: Creating a pod to test substitution in container's command @ 04/23/23 16:39:38.672
  E0423 16:39:39.172459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:40.172662      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:41.172649      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:42.172809      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:39:42.811
  Apr 23 16:39:42.819: INFO: Trying to get logs from node soodi4ja4shi-3 pod var-expansion-204b7f30-0cc4-4959-9db2-155ded920e0d container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 16:39:43.103
  Apr 23 16:39:43.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7824" for this suite. @ 04/23/23 16:39:43.147
• [4.574 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS  E0423 16:39:43.173199      15 retrywatcher.go:130] "Watch failed" err="context canceled"
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/23/23 16:39:43.173
  Apr 23 16:39:43.173: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:39:43.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:43.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:43.219
  STEP: Creating configMap with name configmap-test-volume-map-2a65de07-89be-4f45-8237-80fcdf546fdf @ 04/23/23 16:39:43.238
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:39:43.262
  E0423 16:39:44.173560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:45.174412      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:46.174518      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:47.174760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:48.175775      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:49.175931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:39:49.486
  Apr 23 16:39:49.506: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-220832ac-79ba-415f-85eb-1036a757f309 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:39:49.555
  Apr 23 16:39:49.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3082" for this suite. @ 04/23/23 16:39:49.638
• [6.483 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/23/23 16:39:49.665
  Apr 23 16:39:49.666: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename endpointslice @ 04/23/23 16:39:49.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:49.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:49.717
  STEP: getting /apis @ 04/23/23 16:39:49.722
  STEP: getting /apis/discovery.k8s.io @ 04/23/23 16:39:49.734
  STEP: getting /apis/discovery.k8s.iov1 @ 04/23/23 16:39:49.737
  STEP: creating @ 04/23/23 16:39:49.74
  STEP: getting @ 04/23/23 16:39:49.784
  STEP: listing @ 04/23/23 16:39:49.797
  STEP: watching @ 04/23/23 16:39:49.808
  Apr 23 16:39:49.808: INFO: starting watch
  STEP: cluster-wide listing @ 04/23/23 16:39:49.813
  STEP: cluster-wide watching @ 04/23/23 16:39:49.823
  Apr 23 16:39:49.824: INFO: starting watch
  STEP: patching @ 04/23/23 16:39:49.826
  STEP: updating @ 04/23/23 16:39:49.843
  Apr 23 16:39:49.864: INFO: waiting for watch events with expected annotations
  Apr 23 16:39:49.864: INFO: saw patched and updated annotations
  STEP: deleting @ 04/23/23 16:39:49.865
  STEP: deleting a collection @ 04/23/23 16:39:49.892
  Apr 23 16:39:49.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9860" for this suite. @ 04/23/23 16:39:49.957
• [0.319 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/23/23 16:39:49.984
  Apr 23 16:39:49.984: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:39:49.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:50.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:50.031
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:39:50.036
  E0423 16:39:50.177201      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:51.178873      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:52.180005      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:53.180210      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:39:54.1
  Apr 23 16:39:54.108: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-871dec90-b481-422e-bcce-0c6acb47bf1d container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:39:54.124
  Apr 23 16:39:54.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-820" for this suite. @ 04/23/23 16:39:54.169
  E0423 16:39:54.181732      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/23/23 16:39:54.196
  Apr 23 16:39:54.196: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename init-container @ 04/23/23 16:39:54.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:54.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:54.234
  STEP: creating the pod @ 04/23/23 16:39:54.239
  Apr 23 16:39:54.239: INFO: PodSpec: initContainers in spec.initContainers
  E0423 16:39:55.194418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:56.185840      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:57.186263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:39:57.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6915" for this suite. @ 04/23/23 16:39:58.008
• [3.824 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/23/23 16:39:58.019
  Apr 23 16:39:58.019: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:39:58.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:58.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:58.057
  STEP: Counting existing ResourceQuota @ 04/23/23 16:39:58.062
  E0423 16:39:58.187139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:39:59.187712      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:00.187849      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:01.188241      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:02.188162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 16:40:03.073
  STEP: Ensuring resource quota status is calculated @ 04/23/23 16:40:03.084
  E0423 16:40:03.188480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:04.189506      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 04/23/23 16:40:05.093
  STEP: Ensuring resource quota status captures replicaset creation @ 04/23/23 16:40:05.111
  E0423 16:40:05.190073      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:06.190463      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 04/23/23 16:40:07.118
  STEP: Ensuring resource quota status released usage @ 04/23/23 16:40:07.13
  E0423 16:40:07.190796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:08.190937      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:40:09.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-916" for this suite. @ 04/23/23 16:40:09.148
• [11.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/23/23 16:40:09.176
  Apr 23 16:40:09.176: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 16:40:09.183
  E0423 16:40:09.191908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:40:09.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:40:09.226
  Apr 23 16:40:09.234: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:40:10.192480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:11.192635      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/23/23 16:40:11.32
  Apr 23 16:40:11.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1542 --namespace=crd-publish-openapi-1542 create -f -'
  E0423 16:40:12.192798      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:40:13.007: INFO: stderr: ""
  Apr 23 16:40:13.007: INFO: stdout: "e2e-test-crd-publish-openapi-6107-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 23 16:40:13.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1542 --namespace=crd-publish-openapi-1542 delete e2e-test-crd-publish-openapi-6107-crds test-cr'
  E0423 16:40:13.193790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:40:13.207: INFO: stderr: ""
  Apr 23 16:40:13.207: INFO: stdout: "e2e-test-crd-publish-openapi-6107-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 23 16:40:13.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1542 --namespace=crd-publish-openapi-1542 apply -f -'
  E0423 16:40:14.193986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:40:14.973: INFO: stderr: ""
  Apr 23 16:40:14.973: INFO: stdout: "e2e-test-crd-publish-openapi-6107-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 23 16:40:14.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1542 --namespace=crd-publish-openapi-1542 delete e2e-test-crd-publish-openapi-6107-crds test-cr'
  Apr 23 16:40:15.183: INFO: stderr: ""
  Apr 23 16:40:15.183: INFO: stdout: "e2e-test-crd-publish-openapi-6107-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/23/23 16:40:15.183
  Apr 23 16:40:15.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=crd-publish-openapi-1542 explain e2e-test-crd-publish-openapi-6107-crds'
  E0423 16:40:15.195030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:40:15.881: INFO: stderr: ""
  Apr 23 16:40:15.881: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-6107-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0423 16:40:16.194143      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:17.197225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:18.196431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:40:18.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1542" for this suite. @ 04/23/23 16:40:18.782
• [9.622 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/23/23 16:40:18.8
  Apr 23 16:40:18.800: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 16:40:18.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:40:18.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:40:18.871
  STEP: Create a Replicaset @ 04/23/23 16:40:18.882
  STEP: Verify that the required pods have come up. @ 04/23/23 16:40:18.895
  Apr 23 16:40:18.901: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0423 16:40:19.196014      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:20.196942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:21.197007      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:22.197135      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:23.197194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:40:23.913: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 16:40:23.913
  STEP: Getting /status @ 04/23/23 16:40:23.913
  Apr 23 16:40:23.928: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/23/23 16:40:23.928
  Apr 23 16:40:23.948: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/23/23 16:40:23.949
  Apr 23 16:40:23.952: INFO: Observed &ReplicaSet event: ADDED
  Apr 23 16:40:23.953: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:40:23.953: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:40:23.955: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:40:23.966: INFO: Found replicaset test-rs in namespace replicaset-7569 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 23 16:40:23.966: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/23/23 16:40:23.966
  Apr 23 16:40:23.967: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 23 16:40:23.987: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/23/23 16:40:23.987
  Apr 23 16:40:23.990: INFO: Observed &ReplicaSet event: ADDED
  Apr 23 16:40:23.991: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:40:23.992: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:40:23.993: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:40:23.993: INFO: Observed replicaset test-rs in namespace replicaset-7569 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 16:40:23.994: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:40:23.994: INFO: Found replicaset test-rs in namespace replicaset-7569 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 23 16:40:23.994: INFO: Replicaset test-rs has a patched status
  Apr 23 16:40:23.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7569" for this suite. @ 04/23/23 16:40:24.004
• [5.220 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/23/23 16:40:24.021
  Apr 23 16:40:24.021: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename disruption @ 04/23/23 16:40:24.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:40:24.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:40:24.058
  STEP: creating the pdb @ 04/23/23 16:40:24.064
  STEP: Waiting for the pdb to be processed @ 04/23/23 16:40:24.087
  E0423 16:40:24.197443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:25.197588      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 04/23/23 16:40:26.104
  STEP: Waiting for the pdb to be processed @ 04/23/23 16:40:26.116
  E0423 16:40:26.198607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:27.199033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 04/23/23 16:40:28.134
  STEP: Waiting for the pdb to be processed @ 04/23/23 16:40:28.155
  E0423 16:40:28.198952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:29.199167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 04/23/23 16:40:30.184
  Apr 23 16:40:30.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 16:40:30.200119      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "disruption-8677" for this suite. @ 04/23/23 16:40:30.21
• [6.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/23/23 16:40:30.236
  Apr 23 16:40:30.236: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:40:30.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:40:30.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:40:30.269
  STEP: Creating configMap with name cm-test-opt-del-ecd29961-20d2-46be-9b4c-8c5ab63b753b @ 04/23/23 16:40:30.282
  STEP: Creating configMap with name cm-test-opt-upd-0dbf05af-1d5f-48a1-83be-44e90936dd6f @ 04/23/23 16:40:30.295
  STEP: Creating the pod @ 04/23/23 16:40:30.303
  E0423 16:40:31.201425      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:32.202807      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:33.203255      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:34.203323      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-ecd29961-20d2-46be-9b4c-8c5ab63b753b @ 04/23/23 16:40:34.42
  STEP: Updating configmap cm-test-opt-upd-0dbf05af-1d5f-48a1-83be-44e90936dd6f @ 04/23/23 16:40:34.437
  STEP: Creating configMap with name cm-test-opt-create-429253e3-19d6-485b-bcae-8e2a36b95ac6 @ 04/23/23 16:40:34.444
  STEP: waiting to observe update in volume @ 04/23/23 16:40:34.452
  E0423 16:40:35.205745      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:36.204939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:37.205101      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:38.205480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:39.206332      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:40.206579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:41.206996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:42.206990      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:43.207610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:44.207947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:45.208246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:46.208562      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:47.209482      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:48.209915      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:49.210802      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:50.211096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:51.211886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:52.212524      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:53.212765      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:54.213244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:55.213905      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:56.214199      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:57.214931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:58.215900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:40:59.216245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:00.216514      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:01.216804      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:02.217531      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:03.218581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:04.219741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:05.219744      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:06.220750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:07.221000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:08.221948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:09.222546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:10.223075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:11.223983      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:12.224181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:13.224874      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:14.225727      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:15.227103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:16.226118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:17.226958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:18.227980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:19.229006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:20.229033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:21.229248      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:22.230132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:23.230695      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:24.231187      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:25.231250      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:26.231674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:27.232060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:28.232380      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:29.232496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:30.232686      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:31.233147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:32.233207      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:33.233208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:34.233963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:35.233939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:36.234348      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:37.235647      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:38.235893      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:39.236958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:40.236991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:41.237506      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:42.237839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:43.237910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:44.238852      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:45.239510      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:46.240040      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:47.240125      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:48.240820      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:49.241723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:50.241995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:51.242263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:52.242419      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:53.242901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:54.243677      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:55.243901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:56.244889      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:57.245533      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:58.245994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:41:59.246451      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:00.246606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:01.247092      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:42:01.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1548" for this suite. @ 04/23/23 16:42:01.926
• [91.703 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/23/23 16:42:01.946
  Apr 23 16:42:01.946: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:42:01.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:02.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:02.005
  STEP: Creating configMap with name projected-configmap-test-volume-3ca3fcd1-dab2-4f82-ac34-658ef168b644 @ 04/23/23 16:42:02.028
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:42:02.043
  E0423 16:42:02.247582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:03.247860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:04.248332      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:05.248690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:42:06.09
  Apr 23 16:42:06.097: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-configmaps-507cae18-df96-4ae8-820c-f405255ccd28 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:42:06.113
  Apr 23 16:42:06.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6298" for this suite. @ 04/23/23 16:42:06.164
• [4.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/23/23 16:42:06.195
  Apr 23 16:42:06.195: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:42:06.198
  E0423 16:42:06.249153      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:06.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:06.298
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:42:06.303
  E0423 16:42:07.249631      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:08.249859      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:09.250042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:10.250939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:11.251478      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:12.252451      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:42:12.373
  Apr 23 16:42:12.390: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-bae818b2-5cbb-4372-a48b-2676cc954044 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:42:12.41
  Apr 23 16:42:12.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6475" for this suite. @ 04/23/23 16:42:12.458
• [6.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/23/23 16:42:12.487
  Apr 23 16:42:12.487: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:42:12.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:12.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:12.538
  Apr 23 16:42:12.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5726" for this suite. @ 04/23/23 16:42:12.625
• [0.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/23/23 16:42:12.655
  Apr 23 16:42:12.655: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename endpointslice @ 04/23/23 16:42:12.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:12.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:12.698
  E0423 16:42:13.253369      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:14.253683      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:42:14.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7899" for this suite. @ 04/23/23 16:42:14.963
• [2.349 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/23/23 16:42:15.021
  Apr 23 16:42:15.021: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:42:15.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:15.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:15.083
  E0423 16:42:15.254437      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:16.255259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:17.256306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:18.258332      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:19.258164      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:20.258353      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:21.258770      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:22.259907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:23.260217      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:24.260751      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:25.261006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:26.261872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:27.262172      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:28.262755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:29.263239      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:30.263604      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:31.264575      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/23/23 16:42:32.099
  E0423 16:42:32.265282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:33.265735      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:34.266410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:35.267608      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:36.268088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 16:42:37.107
  STEP: Ensuring resource quota status is calculated @ 04/23/23 16:42:37.123
  E0423 16:42:37.268489      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:38.268507      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 04/23/23 16:42:39.133
  STEP: Ensuring resource quota status captures configMap creation @ 04/23/23 16:42:39.159
  E0423 16:42:39.269404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:40.270000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 04/23/23 16:42:41.168
  STEP: Ensuring resource quota status released usage @ 04/23/23 16:42:41.185
  E0423 16:42:41.270370      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:42.270933      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:42:43.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-57" for this suite. @ 04/23/23 16:42:43.201
• [28.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/23/23 16:42:43.223
  Apr 23 16:42:43.224: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename dns @ 04/23/23 16:42:43.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:43.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:43.262
  STEP: Creating a test headless service @ 04/23/23 16:42:43.267
  E0423 16:42:43.271579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8176.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8176.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/23/23 16:42:43.283
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8176.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8176.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/23/23 16:42:43.284
  STEP: creating a pod to probe DNS @ 04/23/23 16:42:43.284
  STEP: submitting the pod to kubernetes @ 04/23/23 16:42:43.284
  E0423 16:42:44.272089      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:45.273042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:46.273706      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:47.274434      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 16:42:47.344
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:42:47.351
  Apr 23 16:42:47.392: INFO: DNS probes using dns-8176/dns-test-ae80d5d1-17fb-4956-b7a7-3273056a7014 succeeded

  Apr 23 16:42:47.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:42:47.404
  STEP: deleting the test headless service @ 04/23/23 16:42:47.434
  STEP: Destroying namespace "dns-8176" for this suite. @ 04/23/23 16:42:47.465
• [4.282 seconds]
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/23/23 16:42:47.505
  Apr 23 16:42:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename security-context-test @ 04/23/23 16:42:47.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:47.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:47.553
  E0423 16:42:48.274592      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:49.274960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:50.274961      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:51.275483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:52.275971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:53.276694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:54.277754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:55.278413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:42:55.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-43" for this suite. @ 04/23/23 16:42:55.683
• [8.190 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/23/23 16:42:55.701
  Apr 23 16:42:55.701: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 16:42:55.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:55.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:55.743
  STEP: create the container @ 04/23/23 16:42:55.748
  W0423 16:42:55.763471      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/23/23 16:42:55.764
  E0423 16:42:56.278322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:57.278278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:42:58.279351      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/23/23 16:42:58.88
  STEP: the container should be terminated @ 04/23/23 16:42:58.887
  STEP: the termination message should be set @ 04/23/23 16:42:58.888
  Apr 23 16:42:58.888: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/23/23 16:42:58.888
  Apr 23 16:42:58.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5933" for this suite. @ 04/23/23 16:42:58.936
• [3.252 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/23/23 16:42:58.956
  Apr 23 16:42:58.956: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 16:42:58.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:42:58.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:42:58.988
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/23/23 16:42:58.993
  E0423 16:42:59.279855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:00.280671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:01.280849      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:02.281733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:03.282517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:04.283325      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:43:05.053
  Apr 23 16:43:05.060: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-4465da88-5b96-441b-9fda-56379014a955 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 16:43:05.205
  Apr 23 16:43:05.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 16:43:05.284301      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-6512" for this suite. @ 04/23/23 16:43:05.285
• [6.344 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/23/23 16:43:05.302
  Apr 23 16:43:05.302: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 16:43:05.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:43:05.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:43:05.346
  STEP: creating the pod @ 04/23/23 16:43:05.352
  STEP: waiting for pod running @ 04/23/23 16:43:05.371
  E0423 16:43:06.284152      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:07.284489      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:08.284564      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:09.285053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 04/23/23 16:43:09.453
  Apr 23 16:43:09.461: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7865 PodName:var-expansion-07614d58-3f01-4437-9f4c-6f7663bdb246 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:43:09.462: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:43:09.467: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:43:09.468: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7865/pods/var-expansion-07614d58-3f01-4437-9f4c-6f7663bdb246/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/23/23 16:43:09.592
  Apr 23 16:43:09.603: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7865 PodName:var-expansion-07614d58-3f01-4437-9f4c-6f7663bdb246 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:43:09.603: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:43:09.606: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:43:09.607: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7865/pods/var-expansion-07614d58-3f01-4437-9f4c-6f7663bdb246/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/23/23 16:43:09.753
  Apr 23 16:43:10.283: INFO: Successfully updated pod "var-expansion-07614d58-3f01-4437-9f4c-6f7663bdb246"
  STEP: waiting for annotated pod running @ 04/23/23 16:43:10.283
  E0423 16:43:10.285242      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 04/23/23 16:43:10.302
  Apr 23 16:43:10.303: INFO: Deleting pod "var-expansion-07614d58-3f01-4437-9f4c-6f7663bdb246" in namespace "var-expansion-7865"
  Apr 23 16:43:10.326: INFO: Wait up to 5m0s for pod "var-expansion-07614d58-3f01-4437-9f4c-6f7663bdb246" to be fully deleted
  E0423 16:43:11.285891      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:12.286673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:13.287003      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:14.287924      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:15.287968      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:16.288134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:17.288337      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:18.289177      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:19.289367      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:20.289943      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:21.290027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:22.290838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:23.290897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:24.291523      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:25.291603      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:26.292304      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:27.292303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:28.293353      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:29.293472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:30.293974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:31.295054      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:32.295191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:33.295346      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:34.295669      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:35.296596      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:36.296791      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:37.297541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:38.297721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:39.298108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:40.299150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:41.299306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:42.299747      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:43:42.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7865" for this suite. @ 04/23/23 16:43:42.479
• [37.192 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/23/23 16:43:42.496
  Apr 23 16:43:42.496: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/23/23 16:43:42.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:43:42.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:43:42.531
  STEP: creating a target pod @ 04/23/23 16:43:42.534
  E0423 16:43:43.301278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:44.301625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 04/23/23 16:43:44.577
  E0423 16:43:45.302371      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:46.302584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 04/23/23 16:43:46.664
  Apr 23 16:43:46.664: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1378 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:43:46.664: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 16:43:46.668: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:43:46.669: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-1378/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Apr 23 16:43:47.079: INFO: Exec stderr: ""
  Apr 23 16:43:47.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-1378" for this suite. @ 04/23/23 16:43:47.184
• [4.701 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/23/23 16:43:47.199
  Apr 23 16:43:47.199: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename ingressclass @ 04/23/23 16:43:47.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:43:47.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:43:47.237
  STEP: getting /apis @ 04/23/23 16:43:47.253
  STEP: getting /apis/networking.k8s.io @ 04/23/23 16:43:47.262
  STEP: getting /apis/networking.k8s.iov1 @ 04/23/23 16:43:47.267
  STEP: creating @ 04/23/23 16:43:47.269
  E0423 16:43:47.303105      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting @ 04/23/23 16:43:47.313
  STEP: listing @ 04/23/23 16:43:47.328
  STEP: watching @ 04/23/23 16:43:47.338
  Apr 23 16:43:47.338: INFO: starting watch
  STEP: patching @ 04/23/23 16:43:47.342
  STEP: updating @ 04/23/23 16:43:47.36
  Apr 23 16:43:47.370: INFO: waiting for watch events with expected annotations
  Apr 23 16:43:47.370: INFO: saw patched and updated annotations
  STEP: deleting @ 04/23/23 16:43:47.37
  STEP: deleting a collection @ 04/23/23 16:43:47.402
  Apr 23 16:43:47.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-1138" for this suite. @ 04/23/23 16:43:47.493
• [0.328 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/23/23 16:43:47.53
  Apr 23 16:43:47.530: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:43:47.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:43:47.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:43:47.634
  STEP: Discovering how many secrets are in namespace by default @ 04/23/23 16:43:47.641
  E0423 16:43:48.302951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:49.303939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:50.304103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:51.304482      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:52.305311      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/23/23 16:43:52.648
  E0423 16:43:53.305227      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:54.305972      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:55.306247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:56.306597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:57.306794      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 16:43:57.657
  STEP: Ensuring resource quota status is calculated @ 04/23/23 16:43:57.667
  E0423 16:43:58.307593      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:43:59.307921      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 04/23/23 16:43:59.679
  STEP: Ensuring resource quota status captures secret creation @ 04/23/23 16:43:59.714
  E0423 16:44:00.307982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:01.308171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 04/23/23 16:44:01.724
  STEP: Ensuring resource quota status released usage @ 04/23/23 16:44:01.744
  E0423 16:44:02.309065      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:03.309984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:44:03.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5338" for this suite. @ 04/23/23 16:44:03.763
• [16.244 seconds]
------------------------------
S
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/23/23 16:44:03.774
  Apr 23 16:44:03.774: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename containers @ 04/23/23 16:44:03.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:44:03.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:44:03.814
  E0423 16:44:04.310860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:05.311043      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:44:05.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2491" for this suite. @ 04/23/23 16:44:05.887
• [2.128 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/23/23 16:44:05.906
  Apr 23 16:44:05.906: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:44:05.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:44:05.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:44:05.977
  STEP: create deployment with httpd image @ 04/23/23 16:44:05.986
  Apr 23 16:44:05.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-9143 create -f -'
  E0423 16:44:06.311307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:07.312479      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:44:07.861: INFO: stderr: ""
  Apr 23 16:44:07.861: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/23/23 16:44:07.861
  Apr 23 16:44:07.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-9143 diff -f -'
  E0423 16:44:08.313535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:44:08.624: INFO: rc: 1
  Apr 23 16:44:08.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-9143 delete -f -'
  Apr 23 16:44:09.137: INFO: stderr: ""
  Apr 23 16:44:09.137: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 23 16:44:09.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9143" for this suite. @ 04/23/23 16:44:09.165
• [3.307 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/23/23 16:44:09.219
  Apr 23 16:44:09.219: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:44:09.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:44:09.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:44:09.292
  STEP: Creating secret with name secret-test-map-435f9d16-6798-442a-bc46-ff4bca337956 @ 04/23/23 16:44:09.297
  E0423 16:44:09.314278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:44:09.315
  E0423 16:44:10.314383      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:11.314592      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:12.314887      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:13.315026      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:14.316093      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:15.316995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:44:15.388
  Apr 23 16:44:15.401: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-secrets-255249c2-4b5d-4b5b-9930-58352b2400b8 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:44:15.433
  Apr 23 16:44:15.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2211" for this suite. @ 04/23/23 16:44:15.539
• [6.352 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/23/23 16:44:15.573
  Apr 23 16:44:15.573: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:44:15.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:44:15.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:44:15.673
  E0423 16:44:16.317803      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:17.329365      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:18.329488      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:19.329678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:20.330773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:21.331062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:22.332133      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:23.332982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:24.333178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:25.333436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:44:25.898
  Apr 23 16:44:25.910: INFO: Trying to get logs from node soodi4ja4shi-3 pod client-envvars-f9d7d35e-aca7-4767-ab57-c6bf21d78294 container env3cont: <nil>
  STEP: delete the pod @ 04/23/23 16:44:25.929
  Apr 23 16:44:25.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9909" for this suite. @ 04/23/23 16:44:25.967
• [10.407 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/23/23 16:44:25.985
  Apr 23 16:44:25.985: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:44:25.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:44:26.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:44:26.023
  STEP: Counting existing ResourceQuota @ 04/23/23 16:44:26.033
  E0423 16:44:26.334036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:27.335306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:28.335059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:29.335492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:30.336505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 16:44:31.041
  STEP: Ensuring resource quota status is calculated @ 04/23/23 16:44:31.058
  E0423 16:44:31.336723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:32.337148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 04/23/23 16:44:33.066
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/23/23 16:44:33.095
  E0423 16:44:33.337677      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:34.337693      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/23/23 16:44:35.103
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/23/23 16:44:35.107
  STEP: Ensuring a pod cannot update its resource requirements @ 04/23/23 16:44:35.111
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/23/23 16:44:35.119
  E0423 16:44:35.338106      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:36.338347      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/23/23 16:44:37.13
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 16:44:37.162
  E0423 16:44:37.339404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:38.339929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:44:39.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1853" for this suite. @ 04/23/23 16:44:39.188
• [13.221 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/23/23 16:44:39.206
  Apr 23 16:44:39.206: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:44:39.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:44:39.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:44:39.245
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:44:39.25
  E0423 16:44:39.340900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:40.356215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:41.343345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:42.343771      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:44:43.295
  Apr 23 16:44:43.304: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-f0e25aa4-75e6-485b-8073-a9ba0d074d02 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:44:43.335
  E0423 16:44:43.344535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:44:43.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8401" for this suite. @ 04/23/23 16:44:43.377
• [4.185 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/23/23 16:44:43.392
  Apr 23 16:44:43.392: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename init-container @ 04/23/23 16:44:43.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:44:43.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:44:43.451
  STEP: creating the pod @ 04/23/23 16:44:43.458
  Apr 23 16:44:43.459: INFO: PodSpec: initContainers in spec.initContainers
  E0423 16:44:44.344668      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:45.345503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:46.345770      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:47.346427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:44:47.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7855" for this suite. @ 04/23/23 16:44:47.695
• [4.322 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/23/23 16:44:47.715
  Apr 23 16:44:47.715: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename subpath @ 04/23/23 16:44:47.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:44:47.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:44:47.754
  STEP: Setting up data @ 04/23/23 16:44:47.76
  STEP: Creating pod pod-subpath-test-configmap-8crw @ 04/23/23 16:44:47.779
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 16:44:47.78
  E0423 16:44:48.348169      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:49.348371      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:50.349153      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:51.350048      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:52.350318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:53.350580      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:54.351453      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:55.351800      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:56.351576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:57.351812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:58.352808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:44:59.352960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:00.353208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:01.353671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:02.354386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:03.354864      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:04.354914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:05.354966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:06.355651      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:07.355870      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:08.356571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:09.357056      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:10.358183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:11.358324      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:45:11.983
  Apr 23 16:45:11.993: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-subpath-test-configmap-8crw container test-container-subpath-configmap-8crw: <nil>
  STEP: delete the pod @ 04/23/23 16:45:12.014
  STEP: Deleting pod pod-subpath-test-configmap-8crw @ 04/23/23 16:45:12.054
  Apr 23 16:45:12.054: INFO: Deleting pod "pod-subpath-test-configmap-8crw" in namespace "subpath-9669"
  Apr 23 16:45:12.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9669" for this suite. @ 04/23/23 16:45:12.069
• [24.372 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/23/23 16:45:12.09
  Apr 23 16:45:12.090: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename deployment @ 04/23/23 16:45:12.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:45:12.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:45:12.145
  Apr 23 16:45:12.155: INFO: Creating deployment "webserver-deployment"
  Apr 23 16:45:12.167: INFO: Waiting for observed generation 1
  E0423 16:45:12.358468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:13.382052      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:14.194: INFO: Waiting for all required pods to come up
  Apr 23 16:45:14.214: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/23/23 16:45:14.214
  E0423 16:45:14.382742      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:15.383194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:16.385657      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:17.385654      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:18.386704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:19.387594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:20.343: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 23 16:45:20.362: INFO: Updating deployment "webserver-deployment" with a non-existent image
  E0423 16:45:20.388582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:20.419: INFO: Updating deployment webserver-deployment
  Apr 23 16:45:20.420: INFO: Waiting for observed generation 2
  E0423 16:45:21.389020      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:22.389158      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:22.449: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 23 16:45:22.468: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 23 16:45:22.498: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 23 16:45:22.546: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 23 16:45:22.547: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 23 16:45:22.571: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 23 16:45:22.602: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 23 16:45:22.602: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 23 16:45:22.744: INFO: Updating deployment webserver-deployment
  Apr 23 16:45:22.744: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  E0423 16:45:23.389325      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:23.656: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  E0423 16:45:24.390108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:25.390049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:25.960: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Apr 23 16:45:26.338: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-4017  fe4cf55b-9f83-4d66-8a5c-1a3ffa4e9a9b 36973 3 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-23 16:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f5bc88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-23 16:45:23 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-04-23 16:45:24 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  E0423 16:45:26.390731      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:26.711: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-4017  b10e17fc-ac61-4ff4-b604-7d96c07d10b5 36960 3 2023-04-23 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fe4cf55b-9f83-4d66-8a5c-1a3ffa4e9a9b 0xc004da7eb7 0xc004da7eb8}] [] [{kube-controller-manager Update apps/v1 2023-04-23 16:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe4cf55b-9f83-4d66-8a5c-1a3ffa4e9a9b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004da7f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 16:45:26.712: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 23 16:45:26.712: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-4017  6716e4d5-d816-4e9e-bd49-23c32080657b 36956 3 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fe4cf55b-9f83-4d66-8a5c-1a3ffa4e9a9b 0xc004da7dc7 0xc004da7dc8}] [] [{kube-controller-manager Update apps/v1 2023-04-23 16:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe4cf55b-9f83-4d66-8a5c-1a3ffa4e9a9b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004da7e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 16:45:26.824: INFO: Pod "webserver-deployment-67bd4bf6dc-48czl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-48czl webserver-deployment-67bd4bf6dc- deployment-4017  d9689a9c-6d35-478e-b18b-edd29dde8c26 36795 0 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e420e7 0xc003e420e8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vnqr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vnqr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:10.233.64.44,StartTime:2023-04-23 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:45:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://605a9b0508626ca406ac1141668612529c85e48f1155cb8310690d324ead1dda,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.44,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.828: INFO: Pod "webserver-deployment-67bd4bf6dc-48nqr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-48nqr webserver-deployment-67bd4bf6dc- deployment-4017  33c0cd05-94db-4c8a-87a8-3b4ae9afdb06 36994 0 2023-04-23 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e422d7 0xc003e422d8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-25gxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-25gxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.840: INFO: Pod "webserver-deployment-67bd4bf6dc-6swvb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6swvb webserver-deployment-67bd4bf6dc- deployment-4017  42c950f7-2090-4060-9842-cb5637040bfe 36781 0 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e424a7 0xc003e424a8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wl7d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wl7d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:10.233.64.34,StartTime:2023-04-23 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:45:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dd46dc5184f089292a2e200e483aa26fc75fc57f47fc3923b23b76b6fe407498,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.34,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.843: INFO: Pod "webserver-deployment-67bd4bf6dc-89bhh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-89bhh webserver-deployment-67bd4bf6dc- deployment-4017  ae8b8969-9b2b-482c-b428-b1057a4e3ec3 36918 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e42697 0xc003e42698}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6mc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6mc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.844: INFO: Pod "webserver-deployment-67bd4bf6dc-8xfsk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8xfsk webserver-deployment-67bd4bf6dc- deployment-4017  b9554668-efaf-4e15-b2a7-24329f2e3d39 37006 0 2023-04-23 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e42867 0xc003e42868}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6nr6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6nr6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:,StartTime:2023-04-23 16:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.846: INFO: Pod "webserver-deployment-67bd4bf6dc-bzvrw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bzvrw webserver-deployment-67bd4bf6dc- deployment-4017  61edbbef-398b-474f-a110-8bb0f404ddfb 36988 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e42a37 0xc003e42a38}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8pl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8pl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:,StartTime:2023-04-23 16:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.848: INFO: Pod "webserver-deployment-67bd4bf6dc-djtbk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-djtbk webserver-deployment-67bd4bf6dc- deployment-4017  49cd1c07-9c4e-4a59-a136-b511c79e8d17 36703 0 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e42c07 0xc003e42c08}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tcp25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tcp25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:10.233.65.145,StartTime:2023-04-23 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:45:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bb936e4622d22a7ab4594469f93b4e0acb21e7f5ca3f8819cee70c0e92707739,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.145,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.850: INFO: Pod "webserver-deployment-67bd4bf6dc-fb6wr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fb6wr webserver-deployment-67bd4bf6dc- deployment-4017  62d83ebc-4fb1-40b4-ae0c-8acc3806cd19 36949 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e42df7 0xc003e42df8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qh6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qh6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.853: INFO: Pod "webserver-deployment-67bd4bf6dc-fzbtd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fzbtd webserver-deployment-67bd4bf6dc- deployment-4017  3fc1f2d7-8ac9-46df-84a9-b44217187e81 37005 0 2023-04-23 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e42fc7 0xc003e42fc8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhccg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhccg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.856: INFO: Pod "webserver-deployment-67bd4bf6dc-gcrbl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gcrbl webserver-deployment-67bd4bf6dc- deployment-4017  ab65d21d-5728-4fae-a0b0-44d8d9664a68 36940 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e43197 0xc003e43198}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mks46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mks46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:,StartTime:2023-04-23 16:45:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.858: INFO: Pod "webserver-deployment-67bd4bf6dc-gwn99" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gwn99 webserver-deployment-67bd4bf6dc- deployment-4017  8593e9a8-0454-44ae-bb3e-189d1cb5505d 36712 0 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e43367 0xc003e43368}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-flcs6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-flcs6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:10.233.65.214,StartTime:2023-04-23 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:45:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dfa6baf4f56ae72b91639cd627823bfb51c3ee702750c6c5af0e7d64dd7f1c39,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.214,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.860: INFO: Pod "webserver-deployment-67bd4bf6dc-j5lc8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-j5lc8 webserver-deployment-67bd4bf6dc- deployment-4017  938f37ce-95b0-4697-88f3-59547f39dcd4 36727 0 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e43557 0xc003e43558}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-56j4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-56j4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.66.198,StartTime:2023-04-23 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:45:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://31e356922bf7fe3d88d420fb215886161f1e5c37c58fbcf7a4c077131639b72f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.198,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.861: INFO: Pod "webserver-deployment-67bd4bf6dc-kq4bk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kq4bk webserver-deployment-67bd4bf6dc- deployment-4017  a71357c2-cff9-49e4-bc94-d0d1917e02d2 36970 0 2023-04-23 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e43747 0xc003e43748}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8fxh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8fxh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:,StartTime:2023-04-23 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.863: INFO: Pod "webserver-deployment-67bd4bf6dc-krvkj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-krvkj webserver-deployment-67bd4bf6dc- deployment-4017  0527c648-c1f2-46a9-b26d-9024e14b965f 36950 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e43917 0xc003e43918}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffd4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffd4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:,StartTime:2023-04-23 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.864: INFO: Pod "webserver-deployment-67bd4bf6dc-lmffz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lmffz webserver-deployment-67bd4bf6dc- deployment-4017  5bb115a3-e672-49df-b303-6e3805df10ad 36756 0 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e43ae7 0xc003e43ae8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxwgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxwgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.66.64,StartTime:2023-04-23 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:45:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b6e9eb3a183ab695cd53c03c774ae7596abe553808ecd379ff218afb00bbc81d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.64,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.866: INFO: Pod "webserver-deployment-67bd4bf6dc-rnb2p" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rnb2p webserver-deployment-67bd4bf6dc- deployment-4017  3ba47f27-954e-4611-8cdd-00930e0a1158 36719 0 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e43cd7 0xc003e43cd8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8pw79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8pw79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:10.233.65.189,StartTime:2023-04-23 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:45:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://af1038e84da623d91cf4093b3989b74de8908f2ae793a1b2bda4e33c49105784,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.189,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.871: INFO: Pod "webserver-deployment-67bd4bf6dc-rxsn5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rxsn5 webserver-deployment-67bd4bf6dc- deployment-4017  a42ea0b6-94ca-4ffb-b217-bbabd0786eaa 37000 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc003e43ed7 0xc003e43ed8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cq4hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cq4hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:,StartTime:2023-04-23 16:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.872: INFO: Pod "webserver-deployment-67bd4bf6dc-sphzr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sphzr webserver-deployment-67bd4bf6dc- deployment-4017  8f96b6b7-45ec-41f8-82d6-2395f0272987 36899 0 2023-04-23 16:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc0012b00b7 0xc0012b00b8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lk62m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lk62m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.874: INFO: Pod "webserver-deployment-67bd4bf6dc-vnh4s" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vnh4s webserver-deployment-67bd4bf6dc- deployment-4017  08b68b8a-75df-417e-b436-e6f60eaf59f7 36777 0 2023-04-23 16:45:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc0012b0287 0xc0012b0288}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ln9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ln9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:10.233.64.90,StartTime:2023-04-23 16:45:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:45:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://aa0f82a2ff2756927c09ac032ca549d7fe85e7e8e6596ca92a49a1cea604de14,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.90,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.875: INFO: Pod "webserver-deployment-67bd4bf6dc-zfnw6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zfnw6 webserver-deployment-67bd4bf6dc- deployment-4017  6bf98c8c-a7e9-4d65-8f2d-5bde63583782 37003 0 2023-04-23 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6716e4d5-d816-4e9e-bd49-23c32080657b 0xc0012b0657 0xc0012b0658}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6716e4d5-d816-4e9e-bd49-23c32080657b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4fhjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4fhjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.876: INFO: Pod "webserver-deployment-7b75d79cf5-6jt4x" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6jt4x webserver-deployment-7b75d79cf5- deployment-4017  00cef24c-9d54-4188-8089-a6fdf1951d38 36833 0 2023-04-23 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc0012b0ae7 0xc0012b0ae8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nv8hx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nv8hx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:,StartTime:2023-04-23 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.877: INFO: Pod "webserver-deployment-7b75d79cf5-bdtz6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bdtz6 webserver-deployment-7b75d79cf5- deployment-4017  ce807c13-b4ef-4e3f-ada7-a647ffeed604 36921 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc0012b0f47 0xc0012b0f48}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jw8zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jw8zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:,StartTime:2023-04-23 16:45:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.878: INFO: Pod "webserver-deployment-7b75d79cf5-dkhrj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dkhrj webserver-deployment-7b75d79cf5- deployment-4017  f6f04ac8-3e00-4311-b8c0-281bd2e3094d 36946 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327c7a7 0xc00327c7a8}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-th88d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-th88d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:,StartTime:2023-04-23 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.882: INFO: Pod "webserver-deployment-7b75d79cf5-dzw2p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dzw2p webserver-deployment-7b75d79cf5- deployment-4017  6e7966e6-7f4a-4227-930c-da61d21c26ae 36939 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327ce77 0xc00327ce78}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dr4dq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dr4dq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.883: INFO: Pod "webserver-deployment-7b75d79cf5-fkhms" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-fkhms webserver-deployment-7b75d79cf5- deployment-4017  5b81108f-4abe-4a90-be0c-73b8588d5a77 36952 0 2023-04-23 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327d077 0xc00327d078}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7cbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7cbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.885: INFO: Pod "webserver-deployment-7b75d79cf5-l5kq2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-l5kq2 webserver-deployment-7b75d79cf5- deployment-4017  f5c505e0-8af5-458c-b1e0-6ae79a46e706 36948 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327d287 0xc00327d288}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-28w92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28w92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.106,PodIP:,StartTime:2023-04-23 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.889: INFO: Pod "webserver-deployment-7b75d79cf5-ljzwx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ljzwx webserver-deployment-7b75d79cf5- deployment-4017  8690527a-0ada-45fe-81c4-e655d2c7a57b 36944 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327d477 0xc00327d478}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7f26t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7f26t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.890: INFO: Pod "webserver-deployment-7b75d79cf5-tmgn8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tmgn8 webserver-deployment-7b75d79cf5- deployment-4017  ca427adb-d722-4dc3-9f4c-c6a88f90bb89 36861 0 2023-04-23 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327d667 0xc00327d668}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xhf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xhf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:,StartTime:2023-04-23 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.891: INFO: Pod "webserver-deployment-7b75d79cf5-tn9hw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tn9hw webserver-deployment-7b75d79cf5- deployment-4017  dc1cb2df-19c5-4a3f-89c3-06d0b6ac4d1b 36862 0 2023-04-23 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327d857 0xc00327d858}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2fjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2fjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:,StartTime:2023-04-23 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.892: INFO: Pod "webserver-deployment-7b75d79cf5-w6wmh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w6wmh webserver-deployment-7b75d79cf5- deployment-4017  2e969e87-c016-43f0-85df-140ccf38b9ab 36822 0 2023-04-23 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327da47 0xc00327da48}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r67ww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r67ww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.894: INFO: Pod "webserver-deployment-7b75d79cf5-w85l2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w85l2 webserver-deployment-7b75d79cf5- deployment-4017  edcab485-d4f1-48a8-8d78-ad3064951e29 36869 0 2023-04-23 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327dc47 0xc00327dc48}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8j52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8j52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2023-04-23 16:45:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.899: INFO: Pod "webserver-deployment-7b75d79cf5-wkgvc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wkgvc webserver-deployment-7b75d79cf5- deployment-4017  6d389b29-1de8-430e-bf96-b17a7b3f1889 36919 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc00327de57 0xc00327de58}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcdwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcdwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:,StartTime:2023-04-23 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.900: INFO: Pod "webserver-deployment-7b75d79cf5-zt4fg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zt4fg webserver-deployment-7b75d79cf5- deployment-4017  8060a4cc-2e9a-4e2c-bb1c-2998f46097b1 37013 0 2023-04-23 16:45:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b10e17fc-ac61-4ff4-b604-7d96c07d10b5 0xc003f72047 0xc003f72048}] [] [{kube-controller-manager Update v1 2023-04-23 16:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b10e17fc-ac61-4ff4-b604-7d96c07d10b5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5w4kg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5w4kg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:soodi4ja4shi-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.241,PodIP:,StartTime:2023-04-23 16:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:45:26.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4017" for this suite. @ 04/23/23 16:45:27.027
• [15.004 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/23/23 16:45:27.096
  Apr 23 16:45:27.096: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename security-context-test @ 04/23/23 16:45:27.1
  E0423 16:45:27.392251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:45:27.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:45:27.45
  E0423 16:45:28.392931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:29.393696      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:30.394650      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:31.395422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:32.396483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:33.396728      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:34.397622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:35.397808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:36.397995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:37.398275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:38.398534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:39.398866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:40.399355      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:41.399338      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:42.400189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:43.401096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:44.401154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:45.401444      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:46.401547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:47.402004      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:48.402907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:49.450105      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:50.431773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:51.431933      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:52.432039      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:53.432208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:54.433519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:55.433594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:56.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 16:45:56.434400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "security-context-test-8618" for this suite. @ 04/23/23 16:45:56.438
• [29.357 seconds]
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/23/23 16:45:56.454
  Apr 23 16:45:56.454: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:45:56.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:45:56.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:45:56.512
  STEP: creating a replication controller @ 04/23/23 16:45:56.526
  Apr 23 16:45:56.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 create -f -'
  E0423 16:45:57.438183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:45:57.688: INFO: stderr: ""
  Apr 23 16:45:57.688: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/23/23 16:45:57.688
  Apr 23 16:45:57.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:45:58.020: INFO: stderr: ""
  Apr 23 16:45:58.021: INFO: stdout: "update-demo-nautilus-2c9lq update-demo-nautilus-57dzr "
  Apr 23 16:45:58.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-2c9lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:45:58.246: INFO: stderr: ""
  Apr 23 16:45:58.246: INFO: stdout: ""
  Apr 23 16:45:58.246: INFO: update-demo-nautilus-2c9lq is created but not running
  E0423 16:45:58.439209      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:45:59.439259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:00.440012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:01.440180      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:02.440730      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:03.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0423 16:46:03.441152      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:03.515: INFO: stderr: ""
  Apr 23 16:46:03.515: INFO: stdout: "update-demo-nautilus-2c9lq update-demo-nautilus-57dzr "
  Apr 23 16:46:03.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-2c9lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:03.803: INFO: stderr: ""
  Apr 23 16:46:03.803: INFO: stdout: ""
  Apr 23 16:46:03.803: INFO: update-demo-nautilus-2c9lq is created but not running
  E0423 16:46:04.441516      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:05.442389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:06.443027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:07.443247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:08.443344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:08.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:46:09.075: INFO: stderr: ""
  Apr 23 16:46:09.075: INFO: stdout: "update-demo-nautilus-2c9lq update-demo-nautilus-57dzr "
  Apr 23 16:46:09.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-2c9lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:09.314: INFO: stderr: ""
  Apr 23 16:46:09.314: INFO: stdout: ""
  Apr 23 16:46:09.314: INFO: update-demo-nautilus-2c9lq is created but not running
  E0423 16:46:09.443994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:10.445286      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:11.445214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:12.445408      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:13.446480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:14.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0423 16:46:14.446361      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:14.628: INFO: stderr: ""
  Apr 23 16:46:14.628: INFO: stdout: "update-demo-nautilus-2c9lq update-demo-nautilus-57dzr "
  Apr 23 16:46:14.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-2c9lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:14.920: INFO: stderr: ""
  Apr 23 16:46:14.920: INFO: stdout: ""
  Apr 23 16:46:14.920: INFO: update-demo-nautilus-2c9lq is created but not running
  E0423 16:46:15.446510      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:16.447185      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:17.448099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:18.449044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:19.449579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:19.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:46:20.144: INFO: stderr: ""
  Apr 23 16:46:20.144: INFO: stdout: "update-demo-nautilus-2c9lq update-demo-nautilus-57dzr "
  Apr 23 16:46:20.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-2c9lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:20.412: INFO: stderr: ""
  Apr 23 16:46:20.412: INFO: stdout: ""
  Apr 23 16:46:20.413: INFO: update-demo-nautilus-2c9lq is created but not running
  E0423 16:46:20.449883      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:21.450391      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:22.451477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:23.450958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:24.451127      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:25.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0423 16:46:25.452194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:25.608: INFO: stderr: ""
  Apr 23 16:46:25.608: INFO: stdout: "update-demo-nautilus-2c9lq update-demo-nautilus-57dzr "
  Apr 23 16:46:25.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-2c9lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:25.806: INFO: stderr: ""
  Apr 23 16:46:25.806: INFO: stdout: ""
  Apr 23 16:46:25.806: INFO: update-demo-nautilus-2c9lq is created but not running
  E0423 16:46:26.452400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:27.452607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:28.453470      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:29.453433      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:30.453681      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:30.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:46:31.076: INFO: stderr: ""
  Apr 23 16:46:31.076: INFO: stdout: "update-demo-nautilus-2c9lq update-demo-nautilus-57dzr "
  Apr 23 16:46:31.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-2c9lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:31.334: INFO: stderr: ""
  Apr 23 16:46:31.334: INFO: stdout: "true"
  Apr 23 16:46:31.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-2c9lq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0423 16:46:31.454703      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:31.631: INFO: stderr: ""
  Apr 23 16:46:31.631: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:46:31.631: INFO: validating pod update-demo-nautilus-2c9lq
  Apr 23 16:46:31.655: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:46:31.655: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:46:31.655: INFO: update-demo-nautilus-2c9lq is verified up and running
  Apr 23 16:46:31.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-57dzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:31.885: INFO: stderr: ""
  Apr 23 16:46:31.885: INFO: stdout: "true"
  Apr 23 16:46:31.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-57dzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:46:32.136: INFO: stderr: ""
  Apr 23 16:46:32.136: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:46:32.136: INFO: validating pod update-demo-nautilus-57dzr
  Apr 23 16:46:32.157: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:46:32.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:46:32.157: INFO: update-demo-nautilus-57dzr is verified up and running
  STEP: scaling down the replication controller @ 04/23/23 16:46:32.157
  Apr 23 16:46:32.212: INFO: scanned /root for discovery docs: <nil>
  Apr 23 16:46:32.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0423 16:46:32.454951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:33.455115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:33.555: INFO: stderr: ""
  Apr 23 16:46:33.555: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/23/23 16:46:33.555
  Apr 23 16:46:33.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:46:33.783: INFO: stderr: ""
  Apr 23 16:46:33.784: INFO: stdout: "update-demo-nautilus-2c9lq update-demo-nautilus-57dzr "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 04/23/23 16:46:33.784
  E0423 16:46:34.456089      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:35.456294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:36.456501      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:37.456656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:38.456962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:38.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:46:39.012: INFO: stderr: ""
  Apr 23 16:46:39.012: INFO: stdout: "update-demo-nautilus-57dzr "
  Apr 23 16:46:39.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-57dzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:39.199: INFO: stderr: ""
  Apr 23 16:46:39.199: INFO: stdout: "true"
  Apr 23 16:46:39.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-57dzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:46:39.393: INFO: stderr: ""
  Apr 23 16:46:39.393: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:46:39.393: INFO: validating pod update-demo-nautilus-57dzr
  Apr 23 16:46:39.406: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:46:39.406: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:46:39.406: INFO: update-demo-nautilus-57dzr is verified up and running
  STEP: scaling up the replication controller @ 04/23/23 16:46:39.406
  Apr 23 16:46:39.429: INFO: scanned /root for discovery docs: <nil>
  Apr 23 16:46:39.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0423 16:46:39.457400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:40.458010      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:40.660: INFO: stderr: ""
  Apr 23 16:46:40.660: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/23/23 16:46:40.66
  Apr 23 16:46:40.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:46:41.118: INFO: stderr: ""
  Apr 23 16:46:41.118: INFO: stdout: "update-demo-nautilus-57dzr update-demo-nautilus-fxfn5 "
  Apr 23 16:46:41.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-57dzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0423 16:46:41.458545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:41.560: INFO: stderr: ""
  Apr 23 16:46:41.560: INFO: stdout: "true"
  Apr 23 16:46:41.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-57dzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:46:41.783: INFO: stderr: ""
  Apr 23 16:46:41.783: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:46:41.783: INFO: validating pod update-demo-nautilus-57dzr
  Apr 23 16:46:41.791: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:46:41.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:46:41.791: INFO: update-demo-nautilus-57dzr is verified up and running
  Apr 23 16:46:41.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-fxfn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:42.039: INFO: stderr: ""
  Apr 23 16:46:42.039: INFO: stdout: ""
  Apr 23 16:46:42.039: INFO: update-demo-nautilus-fxfn5 is created but not running
  E0423 16:46:42.459703      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:43.460567      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:44.460602      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:45.460762      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:46.460948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:47.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:46:47.252: INFO: stderr: ""
  Apr 23 16:46:47.252: INFO: stdout: "update-demo-nautilus-57dzr update-demo-nautilus-fxfn5 "
  Apr 23 16:46:47.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-57dzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0423 16:46:47.462885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:47.475: INFO: stderr: ""
  Apr 23 16:46:47.475: INFO: stdout: "true"
  Apr 23 16:46:47.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-57dzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:46:47.757: INFO: stderr: ""
  Apr 23 16:46:47.757: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:46:47.757: INFO: validating pod update-demo-nautilus-57dzr
  Apr 23 16:46:47.776: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:46:47.776: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:46:47.776: INFO: update-demo-nautilus-57dzr is verified up and running
  Apr 23 16:46:47.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-fxfn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:46:47.981: INFO: stderr: ""
  Apr 23 16:46:47.981: INFO: stdout: "true"
  Apr 23 16:46:47.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods update-demo-nautilus-fxfn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:46:48.211: INFO: stderr: ""
  Apr 23 16:46:48.211: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:46:48.211: INFO: validating pod update-demo-nautilus-fxfn5
  Apr 23 16:46:48.234: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:46:48.234: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:46:48.234: INFO: update-demo-nautilus-fxfn5 is verified up and running
  STEP: using delete to clean up resources @ 04/23/23 16:46:48.234
  Apr 23 16:46:48.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 delete --grace-period=0 --force -f -'
  E0423 16:46:48.464154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:46:48.562: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 16:46:48.562: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 23 16:46:48.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get rc,svc -l name=update-demo --no-headers'
  Apr 23 16:46:48.901: INFO: stderr: "No resources found in kubectl-2099 namespace.\n"
  Apr 23 16:46:48.901: INFO: stdout: ""
  Apr 23 16:46:48.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-2099 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 23 16:46:49.119: INFO: stderr: ""
  Apr 23 16:46:49.119: INFO: stdout: ""
  Apr 23 16:46:49.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2099" for this suite. @ 04/23/23 16:46:49.132
• [52.778 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/23/23 16:46:49.234
  Apr 23 16:46:49.234: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:46:49.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:46:49.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:46:49.306
  STEP: Creating secret with name s-test-opt-del-2b45b8fb-0c43-4bb0-bab1-328631577c7f @ 04/23/23 16:46:49.365
  STEP: Creating secret with name s-test-opt-upd-2667146f-584c-44ef-ac10-8339ac28ef94 @ 04/23/23 16:46:49.44
  E0423 16:46:49.464734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating the pod @ 04/23/23 16:46:49.498
  E0423 16:46:50.464939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:51.475164      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:52.487181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:53.476892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:54.478458      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:55.478101      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-2b45b8fb-0c43-4bb0-bab1-328631577c7f @ 04/23/23 16:46:55.807
  STEP: Updating secret s-test-opt-upd-2667146f-584c-44ef-ac10-8339ac28ef94 @ 04/23/23 16:46:55.823
  STEP: Creating secret with name s-test-opt-create-f6257f0f-75a7-4693-a5f1-e12c569a4905 @ 04/23/23 16:46:55.838
  STEP: waiting to observe update in volume @ 04/23/23 16:46:55.855
  E0423 16:46:56.480741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:57.479896      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:58.480706      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:46:59.480979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:00.481422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:01.482218      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:02.482219      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:03.482344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:04.483790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:05.483431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:06.484095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:07.484232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:08.485282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:09.486170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:10.486193      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:11.487170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:12.487779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:13.487948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:14.488090      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:15.489209      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:16.490179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:17.490335      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:18.491307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:19.491428      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:20.491565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:21.491699      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:22.492001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:23.492095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:24.493142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:25.493220      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:26.494185      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:27.494710      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:28.494969      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:29.495060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:30.495961      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:31.497726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:32.498000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:33.499560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:34.499817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:35.500151      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:36.501193      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:37.501758      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:38.502019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:39.502401      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:40.503078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:41.503525      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:42.504028      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:43.505024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:44.505963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:45.506571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:46.506585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:47.506908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:48.507230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:49.507455      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:50.508035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:51.508295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:52.508831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:53.509182      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:54.509619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:55.510555      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:56.511927      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:57.511626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:58.511886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:47:59.512146      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:00.512398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:01.512374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:02.513515      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:03.513715      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:04.514454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:05.514546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:06.515097      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:07.515215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:08.515593      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:09.515947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:10.516118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:11.517071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:12.517407      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:13.517743      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:14.517827      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:48:15.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2924" for this suite. @ 04/23/23 16:48:15.371
• [86.155 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/23/23 16:48:15.4
  Apr 23 16:48:15.400: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename job @ 04/23/23 16:48:15.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:48:15.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:48:15.453
  STEP: Creating a job @ 04/23/23 16:48:15.46
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/23/23 16:48:15.476
  E0423 16:48:15.518586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:16.519290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:17.519466      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:18.519944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 04/23/23 16:48:19.495
  E0423 16:48:19.520325      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating /status @ 04/23/23 16:48:19.523
  STEP: get /status @ 04/23/23 16:48:19.551
  Apr 23 16:48:19.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9572" for this suite. @ 04/23/23 16:48:19.577
• [4.207 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/23/23 16:48:19.615
  Apr 23 16:48:19.616: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename dns @ 04/23/23 16:48:19.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:48:19.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:48:19.72
  STEP: Creating a test externalName service @ 04/23/23 16:48:19.737
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9102.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:48:19.764
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9102.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:48:19.764
  STEP: creating a pod to probe DNS @ 04/23/23 16:48:19.764
  STEP: submitting the pod to kubernetes @ 04/23/23 16:48:19.765
  E0423 16:48:20.520577      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:21.521871      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:22.522483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:23.524528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:24.524031      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:25.524331      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:26.524587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:27.586852      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:28.587570      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:29.587951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:30.588223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:31.589345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 16:48:31.911
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:48:31.919
  Apr 23 16:48:31.942: INFO: DNS probes using dns-test-0e8e4d99-4f2c-49b1-b3e2-3fbe5eb99138 succeeded

  STEP: changing the externalName to bar.example.com @ 04/23/23 16:48:31.943
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9102.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:48:31.971
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9102.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:48:31.971
  STEP: creating a second pod to probe DNS @ 04/23/23 16:48:31.971
  STEP: submitting the pod to kubernetes @ 04/23/23 16:48:31.972
  E0423 16:48:32.590462      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:33.590911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:34.591371      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:35.592398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:36.592936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:37.593655      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 16:48:38.032
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:48:38.039
  Apr 23 16:48:38.055: INFO: File wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:38.063: INFO: File jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:38.063: INFO: Lookups using dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 failed for: [wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local]

  E0423 16:48:38.602053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:39.602259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:40.602546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:41.602964      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:42.603674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:48:43.082: INFO: File wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:43.091: INFO: File jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:43.091: INFO: Lookups using dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 failed for: [wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local]

  E0423 16:48:43.603759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:44.604366      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:45.604519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:46.605353      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:47.606095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:48:48.078: INFO: File wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:48.086: INFO: File jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:48.086: INFO: Lookups using dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 failed for: [wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local]

  E0423 16:48:48.605958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:49.606550      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:50.606315      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:51.606530      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:52.607248      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:48:53.079: INFO: File wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:53.089: INFO: File jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:53.089: INFO: Lookups using dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 failed for: [wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local]

  E0423 16:48:53.608702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:54.608591      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:55.608775      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:56.609048      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:57.609707      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:48:58.074: INFO: File wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:58.085: INFO: File jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local from pod  dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 23 16:48:58.085: INFO: Lookups using dns-9102/dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 failed for: [wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local]

  E0423 16:48:58.609956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:48:59.610048      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:00.611207      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:01.612114      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:02.612460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:03.080: INFO: DNS probes using dns-test-cd507349-7c0f-4945-baf5-762b56417bb1 succeeded

  STEP: changing the service to type=ClusterIP @ 04/23/23 16:49:03.08
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9102.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9102.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:49:03.12
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9102.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9102.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:49:03.12
  STEP: creating a third pod to probe DNS @ 04/23/23 16:49:03.12
  STEP: submitting the pod to kubernetes @ 04/23/23 16:49:03.133
  E0423 16:49:03.612806      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:04.613064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:05.613699      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:06.613807      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:07.614322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:08.614442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 16:49:09.271
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:49:09.279
  Apr 23 16:49:09.318: INFO: DNS probes using dns-test-c1eec549-51eb-4c12-9da9-ad89ad926324 succeeded

  Apr 23 16:49:09.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:49:09.337
  STEP: deleting the pod @ 04/23/23 16:49:09.369
  STEP: deleting the pod @ 04/23/23 16:49:09.409
  STEP: deleting the test externalName service @ 04/23/23 16:49:09.47
  E0423 16:49:09.615429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "dns-9102" for this suite. @ 04/23/23 16:49:09.623
• [50.042 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/23/23 16:49:09.659
  Apr 23 16:49:09.660: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 16:49:09.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:49:09.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:49:09.719
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/23/23 16:49:09.727
  Apr 23 16:49:09.752: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0423 16:49:10.615954      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:11.616485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:12.618094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:13.617734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:14.617952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:14.865: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 16:49:14.866
  E0423 16:49:15.618796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:16.619004      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:17.619418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:18.619625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:19.619999      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:20.620270      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:21.620802      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:22.620981      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:23.698853      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:24.631537      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:25.632160      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:26.632530      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 04/23/23 16:49:27.005
  STEP: updating a scale subresource @ 04/23/23 16:49:27.012
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/23/23 16:49:27.025
  STEP: Patch a scale subresource @ 04/23/23 16:49:27.045
  Apr 23 16:49:27.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3385" for this suite. @ 04/23/23 16:49:27.167
• [17.528 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/23/23 16:49:27.192
  Apr 23 16:49:27.192: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:49:27.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:49:27.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:49:27.244
  STEP: Setting up server cert @ 04/23/23 16:49:27.287
  E0423 16:49:27.632816      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:28.633515      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:49:28.791
  STEP: Deploying the webhook pod @ 04/23/23 16:49:28.802
  STEP: Wait for the deployment to be ready @ 04/23/23 16:49:28.837
  Apr 23 16:49:28.880: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 16:49:29.633643      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:30.634342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:30.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 49, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 49, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 49, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 49, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:49:31.634963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:32.636799      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 16:49:32.915
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:49:32.961
  E0423 16:49:33.636749      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:33.963: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/23/23 16:49:33.971
  STEP: create a pod that should be denied by the webhook @ 04/23/23 16:49:34.007
  STEP: create a pod that causes the webhook to hang @ 04/23/23 16:49:34.039
  E0423 16:49:34.637024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:35.637845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:36.638345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:37.639254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:38.640100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:39.640718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:40.641734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:41.642773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:42.643019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:43.644027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 04/23/23 16:49:44.056
  STEP: create a configmap that should be admitted by the webhook @ 04/23/23 16:49:44.083
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/23/23 16:49:44.111
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/23/23 16:49:44.134
  STEP: create a namespace that bypass the webhook @ 04/23/23 16:49:44.147
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/23/23 16:49:44.171
  Apr 23 16:49:44.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-324" for this suite. @ 04/23/23 16:49:44.345
  STEP: Destroying namespace "webhook-markers-9912" for this suite. @ 04/23/23 16:49:44.367
  STEP: Destroying namespace "exempted-namespace-4451" for this suite. @ 04/23/23 16:49:44.394
• [17.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/23/23 16:49:44.418
  Apr 23 16:49:44.419: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:49:44.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:49:44.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:49:44.482
  STEP: Starting the proxy @ 04/23/23 16:49:44.488
  Apr 23 16:49:44.496: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-4647 proxy --unix-socket=/tmp/kubectl-proxy-unix1361936023/test'
  E0423 16:49:44.644541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving proxy /api/ output @ 04/23/23 16:49:44.723
  Apr 23 16:49:44.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4647" for this suite. @ 04/23/23 16:49:44.739
• [0.335 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/23/23 16:49:44.756
  Apr 23 16:49:44.756: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename certificates @ 04/23/23 16:49:44.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:49:44.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:49:44.822
  E0423 16:49:45.645638      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 04/23/23 16:49:45.829
  STEP: getting /apis/certificates.k8s.io @ 04/23/23 16:49:45.838
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/23/23 16:49:45.84
  STEP: creating @ 04/23/23 16:49:45.847
  STEP: getting @ 04/23/23 16:49:45.891
  STEP: listing @ 04/23/23 16:49:45.897
  STEP: watching @ 04/23/23 16:49:45.907
  Apr 23 16:49:45.908: INFO: starting watch
  STEP: patching @ 04/23/23 16:49:45.912
  STEP: updating @ 04/23/23 16:49:45.935
  Apr 23 16:49:45.947: INFO: waiting for watch events with expected annotations
  Apr 23 16:49:45.947: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/23/23 16:49:45.947
  STEP: patching /approval @ 04/23/23 16:49:45.955
  STEP: updating /approval @ 04/23/23 16:49:45.979
  STEP: getting /status @ 04/23/23 16:49:45.991
  STEP: patching /status @ 04/23/23 16:49:46.003
  STEP: updating /status @ 04/23/23 16:49:46.02
  STEP: deleting @ 04/23/23 16:49:46.041
  STEP: deleting a collection @ 04/23/23 16:49:46.076
  Apr 23 16:49:46.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-8949" for this suite. @ 04/23/23 16:49:46.129
• [1.399 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/23/23 16:49:46.163
  Apr 23 16:49:46.163: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:49:46.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:49:46.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:49:46.201
  Apr 23 16:49:46.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-914 create -f -'
  E0423 16:49:46.645764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:47.241: INFO: stderr: ""
  Apr 23 16:49:47.242: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 23 16:49:47.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-914 create -f -'
  E0423 16:49:47.646718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:48.228: INFO: stderr: ""
  Apr 23 16:49:48.228: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/23/23 16:49:48.228
  E0423 16:49:48.646723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:49.236: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:49.236: INFO: Found 0 / 1
  E0423 16:49:49.646936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:50.245: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:50.245: INFO: Found 0 / 1
  E0423 16:49:50.647486      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:51.239: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:51.239: INFO: Found 0 / 1
  E0423 16:49:51.648003      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:52.235: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:52.236: INFO: Found 0 / 1
  E0423 16:49:52.648026      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:53.235: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:53.235: INFO: Found 0 / 1
  E0423 16:49:53.648163      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:54.309: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:54.309: INFO: Found 0 / 1
  E0423 16:49:54.648335      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:55.235: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:55.236: INFO: Found 0 / 1
  E0423 16:49:55.648899      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:56.238: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:56.239: INFO: Found 1 / 1
  Apr 23 16:49:56.239: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 23 16:49:56.246: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:49:56.246: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 23 16:49:56.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-914 describe pod agnhost-primary-hh6c9'
  Apr 23 16:49:56.512: INFO: stderr: ""
  Apr 23 16:49:56.512: INFO: stdout: "Name:             agnhost-primary-hh6c9\nNamespace:        kubectl-914\nPriority:         0\nService Account:  default\nNode:             soodi4ja4shi-3/192.168.121.96\nStart Time:       Sun, 23 Apr 2023 16:49:47 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.103\nIPs:\n  IP:           10.233.66.103\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://cd1f1b39c327e0fedae7ff95ce8c64f9e39a1c5302beff78a272e025e4ed0441\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 23 Apr 2023 16:49:54 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5j69g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5j69g:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  9s    default-scheduler  Successfully assigned kubectl-914/agnhost-primary-hh6c9 to soodi4ja4shi-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Apr 23 16:49:56.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-914 describe rc agnhost-primary'
  E0423 16:49:56.648687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:49:56.820: INFO: stderr: ""
  Apr 23 16:49:56.820: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-914\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  9s    replication-controller  Created pod: agnhost-primary-hh6c9\n"
  Apr 23 16:49:56.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-914 describe service agnhost-primary'
  Apr 23 16:49:57.027: INFO: stderr: ""
  Apr 23 16:49:57.027: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-914\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.24.26\nIPs:               10.233.24.26\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.103:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 23 16:49:57.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-914 describe node soodi4ja4shi-1'
  Apr 23 16:49:57.318: INFO: stderr: ""
  Apr 23 16:49:57.319: INFO: stdout: "Name:               soodi4ja4shi-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=soodi4ja4shi-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 23 Apr 2023 14:55:01 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  soodi4ja4shi-1\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 23 Apr 2023 16:49:55 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 23 Apr 2023 14:59:53 +0000   Sun, 23 Apr 2023 14:59:53 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Sun, 23 Apr 2023 16:44:57 +0000   Sun, 23 Apr 2023 14:54:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 23 Apr 2023 16:44:57 +0000   Sun, 23 Apr 2023 14:54:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 23 Apr 2023 16:44:57 +0000   Sun, 23 Apr 2023 14:54:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 23 Apr 2023 16:44:57 +0000   Sun, 23 Apr 2023 14:56:30 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.241\n  Hostname:    soodi4ja4shi-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  122749536Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8140196Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  119410748528\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3290532Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 7c03b8aa119c4fb4967a08240d54a7ec\n  System UUID:                7c03b8aa-119c-4fb4-967a-08240d54a7ec\n  Boot ID:                    372ae64d-58a7-4825-a402-b6f2f124894f\n  Kernel Version:             5.19.0-40-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.0\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-8bln9                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         111m\n  kube-system                 cilium-node-init-7rprd                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         111m\n  kube-system                 coredns-5d78c9869d-rztdc                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     107m\n  kube-system                 kube-addon-manager-soodi4ja4shi-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         111m\n  kube-system                 kube-apiserver-soodi4ja4shi-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 kube-controller-manager-soodi4ja4shi-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 kube-proxy-lgs9b                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 kube-scheduler-soodi4ja4shi-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         114m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-gg8kg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                855m (53%)  0 (0%)\n  memory             320Mi (9%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  Apr 23 16:49:57.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-914 describe namespace kubectl-914'
  Apr 23 16:49:57.568: INFO: stderr: ""
  Apr 23 16:49:57.568: INFO: stdout: "Name:         kubectl-914\nLabels:       e2e-framework=kubectl\n              e2e-run=88793e81-66ba-458e-8c2f-bc082b701f21\n              kubernetes.io/metadata.name=kubectl-914\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 23 16:49:57.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-914" for this suite. @ 04/23/23 16:49:57.576
• [11.435 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/23/23 16:49:57.601
  Apr 23 16:49:57.601: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 16:49:57.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:49:57.645
  E0423 16:49:57.648679      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:49:57.651
  STEP: Creating pod liveness-7d74a283-fbb1-4eaa-b585-7cb5953f0b05 in namespace container-probe-7080 @ 04/23/23 16:49:57.656
  E0423 16:49:58.649322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:49:59.649967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:00.649860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:01.651226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:02.651746      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:03.654893      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:03.804: INFO: Started pod liveness-7d74a283-fbb1-4eaa-b585-7cb5953f0b05 in namespace container-probe-7080
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 16:50:03.812
  Apr 23 16:50:03.822: INFO: Initial restart count of pod liveness-7d74a283-fbb1-4eaa-b585-7cb5953f0b05 is 0
  E0423 16:50:04.654992      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:05.656622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:06.656538      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:07.657357      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:08.657660      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:09.657725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:10.658498      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:11.658982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:12.659473      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:13.659842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:14.661084      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:15.661116      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:16.662189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:17.662974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:18.663335      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:19.663786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:20.663978      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:21.664313      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:22.664751      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:23.665549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:23.938: INFO: Restart count of pod container-probe-7080/liveness-7d74a283-fbb1-4eaa-b585-7cb5953f0b05 is now 1 (20.115338448s elapsed)
  Apr 23 16:50:23.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:50:23.959
  STEP: Destroying namespace "container-probe-7080" for this suite. @ 04/23/23 16:50:24
• [26.422 seconds]
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/23/23 16:50:24.024
  Apr 23 16:50:24.024: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 16:50:24.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:50:24.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:50:24.091
  STEP: apply creating a deployment @ 04/23/23 16:50:24.097
  Apr 23 16:50:24.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4035" for this suite. @ 04/23/23 16:50:24.138
• [0.140 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/23/23 16:50:24.165
  Apr 23 16:50:24.165: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 16:50:24.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:50:24.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:50:24.221
  STEP: Given a ReplicationController is created @ 04/23/23 16:50:24.228
  STEP: When the matched label of one of its pods change @ 04/23/23 16:50:24.239
  Apr 23 16:50:24.244: INFO: Pod name pod-release: Found 0 pods out of 1
  E0423 16:50:24.666115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:25.666487      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:26.667132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:27.668135      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:28.668566      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:29.259: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/23/23 16:50:29.281
  Apr 23 16:50:29.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8776" for this suite. @ 04/23/23 16:50:29.357
• [5.236 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/23/23 16:50:29.403
  Apr 23 16:50:29.403: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:50:29.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:50:29.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:50:29.485
  STEP: Setting up server cert @ 04/23/23 16:50:29.57
  E0423 16:50:29.669112      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:50:30.489
  STEP: Deploying the webhook pod @ 04/23/23 16:50:30.498
  STEP: Wait for the deployment to be ready @ 04/23/23 16:50:30.522
  Apr 23 16:50:30.542: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0423 16:50:30.670164      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:31.670920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:32.564: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 50, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 50, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 50, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 50, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:50:32.671370      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:33.671520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:34.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 50, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 50, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 50, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 50, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:50:34.674095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:35.674257      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 16:50:36.571
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:50:36.61
  E0423 16:50:36.674414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:37.611: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/23/23 16:50:37.623
  E0423 16:50:37.675749      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be updated by the webhook @ 04/23/23 16:50:37.676
  Apr 23 16:50:37.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3977" for this suite. @ 04/23/23 16:50:37.878
  STEP: Destroying namespace "webhook-markers-8370" for this suite. @ 04/23/23 16:50:37.903
• [8.524 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/23/23 16:50:37.932
  Apr 23 16:50:37.932: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/23/23 16:50:37.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:50:37.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:50:38.004
  STEP: create the container to handle the HTTPGet hook request. @ 04/23/23 16:50:38.028
  E0423 16:50:38.676480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:39.678150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:40.677740      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:41.677628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/23/23 16:50:42.1
  E0423 16:50:42.677944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:43.743242      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:44.744248      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:45.744335      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/23/23 16:50:46.161
  STEP: delete the pod with lifecycle hook @ 04/23/23 16:50:46.211
  E0423 16:50:46.744608      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:47.745525      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:48.746557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:49.746951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:50.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9287" for this suite. @ 04/23/23 16:50:50.276
• [12.374 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/23/23 16:50:50.309
  Apr 23 16:50:50.309: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:50:50.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:50:50.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:50:50.363
  STEP: creating a ConfigMap @ 04/23/23 16:50:50.368
  STEP: fetching the ConfigMap @ 04/23/23 16:50:50.376
  STEP: patching the ConfigMap @ 04/23/23 16:50:50.386
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/23/23 16:50:50.403
  STEP: deleting the ConfigMap by collection with a label selector @ 04/23/23 16:50:50.432
  STEP: listing all ConfigMaps in test namespace @ 04/23/23 16:50:50.475
  Apr 23 16:50:50.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2045" for this suite. @ 04/23/23 16:50:50.515
• [0.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/23/23 16:50:50.56
  Apr 23 16:50:50.561: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-webhook @ 04/23/23 16:50:50.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:50:50.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:50:50.606
  STEP: Setting up server cert @ 04/23/23 16:50:50.612
  E0423 16:50:50.747892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/23/23 16:50:51.417
  STEP: Deploying the custom resource conversion webhook pod @ 04/23/23 16:50:51.432
  STEP: Wait for the deployment to be ready @ 04/23/23 16:50:51.457
  Apr 23 16:50:51.489: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0423 16:50:51.748600      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:52.748662      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:53.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 50, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 50, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 50, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 50, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:50:53.749799      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:54.753386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:55.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 50, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 50, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 50, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 50, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:50:55.754343      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:56.754579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 16:50:57.554
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:50:57.573
  E0423 16:50:57.755156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:50:58.573: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 23 16:50:58.603: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:50:58.755798      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:50:59.756804      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:00.757508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/23/23 16:51:01.429
  STEP: Create a v2 custom resource @ 04/23/23 16:51:01.496
  E0423 16:51:01.757362      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: List CRs in v1 @ 04/23/23 16:51:01.803
  STEP: List CRs in v2 @ 04/23/23 16:51:01.817
  Apr 23 16:51:01.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-7713" for this suite. @ 04/23/23 16:51:02.516
• [12.021 seconds]
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/23/23 16:51:02.583
  Apr 23 16:51:02.583: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 16:51:02.586
  E0423 16:51:02.757568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:51:02.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:51:02.852
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/23/23 16:51:02.869
  E0423 16:51:03.758481      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:04.758901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:05.759441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:06.759313      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:07.759493      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:08.759591      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:51:08.99
  Apr 23 16:51:08.997: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-ea8c13cb-fd33-483c-8a1f-1ab5a69e287f container test-container: <nil>
  STEP: delete the pod @ 04/23/23 16:51:09.114
  Apr 23 16:51:09.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9045" for this suite. @ 04/23/23 16:51:09.181
• [6.617 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/23/23 16:51:09.201
  Apr 23 16:51:09.201: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:51:09.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:51:09.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:51:09.26
  STEP: Creating configMap with name configmap-test-volume-0789f70c-88be-4e5d-a081-4f0c0cfcb899 @ 04/23/23 16:51:09.274
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:51:09.282
  E0423 16:51:09.759852      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:10.760345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:11.761558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:12.762794      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:13.763622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:14.764230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:15.764535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:16.765212      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:51:17.427
  Apr 23 16:51:17.440: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-e10af78f-2dfd-428b-bd2f-911f6d15f377 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:51:17.46
  Apr 23 16:51:17.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5148" for this suite. @ 04/23/23 16:51:17.554
• [8.377 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/23/23 16:51:17.584
  Apr 23 16:51:17.584: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:51:17.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:51:17.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:51:17.63
  STEP: Setting up server cert @ 04/23/23 16:51:17.674
  E0423 16:51:17.766205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:51:18.605
  STEP: Deploying the webhook pod @ 04/23/23 16:51:18.625
  STEP: Wait for the deployment to be ready @ 04/23/23 16:51:18.678
  E0423 16:51:18.766963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:51:18.809: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 16:51:19.767156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:20.767394      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:51:20.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 51, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:51:21.767589      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:22.774144      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:51:22.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 16, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 16, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 16, 51, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 16:51:23.774587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:24.774620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 16:51:24.935
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:51:24.969
  E0423 16:51:25.774991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:51:25.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/23/23 16:51:25.978
  STEP: create a pod @ 04/23/23 16:51:26.019
  E0423 16:51:26.775994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:27.777177      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:28.777723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:29.777855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/23/23 16:51:30.057
  Apr 23 16:51:30.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=webhook-446 attach --namespace=webhook-446 to-be-attached-pod -i -c=container1'
  Apr 23 16:51:30.333: INFO: rc: 1
  Apr 23 16:51:30.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-446" for this suite. @ 04/23/23 16:51:30.52
  STEP: Destroying namespace "webhook-markers-9405" for this suite. @ 04/23/23 16:51:30.56
• [13.010 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/23/23 16:51:30.604
  Apr 23 16:51:30.604: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename dns @ 04/23/23 16:51:30.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:51:30.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:51:30.728
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/23/23 16:51:30.76
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/23/23 16:51:30.76
  STEP: creating a pod to probe DNS @ 04/23/23 16:51:30.76
  STEP: submitting the pod to kubernetes @ 04/23/23 16:51:30.761
  E0423 16:51:30.782404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:31.782436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:32.783028      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:33.783212      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:34.784289      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:35.783837      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:36.784076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:37.784732      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:38.786354      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 16:51:38.91
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:51:38.925
  Apr 23 16:51:38.978: INFO: DNS probes using dns-5976/dns-test-8e8f6a59-0eae-4336-a2e3-60fb2fd56f83 succeeded

  Apr 23 16:51:38.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:51:38.991
  STEP: Destroying namespace "dns-5976" for this suite. @ 04/23/23 16:51:39.028
• [8.455 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/23/23 16:51:39.071
  Apr 23 16:51:39.072: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/23/23 16:51:39.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:51:39.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:51:39.15
  E0423 16:51:39.785616      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:40.786099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:41.785786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:42.786905      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:43.787448      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:44.788060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:45.788348      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:46.788953      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:47.789585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:48.789604      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:49.790710      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:50.790972      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:51.792205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:52.793122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:53.793137      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:54.793883      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:51:55.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/23/23 16:51:55.346
  STEP: Cleaning up the configmap @ 04/23/23 16:51:55.36
  STEP: Cleaning up the pod @ 04/23/23 16:51:55.374
  STEP: Destroying namespace "emptydir-wrapper-7528" for this suite. @ 04/23/23 16:51:55.405
• [16.353 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/23/23 16:51:55.435
  Apr 23 16:51:55.435: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename prestop @ 04/23/23 16:51:55.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:51:55.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:51:55.488
  STEP: Creating server pod server in namespace prestop-2161 @ 04/23/23 16:51:55.497
  STEP: Waiting for pods to come up. @ 04/23/23 16:51:55.52
  E0423 16:51:55.795080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:56.795976      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:57.797389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:58.798016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:51:59.799057      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:00.800188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-2161 @ 04/23/23 16:52:01.595
  E0423 16:52:01.801023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:02.801447      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:03.802729      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:04.803062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:05.803080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:06.803538      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:07.804611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:08.805416      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:09.806153      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:10.806906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 04/23/23 16:52:11.717
  E0423 16:52:11.807037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:12.807946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:13.808374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:14.808578      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:15.808831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:52:16.751: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 23 16:52:16.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/23/23 16:52:16.771
  STEP: Destroying namespace "prestop-2161" for this suite. @ 04/23/23 16:52:16.8
  E0423 16:52:16.809948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [21.425 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/23/23 16:52:16.861
  Apr 23 16:52:16.861: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:52:16.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:16.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:16.984
  STEP: Creating projection with secret that has name projected-secret-test-map-8c7b5126-3d82-4c54-8efe-899d75f2391d @ 04/23/23 16:52:16.991
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:52:17.019
  E0423 16:52:17.811184      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:18.811374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:19.811459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:20.813202      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:21.813793      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:22.814995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:23.818944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:24.820833      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:25.831212      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:26.831252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:52:27.222
  Apr 23 16:52:27.230: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-secrets-0cbc95ad-98ed-4475-865c-4f508b35009d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:52:27.272
  Apr 23 16:52:27.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9266" for this suite. @ 04/23/23 16:52:27.329
• [10.483 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/23/23 16:52:27.366
  Apr 23 16:52:27.366: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:52:27.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:27.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:27.438
  STEP: creating service in namespace services-8701 @ 04/23/23 16:52:27.447
  STEP: creating service affinity-clusterip in namespace services-8701 @ 04/23/23 16:52:27.447
  STEP: creating replication controller affinity-clusterip in namespace services-8701 @ 04/23/23 16:52:27.472
  I0423 16:52:27.499863      15 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-8701, replica count: 3
  E0423 16:52:27.831867      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:28.832063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:29.832870      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:52:30.553841      15 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:52:30.833566      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:31.833762      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:32.833880      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:52:33.554999      15 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:52:33.834789      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:34.835016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:35.835894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:52:36.555273      15 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:52:36.837083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:37.838288      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:38.839137      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:52:39.558781      15 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:52:39.839928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:40.840395      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:41.840518      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:52:42.559890      15 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:52:42.577: INFO: Creating new exec pod
  E0423 16:52:42.841632      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:43.842270      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:44.843261      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:45.844172      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:46.844764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:47.851793      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:48.852397      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:49.852541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:50.853484      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:52:51.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8701 exec execpod-affinity96ld6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  E0423 16:52:51.855107      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:52:52.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 23 16:52:52.238: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:52:52.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8701 exec execpod-affinity96ld6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.46.133 80'
  Apr 23 16:52:52.813: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.46.133 80\nConnection to 10.233.46.133 80 port [tcp/http] succeeded!\n"
  Apr 23 16:52:52.813: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:52:52.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8701 exec execpod-affinity96ld6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.46.133:80/ ; done'
  E0423 16:52:52.855675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:52:53.587: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.133:80/\n"
  Apr 23 16:52:53.587: INFO: stdout: "\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j\naffinity-clusterip-zrw9j"
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Received response from host: affinity-clusterip-zrw9j
  Apr 23 16:52:53.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:52:53.602: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-8701, will wait for the garbage collector to delete the pods @ 04/23/23 16:52:53.649
  Apr 23 16:52:53.741: INFO: Deleting ReplicationController affinity-clusterip took: 27.426368ms
  Apr 23 16:52:53.842: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.955397ms
  E0423 16:52:53.855783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:54.856181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:55.857584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:56.858526      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:57.859030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:58.859890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:52:59.860637      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:00.861340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:01.861652      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:02.862200      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:03.862922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:04.863459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:05.864477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:06.865061      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:07.865188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:08.865663      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:09.865855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:10.866678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:11.867546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8701" for this suite. @ 04/23/23 16:53:12.111
• [44.774 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/23/23 16:53:12.141
  Apr 23 16:53:12.141: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 16:53:12.145
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:53:12.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:53:12.211
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/23/23 16:53:12.222
  E0423 16:53:12.868720      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:13.869101      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:14.869148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:15.869779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:16.870598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:17.871337      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:18.871266      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:19.871396      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:53:20.325
  Apr 23 16:53:20.334: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-cb8fd4e1-6931-4eb2-bef1-327c49c691ca container test-container: <nil>
  STEP: delete the pod @ 04/23/23 16:53:20.378
  Apr 23 16:53:20.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2854" for this suite. @ 04/23/23 16:53:20.453
• [8.369 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/23/23 16:53:20.518
  Apr 23 16:53:20.518: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:53:20.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:53:20.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:53:20.572
  STEP: Creating configMap with name configmap-test-volume-66ebf580-0d01-4b76-a3c4-f44fad985fb5 @ 04/23/23 16:53:20.587
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:53:20.6
  E0423 16:53:20.871884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:21.872309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:22.872622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:23.873181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:24.873337      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:25.874105      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:26.874537      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:27.875487      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:28.876465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:29.877747      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:30.878432      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:31.878931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:32.879345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:33.880360      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 16:53:34.818
  Apr 23 16:53:34.858: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-configmaps-a36219f6-0830-403e-8991-cdc727499029 container configmap-volume-test: <nil>
  E0423 16:53:34.880628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/23/23 16:53:35.301
  Apr 23 16:53:35.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7782" for this suite. @ 04/23/23 16:53:35.348
• [14.843 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/23/23 16:53:35.368
  Apr 23 16:53:35.368: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename gc @ 04/23/23 16:53:35.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:53:35.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:53:35.413
  STEP: create the rc @ 04/23/23 16:53:35.43
  W0423 16:53:35.454085      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0423 16:53:35.881498      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:36.881735      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:37.882509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:38.882427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:39.882934      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/23/23 16:53:40.47
  STEP: wait for all pods to be garbage collected @ 04/23/23 16:53:40.485
  E0423 16:53:40.884009      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:41.886899      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:42.884749      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:43.884907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:44.885087      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/23/23 16:53:45.544
  Apr 23 16:53:45.838: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 16:53:45.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-549" for this suite. @ 04/23/23 16:53:45.858
• [10.503 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/23/23 16:53:45.88
  Apr 23 16:53:45.880: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 16:53:45.885
  E0423 16:53:45.885567      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:53:45.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:53:45.926
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/23/23 16:53:45.933
  Apr 23 16:53:45.935: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:53:46.885962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:47.886471      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:48.887000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:49.888272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:50.889595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:51.891064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:52.893206      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:53.900226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:54.899838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:55.901080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:56.900409      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/23/23 16:53:57.528
  Apr 23 16:53:57.537: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:53:57.900793      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:53:58.900921      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:53:59.729: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 16:53:59.902019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:00.902595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:01.903187      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:02.913947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:03.914124      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:04.914347      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:05.927183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:06.927948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:07.927975      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:08.928121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:54:09.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4225" for this suite. @ 04/23/23 16:54:09.118
• [23.265 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/23/23 16:54:09.152
  Apr 23 16:54:09.152: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename init-container @ 04/23/23 16:54:09.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:09.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:09.244
  STEP: creating the pod @ 04/23/23 16:54:09.252
  Apr 23 16:54:09.253: INFO: PodSpec: initContainers in spec.initContainers
  E0423 16:54:09.928696      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:10.929389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:11.929801      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:12.929747      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:13.930178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:14.930002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:15.930155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:16.930822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:17.930948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:18.931108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:19.931726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:20.932074      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:21.932150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:22.932814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:23.932967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:24.933206      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:25.933836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:26.933974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:27.935058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:28.935914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:29.936121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:30.937254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:31.937435      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:32.938467      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:33.982845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:34.983414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:35.984898      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:36.985217      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:37.985625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:38.985781      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:54:39.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4141" for this suite. @ 04/23/23 16:54:39.492
• [30.364 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/23/23 16:54:39.518
  Apr 23 16:54:39.518: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 16:54:39.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:39.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:39.572
  E0423 16:54:39.986354      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:40.987249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:41.987717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:42.987870      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:43.988383      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:44.989764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:45.990669      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:46.990858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:47.991803      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:48.991846      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:49.992320      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:50.992774      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:51.993271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:52.993662      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:53.994779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:54.995170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:55.995298      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:56.996119      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:57.997831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:58.998234      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:54:59.999029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:00.999800      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:02.000573      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:03.000941      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:04.001031      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:05.002179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:06.002334      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:07.003038      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:08.003443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:09.004251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:10.004328      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:11.004556      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:12.005523      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:13.005521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:14.006585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:15.006848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:16.007437      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:17.008271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:18.008889      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:19.010132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:20.010994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:21.011397      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:22.012044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:23.012247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:24.012292      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:25.012458      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:26.012606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:27.013140      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:28.013391      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:29.013340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:30.013510      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:31.013956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:32.014215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:33.014875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:34.015532      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:35.016067      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:36.016310      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:37.017099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:38.018077      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:39.019075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:55:39.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6371" for this suite. @ 04/23/23 16:55:39.626
• [60.127 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/23/23 16:55:39.647
  Apr 23 16:55:39.647: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 16:55:39.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:39.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:39.69
  Apr 23 16:55:39.694: INFO: Creating ReplicaSet my-hostname-basic-5972ae98-5504-4673-8ac3-d5cb30ec630e
  Apr 23 16:55:39.714: INFO: Pod name my-hostname-basic-5972ae98-5504-4673-8ac3-d5cb30ec630e: Found 0 pods out of 1
  E0423 16:55:40.019673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:41.020511      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:42.021048      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:43.021027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:44.022333      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:55:44.730: INFO: Pod name my-hostname-basic-5972ae98-5504-4673-8ac3-d5cb30ec630e: Found 1 pods out of 1
  Apr 23 16:55:44.730: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5972ae98-5504-4673-8ac3-d5cb30ec630e" is running
  Apr 23 16:55:44.741: INFO: Pod "my-hostname-basic-5972ae98-5504-4673-8ac3-d5cb30ec630e-r22gt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 16:55:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 16:55:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 16:55:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 16:55:39 +0000 UTC Reason: Message:}])
  Apr 23 16:55:44.741: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/23/23 16:55:44.742
  Apr 23 16:55:44.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7387" for this suite. @ 04/23/23 16:55:44.815
• [5.197 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/23/23 16:55:44.844
  Apr 23 16:55:44.844: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 16:55:44.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:44.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:44.929
  STEP: Updating Namespace "namespaces-9134" @ 04/23/23 16:55:44.936
  Apr 23 16:55:44.963: INFO: Namespace "namespaces-9134" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"88793e81-66ba-458e-8c2f-bc082b701f21", "kubernetes.io/metadata.name":"namespaces-9134", "namespaces-9134":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 23 16:55:44.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9134" for this suite. @ 04/23/23 16:55:44.976
• [0.156 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/23/23 16:55:45.002
  Apr 23 16:55:45.002: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:55:45.004
  E0423 16:55:45.022106      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:45.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:45.072
  STEP: Creating a ResourceQuota @ 04/23/23 16:55:45.08
  STEP: Getting a ResourceQuota @ 04/23/23 16:55:45.109
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/23/23 16:55:45.128
  STEP: Patching the ResourceQuota @ 04/23/23 16:55:45.138
  STEP: Deleting a Collection of ResourceQuotas @ 04/23/23 16:55:45.172
  STEP: Verifying the deleted ResourceQuota @ 04/23/23 16:55:45.227
  Apr 23 16:55:45.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-261" for this suite. @ 04/23/23 16:55:45.253
• [0.276 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/23/23 16:55:45.279
  Apr 23 16:55:45.279: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:55:45.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:45.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:45.368
  STEP: creating service in namespace services-5100 @ 04/23/23 16:55:45.375
  STEP: creating service affinity-clusterip-transition in namespace services-5100 @ 04/23/23 16:55:45.376
  STEP: creating replication controller affinity-clusterip-transition in namespace services-5100 @ 04/23/23 16:55:45.414
  I0423 16:55:45.484227      15 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-5100, replica count: 3
  E0423 16:55:46.023082      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:47.023030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:48.023281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:55:48.536222      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:55:49.024259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:50.024816      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:51.025582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:55:51.536987      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:55:52.025284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:53.026415      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:54.026147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:55:54.537773      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:55:55.026334      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:56.027349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:57.027480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:55:57.538173      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:55:58.028088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:55:59.028627      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:00.029312      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:56:00.539347      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:56:01.030244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:02.030390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:03.031367      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:56:03.539735      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:56:04.031540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:05.031687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:06.031829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:56:06.540208      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:56:07.032026      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:08.038564      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:09.039959      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:56:09.541039      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:56:10.039406      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:11.039672      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:12.039752      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:56:12.542278      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:56:12.571: INFO: Creating new exec pod
  E0423 16:56:13.040900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:14.041179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:15.041117      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:16.041569      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:17.042545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:18.043723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:19.044772      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:20.045726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:21.046100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:22.046847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:23.047569      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:24.048389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:25.048495      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:26.050875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:27.051420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:28.052048      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:29.052541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:56:29.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-5100 exec execpod-affinitylmg9x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  E0423 16:56:30.060436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:56:30.281: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 23 16:56:30.281: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:56:30.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-5100 exec execpod-affinitylmg9x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.6.205 80'
  Apr 23 16:56:30.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.6.205 80\nConnection to 10.233.6.205 80 port [tcp/http] succeeded!\n"
  Apr 23 16:56:30.758: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:56:30.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-5100 exec execpod-affinitylmg9x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.6.205:80/ ; done'
  E0423 16:56:31.060913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:32.061115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:56:32.092: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n"
  Apr 23 16:56:32.092: INFO: stdout: "\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-wwht8\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-jp7xh\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-jp7xh\naffinity-clusterip-transition-jp7xh\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-wwht8\naffinity-clusterip-transition-jp7xh\naffinity-clusterip-transition-wwht8\naffinity-clusterip-transition-jp7xh\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q"
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-wwht8
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-jp7xh
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-jp7xh
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-jp7xh
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-wwht8
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-jp7xh
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-wwht8
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-jp7xh
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:32.092: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:32.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-5100 exec execpod-affinitylmg9x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.6.205:80/ ; done'
  E0423 16:56:33.061062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:56:33.089: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.6.205:80/\n"
  Apr 23 16:56:33.090: INFO: stdout: "\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q\naffinity-clusterip-transition-ghd4q"
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Received response from host: affinity-clusterip-transition-ghd4q
  Apr 23 16:56:33.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:56:33.102: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5100, will wait for the garbage collector to delete the pods @ 04/23/23 16:56:33.125
  Apr 23 16:56:33.212: INFO: Deleting ReplicationController affinity-clusterip-transition took: 18.478606ms
  Apr 23 16:56:33.313: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.207722ms
  E0423 16:56:34.061417      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:35.062148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:36.062294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:37.062751      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:38.063405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:39.064162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:40.064389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:41.065422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:42.065553      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:43.066267      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:44.066603      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:45.067498      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:46.068586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:47.069271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:48.069356      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:49.070025      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:50.071714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:51.071763      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:52.072276      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:53.073336      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:54.074032      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:55.074245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:56.074522      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:57.075012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:58.075490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:56:59.075669      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5100" for this suite. @ 04/23/23 16:56:59.68
• [74.415 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/23/23 16:56:59.695
  Apr 23 16:56:59.695: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename services @ 04/23/23 16:56:59.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:56:59.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:56:59.76
  STEP: creating service in namespace services-8552 @ 04/23/23 16:56:59.774
  STEP: creating service affinity-nodeport in namespace services-8552 @ 04/23/23 16:56:59.774
  STEP: creating replication controller affinity-nodeport in namespace services-8552 @ 04/23/23 16:56:59.811
  I0423 16:56:59.839855      15 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-8552, replica count: 3
  E0423 16:57:00.075933      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:01.076103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:02.076284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:57:02.891788      15 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:57:03.076377      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:04.076508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:05.076676      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:57:05.893160      15 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 16:57:06.077024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:07.077469      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:08.078740      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 16:57:08.893688      15 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:57:08.919: INFO: Creating new exec pod
  E0423 16:57:09.079099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:10.080065      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:11.080734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:12.081092      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:13.081727      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:14.081956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:15.082782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:16.082897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:17.083470      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:18.085006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:19.085139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:20.086939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:21.087678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:22.088441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:23.089002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:24.089151      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:25.089195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:26.089979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:27.090822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:28.091785      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:29.092829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:30.093979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:31.093965      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:57:32.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8552 exec execpod-affinity9fs9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E0423 16:57:32.094706      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:57:32.738: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 23 16:57:32.738: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:57:32.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8552 exec execpod-affinity9fs9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.11.95 80'
  E0423 16:57:33.095355      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:57:33.561: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.11.95 80\nConnection to 10.233.11.95 80 port [tcp/http] succeeded!\n"
  Apr 23 16:57:33.561: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:57:33.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8552 exec execpod-affinity9fs9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.241 30697'
  Apr 23 16:57:34.061: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.241 30697\nConnection to 192.168.121.241 30697 port [tcp/*] succeeded!\n"
  Apr 23 16:57:34.061: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:57:34.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8552 exec execpod-affinity9fs9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.96 30697'
  E0423 16:57:34.096621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:57:34.464: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.96 30697\nConnection to 192.168.121.96 30697 port [tcp/*] succeeded!\n"
  Apr 23 16:57:34.464: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 16:57:34.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=services-8552 exec execpod-affinity9fs9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.241:30697/ ; done'
  E0423 16:57:35.097030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 16:57:35.434: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.241:30697/\n"
  Apr 23 16:57:35.434: INFO: stdout: "\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws\naffinity-nodeport-k5pws"
  Apr 23 16:57:35.434: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.434: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.434: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.434: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.434: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.435: INFO: Received response from host: affinity-nodeport-k5pws
  Apr 23 16:57:35.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:57:35.453: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-8552, will wait for the garbage collector to delete the pods @ 04/23/23 16:57:35.48
  Apr 23 16:57:35.564: INFO: Deleting ReplicationController affinity-nodeport took: 25.298478ms
  Apr 23 16:57:35.664: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.449329ms
  E0423 16:57:36.097817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:37.097801      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:38.098258      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:39.098931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:40.098928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:41.099829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:42.100533      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:43.101289      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:44.102409      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:45.102458      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:46.103416      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:47.103442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:48.104475      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:49.105019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:50.105599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:51.106684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:52.107109      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:53.107116      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:54.107987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:55.108205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:56.109034      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:57.109713      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:58.109720      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:57:59.110325      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:00.111368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:01.112298      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:02.113408      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:03.113479      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:04.114332      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:05.114601      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:06.114987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:07.116011      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:08.116715      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:09.118695      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:10.118841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:11.118949      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:12.119373      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:13.120167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:14.120290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:15.120662      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:16.121009      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:17.121662      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:18.122022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:19.122350      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:20.123030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:21.124019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:22.124869      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:23.125889      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:24.126379      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:25.126833      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:26.127215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:27.128291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:28.129817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:29.130604      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:30.131307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:31.131967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:32.132960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:33.133884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:34.135225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:35.136322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:36.136298      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:37.136991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:38.137828      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:39.138456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:40.138810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:41.138766      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:42.139330      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:43.139410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:44.140159      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:45.140725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:46.141160      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:47.141434      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:48.141422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:49.141700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:50.142398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:51.143178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:52.143413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:53.143894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:54.144480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:55.144751      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:56.144902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:57.145565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:58.145971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:58:59.146750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:00.147598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:01.148461      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:02.149125      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:03.149413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:04.149621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:05.150938      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:06.151879      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:07.152577      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:08.152681      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:09.153407      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:10.153586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:11.154415      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:12.154929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:13.155824      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:14.156787      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:15.157502      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:16.158350      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:17.159519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:18.160143      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:19.160239      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:20.161224      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:21.162230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:22.162294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:23.162384      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:24.162924      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:25.163025      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:26.163712      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:27.164302      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:28.164747      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:29.165526      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:30.165773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:31.167243      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:32.167223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:33.168097      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:34.168069      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:35.168172      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:36.169303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:37.169746      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:38.170051      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:39.170889      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:40.171402      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:41.172379      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:42.172594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:43.173528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:44.173657      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:45.174576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:46.174909      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:47.175702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:48.176744      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:49.177741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:50.178305      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:51.179443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:52.179748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:53.180649      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:54.181222      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:55.181826      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:56.182331      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:57.182402      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:58.182904      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 16:59:59.183722      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:00.184344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:01.184805      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:02.185374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:03.186066      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:04.187232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:05.188292      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:06.189376      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:07.189417      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:08.190131      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:09.190248      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:10.191725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:11.192541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:12.193487      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:13.194319      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:14.195224      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:15.195938      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:16.196017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:17.196256      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:18.196446      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:19.197281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:20.197719      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:21.198080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8552" for this suite. @ 04/23/23 17:00:21.84
• [202.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/23/23 17:00:21.86
  Apr 23 17:00:21.860: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 17:00:21.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:00:21.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:00:21.919
  STEP: set up a multi version CRD @ 04/23/23 17:00:21.929
  Apr 23 17:00:21.931: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 17:00:22.198435      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:23.199279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:24.199884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:25.206782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:26.201671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:27.201713      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:28.203021      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 04/23/23 17:00:29.135
  STEP: check the unserved version gets removed @ 04/23/23 17:00:29.197
  E0423 17:00:29.203381      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:30.203619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/23/23 17:00:30.919
  E0423 17:00:31.204223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:32.204555      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:33.204553      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:34.204694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:35.204957      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:35.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6988" for this suite. @ 04/23/23 17:00:35.653
• [13.808 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/23/23 17:00:35.671
  Apr 23 17:00:35.671: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename disruption @ 04/23/23 17:00:35.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:00:35.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:00:35.732
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:00:35.75
  E0423 17:00:36.205773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:37.205875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/23/23 17:00:37.825
  Apr 23 17:00:37.852: INFO: running pods: 0 < 3
  E0423 17:00:38.205843      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:39.206282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:39.861: INFO: running pods: 0 < 3
  E0423 17:00:40.207599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:41.208457      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:41.870: INFO: running pods: 0 < 3
  E0423 17:00:42.208512      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:43.208817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:43.862: INFO: running pods: 0 < 3
  E0423 17:00:44.209108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:45.209335      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:45.860: INFO: running pods: 0 < 3
  E0423 17:00:46.209845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:47.210717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:47.861: INFO: running pods: 0 < 3
  E0423 17:00:48.210720      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:49.210902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:49.860: INFO: running pods: 0 < 3
  E0423 17:00:50.211480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:51.212018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:51.863: INFO: running pods: 0 < 3
  E0423 17:00:52.212595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:53.213021      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:53.865: INFO: running pods: 0 < 3
  E0423 17:00:54.213253      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:55.213260      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:55.862: INFO: running pods: 0 < 3
  E0423 17:00:56.214942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:57.215544      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:00:57.867: INFO: running pods: 0 < 3
  E0423 17:00:58.216451      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:00:59.216911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:00.021: INFO: running pods: 0 < 3
  E0423 17:01:00.217723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:01.218254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:01.866: INFO: running pods: 0 < 3
  E0423 17:01:02.220010      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:03.219508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:03.865: INFO: running pods: 0 < 3
  E0423 17:01:04.219954      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:05.220166      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:05.868: INFO: running pods: 0 < 3
  E0423 17:01:06.220646      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:07.220531      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:07.862: INFO: running pods: 0 < 3
  E0423 17:01:08.220799      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:09.220875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:09.860: INFO: running pods: 0 < 3
  E0423 17:01:10.221384      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:11.221986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:11.865: INFO: running pods: 0 < 3
  E0423 17:01:12.222188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:13.223124      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:13.860: INFO: running pods: 0 < 3
  E0423 17:01:14.223225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:15.224018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:15.861: INFO: running pods: 0 < 3
  E0423 17:01:16.224438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:17.224551      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:17.864: INFO: running pods: 0 < 3
  E0423 17:01:18.224658      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:19.224761      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:19.860: INFO: running pods: 0 < 3
  E0423 17:01:20.225283      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:21.225353      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:21.862: INFO: running pods: 0 < 3
  E0423 17:01:22.226345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:23.226920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:23.864: INFO: running pods: 0 < 3
  E0423 17:01:24.227031      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:25.229038      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:25.864: INFO: running pods: 0 < 3
  E0423 17:01:26.229641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:27.229832      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:27.861: INFO: running pods: 2 < 3
  E0423 17:01:28.230405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:29.230526      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:29.863: INFO: running pods: 2 < 3
  E0423 17:01:30.230912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:31.231472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:01:31.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4408" for this suite. @ 04/23/23 17:01:31.885
• [56.233 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/23/23 17:01:31.904
  Apr 23 17:01:31.904: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 17:01:31.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:01:31.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:01:31.958
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/23/23 17:01:31.964
  E0423 17:01:32.231857      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:33.232849      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:34.233107      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:35.233617      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:36.234340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:37.234966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:38.236122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:39.239001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:40.239176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:41.240001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:42.241084      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:43.242159      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:44.242620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:45.242987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:46.244161      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:47.244653      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:48.245694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:49.245900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:50.246325      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:51.247345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:52.248523      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:53.249066      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:54.249230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:55.249337      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:56.249453      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:57.249766      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:58.249990      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:01:59.250208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:00.251292      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:01.251335      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:02.252181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:03.253064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:04.254400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:05.254459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:06.255504      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:07.256249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:08.257091      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:09.257833      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:10.258629      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:11.258965      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:12.259130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:13.259972      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:14.260131      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:15.260304      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:16.261290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:17.261415      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:18.261614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:19.262049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:20.262496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:21.263276      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:22.263960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:23.264063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:24.264635      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:25.265328      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:26.265840      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:27.266536      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:28.267490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:29.268287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:30.289538      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:31.289728      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:32.290300      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 04/23/23 17:02:32.378
  STEP: Then the orphan pod is adopted @ 04/23/23 17:02:32.401
  E0423 17:02:33.290234      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:02:33.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6191" for this suite. @ 04/23/23 17:02:33.441
• [61.562 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/23/23 17:02:33.472
  Apr 23 17:02:33.472: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 17:02:33.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:02:33.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:02:33.553
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/23/23 17:02:33.582
  E0423 17:02:34.291538      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:35.292140      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:36.296727      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:37.294546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:38.294817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:39.294920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:40.295045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:41.296155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:42.296342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:43.297300      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:44.298096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:45.298814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:46.298971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:47.300141      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:48.300842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:49.301056      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:50.301315      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:51.301374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:52.301615      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:53.301992      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:54.302204      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:55.303944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:56.304219      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:57.306093      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:58.306710      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:02:59.307678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:00.307989      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:01.308398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:02.308695      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:03.309499      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:04.309792      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:05.310787      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:06.311612      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:07.312597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:08.312976      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:09.313760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:10.316321      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:11.316071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:12.316702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:13.316808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:14.317001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:15.317962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:16.318694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:17.319552      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:18.319725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:19.320761      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:20.321368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:21.322233      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:22.322749      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:23.323466      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:24.324440      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:25.325789      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:26.325911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:27.326556      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:28.326737      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:29.327222      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:30.327676      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:31.328381      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:32.329078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:33.329923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:34.330160      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:35.330204      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:36.331093      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:37.331198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:38.331308      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:39.332313      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:40.332439      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:41.332874      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:42.333760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:43.333874      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:44.334243      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:45.337778      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:46.338360      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:47.339011      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:48.340275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:49.341238      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:50.341512      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:51.341938      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:52.342848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:53.343004      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:54.343083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:55.343614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:56.344814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:57.345367      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:58.345944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:03:59.346005      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:00.346595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:01.346908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:02.347812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:03.348458      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:04.349265      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/23/23 17:04:04.612
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/23/23 17:04:04.621
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/23/23 17:04:04.636
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/23/23 17:04:04.636
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/23/23 17:04:04.743
  E0423 17:04:05.349506      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:06.349989      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:07.350093      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:08.351674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:09.352152      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:10.352494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:11.353647      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:12.354915      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:13.355560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:14.356387      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:15.356898      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:16.357067      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:17.358038      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:18.358948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:19.359996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:20.360438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:21.361128      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:22.361306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:23.362243      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:24.362857      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:25.363140      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:26.363323      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:27.363482      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:28.363574      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:29.364007      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:30.364698      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:31.364792      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:32.365286      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:33.366037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:34.366624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:35.367511      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:36.368070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:37.368219      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:38.368822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:39.369848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:40.370573      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:41.371719      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:42.372149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:43.372733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:44.373786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:45.373993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:46.375038      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:47.375914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:48.377199      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:49.378983      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:50.378418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:51.378406      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:52.379952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:53.381043      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:54.381178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:55.381897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:56.382269      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:57.382590      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:58.383462      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:04:59.383665      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:00.384673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:01.385327      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:02.386068      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:03.387055      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:04.388435      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:05.389490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:06.389671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:07.390013      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:08.390272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:09.391205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:10.392042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:11.392776      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:12.393075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:13.393362      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:14.394327      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:15.394533      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:16.395627      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:17.395626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:18.395908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:19.396366      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:20.396850      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:21.396988      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:22.397203      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/23/23 17:05:22.627
  E0423 17:05:23.397375      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/23/23 17:05:23.711
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/23/23 17:05:23.732
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/23/23 17:05:23.732
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/23/23 17:05:23.808
  E0423 17:05:24.397910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/23/23 17:05:24.834
  E0423 17:05:25.398837      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:26.398994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:27.399037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:28.400091      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:29.400895      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:30.401879      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:31.401983      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:32.402011      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:33.403266      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:34.403374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:35.403583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:36.404303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:37.404326      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:38.405047      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:39.405909      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:40.406134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:41.406919      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:42.407419      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:43.407416      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:44.408459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:45.409570      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:46.409624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:47.410971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:48.411427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:49.411540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:50.412155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:51.413234      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:52.413559      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:53.414292      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:54.414499      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:55.414903      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:56.415196      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:57.415297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:58.416391      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:05:59.417279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:00.417288      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:01.418387      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:02.419311      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:03.420195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:04.421131      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:05.422067      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:06.422791      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:07.423694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:08.423957      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:09.424166      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:10.425275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:11.426191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:12.426962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:13.427561      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:14.427736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:15.429223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:16.429823      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:17.430066      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:18.430878      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:19.432116      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:20.433128      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:21.433683      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:22.436333      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:23.436837      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:24.437186      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:25.438200      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:26.438908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:27.439987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:28.440591      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:29.441463      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:30.442420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:31.442867      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:32.443454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:33.443944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:34.444133      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:35.445060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:36.445277      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:37.445503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:38.445649      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:39.446269      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:40.446753      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:41.446814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:42.447203      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:43.447719      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/23/23 17:06:43.657
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/23/23 17:06:43.684
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/23/23 17:06:43.684
  Apr 23 17:06:43.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3287" for this suite. @ 04/23/23 17:06:43.783
• [250.342 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/23/23 17:06:43.819
  Apr 23 17:06:43.819: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 17:06:43.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:43.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:43.868
  Apr 23 17:06:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 17:06:44.448789      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:06:44.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4101" for this suite. @ 04/23/23 17:06:44.997
• [1.201 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/23/23 17:06:45.021
  Apr 23 17:06:45.021: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 17:06:45.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:45.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:45.09
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/23/23 17:06:45.096
  Apr 23 17:06:45.099: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 17:06:45.449542      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:46.449923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:47.450418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:06:47.793: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 17:06:48.450986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:49.451338      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:50.452399      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:51.453346      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:52.453157      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:53.453846      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:54.454754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:55.455766      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:56.467775      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:06:57.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6639" for this suite. @ 04/23/23 17:06:57.433
  E0423 17:06:57.468758      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [12.492 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 04/23/23 17:06:57.519
  Apr 23 17:06:57.519: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 17:06:57.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:57.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:57.57
  STEP: Creating service test in namespace statefulset-8665 @ 04/23/23 17:06:57.575
  STEP: Creating statefulset ss in namespace statefulset-8665 @ 04/23/23 17:06:57.583
  Apr 23 17:06:57.624: INFO: Found 0 stateful pods, waiting for 1
  E0423 17:06:58.469557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:06:59.469641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:00.470466      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:01.470456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:02.471058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:03.471271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:04.471607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:05.471880      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:06.472272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:07.472573      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:07:07.635: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:07:08.472625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:09.472844      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:10.473295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:11.473675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:12.474452      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:13.474942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:14.476216      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:15.476499      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:16.476862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:17.477427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:07:17.634: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:07:18.477617      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:19.477789      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:20.478055      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:21.478101      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:22.478298      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:23.479400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:24.480131      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:25.481457      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:26.481241      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:27.481259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:07:27.641: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:07:28.482408      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:29.482136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:30.482731      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:31.482574      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:32.483239      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:33.483991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:34.484244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:35.484357      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:36.484559      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:37.484991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:07:37.632: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:07:38.485443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:39.485534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:40.486940      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:41.486903      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:42.487042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:43.488532      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:44.488781      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:45.488922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:46.489092      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:47.489227      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:07:47.632: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:07:48.489455      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:49.490006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:50.490430      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:51.490875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:52.491758      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:53.491847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:54.492738      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:55.493727      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:56.493879      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:57.494390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:07:57.634: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:07:58.495226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:07:59.495620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:00.495934      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:01.497049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:02.497839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:03.498087      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:04.498823      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:05.498702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:06.498946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:07.499136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:08:07.632: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:08:08.500033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:09.500206      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:10.500342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:11.501018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:12.501431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:13.501568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:14.501664      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:15.501813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:16.501973      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:17.502809      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:08:17.635: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:08:18.503043      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:19.503415      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:20.504280      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:21.505157      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:22.506278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:23.506522      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:24.506921      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:25.507797      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:26.511510      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:27.511594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:08:27.632: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0423 17:08:28.511778      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:29.512892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:30.512503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:31.512940      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:32.513425      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:33.513495      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:34.513571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:35.513957      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:36.515075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:37.515180      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:08:37.640: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/23/23 17:08:37.66
  STEP: updating a scale subresource @ 04/23/23 17:08:37.671
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/23/23 17:08:37.686
  STEP: Patch a scale subresource @ 04/23/23 17:08:37.695
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/23/23 17:08:37.714
  Apr 23 17:08:37.724: INFO: Deleting all statefulset in ns statefulset-8665
  Apr 23 17:08:37.741: INFO: Scaling statefulset ss to 0
  E0423 17:08:38.515409      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:39.515965      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:40.517193      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:41.518279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:42.518716      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:43.518856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:44.519043      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:45.519205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:46.519368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:47.519879      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:48.520856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:49.521068      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:50.521252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:51.521520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:52.525580      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:53.525455      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:54.525625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:55.525904      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:56.526082      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:57.527587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:58.527798      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:08:59.528356      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:00.528519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:01.529185      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:02.529929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:03.530072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:04.530180      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:05.530389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:06.531081      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:07.531222      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:08.531416      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:09.531620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:10.532409      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:11.532456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:12.533235      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:13.533594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:14.533714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:15.533912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:16.533991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:17.534208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:18.534788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:19.534930      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:20.535389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:21.536017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:22.536381      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:23.536838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:24.537990      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:25.538692      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:26.538958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:27.539398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:28.540436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:29.540897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:30.541850      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:31.541812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:32.542160      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:33.542385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:34.542714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:35.542920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:36.543049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:37.543122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:38.543984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:39.544073      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:40.544233      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:41.544480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:42.545454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:43.545776      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:44.546556      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:45.546861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:46.548027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:47.548379      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:48.548540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:49.548706      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:50.549824      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:51.550565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:52.551417      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:53.551539      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:54.551756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:55.553801      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:56.553802      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:57.554192      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:58.554554      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:09:59.554840      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:00.555003      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:01.556029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:02.556616      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:03.556721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:04.557158      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:05.557187      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:06.557191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:07.558032      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:08.560502      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:09.559471      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:10.559710      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:11.559798      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:12.560511      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:13.560614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:14.561041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:15.561146      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:16.561291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:17.561822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:18.562446      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:19.562520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:20.562932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:21.563039      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:22.563553      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:23.563627      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:24.563928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:25.564160      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:26.564183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:27.565465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:28.566494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:29.567188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:30.568065      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:31.568871      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:32.569049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:33.569414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:34.570086      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:35.570521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:36.571100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:37.571429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:38.571568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:39.571815      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:40.573528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:41.574019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:42.575378      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:43.575959      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:44.576733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:45.577625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:46.578707      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:47.579198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:48.580023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:49.580436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:50.581212      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:51.581520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:52.581950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:53.582422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:54.582951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:55.583092      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:56.584095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:57.584950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:58.585617      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:10:59.585670      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:00.586558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:01.586877      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:02.587638      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:03.587833      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:04.588156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:05.588415      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:06.589840      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:07.590266      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:08.591214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:09.592239      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:10.592980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:11.593415      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:12.593711      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:13.594278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:14.594426      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:15.595161      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:16.595489      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:17.596553      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:18.596912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:19.597820      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:20.597818      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:21.597970      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:22.599087      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:23.599182      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:24.599342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:25.599525      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:26.600576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:27.601284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:28.601410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:29.601508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:30.601683      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:31.601914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:32.602016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:33.603103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:34.604346      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:35.604438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:36.604517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:37.604759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:38.605684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:39.605923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:40.606183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:41.606340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:42.607430      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:43.607688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:44.607858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:45.607999      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:46.608164      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:47.609058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:48.611169      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:49.612131      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:50.612847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:51.612352      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:52.612983      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:53.612904      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:54.613610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:55.613999      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:56.614454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:57.615577      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:58.615862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:11:59.616150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:00.616931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:01.617081      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:02.618278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:03.618856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:04.618954      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:05.619400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:06.619471      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:07.620467      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:08.620558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:09.621549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:10.622504      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:11.623192      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:12.623488      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:13.624264      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:14.624543      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:15.624593      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:16.624818      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:17.625750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:18.626624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:19.626936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:20.628359      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:21.628947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:22.630144      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:23.630530      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:24.631090      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:25.631963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:26.632230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:27.632755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:28.632880      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:29.633885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:30.634449      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:31.635457      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:32.636674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:33.637038      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:34.637567      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:35.637850      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:36.638307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:37.639045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:38.639225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:39.640080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:40.640259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:41.640391      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:42.640812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:43.640894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:44.641036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:45.641670      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:46.641909      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:47.642871      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:48.643139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:49.644016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:50.644687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:51.644857      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:52.645675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:53.646167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:54.646951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:55.647995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:56.648359      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:57.649070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:58.650319      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:12:59.649626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:00.650343      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:01.651139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:02.652409      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:03.652607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:04.652922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:05.652894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:06.653120      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:07.653817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:08.653864      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:09.654055      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:10.654196      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:11.655148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:12.655578      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:13.656380      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:14.657418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:15.657748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:16.657694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:17.657736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:18.657939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:19.658123      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:20.658986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:21.659304      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:22.659875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:23.659952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:24.660098      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:25.660283      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:26.660534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:27.660944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:28.661691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:29.662073      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:30.662600      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:31.663122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:32.663702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:33.663850      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:34.664119      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:35.664220      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:36.665486      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:37.666366      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:38.666589      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:39.666838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:40.667352      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:41.668110      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:42.668935      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:43.669671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:44.669818      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:45.670072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:46.670882      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:47.671629      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:48.672292      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:49.672297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:50.672823      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:51.673171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:52.673717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:53.674893      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:54.674902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:55.676083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:56.677146      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:57.678019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:58.678856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:13:59.678985      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:00.680106      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:01.680786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:02.682099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:03.682272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:04.683279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:05.685143      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:06.685558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:07.686331      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:08.686430      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:09.686572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:10.686585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:11.686953      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:12.688601      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:13.687951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:14.688660      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:15.688947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:16.689659      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:17.690076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:18.690446      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:19.690916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:20.691993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:21.692516      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:22.693784      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:23.694130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:24.694516      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:25.695229      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:26.696046      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:27.696748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:28.697465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:29.697643      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:30.699044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:31.699239      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:32.699456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:33.699685      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:34.700016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:35.700730      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:36.700800      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:37.701367      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:38.701507      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:39.701781      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:40.702890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:41.703023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:42.703720      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:43.704034      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:44.704060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:45.704351      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:46.704488      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:47.705302      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:48.705916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:49.706017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:50.706106      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:51.706308      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:52.707337      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:53.707656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:54.707886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:55.708570      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:56.708631      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:57.709001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:14:57.873: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 17:14:57.879: INFO: Deleting statefulset ss
  Apr 23 17:14:57.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8665" for this suite. @ 04/23/23 17:14:57.932
• [480.434 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/23/23 17:14:57.974
  Apr 23 17:14:57.975: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:14:57.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:14:58.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:14:58.033
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:14:58.037
  E0423 17:14:58.709446      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:14:59.713012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:00.712086      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:01.713053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:15:02.094
  Apr 23 17:15:02.101: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-04338deb-e182-4d82-b70c-4cde98c758c3 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:15:02.14
  Apr 23 17:15:02.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3297" for this suite. @ 04/23/23 17:15:02.183
• [4.224 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/23/23 17:15:02.203
  Apr 23 17:15:02.203: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:15:02.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:15:02.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:15:02.244
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/23/23 17:15:02.254
  E0423 17:15:02.713848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:03.714930      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:04.715547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:05.715994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:06.716759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:07.717580      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:15:08.306
  Apr 23 17:15:08.316: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-2ccf5ca9-460b-413d-b153-70fa296921be container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:15:08.349
  Apr 23 17:15:08.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7434" for this suite. @ 04/23/23 17:15:08.399
• [6.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/23/23 17:15:08.422
  Apr 23 17:15:08.423: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename lease-test @ 04/23/23 17:15:08.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:15:08.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:15:08.482
  Apr 23 17:15:08.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-4998" for this suite. @ 04/23/23 17:15:08.677
• [0.270 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/23/23 17:15:08.708
  Apr 23 17:15:08.708: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 17:15:08.714
  E0423 17:15:08.718933      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:15:08.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:15:08.747
  STEP: Create a ReplicaSet @ 04/23/23 17:15:08.753
  STEP: Verify that the required pods have come up @ 04/23/23 17:15:08.763
  Apr 23 17:15:08.771: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0423 17:15:09.719749      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:10.720633      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:11.721185      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:12.722145      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:13.723250      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:13.787: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/23/23 17:15:13.787
  Apr 23 17:15:13.797: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/23/23 17:15:13.798
  STEP: DeleteCollection of the ReplicaSets @ 04/23/23 17:15:13.81
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/23/23 17:15:13.835
  Apr 23 17:15:13.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2053" for this suite. @ 04/23/23 17:15:13.864
• [5.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/23/23 17:15:13.954
  Apr 23 17:15:13.954: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 17:15:13.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:15:14.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:15:14.064
  STEP: creating Agnhost RC @ 04/23/23 17:15:14.074
  Apr 23 17:15:14.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-3281 create -f -'
  E0423 17:15:14.723996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:15.470: INFO: stderr: ""
  Apr 23 17:15:15.470: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/23/23 17:15:15.47
  E0423 17:15:15.724865      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:16.478: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:15:16.478: INFO: Found 0 / 1
  E0423 17:15:16.726494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:17.477: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:15:17.477: INFO: Found 0 / 1
  E0423 17:15:17.726566      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:18.483: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:15:18.483: INFO: Found 0 / 1
  E0423 17:15:18.727879      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:19.519: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:15:19.578: INFO: Found 0 / 1
  E0423 17:15:19.728446      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:20.481: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:15:20.481: INFO: Found 0 / 1
  E0423 17:15:20.729427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:21.505: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:15:21.505: INFO: Found 1 / 1
  Apr 23 17:15:21.505: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/23/23 17:15:21.505
  Apr 23 17:15:21.521: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:15:21.521: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 23 17:15:21.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-3281 patch pod agnhost-primary-5gth7 -p {"metadata":{"annotations":{"x":"y"}}}'
  E0423 17:15:21.729952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:21.828: INFO: stderr: ""
  Apr 23 17:15:21.828: INFO: stdout: "pod/agnhost-primary-5gth7 patched\n"
  STEP: checking annotations @ 04/23/23 17:15:21.829
  Apr 23 17:15:21.836: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:15:21.836: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 23 17:15:21.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3281" for this suite. @ 04/23/23 17:15:21.847
• [7.925 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/23/23 17:15:21.88
  Apr 23 17:15:21.880: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:15:21.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:15:21.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:15:21.931
  STEP: Setting up server cert @ 04/23/23 17:15:21.983
  E0423 17:15:22.730839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:15:22.887
  STEP: Deploying the webhook pod @ 04/23/23 17:15:22.899
  STEP: Wait for the deployment to be ready @ 04/23/23 17:15:22.923
  Apr 23 17:15:22.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:15:23.731191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:24.731422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:24.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:15:25.731557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:26.731699      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:26.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:15:27.732721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:28.733092      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:28.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:15:29.733962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:30.734466      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:30.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:15:31.735365      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:32.736625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:32.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 15, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:15:33.736980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:34.737666      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:15:34.981
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:15:35.03
  E0423 17:15:35.737787      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:36.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 17:15:36.039: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2818-crds.webhook.example.com via the AdmissionRegistration API @ 04/23/23 17:15:36.568
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/23/23 17:15:36.628
  E0423 17:15:36.737764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:37.738872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:38.739136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:15:39.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 17:15:39.753443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8238" for this suite. @ 04/23/23 17:15:39.907
  STEP: Destroying namespace "webhook-markers-8608" for this suite. @ 04/23/23 17:15:39.928
• [18.069 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/23/23 17:15:39.949
  Apr 23 17:15:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pod-network-test @ 04/23/23 17:15:39.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:15:40.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:15:40.023
  STEP: Performing setup for networking test in namespace pod-network-test-9488 @ 04/23/23 17:15:40.032
  STEP: creating a selector @ 04/23/23 17:15:40.032
  STEP: Creating the service pods in kubernetes @ 04/23/23 17:15:40.033
  Apr 23 17:15:40.033: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0423 17:15:40.754355      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:41.755045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:42.755587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:43.755907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:44.757189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:45.757661      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:46.758475      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:47.762141      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:48.762251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:49.762972      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:50.762886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:51.763407      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:52.764152      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:53.764495      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:54.764672      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:55.764877      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:56.765760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:57.766793      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:58.766892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:15:59.767226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:00.768018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:01.768354      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:02.768782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:03.770109      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/23/23 17:16:04.273
  E0423 17:16:04.770146      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:05.771084      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:06.771528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:07.772190      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:16:08.343: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 23 17:16:08.343: INFO: Breadth first check of 10.233.64.117 on host 192.168.121.241...
  Apr 23 17:16:08.353: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.220:9080/dial?request=hostname&protocol=udp&host=10.233.64.117&port=8081&tries=1'] Namespace:pod-network-test-9488 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:16:08.354: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 17:16:08.356: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:16:08.356: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9488/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.220%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 17:16:08.532: INFO: Waiting for responses: map[]
  Apr 23 17:16:08.532: INFO: reached 10.233.64.117 after 0/1 tries
  Apr 23 17:16:08.533: INFO: Breadth first check of 10.233.65.118 on host 192.168.121.106...
  Apr 23 17:16:08.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.220:9080/dial?request=hostname&protocol=udp&host=10.233.65.118&port=8081&tries=1'] Namespace:pod-network-test-9488 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:16:08.540: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 17:16:08.545: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:16:08.545: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9488/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.220%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.118%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 17:16:08.694: INFO: Waiting for responses: map[]
  Apr 23 17:16:08.695: INFO: reached 10.233.65.118 after 0/1 tries
  Apr 23 17:16:08.695: INFO: Breadth first check of 10.233.66.42 on host 192.168.121.96...
  Apr 23 17:16:08.702: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.220:9080/dial?request=hostname&protocol=udp&host=10.233.66.42&port=8081&tries=1'] Namespace:pod-network-test-9488 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:16:08.702: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 17:16:08.703: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:16:08.704: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9488/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.220%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.42%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0423 17:16:08.775019      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:16:09.379: INFO: Waiting for responses: map[]
  Apr 23 17:16:09.379: INFO: reached 10.233.66.42 after 0/1 tries
  Apr 23 17:16:09.379: INFO: Going to retry 0 out of 3 pods....
  Apr 23 17:16:09.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9488" for this suite. @ 04/23/23 17:16:09.398
• [29.480 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/23/23 17:16:09.436
  Apr 23 17:16:09.436: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:16:09.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:16:09.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:16:09.479
  STEP: Setting up server cert @ 04/23/23 17:16:09.529
  E0423 17:16:09.775511      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:16:10.027
  STEP: Deploying the webhook pod @ 04/23/23 17:16:10.042
  STEP: Wait for the deployment to be ready @ 04/23/23 17:16:10.069
  Apr 23 17:16:10.086: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0423 17:16:10.776000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:11.776529      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:16:12.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 16, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 16, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 16, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 16, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:16:12.776884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:13.777353      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:16:14.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 16, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 16, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 16, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 16, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:16:14.777558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:15.778774      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:16:16.12
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:16:16.158
  E0423 17:16:16.778906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:16:17.158: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/23/23 17:16:17.165
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/23/23 17:16:17.213
  STEP: Creating a dummy validating-webhook-configuration object @ 04/23/23 17:16:17.245
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/23/23 17:16:17.261
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/23/23 17:16:17.275
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/23/23 17:16:17.295
  Apr 23 17:16:17.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7667" for this suite. @ 04/23/23 17:16:17.52
  STEP: Destroying namespace "webhook-markers-8336" for this suite. @ 04/23/23 17:16:17.539
• [8.158 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/23/23 17:16:17.597
  Apr 23 17:16:17.597: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:16:17.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:16:17.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:16:17.661
  STEP: Creating configMap with name projected-configmap-test-volume-map-70aa7943-a630-426f-87a9-3407c7b1b685 @ 04/23/23 17:16:17.664
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:16:17.675
  E0423 17:16:17.779039      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:18.779170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:19.779767      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:20.779982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:21.780190      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:22.780673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:23.781564      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:24.781615      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:25.782026      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:26.782933      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:27.783687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:28.794957      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:29.795290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:30.795329      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:31.795486      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:32.795700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:33.795779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:16:33.886
  Apr 23 17:16:33.895: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-projected-configmaps-8a083c55-9ab4-41ea-a1c9-363738c85660 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:16:33.915
  Apr 23 17:16:33.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7398" for this suite. @ 04/23/23 17:16:33.972
• [16.396 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/23/23 17:16:33.996
  Apr 23 17:16:33.996: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/23/23 17:16:33.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:16:34.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:16:34.051
  STEP: creating @ 04/23/23 17:16:34.069
  STEP: getting @ 04/23/23 17:16:34.12
  STEP: listing @ 04/23/23 17:16:34.145
  STEP: deleting @ 04/23/23 17:16:34.16
  Apr 23 17:16:34.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1094" for this suite. @ 04/23/23 17:16:34.248
• [0.273 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/23/23 17:16:34.275
  Apr 23 17:16:34.275: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubectl-logs @ 04/23/23 17:16:34.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:16:34.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:16:34.335
  STEP: creating an pod @ 04/23/23 17:16:34.356
  Apr 23 17:16:34.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-logs-9426 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Apr 23 17:16:34.657: INFO: stderr: ""
  Apr 23 17:16:34.658: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/23/23 17:16:34.658
  Apr 23 17:16:34.658: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0423 17:16:34.796269      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:35.796436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:36.797477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:37.797733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:38.797719      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:39.797924      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:40.798068      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:41.798273      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:42.798373      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:43.798926      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:44.799548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:45.799410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:46.801263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:47.801824      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:48.801272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:49.801584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:50.801673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:51.802114      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:52.802692      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:53.802966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:54.803987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:55.804709      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:56.805170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:57.805465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:58.805535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:16:59.814089      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:00.814247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:01.814817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:02.815871      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:17:02.918: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/23/23 17:17:02.919
  Apr 23 17:17:02.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-logs-9426 logs logs-generator logs-generator'
  Apr 23 17:17:03.369: INFO: stderr: ""
  Apr 23 17:17:03.369: INFO: stdout: "I0423 17:17:00.941661       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/5v6v 533\nI0423 17:17:01.141817       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/7wxf 475\nI0423 17:17:01.342355       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/pzg 323\nI0423 17:17:01.542879       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/lkzh 457\nI0423 17:17:01.742412       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/mh5t 328\nI0423 17:17:01.942828       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/9mnb 492\nI0423 17:17:02.142364       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/nn4 483\nI0423 17:17:02.341717       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/66rk 547\nI0423 17:17:02.542140       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/98rw 331\nI0423 17:17:02.742597       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/v4q 483\nI0423 17:17:02.942061       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/qk9 242\nI0423 17:17:03.142485       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/jn2j 201\nI0423 17:17:03.341896       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/kcs 244\n"
  STEP: limiting log lines @ 04/23/23 17:17:03.369
  Apr 23 17:17:03.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-logs-9426 logs logs-generator logs-generator --tail=1'
  Apr 23 17:17:03.595: INFO: stderr: ""
  Apr 23 17:17:03.595: INFO: stdout: "I0423 17:17:03.546876       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/nxk 468\n"
  Apr 23 17:17:03.595: INFO: got output "I0423 17:17:03.546876       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/nxk 468\n"
  STEP: limiting log bytes @ 04/23/23 17:17:03.595
  Apr 23 17:17:03.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-logs-9426 logs logs-generator logs-generator --limit-bytes=1'
  E0423 17:17:03.816656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:17:03.894: INFO: stderr: ""
  Apr 23 17:17:03.894: INFO: stdout: "I"
  Apr 23 17:17:03.894: INFO: got output "I"
  STEP: exposing timestamps @ 04/23/23 17:17:03.894
  Apr 23 17:17:03.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-logs-9426 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 23 17:17:04.109: INFO: stderr: ""
  Apr 23 17:17:04.109: INFO: stdout: "2023-04-23T17:17:03.943451964Z I0423 17:17:03.941900       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/27z 580\n"
  Apr 23 17:17:04.109: INFO: got output "2023-04-23T17:17:03.943451964Z I0423 17:17:03.941900       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/27z 580\n"
  STEP: restricting to a time range @ 04/23/23 17:17:04.109
  E0423 17:17:04.816828      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:05.816942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:17:06.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-logs-9426 logs logs-generator logs-generator --since=1s'
  E0423 17:17:06.817540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:17:07.016: INFO: stderr: ""
  Apr 23 17:17:07.016: INFO: stdout: "I0423 17:17:06.142031       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/bzn 484\nI0423 17:17:06.342393       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/xj7r 340\nI0423 17:17:06.541935       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/zt5p 450\nI0423 17:17:06.745453       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/g9k 231\nI0423 17:17:06.941703       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/mmz 256\n"
  Apr 23 17:17:07.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-logs-9426 logs logs-generator logs-generator --since=24h'
  Apr 23 17:17:07.524: INFO: stderr: ""
  Apr 23 17:17:07.525: INFO: stdout: "I0423 17:17:00.941661       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/5v6v 533\nI0423 17:17:01.141817       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/7wxf 475\nI0423 17:17:01.342355       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/pzg 323\nI0423 17:17:01.542879       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/lkzh 457\nI0423 17:17:01.742412       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/mh5t 328\nI0423 17:17:01.942828       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/9mnb 492\nI0423 17:17:02.142364       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/nn4 483\nI0423 17:17:02.341717       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/66rk 547\nI0423 17:17:02.542140       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/98rw 331\nI0423 17:17:02.742597       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/v4q 483\nI0423 17:17:02.942061       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/qk9 242\nI0423 17:17:03.142485       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/jn2j 201\nI0423 17:17:03.341896       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/kcs 244\nI0423 17:17:03.546876       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/nxk 468\nI0423 17:17:03.742368       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/95sm 242\nI0423 17:17:03.941900       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/27z 580\nI0423 17:17:04.142350       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/25h9 222\nI0423 17:17:04.342773       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/4llt 463\nI0423 17:17:04.542221       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/c2zc 495\nI0423 17:17:04.743589       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/bwxf 406\nI0423 17:17:04.942252       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/vhlk 569\nI0423 17:17:05.142717       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/bz7b 429\nI0423 17:17:05.342198       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/rsjf 423\nI0423 17:17:05.542668       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/bmms 211\nI0423 17:17:05.741937       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/59z 251\nI0423 17:17:05.942600       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/2tc4 484\nI0423 17:17:06.142031       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/bzn 484\nI0423 17:17:06.342393       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/xj7r 340\nI0423 17:17:06.541935       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/zt5p 450\nI0423 17:17:06.745453       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/g9k 231\nI0423 17:17:06.941703       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/mmz 256\nI0423 17:17:07.142086       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/kube-system/pods/r8d 314\nI0423 17:17:07.342997       1 logs_generator.go:76] 32 GET /api/v1/namespaces/ns/pods/kgtf 284\n"
  Apr 23 17:17:07.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-478447926 --namespace=kubectl-logs-9426 delete pod logs-generator'
  E0423 17:17:07.817595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:08.818734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:09.819003      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:17:10.804: INFO: stderr: ""
  Apr 23 17:17:10.804: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 23 17:17:10.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-9426" for this suite. @ 04/23/23 17:17:10.816
  E0423 17:17:10.819420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [36.579 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/23/23 17:17:10.856
  Apr 23 17:17:10.856: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename sched-pred @ 04/23/23 17:17:10.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:17:10.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:17:10.909
  Apr 23 17:17:10.914: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 23 17:17:10.935: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 17:17:10.943: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-1 before test
  Apr 23 17:17:10.960: INFO: cilium-8bln9 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.960: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 17:17:10.960: INFO: cilium-node-init-7rprd from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.960: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 17:17:10.960: INFO: coredns-5d78c9869d-rztdc from kube-system started at 2023-04-23 15:02:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.960: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 17:17:10.960: INFO: kube-addon-manager-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:58:03 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.961: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 23 17:17:10.961: INFO: kube-apiserver-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.961: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 23 17:17:10.961: INFO: kube-controller-manager-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.961: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 23 17:17:10.961: INFO: kube-proxy-lgs9b from kube-system started at 2023-04-23 14:55:18 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.961: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 17:17:10.961: INFO: kube-scheduler-soodi4ja4shi-1 from kube-system started at 2023-04-23 14:56:20 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.961: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 23 17:17:10.961: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-gg8kg from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 17:17:10.961: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:17:10.961: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 17:17:10.961: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-2 before test
  Apr 23 17:17:10.983: INFO: cilium-node-init-s5qwm from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.983: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 17:17:10.983: INFO: cilium-q8j55 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.984: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 17:17:10.984: INFO: coredns-5d78c9869d-jk5pb from kube-system started at 2023-04-23 14:59:51 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.984: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 17:17:10.984: INFO: kube-addon-manager-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:58:02 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.984: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 23 17:17:10.984: INFO: kube-apiserver-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.985: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 23 17:17:10.985: INFO: kube-controller-manager-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.985: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 23 17:17:10.985: INFO: kube-proxy-7j684 from kube-system started at 2023-04-23 14:55:59 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.985: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 17:17:10.985: INFO: kube-scheduler-soodi4ja4shi-2 from kube-system started at 2023-04-23 14:56:21 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:10.986: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 23 17:17:10.986: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-z9sfh from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 17:17:10.986: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:17:10.986: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 17:17:10.986: INFO: 
  Logging pods the apiserver thinks is on node soodi4ja4shi-3 before test
  Apr 23 17:17:11.005: INFO: cilium-hqn2w from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:11.005: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 23 17:17:11.005: INFO: cilium-node-init-hq966 from kube-system started at 2023-04-23 14:58:16 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:11.005: INFO: 	Container node-init ready: true, restart count 0
  Apr 23 17:17:11.005: INFO: cilium-operator-85fcfcb8b4-lvsvw from kube-system started at 2023-04-23 14:58:17 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:11.005: INFO: 	Container cilium-operator ready: true, restart count 0
  Apr 23 17:17:11.005: INFO: kube-proxy-qhxf2 from kube-system started at 2023-04-23 14:56:32 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:11.005: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 23 17:17:11.005: INFO: sonobuoy from sonobuoy started at 2023-04-23 14:59:49 +0000 UTC (1 container statuses recorded)
  Apr 23 17:17:11.005: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 23 17:17:11.005: INFO: sonobuoy-e2e-job-e20a28f544554453 from sonobuoy started at 2023-04-23 15:00:04 +0000 UTC (2 container statuses recorded)
  Apr 23 17:17:11.005: INFO: 	Container e2e ready: true, restart count 0
  Apr 23 17:17:11.005: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:17:11.005: INFO: sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-pxgxt from sonobuoy started at 2023-04-23 15:00:05 +0000 UTC (2 container statuses recorded)
  Apr 23 17:17:11.005: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:17:11.005: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node soodi4ja4shi-1 @ 04/23/23 17:17:11.043
  STEP: verifying the node has the label node soodi4ja4shi-2 @ 04/23/23 17:17:11.083
  STEP: verifying the node has the label node soodi4ja4shi-3 @ 04/23/23 17:17:11.167
  Apr 23 17:17:11.639: INFO: Pod cilium-8bln9 requesting resource cpu=0m on Node soodi4ja4shi-1
  Apr 23 17:17:11.639: INFO: Pod cilium-hqn2w requesting resource cpu=0m on Node soodi4ja4shi-3
  Apr 23 17:17:11.639: INFO: Pod cilium-node-init-7rprd requesting resource cpu=100m on Node soodi4ja4shi-1
  Apr 23 17:17:11.639: INFO: Pod cilium-node-init-hq966 requesting resource cpu=100m on Node soodi4ja4shi-3
  Apr 23 17:17:11.639: INFO: Pod cilium-node-init-s5qwm requesting resource cpu=100m on Node soodi4ja4shi-2
  Apr 23 17:17:11.639: INFO: Pod cilium-operator-85fcfcb8b4-lvsvw requesting resource cpu=0m on Node soodi4ja4shi-3
  Apr 23 17:17:11.639: INFO: Pod cilium-q8j55 requesting resource cpu=0m on Node soodi4ja4shi-2
  Apr 23 17:17:11.639: INFO: Pod coredns-5d78c9869d-jk5pb requesting resource cpu=100m on Node soodi4ja4shi-2
  Apr 23 17:17:11.639: INFO: Pod coredns-5d78c9869d-rztdc requesting resource cpu=100m on Node soodi4ja4shi-1
  Apr 23 17:17:11.639: INFO: Pod kube-addon-manager-soodi4ja4shi-1 requesting resource cpu=5m on Node soodi4ja4shi-1
  Apr 23 17:17:11.639: INFO: Pod kube-addon-manager-soodi4ja4shi-2 requesting resource cpu=5m on Node soodi4ja4shi-2
  Apr 23 17:17:11.640: INFO: Pod kube-apiserver-soodi4ja4shi-1 requesting resource cpu=250m on Node soodi4ja4shi-1
  Apr 23 17:17:11.640: INFO: Pod kube-apiserver-soodi4ja4shi-2 requesting resource cpu=250m on Node soodi4ja4shi-2
  Apr 23 17:17:11.640: INFO: Pod kube-controller-manager-soodi4ja4shi-1 requesting resource cpu=200m on Node soodi4ja4shi-1
  Apr 23 17:17:11.640: INFO: Pod kube-controller-manager-soodi4ja4shi-2 requesting resource cpu=200m on Node soodi4ja4shi-2
  Apr 23 17:17:11.640: INFO: Pod kube-proxy-7j684 requesting resource cpu=0m on Node soodi4ja4shi-2
  Apr 23 17:17:11.640: INFO: Pod kube-proxy-lgs9b requesting resource cpu=0m on Node soodi4ja4shi-1
  Apr 23 17:17:11.640: INFO: Pod kube-proxy-qhxf2 requesting resource cpu=0m on Node soodi4ja4shi-3
  Apr 23 17:17:11.640: INFO: Pod kube-scheduler-soodi4ja4shi-1 requesting resource cpu=100m on Node soodi4ja4shi-1
  Apr 23 17:17:11.640: INFO: Pod kube-scheduler-soodi4ja4shi-2 requesting resource cpu=100m on Node soodi4ja4shi-2
  Apr 23 17:17:11.640: INFO: Pod sonobuoy requesting resource cpu=0m on Node soodi4ja4shi-3
  Apr 23 17:17:11.640: INFO: Pod sonobuoy-e2e-job-e20a28f544554453 requesting resource cpu=0m on Node soodi4ja4shi-3
  Apr 23 17:17:11.640: INFO: Pod sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-gg8kg requesting resource cpu=0m on Node soodi4ja4shi-1
  Apr 23 17:17:11.640: INFO: Pod sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-pxgxt requesting resource cpu=0m on Node soodi4ja4shi-3
  Apr 23 17:17:11.640: INFO: Pod sonobuoy-systemd-logs-daemon-set-8fa64e80e29e49c7-z9sfh requesting resource cpu=0m on Node soodi4ja4shi-2
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/23/23 17:17:11.64
  Apr 23 17:17:11.640: INFO: Creating a pod which consumes cpu=591m on Node soodi4ja4shi-1
  Apr 23 17:17:11.692: INFO: Creating a pod which consumes cpu=591m on Node soodi4ja4shi-2
  Apr 23 17:17:11.771: INFO: Creating a pod which consumes cpu=1050m on Node soodi4ja4shi-3
  E0423 17:17:11.974063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:13.909251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:14.223214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:15.223427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:16.224361      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:17.224734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:18.224900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:19.225284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:20.226155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/23/23 17:17:20.296
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-17783ecf-1d60-4a9f-ae54-3b101b6deedc.17589fdcde29826a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1631/filler-pod-17783ecf-1d60-4a9f-ae54-3b101b6deedc to soodi4ja4shi-2] @ 04/23/23 17:17:20.303
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-17783ecf-1d60-4a9f-ae54-3b101b6deedc.17589fdd26cd3249], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/23/23 17:17:20.303
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-17783ecf-1d60-4a9f-ae54-3b101b6deedc.17589fdd4b9c4490], Reason = [Created], Message = [Created container filler-pod-17783ecf-1d60-4a9f-ae54-3b101b6deedc] @ 04/23/23 17:17:20.304
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-17783ecf-1d60-4a9f-ae54-3b101b6deedc.17589fdd5023700f], Reason = [Started], Message = [Started container filler-pod-17783ecf-1d60-4a9f-ae54-3b101b6deedc] @ 04/23/23 17:17:20.304
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-59a81c14-9589-4dad-9202-11a73a14a020.17589fdcd9c611ea], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1631/filler-pod-59a81c14-9589-4dad-9202-11a73a14a020 to soodi4ja4shi-1] @ 04/23/23 17:17:20.304
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-59a81c14-9589-4dad-9202-11a73a14a020.17589fdd2ffca87b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/23/23 17:17:20.304
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-59a81c14-9589-4dad-9202-11a73a14a020.17589fdd6146f5e7], Reason = [Created], Message = [Created container filler-pod-59a81c14-9589-4dad-9202-11a73a14a020] @ 04/23/23 17:17:20.304
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-59a81c14-9589-4dad-9202-11a73a14a020.17589fdd6a519944], Reason = [Started], Message = [Started container filler-pod-59a81c14-9589-4dad-9202-11a73a14a020] @ 04/23/23 17:17:20.304
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-db61b62d-49ca-4b57-9c00-c159c5f3595b.17589fdcec0a9949], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1631/filler-pod-db61b62d-49ca-4b57-9c00-c159c5f3595b to soodi4ja4shi-3] @ 04/23/23 17:17:20.305
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-db61b62d-49ca-4b57-9c00-c159c5f3595b.17589fde31251020], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/23/23 17:17:20.305
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-db61b62d-49ca-4b57-9c00-c159c5f3595b.17589fde3ed5043b], Reason = [Created], Message = [Created container filler-pod-db61b62d-49ca-4b57-9c00-c159c5f3595b] @ 04/23/23 17:17:20.305
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-db61b62d-49ca-4b57-9c00-c159c5f3595b.17589fde42005612], Reason = [Started], Message = [Started container filler-pod-db61b62d-49ca-4b57-9c00-c159c5f3595b] @ 04/23/23 17:17:20.305
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17589fdeda5e856a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 04/23/23 17:17:20.34
  E0423 17:17:21.226989      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node soodi4ja4shi-1 @ 04/23/23 17:17:21.343
  STEP: verifying the node doesn't have the label node @ 04/23/23 17:17:21.381
  STEP: removing the label node off the node soodi4ja4shi-2 @ 04/23/23 17:17:21.417
  STEP: verifying the node doesn't have the label node @ 04/23/23 17:17:21.508
  STEP: removing the label node off the node soodi4ja4shi-3 @ 04/23/23 17:17:21.516
  STEP: verifying the node doesn't have the label node @ 04/23/23 17:17:21.577
  Apr 23 17:17:21.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1631" for this suite. @ 04/23/23 17:17:21.8
• [11.191 seconds]
------------------------------
  E0423 17:17:22.227391      15 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/23/23 17:17:23.041
  Apr 23 17:17:23.041: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 17:17:23.231089      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:17:23.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:17:24.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:17:24.154
  STEP: Counting existing ResourceQuota @ 04/23/23 17:17:24.161
  E0423 17:17:24.232228      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:25.232814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:26.233779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:27.234963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:28.235371      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 17:17:29.17
  STEP: Ensuring resource quota status is calculated @ 04/23/23 17:17:29.19
  E0423 17:17:29.235813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:30.235824      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:17:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1682" for this suite. @ 04/23/23 17:17:31.221
  E0423 17:17:31.235902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [8.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/23/23 17:17:31.268
  Apr 23 17:17:31.269: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 17:17:31.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:17:31.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:17:31.359
  STEP: Creating ReplicationController "e2e-rc-f6rbg" @ 04/23/23 17:17:31.366
  Apr 23 17:17:31.392: INFO: Get Replication Controller "e2e-rc-f6rbg" to confirm replicas
  E0423 17:17:32.236230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:17:32.418: INFO: Get Replication Controller "e2e-rc-f6rbg" to confirm replicas
  Apr 23 17:17:32.425: INFO: Found 1 replicas for "e2e-rc-f6rbg" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-f6rbg" @ 04/23/23 17:17:32.426
  STEP: Updating a scale subresource @ 04/23/23 17:17:32.432
  STEP: Verifying replicas where modified for replication controller "e2e-rc-f6rbg" @ 04/23/23 17:17:32.448
  Apr 23 17:17:32.448: INFO: Get Replication Controller "e2e-rc-f6rbg" to confirm replicas
  E0423 17:17:33.237049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:17:33.475: INFO: Get Replication Controller "e2e-rc-f6rbg" to confirm replicas
  Apr 23 17:17:33.490: INFO: Found 2 replicas for "e2e-rc-f6rbg" replication controller
  Apr 23 17:17:33.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5158" for this suite. @ 04/23/23 17:17:33.509
• [2.277 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/23/23 17:17:33.55
  Apr 23 17:17:33.551: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 17:17:33.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:17:33.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:17:33.604
  STEP: Create a pod @ 04/23/23 17:17:33.613
  E0423 17:17:34.237386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:35.237661      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:36.237814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:37.238377      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:38.239288      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:39.241171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:40.240295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:41.240845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:42.240810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:43.240839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:44.241397      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:45.242219      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:46.242979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:47.243243      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:48.243702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:49.244413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:50.244657      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:51.245690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:52.246041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:53.247368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:54.250108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:55.250575      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 04/23/23 17:17:55.792
  Apr 23 17:17:55.815: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 23 17:17:55.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7153" for this suite. @ 04/23/23 17:17:55.833
• [22.305 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/23/23 17:17:55.86
  Apr 23 17:17:55.860: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/23/23 17:17:55.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:17:55.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:17:55.939
  STEP: create the container to handle the HTTPGet hook request. @ 04/23/23 17:17:55.962
  E0423 17:17:56.251414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:57.252025      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/23/23 17:17:58.036
  E0423 17:17:58.252757      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:17:59.253682      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:00.254198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:01.254764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:02.255774      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:03.255966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:04.256786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:05.257005      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:06.258043      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:07.258245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:08.259234      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:09.260003      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:10.260349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:11.260581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:12.261232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:13.263531      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:14.263784      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:15.264788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/23/23 17:18:16.158
  E0423 17:18:16.264992      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:17.265040      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:18.265423      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:19.265547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:20.272421      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:21.272663      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:22.273148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:23.273362      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/23/23 17:18:24.222
  Apr 23 17:18:24.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 17:18:24.274491      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "container-lifecycle-hook-5140" for this suite. @ 04/23/23 17:18:24.275
• [28.435 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 04/23/23 17:18:24.299
  Apr 23 17:18:24.299: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename endpointslice @ 04/23/23 17:18:24.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:18:24.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:18:24.366
  E0423 17:18:25.275011      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:26.275408      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:27.275626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:28.275917      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:29.276090      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 04/23/23 17:18:29.659
  E0423 17:18:30.276148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:31.276643      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:32.277496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:33.277910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:34.277973      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 04/23/23 17:18:34.676
  E0423 17:18:35.278095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:36.279583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:37.279281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:38.280812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:39.279938      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/23/23 17:18:39.692
  E0423 17:18:40.280444      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:41.280565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:42.280948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:43.281657      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:44.281797      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 04/23/23 17:18:44.711
  Apr 23 17:18:44.766: INFO: EndpointSlice for Service endpointslice-37/example-named-port not found
  E0423 17:18:45.282894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:46.283122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:47.283978      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:48.285009      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:49.285771      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:50.286885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:51.288052      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:52.288588      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:53.290547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:54.290836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:18:54.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-37" for this suite. @ 04/23/23 17:18:54.799
• [30.529 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/23/23 17:18:54.832
  Apr 23 17:18:54.832: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename security-context-test @ 04/23/23 17:18:54.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:18:54.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:18:54.883
  E0423 17:18:55.292490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:56.293242      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:57.294119      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:58.294317      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:18:59.295435      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:00.295512      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:01.295740      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:19:01.529: INFO: Got logs for pod "busybox-privileged-false-05cba0d4-508a-462e-ae6f-17bcc1f5bf4e": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 23 17:19:01.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8770" for this suite. @ 04/23/23 17:19:01.553
• [6.742 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/23/23 17:19:01.597
  Apr 23 17:19:01.597: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 17:19:01.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:19:01.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:19:01.662
  E0423 17:19:02.296237      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:03.296954      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:04.297181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:05.297276      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:06.298378      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:07.299514      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:08.300476      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:09.301324      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:10.301794      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:11.303117      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:12.304171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:13.304471      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:14.304906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:15.305859      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:16.306463      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:17.306943      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:18.308066      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:19.308148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:20.308721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:21.308863      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:19:21.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-790" for this suite. @ 04/23/23 17:19:21.778
• [20.193 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/23/23 17:19:21.794
  Apr 23 17:19:21.795: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:19:21.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:19:21.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:19:21.845
  STEP: Creating the pod @ 04/23/23 17:19:21.851
  E0423 17:19:22.309024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:23.310062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:24.310672      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:25.311604      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:26.312358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:27.312982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:28.313716      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:29.314145      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:30.314583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:31.314914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:32.315987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:33.317212      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:34.318205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:19:34.490: INFO: Successfully updated pod "labelsupdate4fd92bc3-49fe-4863-b362-8c1be8f864b7"
  E0423 17:19:35.319280      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:36.319429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:19:36.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7854" for this suite. @ 04/23/23 17:19:36.581
• [14.803 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/23/23 17:19:36.602
  Apr 23 17:19:36.602: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename secrets @ 04/23/23 17:19:36.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:19:36.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:19:36.66
  STEP: Creating secret with name secret-test-e1f93009-8c82-4b37-a6cd-6a1eb21a890e @ 04/23/23 17:19:36.667
  STEP: Creating a pod to test consume secrets @ 04/23/23 17:19:36.677
  E0423 17:19:37.319767      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:38.320023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:39.320923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:40.321049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:41.321221      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:42.321841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:43.322071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:44.322274      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:45.323243      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:46.324226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:47.324248      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:48.324403      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:49.325691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:50.326477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:51.326539      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:52.326923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:53.327612      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:54.327725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:55.328145      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:56.328277      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:57.328411      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:58.328974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:19:59.329171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:00.329630      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:01.329771      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:02.330112      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:03.330979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:04.331165      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:20:04.931
  Apr 23 17:20:04.937: INFO: Trying to get logs from node soodi4ja4shi-3 pod pod-secrets-ab13c528-b928-4b91-b28d-7aedc7d098ca container secret-volume-test: <nil>
  E0423 17:20:05.331596      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/23/23 17:20:05.366
  Apr 23 17:20:05.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7882" for this suite. @ 04/23/23 17:20:05.412
• [28.826 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/23/23 17:20:05.429
  Apr 23 17:20:05.429: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 17:20:05.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:20:05.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:20:05.475
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/23/23 17:20:05.482
  Apr 23 17:20:05.485: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 17:20:06.331662      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:07.339413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:20:07.826: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  E0423 17:20:08.335548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:09.336423      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:10.337381      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:11.338309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:12.344487      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:13.344938      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:14.368611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:15.368723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:16.368779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:20:17.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8872" for this suite. @ 04/23/23 17:20:17.321
• [11.911 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/23/23 17:20:17.34
  Apr 23 17:20:17.340: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 17:20:17.343
  E0423 17:20:17.369645      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:20:17.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:20:17.395
  E0423 17:20:18.369797      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:19.369762      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:20.370004      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:21.370229      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:22.370917      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:23.371088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:24.372066      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:25.372608      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:26.372786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:27.373198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:28.373386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:29.373425      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:30.373874      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:31.373982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:32.374097      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:33.374958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:20:33.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 17:20:33.572: INFO: Deleting pod "var-expansion-cb3ae6f5-c12d-4678-aa35-d898c92ff6e5" in namespace "var-expansion-2702"
  Apr 23 17:20:33.588: INFO: Wait up to 5m0s for pod "var-expansion-cb3ae6f5-c12d-4678-aa35-d898c92ff6e5" to be fully deleted
  E0423 17:20:34.375012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:35.375375      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:36.375265      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:37.375503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:38.376001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:39.376302      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:40.376748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:41.376704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-2702" for this suite. @ 04/23/23 17:20:41.632
• [24.309 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/23/23 17:20:41.65
  Apr 23 17:20:41.650: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename watch @ 04/23/23 17:20:41.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:20:41.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:20:41.699
  STEP: creating a new configmap @ 04/23/23 17:20:41.705
  STEP: modifying the configmap once @ 04/23/23 17:20:41.718
  STEP: modifying the configmap a second time @ 04/23/23 17:20:41.736
  STEP: deleting the configmap @ 04/23/23 17:20:41.752
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/23/23 17:20:41.767
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/23/23 17:20:41.77
  Apr 23 17:20:41.770: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3232  d966c14a-d5ca-4478-83bf-5880a41c5e2d 45235 0 2023-04-23 17:20:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-23 17:20:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 17:20:41.770: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3232  d966c14a-d5ca-4478-83bf-5880a41c5e2d 45236 0 2023-04-23 17:20:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-23 17:20:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 17:20:41.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3232" for this suite. @ 04/23/23 17:20:41.782
• [0.147 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/23/23 17:20:41.798
  Apr 23 17:20:41.798: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename pods @ 04/23/23 17:20:41.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:20:41.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:20:41.884
  STEP: creating a Pod with a static label @ 04/23/23 17:20:41.908
  STEP: watching for Pod to be ready @ 04/23/23 17:20:41.929
  Apr 23 17:20:41.932: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 23 17:20:41.941: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:41 +0000 UTC  }]
  Apr 23 17:20:41.986: INFO: observed Pod pod-test in namespace pods-4783 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:41 +0000 UTC  }]
  E0423 17:20:42.377396      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:43.378023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:44.378460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:45.479116      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:46.479440      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:20:46.658: INFO: Found Pod pod-test in namespace pods-4783 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:20:41 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/23/23 17:20:46.687
  STEP: getting the Pod and ensuring that it's patched @ 04/23/23 17:20:46.774
  STEP: replacing the Pod's status Ready condition to False @ 04/23/23 17:20:46.828
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/23/23 17:20:46.869
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/23/23 17:20:46.869
  STEP: watching for the Pod to be deleted @ 04/23/23 17:20:46.914
  Apr 23 17:20:46.920: INFO: observed event type MODIFIED
  E0423 17:20:47.479509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:20:47.694: INFO: observed event type MODIFIED
  Apr 23 17:20:48.032: INFO: observed event type MODIFIED
  E0423 17:20:48.479980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:20:48.664: INFO: observed event type MODIFIED
  E0423 17:20:49.479979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:50.480105      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:51.480214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:52.480727      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:53.480840      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:54.480984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:55.481119      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:56.481297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:20:56.992: INFO: observed event type MODIFIED
  Apr 23 17:20:57.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4783" for this suite. @ 04/23/23 17:20:57.021
• [15.243 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/23/23 17:20:57.045
  Apr 23 17:20:57.045: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename security-context @ 04/23/23 17:20:57.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:20:57.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:20:57.115
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/23/23 17:20:57.123
  E0423 17:20:57.481529      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:58.481931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:20:59.482022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:00.483016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:01.485149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:02.486096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:03.486260      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:04.486819      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:21:05.203
  Apr 23 17:21:05.213: INFO: Trying to get logs from node soodi4ja4shi-3 pod security-context-ef92ab42-a32d-484d-962b-ec5946289a81 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:21:05.253
  Apr 23 17:21:05.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3753" for this suite. @ 04/23/23 17:21:05.301
• [8.281 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/23/23 17:21:05.328
  Apr 23 17:21:05.328: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:21:05.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:05.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:05.38
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:21:05.388
  E0423 17:21:05.487557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:06.488452      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:07.489291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:08.489451      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:09.489937      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:10.490118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:11.491052      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:12.491958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:13.492684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:21:13.56
  Apr 23 17:21:13.567: INFO: Trying to get logs from node soodi4ja4shi-3 pod downwardapi-volume-88a6cf86-8976-4354-8e64-8aaae326281f container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:21:13.581
  Apr 23 17:21:13.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8081" for this suite. @ 04/23/23 17:21:13.633
• [8.318 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/23/23 17:21:13.653
  Apr 23 17:21:13.653: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:21:13.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:13.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:13.825
  STEP: Setting up server cert @ 04/23/23 17:21:13.915
  E0423 17:21:14.492685      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:15.492910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:21:15.524
  STEP: Deploying the webhook pod @ 04/23/23 17:21:15.543
  STEP: Wait for the deployment to be ready @ 04/23/23 17:21:15.572
  Apr 23 17:21:15.600: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:21:16.493640      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:17.493755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:21:17.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:21:18.494895      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:19.495111      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:21:19.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:21:20.495973      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:21.506872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:21:21.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 21, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:21:22.503684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:21:23.504321      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:21:23.646
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:21:23.676
  E0423 17:21:24.504623      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:21:24.677: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/23/23 17:21:24.687
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/23/23 17:21:24.747
  Apr 23 17:21:24.747: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  Apr 23 17:21:24.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2101" for this suite. @ 04/23/23 17:21:24.911
  STEP: Destroying namespace "webhook-markers-5419" for this suite. @ 04/23/23 17:21:24.942
• [11.323 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/23/23 17:21:24.977
  Apr 23 17:21:24.977: INFO: >>> kubeConfig: /tmp/kubeconfig-478447926
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/23/23 17:21:24.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:25.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:25.067
  STEP: creating @ 04/23/23 17:21:25.072
  STEP: getting @ 04/23/23 17:21:25.127
  STEP: listing in namespace @ 04/23/23 17:21:25.157
  STEP: patching @ 04/23/23 17:21:25.166
  STEP: deleting @ 04/23/23 17:21:25.195
  Apr 23 17:21:25.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3768" for this suite. @ 04/23/23 17:21:25.244
• [0.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 23 17:21:25.280: INFO: Running AfterSuite actions on node 1
  Apr 23 17:21:25.281: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.001 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.151 seconds]
------------------------------

Ran 378 of 7207 Specs in 8442.306 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 2h20m43.364967001s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

