  I0423 16:37:27.048562      21 e2e.go:117] Starting e2e run "cdc5b9e0-b085-436c-af1d-79cea00479c8" on Ginkgo node 1
  Apr 23 16:37:27.079: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682267846 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 23 16:37:27.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:37:27.326: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 23 16:37:27.371: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 23 16:37:27.374: INFO: e2e test version: v1.27.1
  Apr 23 16:37:27.376: INFO: kube-apiserver version: v1.27.1
  Apr 23 16:37:27.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:37:27.393: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/23/23 16:37:27.944
  Apr 23 16:37:27.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename subpath @ 04/23/23 16:37:27.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:27.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:27.971
  STEP: Setting up data @ 04/23/23 16:37:27.978
  STEP: Creating pod pod-subpath-test-secret-njlk @ 04/23/23 16:37:27.991
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 16:37:27.991
  STEP: Saw pod success @ 04/23/23 16:37:54.088
  Apr 23 16:37:54.094: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-subpath-test-secret-njlk container test-container-subpath-secret-njlk: <nil>
  STEP: delete the pod @ 04/23/23 16:37:54.117
  STEP: Deleting pod pod-subpath-test-secret-njlk @ 04/23/23 16:37:54.138
  Apr 23 16:37:54.138: INFO: Deleting pod "pod-subpath-test-secret-njlk" in namespace "subpath-6067"
  Apr 23 16:37:54.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6067" for this suite. @ 04/23/23 16:37:54.148
• [26.211 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/23/23 16:37:54.156
  Apr 23 16:37:54.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 16:37:54.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:54.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:54.192
  STEP: Create a Replicaset @ 04/23/23 16:37:54.265
  STEP: Verify that the required pods have come up. @ 04/23/23 16:37:54.272
  Apr 23 16:37:54.276: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 23 16:37:59.282: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 16:37:59.282
  STEP: Getting /status @ 04/23/23 16:37:59.282
  Apr 23 16:37:59.288: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/23/23 16:37:59.288
  Apr 23 16:37:59.304: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/23/23 16:37:59.304
  Apr 23 16:37:59.307: INFO: Observed &ReplicaSet event: ADDED
  Apr 23 16:37:59.307: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:37:59.307: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:37:59.308: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:37:59.308: INFO: Found replicaset test-rs in namespace replicaset-7389 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 23 16:37:59.308: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/23/23 16:37:59.308
  Apr 23 16:37:59.308: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 23 16:37:59.317: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/23/23 16:37:59.317
  Apr 23 16:37:59.319: INFO: Observed &ReplicaSet event: ADDED
  Apr 23 16:37:59.320: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:37:59.320: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:37:59.320: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:37:59.321: INFO: Observed replicaset test-rs in namespace replicaset-7389 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 16:37:59.321: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 23 16:37:59.321: INFO: Found replicaset test-rs in namespace replicaset-7389 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 23 16:37:59.321: INFO: Replicaset test-rs has a patched status
  Apr 23 16:37:59.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7389" for this suite. @ 04/23/23 16:37:59.326
• [5.179 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/23/23 16:37:59.335
  Apr 23 16:37:59.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:37:59.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:37:59.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:37:59.369
  STEP: Creating configMap with name projected-configmap-test-volume-map-e387636a-f4d2-4496-a935-0026b26079da @ 04/23/23 16:37:59.379
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:37:59.385
  STEP: Saw pod success @ 04/23/23 16:38:03.417
  Apr 23 16:38:03.420: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-configmaps-c3c57784-93f3-4bb3-b73d-964701659ad4 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:38:03.429
  Apr 23 16:38:03.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2421" for this suite. @ 04/23/23 16:38:03.458
• [4.131 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/23/23 16:38:03.466
  Apr 23 16:38:03.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename watch @ 04/23/23 16:38:03.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:03.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:03.493
  STEP: creating a new configmap @ 04/23/23 16:38:03.497
  STEP: modifying the configmap once @ 04/23/23 16:38:03.503
  STEP: modifying the configmap a second time @ 04/23/23 16:38:03.516
  STEP: deleting the configmap @ 04/23/23 16:38:03.528
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/23/23 16:38:03.536
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/23/23 16:38:03.537
  Apr 23 16:38:03.537: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9741  deb88fdd-3f67-4260-834e-5af40f3020a4 2851 0 2023-04-23 16:38:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-23 16:38:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:38:03.537: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9741  deb88fdd-3f67-4260-834e-5af40f3020a4 2852 0 2023-04-23 16:38:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-23 16:38:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:38:03.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9741" for this suite. @ 04/23/23 16:38:03.543
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/23/23 16:38:03.557
  Apr 23 16:38:03.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename security-context @ 04/23/23 16:38:03.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:03.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:03.58
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/23/23 16:38:03.584
  STEP: Saw pod success @ 04/23/23 16:38:07.613
  Apr 23 16:38:07.617: INFO: Trying to get logs from node ip-172-31-70-241 pod security-context-706e4486-e02a-4ee2-9f7b-c0be0405fd3a container test-container: <nil>
  STEP: delete the pod @ 04/23/23 16:38:07.625
  Apr 23 16:38:07.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-154" for this suite. @ 04/23/23 16:38:07.651
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/23/23 16:38:07.659
  Apr 23 16:38:07.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:38:07.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:07.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:07.696
  STEP: Creating configMap with name projected-configmap-test-volume-135e1b61-3518-428c-8f3f-b2af39a577db @ 04/23/23 16:38:07.701
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:38:07.709
  STEP: Saw pod success @ 04/23/23 16:38:11.744
  Apr 23 16:38:11.749: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-configmaps-9beff22f-75cc-400e-b818-9b99656164f9 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 16:38:11.757
  Apr 23 16:38:11.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5171" for this suite. @ 04/23/23 16:38:11.782
• [4.132 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/23/23 16:38:11.794
  Apr 23 16:38:11.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 16:38:11.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:11.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:11.827
  Apr 23 16:38:13.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4730" for this suite. @ 04/23/23 16:38:13.874
• [2.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/23/23 16:38:13.888
  Apr 23 16:38:13.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:38:13.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:13.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:13.92
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:38:13.925
  STEP: Saw pod success @ 04/23/23 16:38:17.954
  Apr 23 16:38:17.959: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-c4aa7676-cc5a-44cb-8f0e-8acff2537370 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:38:17.967
  Apr 23 16:38:17.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9925" for this suite. @ 04/23/23 16:38:18.001
• [4.120 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/23/23 16:38:18.009
  Apr 23 16:38:18.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename server-version @ 04/23/23 16:38:18.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:18.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:18.035
  STEP: Request ServerVersion @ 04/23/23 16:38:18.038
  STEP: Confirm major version @ 04/23/23 16:38:18.04
  Apr 23 16:38:18.040: INFO: Major version: 1
  STEP: Confirm minor version @ 04/23/23 16:38:18.041
  Apr 23 16:38:18.041: INFO: cleanMinorVersion: 27
  Apr 23 16:38:18.041: INFO: Minor version: 27
  Apr 23 16:38:18.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-2730" for this suite. @ 04/23/23 16:38:18.046
• [0.044 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/23/23 16:38:18.055
  Apr 23 16:38:18.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:38:18.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:18.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:18.079
  STEP: Creating projection with secret that has name projected-secret-test-000d5489-bdd5-4563-9e11-6bceccf46760 @ 04/23/23 16:38:18.084
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:38:18.09
  STEP: Saw pod success @ 04/23/23 16:38:22.115
  Apr 23 16:38:22.118: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-secrets-a30a31f5-2bc0-40f0-bc17-40f7724d39c4 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:38:22.132
  Apr 23 16:38:22.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5420" for this suite. @ 04/23/23 16:38:22.161
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/23/23 16:38:22.172
  Apr 23 16:38:22.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/23/23 16:38:22.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:22.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:22.201
  STEP: Setting up the test @ 04/23/23 16:38:22.211
  STEP: Creating hostNetwork=false pod @ 04/23/23 16:38:22.211
  STEP: Creating hostNetwork=true pod @ 04/23/23 16:38:24.239
  STEP: Running the test @ 04/23/23 16:38:28.267
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/23/23 16:38:28.267
  Apr 23 16:38:28.267: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.267: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.267: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 23 16:38:28.361: INFO: Exec stderr: ""
  Apr 23 16:38:28.361: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.362: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.362: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 23 16:38:28.435: INFO: Exec stderr: ""
  Apr 23 16:38:28.436: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.436: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.437: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 23 16:38:28.514: INFO: Exec stderr: ""
  Apr 23 16:38:28.514: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.514: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.515: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 23 16:38:28.576: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/23/23 16:38:28.576
  Apr 23 16:38:28.576: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.577: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.577: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 23 16:38:28.650: INFO: Exec stderr: ""
  Apr 23 16:38:28.650: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.651: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.651: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 23 16:38:28.730: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/23/23 16:38:28.731
  Apr 23 16:38:28.731: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.731: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.731: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 23 16:38:28.799: INFO: Exec stderr: ""
  Apr 23 16:38:28.799: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.800: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.801: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 23 16:38:28.876: INFO: Exec stderr: ""
  Apr 23 16:38:28.876: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.876: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.876: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 23 16:38:28.955: INFO: Exec stderr: ""
  Apr 23 16:38:28.955: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8140 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:38:28.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:28.956: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:38:28.956: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8140/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 23 16:38:29.027: INFO: Exec stderr: ""
  Apr 23 16:38:29.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-8140" for this suite. @ 04/23/23 16:38:29.033
• [6.869 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/23/23 16:38:29.043
  Apr 23 16:38:29.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sysctl @ 04/23/23 16:38:29.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:29.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:29.068
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/23/23 16:38:29.074
  STEP: Watching for error events or started pod @ 04/23/23 16:38:29.084
  STEP: Waiting for pod completion @ 04/23/23 16:38:31.09
  STEP: Checking that the pod succeeded @ 04/23/23 16:38:33.107
  STEP: Getting logs from the pod @ 04/23/23 16:38:33.107
  STEP: Checking that the sysctl is actually updated @ 04/23/23 16:38:33.115
  Apr 23 16:38:33.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-988" for this suite. @ 04/23/23 16:38:33.12
• [4.087 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/23/23 16:38:33.131
  Apr 23 16:38:33.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename endpointslice @ 04/23/23 16:38:33.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:33.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:33.156
  Apr 23 16:38:33.174: INFO: Endpoints addresses: [172.31.16.187 172.31.92.52] , ports: [6443]
  Apr 23 16:38:33.174: INFO: EndpointSlices addresses: [172.31.16.187 172.31.92.52] , ports: [6443]
  Apr 23 16:38:33.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2945" for this suite. @ 04/23/23 16:38:33.181
• [0.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/23/23 16:38:33.192
  Apr 23 16:38:33.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 16:38:33.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:33.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:33.218
  STEP: Creating configMap configmap-124/configmap-test-9c88d99c-31c2-4b78-a098-a89c67d15825 @ 04/23/23 16:38:33.222
  STEP: Creating a pod to test consume configMaps @ 04/23/23 16:38:33.229
  STEP: Saw pod success @ 04/23/23 16:38:37.253
  Apr 23 16:38:37.257: INFO: Trying to get logs from node ip-172-31-86-26 pod pod-configmaps-266b8d56-07e2-432a-a4f6-1c7a3f57f9ef container env-test: <nil>
  STEP: delete the pod @ 04/23/23 16:38:37.28
  Apr 23 16:38:37.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-124" for this suite. @ 04/23/23 16:38:37.306
• [4.124 seconds]
------------------------------
SSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/23/23 16:38:37.316
  Apr 23 16:38:37.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:38:37.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:37.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:37.348
  STEP: creating secret secrets-9524/secret-test-66616fe9-8a69-4e20-8c63-383f7a9f1fc2 @ 04/23/23 16:38:37.351
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:38:37.357
  STEP: Saw pod success @ 04/23/23 16:38:41.393
  Apr 23 16:38:41.397: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-921c3135-f685-448c-9d64-f5e96cd4cf5e container env-test: <nil>
  STEP: delete the pod @ 04/23/23 16:38:41.405
  Apr 23 16:38:41.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9524" for this suite. @ 04/23/23 16:38:41.44
• [4.133 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/23/23 16:38:41.45
  Apr 23 16:38:41.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename job @ 04/23/23 16:38:41.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:41.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:41.477
  STEP: Creating a job @ 04/23/23 16:38:41.48
  STEP: Ensuring active pods == parallelism @ 04/23/23 16:38:41.491
  STEP: Orphaning one of the Job's Pods @ 04/23/23 16:38:43.498
  Apr 23 16:38:44.021: INFO: Successfully updated pod "adopt-release-ggmvd"
  STEP: Checking that the Job readopts the Pod @ 04/23/23 16:38:44.021
  STEP: Removing the labels from the Job's Pod @ 04/23/23 16:38:46.031
  Apr 23 16:38:46.546: INFO: Successfully updated pod "adopt-release-ggmvd"
  STEP: Checking that the Job releases the Pod @ 04/23/23 16:38:46.546
  Apr 23 16:38:48.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3880" for this suite. @ 04/23/23 16:38:48.561
• [7.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/23/23 16:38:48.57
  Apr 23 16:38:48.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 16:38:48.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:48.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:48.601
  Apr 23 16:38:48.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:38:49.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8486" for this suite. @ 04/23/23 16:38:49.18
• [0.626 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/23/23 16:38:49.198
  Apr 23 16:38:49.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename job @ 04/23/23 16:38:49.199
  Apr 23 16:38:49.210: INFO: Namespace name "job-3880" was already taken, generate a new name and retry
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:51.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:51.239
  STEP: Creating Indexed job @ 04/23/23 16:38:51.242
  STEP: Ensuring job reaches completions @ 04/23/23 16:38:51.252
  STEP: Ensuring pods with index for job exist @ 04/23/23 16:38:59.261
  Apr 23 16:38:59.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7701" for this suite. @ 04/23/23 16:38:59.281
• [10.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/23/23 16:38:59.295
  Apr 23 16:38:59.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:38:59.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:59.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:59.333
  STEP: Starting the proxy @ 04/23/23 16:38:59.34
  Apr 23 16:38:59.340: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-7657 proxy --unix-socket=/tmp/kubectl-proxy-unix2984260944/test'
  STEP: retrieving proxy /api/ output @ 04/23/23 16:38:59.397
  Apr 23 16:38:59.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7657" for this suite. @ 04/23/23 16:38:59.407
• [0.121 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/23/23 16:38:59.416
  Apr 23 16:38:59.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename gc @ 04/23/23 16:38:59.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:38:59.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:38:59.44
  Apr 23 16:38:59.498: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"62befde1-42ce-4325-9c72-70665e791576", Controller:(*bool)(0xc00456a09e), BlockOwnerDeletion:(*bool)(0xc00456a09f)}}
  Apr 23 16:38:59.509: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4565411b-8949-4555-98f9-cd520ef460cd", Controller:(*bool)(0xc003f17c66), BlockOwnerDeletion:(*bool)(0xc003f17c67)}}
  Apr 23 16:38:59.519: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"25a36169-3ee0-41ef-9d1e-6e722b5dd8d3", Controller:(*bool)(0xc003f17e96), BlockOwnerDeletion:(*bool)(0xc003f17e97)}}
  Apr 23 16:39:04.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9048" for this suite. @ 04/23/23 16:39:04.556
• [5.146 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/23/23 16:39:04.563
  Apr 23 16:39:04.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:39:04.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:04.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:04.594
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:39:04.597
  STEP: Saw pod success @ 04/23/23 16:39:08.627
  Apr 23 16:39:08.631: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-c681dcac-4bb2-4800-9a1c-7038c63805f4 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:39:08.642
  Apr 23 16:39:08.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6630" for this suite. @ 04/23/23 16:39:08.669
• [4.117 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/23/23 16:39:08.681
  Apr 23 16:39:08.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 16:39:08.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:08.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:08.758
  STEP: Creating a pod to test substitution in volume subpath @ 04/23/23 16:39:08.763
  STEP: Saw pod success @ 04/23/23 16:39:12.79
  Apr 23 16:39:12.800: INFO: Trying to get logs from node ip-172-31-70-241 pod var-expansion-3e0fcade-d4d7-4493-a611-b39eb752c012 container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 16:39:12.812
  Apr 23 16:39:12.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1448" for this suite. @ 04/23/23 16:39:12.849
• [4.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/23/23 16:39:12.862
  Apr 23 16:39:12.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 16:39:12.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:12.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:12.9
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/23/23 16:39:12.91
  STEP: Saw pod success @ 04/23/23 16:39:16.955
  Apr 23 16:39:16.959: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-ada82e1c-525e-4551-ba25-e777e754575a container test-container: <nil>
  STEP: delete the pod @ 04/23/23 16:39:16.969
  Apr 23 16:39:16.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6990" for this suite. @ 04/23/23 16:39:16.994
• [4.143 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/23/23 16:39:17.005
  Apr 23 16:39:17.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 16:39:17.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:17.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:17.039
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-6620 @ 04/23/23 16:39:17.047
  STEP: changing the ExternalName service to type=ClusterIP @ 04/23/23 16:39:17.056
  STEP: creating replication controller externalname-service in namespace services-6620 @ 04/23/23 16:39:17.076
  I0423 16:39:17.084791      21 runners.go:194] Created replication controller with name: externalname-service, namespace: services-6620, replica count: 2
  I0423 16:39:20.135272      21 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:39:20.135: INFO: Creating new exec pod
  Apr 23 16:39:23.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-6620 exec execpodfcvds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 23 16:39:23.316: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 23 16:39:23.316: INFO: stdout: "externalname-service-zmr2q"
  Apr 23 16:39:23.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-6620 exec execpodfcvds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.200 80'
  Apr 23 16:39:23.479: INFO: stderr: "+ nc -v -t -w 2 10.152.183.200 80\n+ echo hostName\nConnection to 10.152.183.200 80 port [tcp/http] succeeded!\n"
  Apr 23 16:39:23.479: INFO: stdout: "externalname-service-zmr2q"
  Apr 23 16:39:23.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:39:23.484: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-6620" for this suite. @ 04/23/23 16:39:23.51
• [6.514 seconds]
------------------------------
SS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/23/23 16:39:23.519
  Apr 23 16:39:23.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:39:23.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:23.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:23.551
  STEP: Creating secret with name secret-test-947acb89-7b92-4585-a069-3ffe6a2823ee @ 04/23/23 16:39:23.555
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:39:23.56
  STEP: Saw pod success @ 04/23/23 16:39:27.591
  Apr 23 16:39:27.595: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-secrets-5a2c6547-f365-4631-8d85-82d8345c1366 container secret-env-test: <nil>
  STEP: delete the pod @ 04/23/23 16:39:27.604
  Apr 23 16:39:27.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3244" for this suite. @ 04/23/23 16:39:27.633
• [4.125 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/23/23 16:39:27.646
  Apr 23 16:39:27.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:39:27.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:27.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:27.678
  STEP: Creating secret with name secret-test-b45a6022-250b-490d-a759-eea073fb4820 @ 04/23/23 16:39:27.684
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:39:27.692
  STEP: Saw pod success @ 04/23/23 16:39:31.719
  Apr 23 16:39:31.723: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-secrets-a286d29f-d8d3-43ed-9cc4-60881b75efad container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:39:31.733
  Apr 23 16:39:31.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6134" for this suite. @ 04/23/23 16:39:31.759
• [4.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/23/23 16:39:31.769
  Apr 23 16:39:31.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename events @ 04/23/23 16:39:31.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:31.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:31.792
  STEP: Create set of events @ 04/23/23 16:39:31.796
  STEP: get a list of Events with a label in the current namespace @ 04/23/23 16:39:31.814
  STEP: delete a list of events @ 04/23/23 16:39:31.818
  Apr 23 16:39:31.818: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/23/23 16:39:31.844
  Apr 23 16:39:31.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7307" for this suite. @ 04/23/23 16:39:31.853
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/23/23 16:39:31.866
  Apr 23 16:39:31.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 16:39:31.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:31.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:31.89
  Apr 23 16:39:31.896: INFO: Got root ca configmap in namespace "svcaccounts-1792"
  Apr 23 16:39:31.905: INFO: Deleted root ca configmap in namespace "svcaccounts-1792"
  STEP: waiting for a new root ca configmap created @ 04/23/23 16:39:32.405
  Apr 23 16:39:32.409: INFO: Recreated root ca configmap in namespace "svcaccounts-1792"
  Apr 23 16:39:32.416: INFO: Updated root ca configmap in namespace "svcaccounts-1792"
  STEP: waiting for the root ca configmap reconciled @ 04/23/23 16:39:32.917
  Apr 23 16:39:32.923: INFO: Reconciled root ca configmap in namespace "svcaccounts-1792"
  Apr 23 16:39:32.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1792" for this suite. @ 04/23/23 16:39:32.93
• [1.074 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/23/23 16:39:32.944
  Apr 23 16:39:32.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:39:32.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:32.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:32.988
  Apr 23 16:39:32.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-191 create -f -'
  Apr 23 16:39:33.746: INFO: stderr: ""
  Apr 23 16:39:33.746: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 23 16:39:33.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-191 create -f -'
  Apr 23 16:39:34.039: INFO: stderr: ""
  Apr 23 16:39:34.039: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/23/23 16:39:34.039
  Apr 23 16:39:35.044: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:39:35.044: INFO: Found 0 / 1
  Apr 23 16:39:36.045: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:39:36.045: INFO: Found 1 / 1
  Apr 23 16:39:36.045: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 23 16:39:36.049: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 16:39:36.049: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 23 16:39:36.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-191 describe pod agnhost-primary-np7rj'
  Apr 23 16:39:36.194: INFO: stderr: ""
  Apr 23 16:39:36.194: INFO: stdout: "Name:             agnhost-primary-np7rj\nNamespace:        kubectl-191\nPriority:         0\nService Account:  default\nNode:             ip-172-31-70-241/172.31.70.241\nStart Time:       Sun, 23 Apr 2023 16:39:33 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.6.217\nIPs:\n  IP:           192.168.6.217\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://9ab452bd641c2a7e12aa2821415677e7e476c00685454b79d5120bf57be1d772\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 23 Apr 2023 16:39:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pscbb (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pscbb:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-191/agnhost-primary-np7rj to ip-172-31-70-241\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Apr 23 16:39:36.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-191 describe rc agnhost-primary'
  Apr 23 16:39:36.331: INFO: stderr: ""
  Apr 23 16:39:36.331: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-191\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-np7rj\n"
  Apr 23 16:39:36.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-191 describe service agnhost-primary'
  Apr 23 16:39:36.491: INFO: stderr: ""
  Apr 23 16:39:36.491: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-191\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.250\nIPs:               10.152.183.250\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.6.217:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 23 16:39:36.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-191 describe node ip-172-31-16-187'
  Apr 23 16:39:36.661: INFO: stderr: ""
  Apr 23 16:39:36.661: INFO: stdout: "Name:               ip-172-31-16-187\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-control-plane\n                    juju-charm=kubernetes-control-plane\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-16-187\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 23 Apr 2023 16:23:32 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-16-187\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 23 Apr 2023 16:39:32 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sun, 23 Apr 2023 16:37:38 +0000   Sun, 23 Apr 2023 16:23:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sun, 23 Apr 2023 16:37:38 +0000   Sun, 23 Apr 2023 16:23:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sun, 23 Apr 2023 16:37:38 +0000   Sun, 23 Apr 2023 16:23:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sun, 23 Apr 2023 16:37:38 +0000   Sun, 23 Apr 2023 16:23:42 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.16.187\n  Hostname:    ip-172-31-16-187\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8035892Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7933492Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec2bdb66f8fee1fb2044f43f0e60110f\n  System UUID:                ec2bdb66-f8fe-e1fb-2044-f43f0e60110f\n  Boot ID:                    f5addce6-ca95-4f48-9437-bf87e1a96fad\n  Kernel Version:             5.19.0-1023-aws\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nNon-terminated Pods:          (1 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-c5mcd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m21s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\n  hugepages-1Gi      0 (0%)    0 (0%)\n  hugepages-2Mi      0 (0%)    0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  RegisteredNode  16m   node-controller  Node ip-172-31-16-187 event: Registered Node ip-172-31-16-187 in Controller\n  Normal  RegisteredNode  15m   node-controller  Node ip-172-31-16-187 event: Registered Node ip-172-31-16-187 in Controller\n"
  Apr 23 16:39:36.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-191 describe namespace kubectl-191'
  Apr 23 16:39:36.738: INFO: stderr: ""
  Apr 23 16:39:36.738: INFO: stdout: "Name:         kubectl-191\nLabels:       e2e-framework=kubectl\n              e2e-run=cdc5b9e0-b085-436c-af1d-79cea00479c8\n              kubernetes.io/metadata.name=kubectl-191\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 23 16:39:36.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-191" for this suite. @ 04/23/23 16:39:36.744
• [3.809 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/23/23 16:39:36.754
  Apr 23 16:39:36.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 16:39:36.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:36.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:36.781
  STEP: Waiting for pod completion @ 04/23/23 16:39:36.801
  Apr 23 16:39:40.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1046" for this suite. @ 04/23/23 16:39:40.83
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/23/23 16:39:40.838
  Apr 23 16:39:40.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 16:39:40.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:39:40.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:39:40.877
  STEP: Creating a cronjob @ 04/23/23 16:39:40.88
  STEP: Ensuring more than one job is running at a time @ 04/23/23 16:39:40.898
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/23/23 16:41:00.903
  STEP: Removing cronjob @ 04/23/23 16:41:00.907
  Apr 23 16:41:00.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-459" for this suite. @ 04/23/23 16:41:00.919
• [80.089 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 04/23/23 16:41:00.93
  Apr 23 16:41:00.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 16:41:00.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:00.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:41:00.975
  Apr 23 16:41:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  W0423 16:41:00.989901      21 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc000ef7820 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0423 16:41:03.560519      21 warnings.go:70] unknown field "alpha"
  W0423 16:41:03.560543      21 warnings.go:70] unknown field "beta"
  W0423 16:41:03.560549      21 warnings.go:70] unknown field "delta"
  W0423 16:41:03.560555      21 warnings.go:70] unknown field "epsilon"
  W0423 16:41:03.560561      21 warnings.go:70] unknown field "gamma"
  Apr 23 16:41:03.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6024" for this suite. @ 04/23/23 16:41:03.618
• [2.697 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/23/23 16:41:03.627
  Apr 23 16:41:03.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 16:41:03.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:03.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:41:03.665
  STEP: Creating a test namespace @ 04/23/23 16:41:03.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:03.702
  STEP: Creating a pod in the namespace @ 04/23/23 16:41:03.706
  STEP: Waiting for the pod to have running status @ 04/23/23 16:41:03.721
  STEP: Deleting the namespace @ 04/23/23 16:41:05.73
  STEP: Waiting for the namespace to be removed. @ 04/23/23 16:41:05.738
  STEP: Recreating the namespace @ 04/23/23 16:41:16.742
  STEP: Verifying there are no pods in the namespace @ 04/23/23 16:41:16.766
  Apr 23 16:41:16.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-744" for this suite. @ 04/23/23 16:41:16.778
  STEP: Destroying namespace "nsdeletetest-8671" for this suite. @ 04/23/23 16:41:16.785
  Apr 23 16:41:16.790: INFO: Namespace nsdeletetest-8671 was already deleted
  STEP: Destroying namespace "nsdeletetest-6560" for this suite. @ 04/23/23 16:41:16.79
• [13.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/23/23 16:41:16.802
  Apr 23 16:41:16.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename tables @ 04/23/23 16:41:16.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:16.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:41:16.829
  Apr 23 16:41:16.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-3042" for this suite. @ 04/23/23 16:41:16.84
• [0.046 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/23/23 16:41:16.848
  Apr 23 16:41:16.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:41:16.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:16.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:41:16.869
  STEP: create deployment with httpd image @ 04/23/23 16:41:16.871
  Apr 23 16:41:16.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6025 create -f -'
  Apr 23 16:41:17.676: INFO: stderr: ""
  Apr 23 16:41:17.676: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/23/23 16:41:17.676
  Apr 23 16:41:17.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6025 diff -f -'
  Apr 23 16:41:17.926: INFO: rc: 1
  Apr 23 16:41:17.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6025 delete -f -'
  Apr 23 16:41:18.003: INFO: stderr: ""
  Apr 23 16:41:18.003: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 23 16:41:18.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6025" for this suite. @ 04/23/23 16:41:18.009
• [1.168 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/23/23 16:41:18.016
  Apr 23 16:41:18.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename dns @ 04/23/23 16:41:18.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:18.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:41:18.043
  STEP: Creating a test externalName service @ 04/23/23 16:41:18.046
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6297.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6297.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:41:18.054
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6297.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6297.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:41:18.054
  STEP: creating a pod to probe DNS @ 04/23/23 16:41:18.054
  STEP: submitting the pod to kubernetes @ 04/23/23 16:41:18.054
  STEP: retrieving the pod @ 04/23/23 16:41:24.093
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:41:24.101
  Apr 23 16:41:24.112: INFO: DNS probes using dns-test-37705f19-bd5f-458b-ba45-f5dd901ee8ca succeeded

  STEP: changing the externalName to bar.example.com @ 04/23/23 16:41:24.112
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6297.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6297.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:41:24.123
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6297.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6297.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:41:24.123
  STEP: creating a second pod to probe DNS @ 04/23/23 16:41:24.124
  STEP: submitting the pod to kubernetes @ 04/23/23 16:41:24.124
  STEP: retrieving the pod @ 04/23/23 16:41:30.158
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:41:30.164
  Apr 23 16:41:30.173: INFO: DNS probes using dns-test-87a078f4-f427-49ec-b094-507dfa59a670 succeeded

  STEP: changing the service to type=ClusterIP @ 04/23/23 16:41:30.173
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6297.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6297.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:41:30.19
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6297.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6297.svc.cluster.local; sleep 1; done
   @ 04/23/23 16:41:30.19
  STEP: creating a third pod to probe DNS @ 04/23/23 16:41:30.19
  STEP: submitting the pod to kubernetes @ 04/23/23 16:41:30.196
  STEP: retrieving the pod @ 04/23/23 16:41:40.243
  STEP: looking for the results for each expected name from probers @ 04/23/23 16:41:40.25
  Apr 23 16:41:40.264: INFO: DNS probes using dns-test-4a37209f-f0e6-4a98-b166-2780b9382aee succeeded

  Apr 23 16:41:40.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:41:40.269
  STEP: deleting the pod @ 04/23/23 16:41:40.293
  STEP: deleting the pod @ 04/23/23 16:41:40.311
  STEP: deleting the test externalName service @ 04/23/23 16:41:40.335
  STEP: Destroying namespace "dns-6297" for this suite. @ 04/23/23 16:41:40.354
• [22.348 seconds]
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/23/23 16:41:40.365
  Apr 23 16:41:40.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:41:40.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:40.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:41:40.395
  STEP: creating the pod @ 04/23/23 16:41:40.399
  STEP: submitting the pod to kubernetes @ 04/23/23 16:41:40.399
  STEP: verifying QOS class is set on the pod @ 04/23/23 16:41:40.413
  Apr 23 16:41:40.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3215" for this suite. @ 04/23/23 16:41:40.426
• [0.071 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/23/23 16:41:40.437
  Apr 23 16:41:40.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:41:40.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:40.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:41:40.482
  STEP: Creating a ResourceQuota with terminating scope @ 04/23/23 16:41:40.489
  STEP: Ensuring ResourceQuota status is calculated @ 04/23/23 16:41:40.495
  STEP: Creating a ResourceQuota with not terminating scope @ 04/23/23 16:41:42.501
  STEP: Ensuring ResourceQuota status is calculated @ 04/23/23 16:41:42.507
  STEP: Creating a long running pod @ 04/23/23 16:41:44.512
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/23/23 16:41:44.529
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/23/23 16:41:46.533
  STEP: Deleting the pod @ 04/23/23 16:41:48.54
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 16:41:48.555
  STEP: Creating a terminating pod @ 04/23/23 16:41:50.56
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/23/23 16:41:50.571
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/23/23 16:41:52.577
  STEP: Deleting the pod @ 04/23/23 16:41:54.582
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 16:41:54.601
  Apr 23 16:41:56.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9138" for this suite. @ 04/23/23 16:41:56.611
• [16.183 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/23/23 16:41:56.62
  Apr 23 16:41:56.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 16:41:56.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:41:56.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:41:56.654
  STEP: Creating a ForbidConcurrent cronjob @ 04/23/23 16:41:56.659
  STEP: Ensuring a job is scheduled @ 04/23/23 16:41:56.669
  STEP: Ensuring exactly one is scheduled @ 04/23/23 16:42:00.674
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/23/23 16:42:00.679
  STEP: Ensuring no more jobs are scheduled @ 04/23/23 16:42:00.683
  STEP: Removing cronjob @ 04/23/23 16:47:00.693
  Apr 23 16:47:00.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6062" for this suite. @ 04/23/23 16:47:00.707
• [304.095 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/23/23 16:47:00.716
  Apr 23 16:47:00.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 16:47:00.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:47:00.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:47:00.75
  STEP: Creating pod test-webserver-a6c09632-9e94-4cb8-8791-49ec81973fc9 in namespace container-probe-5840 @ 04/23/23 16:47:00.754
  Apr 23 16:47:02.777: INFO: Started pod test-webserver-a6c09632-9e94-4cb8-8791-49ec81973fc9 in namespace container-probe-5840
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 16:47:02.777
  Apr 23 16:47:02.780: INFO: Initial restart count of pod test-webserver-a6c09632-9e94-4cb8-8791-49ec81973fc9 is 0
  Apr 23 16:51:03.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 16:51:03.462
  STEP: Destroying namespace "container-probe-5840" for this suite. @ 04/23/23 16:51:03.48
• [242.774 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/23/23 16:51:03.49
  Apr 23 16:51:03.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:51:03.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:51:03.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:51:03.515
  Apr 23 16:51:03.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-7993 version'
  Apr 23 16:51:03.592: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 23 16:51:03.592: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-15T02:05:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 23 16:51:03.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7993" for this suite. @ 04/23/23 16:51:03.599
• [0.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/23/23 16:51:03.611
  Apr 23 16:51:03.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-preemption @ 04/23/23 16:51:03.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:51:03.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:51:03.638
  Apr 23 16:51:03.670: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 23 16:52:03.692: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/23/23 16:52:03.698
  Apr 23 16:52:03.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/23/23 16:52:03.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:03.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:03.733
  STEP: Finding an available node @ 04/23/23 16:52:03.737
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/23/23 16:52:03.737
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/23/23 16:52:05.767
  Apr 23 16:52:05.782: INFO: found a healthy node: ip-172-31-70-241
  Apr 23 16:52:11.887: INFO: pods created so far: [1 1 1]
  Apr 23 16:52:11.887: INFO: length of pods created so far: 3
  Apr 23 16:52:13.898: INFO: pods created so far: [2 2 1]
  Apr 23 16:52:20.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:52:20.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-9440" for this suite. @ 04/23/23 16:52:21.001
  STEP: Destroying namespace "sched-preemption-1230" for this suite. @ 04/23/23 16:52:21.013
• [77.410 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/23/23 16:52:21.031
  Apr 23 16:52:21.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 16:52:21.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:21.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:21.059
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:52:21.065
  STEP: Saw pod success @ 04/23/23 16:52:25.089
  Apr 23 16:52:25.093: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-7450fc51-33e8-49aa-a07e-a4644209e6b5 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:52:25.118
  Apr 23 16:52:25.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8507" for this suite. @ 04/23/23 16:52:25.143
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/23/23 16:52:25.155
  Apr 23 16:52:25.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 16:52:25.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:25.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:25.188
  STEP: Creating secret with name secret-test-248e9502-15f0-4566-ae9e-109ae768f839 @ 04/23/23 16:52:25.191
  STEP: Creating a pod to test consume secrets @ 04/23/23 16:52:25.197
  STEP: Saw pod success @ 04/23/23 16:52:29.219
  Apr 23 16:52:29.225: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-secrets-b36d6c66-4fdd-4024-82b6-4daed52681a1 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 16:52:29.232
  Apr 23 16:52:29.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-224" for this suite. @ 04/23/23 16:52:29.259
• [4.111 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 04/23/23 16:52:29.267
  Apr 23 16:52:29.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 16:52:29.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:29.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:29.295
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/23/23 16:52:29.32
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 16:52:29.329
  Apr 23 16:52:29.336: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:29.336: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:29.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 16:52:29.339: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 16:52:30.345: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:30.345: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:30.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 16:52:30.349: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 16:52:31.346: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:31.346: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:31.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 16:52:31.351: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 16:52:32.345: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:32.345: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:32.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 16:52:32.350: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 16:52:33.345: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:33.345: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:33.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 16:52:33.349: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 16:52:34.344: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:34.344: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:34.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 16:52:34.349: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 16:52:35.345: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:35.345: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:35.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 16:52:35.349: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/23/23 16:52:35.354
  Apr 23 16:52:35.376: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:35.376: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 16:52:35.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 16:52:35.382: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/23/23 16:52:35.382
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 16:52:35.396
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1738, will wait for the garbage collector to delete the pods @ 04/23/23 16:52:35.396
  Apr 23 16:52:35.463: INFO: Deleting DaemonSet.extensions daemon-set took: 9.389106ms
  Apr 23 16:52:35.563: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.475503ms
  Apr 23 16:52:37.770: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 16:52:37.770: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 16:52:37.774: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6540"},"items":null}

  Apr 23 16:52:37.783: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6540"},"items":null}

  Apr 23 16:52:37.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1738" for this suite. @ 04/23/23 16:52:37.814
• [8.558 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/23/23 16:52:37.826
  Apr 23 16:52:37.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename endpointslice @ 04/23/23 16:52:37.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:37.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:37.852
  Apr 23 16:52:39.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5141" for this suite. @ 04/23/23 16:52:39.938
• [2.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/23/23 16:52:39.947
  Apr 23 16:52:39.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 16:52:39.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:39.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:39.978
  STEP: Discovering how many secrets are in namespace by default @ 04/23/23 16:52:39.982
  STEP: Counting existing ResourceQuota @ 04/23/23 16:52:44.987
  STEP: Creating a ResourceQuota @ 04/23/23 16:52:49.991
  STEP: Ensuring resource quota status is calculated @ 04/23/23 16:52:49.999
  STEP: Creating a Secret @ 04/23/23 16:52:52.02
  STEP: Ensuring resource quota status captures secret creation @ 04/23/23 16:52:52.043
  STEP: Deleting a secret @ 04/23/23 16:52:54.048
  STEP: Ensuring resource quota status released usage @ 04/23/23 16:52:54.056
  Apr 23 16:52:56.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8190" for this suite. @ 04/23/23 16:52:56.065
• [16.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/23/23 16:52:56.075
  Apr 23 16:52:56.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename deployment @ 04/23/23 16:52:56.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:56.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:56.1
  Apr 23 16:52:56.102: INFO: Creating simple deployment test-new-deployment
  Apr 23 16:52:56.135: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 04/23/23 16:52:58.161
  STEP: updating a scale subresource @ 04/23/23 16:52:58.167
  STEP: verifying the deployment Spec.Replicas was modified @ 04/23/23 16:52:58.182
  STEP: Patch a scale subresource @ 04/23/23 16:52:58.187
  Apr 23 16:52:58.218: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-7833  33b8cc7e-67d8-49d8-8a92-a481ef778a37 6678 3 2023-04-23 16:52:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-23 16:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 16:52:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d62c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-23 16:52:57 +0000 UTC,LastTransitionTime:2023-04-23 16:52:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-04-23 16:52:57 +0000 UTC,LastTransitionTime:2023-04-23 16:52:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 23 16:52:58.223: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-7833  0e2b28e6-6b23-4842-a383-457bb0b0a95d 6683 3 2023-04-23 16:52:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 33b8cc7e-67d8-49d8-8a92-a481ef778a37 0xc004e1e8f0 0xc004e1e8f1}] [] [{kube-controller-manager Update apps/v1 2023-04-23 16:52:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-23 16:52:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33b8cc7e-67d8-49d8-8a92-a481ef778a37\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e1e978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 16:52:58.233: INFO: Pod "test-new-deployment-67bd4bf6dc-2npg9" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-2npg9 test-new-deployment-67bd4bf6dc- deployment-7833  97966b16-e68d-4371-b004-c4c0503a0139 6671 0 2023-04-23 16:52:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 0e2b28e6-6b23-4842-a383-457bb0b0a95d 0xc004d63070 0xc004d63071}] [] [{kube-controller-manager Update v1 2023-04-23 16:52:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e2b28e6-6b23-4842-a383-457bb0b0a95d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 16:52:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bd4d4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bd4d4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:52:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:52:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:52:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:52:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.234,StartTime:2023-04-23 16:52:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 16:52:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d965316ddb03dad68e8900d92b354248f11784a2db2cdf4b6a115e76205983f7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.234,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:52:58.233: INFO: Pod "test-new-deployment-67bd4bf6dc-l5nwx" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-l5nwx test-new-deployment-67bd4bf6dc- deployment-7833  4ae96ddc-d541-4503-8334-6f1c7a3c1e61 6681 0 2023-04-23 16:52:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 0e2b28e6-6b23-4842-a383-457bb0b0a95d 0xc004d63257 0xc004d63258}] [] [{kube-controller-manager Update v1 2023-04-23 16:52:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e2b28e6-6b23-4842-a383-457bb0b0a95d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zn2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zn2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 16:52:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 16:52:58.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7833" for this suite. @ 04/23/23 16:52:58.246
• [2.185 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/23/23 16:52:58.26
  Apr 23 16:52:58.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:52:58.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:52:58.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:52:58.302
  STEP: Setting up server cert @ 04/23/23 16:52:58.335
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:52:58.635
  STEP: Deploying the webhook pod @ 04/23/23 16:52:58.647
  STEP: Wait for the deployment to be ready @ 04/23/23 16:52:58.664
  Apr 23 16:52:58.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/23/23 16:53:00.687
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:53:00.699
  Apr 23 16:53:01.699: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/23/23 16:53:01.705
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/23/23 16:53:01.731
  STEP: Creating a configMap that should not be mutated @ 04/23/23 16:53:01.744
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/23/23 16:53:01.757
  STEP: Creating a configMap that should be mutated @ 04/23/23 16:53:01.767
  Apr 23 16:53:01.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1114" for this suite. @ 04/23/23 16:53:01.871
  STEP: Destroying namespace "webhook-markers-3690" for this suite. @ 04/23/23 16:53:01.882
• [3.635 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/23/23 16:53:01.895
  Apr 23 16:53:01.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 16:53:01.896
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:53:01.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:53:01.92
  STEP: Deleting RuntimeClass runtimeclass-8548-delete-me @ 04/23/23 16:53:01.931
  STEP: Waiting for the RuntimeClass to disappear @ 04/23/23 16:53:01.941
  Apr 23 16:53:01.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8548" for this suite. @ 04/23/23 16:53:01.96
• [0.074 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/23/23 16:53:01.97
  Apr 23 16:53:01.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 16:53:01.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:53:01.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:53:01.996
  STEP: Creating a test namespace @ 04/23/23 16:53:02
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:53:02.023
  STEP: Creating a service in the namespace @ 04/23/23 16:53:02.028
  STEP: Deleting the namespace @ 04/23/23 16:53:02.042
  STEP: Waiting for the namespace to be removed. @ 04/23/23 16:53:02.051
  STEP: Recreating the namespace @ 04/23/23 16:53:08.056
  STEP: Verifying there is no service in the namespace @ 04/23/23 16:53:08.083
  Apr 23 16:53:08.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3219" for this suite. @ 04/23/23 16:53:08.102
  STEP: Destroying namespace "nsdeletetest-7845" for this suite. @ 04/23/23 16:53:08.127
  Apr 23 16:53:08.131: INFO: Namespace nsdeletetest-7845 was already deleted
  STEP: Destroying namespace "nsdeletetest-7228" for this suite. @ 04/23/23 16:53:08.131
• [6.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 04/23/23 16:53:08.14
  Apr 23 16:53:08.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 16:53:08.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:53:08.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:53:08.167
  STEP: Creating service test in namespace statefulset-5209 @ 04/23/23 16:53:08.171
  STEP: Creating a new StatefulSet @ 04/23/23 16:53:08.177
  Apr 23 16:53:08.188: INFO: Found 0 stateful pods, waiting for 3
  Apr 23 16:53:18.195: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 16:53:18.195: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 16:53:18.195: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 16:53:18.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-5209 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 16:53:18.364: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 16:53:18.364: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 16:53:18.364: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/23/23 16:53:28.39
  Apr 23 16:53:28.415: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/23/23 16:53:28.415
  STEP: Updating Pods in reverse ordinal order @ 04/23/23 16:53:38.436
  Apr 23 16:53:38.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-5209 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 16:53:38.612: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 16:53:38.612: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 16:53:38.612: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 04/23/23 16:53:58.64
  Apr 23 16:53:58.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-5209 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 16:53:58.774: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 16:53:58.774: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 16:53:58.774: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 16:54:08.815: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 04/23/23 16:54:18.832
  Apr 23 16:54:18.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-5209 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 16:54:18.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 16:54:18.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 16:54:18.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 16:54:29.034: INFO: Deleting all statefulset in ns statefulset-5209
  Apr 23 16:54:29.039: INFO: Scaling statefulset ss2 to 0
  Apr 23 16:54:39.060: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 16:54:39.065: INFO: Deleting statefulset ss2
  Apr 23 16:54:39.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5209" for this suite. @ 04/23/23 16:54:39.089
• [90.960 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/23/23 16:54:39.101
  Apr 23 16:54:39.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/23/23 16:54:39.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:39.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:39.132
  STEP: creating @ 04/23/23 16:54:39.136
  STEP: getting @ 04/23/23 16:54:39.162
  STEP: listing in namespace @ 04/23/23 16:54:39.167
  STEP: patching @ 04/23/23 16:54:39.171
  STEP: deleting @ 04/23/23 16:54:39.187
  Apr 23 16:54:39.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9690" for this suite. @ 04/23/23 16:54:39.216
• [0.139 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/23/23 16:54:39.24
  Apr 23 16:54:39.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename watch @ 04/23/23 16:54:39.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:39.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:39.287
  STEP: creating a watch on configmaps @ 04/23/23 16:54:39.292
  STEP: creating a new configmap @ 04/23/23 16:54:39.301
  STEP: modifying the configmap once @ 04/23/23 16:54:39.31
  STEP: closing the watch once it receives two notifications @ 04/23/23 16:54:39.323
  Apr 23 16:54:39.323: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-967  42297c24-dfc5-43dd-a1df-2a12b4e0dd6f 7490 0 2023-04-23 16:54:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-23 16:54:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:54:39.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-967  42297c24-dfc5-43dd-a1df-2a12b4e0dd6f 7491 0 2023-04-23 16:54:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-23 16:54:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/23/23 16:54:39.323
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/23/23 16:54:39.355
  STEP: deleting the configmap @ 04/23/23 16:54:39.357
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/23/23 16:54:39.379
  Apr 23 16:54:39.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-967  42297c24-dfc5-43dd-a1df-2a12b4e0dd6f 7492 0 2023-04-23 16:54:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-23 16:54:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:54:39.379: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-967  42297c24-dfc5-43dd-a1df-2a12b4e0dd6f 7493 0 2023-04-23 16:54:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-23 16:54:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 16:54:39.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-967" for this suite. @ 04/23/23 16:54:39.386
• [0.159 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/23/23 16:54:39.403
  Apr 23 16:54:39.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:54:39.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:39.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:39.436
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 16:54:39.443
  STEP: Saw pod success @ 04/23/23 16:54:43.471
  Apr 23 16:54:43.476: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-2f59ad2e-cf4d-4822-9d7c-ccc59c877f8c container client-container: <nil>
  STEP: delete the pod @ 04/23/23 16:54:43.501
  Apr 23 16:54:43.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7392" for this suite. @ 04/23/23 16:54:43.526
• [4.132 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/23/23 16:54:43.535
  Apr 23 16:54:43.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 16:54:43.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:43.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:43.569
  STEP: create the container @ 04/23/23 16:54:43.576
  W0423 16:54:43.597318      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/23/23 16:54:43.597
  STEP: get the container status @ 04/23/23 16:54:46.62
  STEP: the container should be terminated @ 04/23/23 16:54:46.625
  STEP: the termination message should be set @ 04/23/23 16:54:46.625
  Apr 23 16:54:46.625: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/23/23 16:54:46.625
  Apr 23 16:54:46.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2472" for this suite. @ 04/23/23 16:54:46.651
• [3.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/23/23 16:54:46.662
  Apr 23 16:54:46.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 16:54:46.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:46.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:46.692
  STEP: creating a Namespace @ 04/23/23 16:54:46.695
  STEP: patching the Namespace @ 04/23/23 16:54:46.715
  STEP: get the Namespace and ensuring it has the label @ 04/23/23 16:54:46.726
  Apr 23 16:54:46.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6797" for this suite. @ 04/23/23 16:54:46.739
  STEP: Destroying namespace "nspatchtest-07afe58e-6b25-4074-9283-7355464e242a-2695" for this suite. @ 04/23/23 16:54:46.748
• [0.094 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/23/23 16:54:46.757
  Apr 23 16:54:46.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 16:54:46.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:46.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:46.781
  STEP: Setting up server cert @ 04/23/23 16:54:46.814
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 16:54:47.567
  STEP: Deploying the webhook pod @ 04/23/23 16:54:47.576
  STEP: Wait for the deployment to be ready @ 04/23/23 16:54:47.592
  Apr 23 16:54:47.605: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/23/23 16:54:49.62
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:54:49.632
  Apr 23 16:54:50.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/23/23 16:54:50.637
  STEP: create a namespace for the webhook @ 04/23/23 16:54:50.663
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/23/23 16:54:50.684
  Apr 23 16:54:50.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-51" for this suite. @ 04/23/23 16:54:50.841
  STEP: Destroying namespace "webhook-markers-5703" for this suite. @ 04/23/23 16:54:50.851
  STEP: Destroying namespace "fail-closed-namespace-605" for this suite. @ 04/23/23 16:54:50.86
• [4.112 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/23/23 16:54:50.869
  Apr 23 16:54:50.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 16:54:50.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:50.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:50.893
  STEP: creating the pod @ 04/23/23 16:54:50.897
  STEP: setting up watch @ 04/23/23 16:54:50.897
  STEP: submitting the pod to kubernetes @ 04/23/23 16:54:51.002
  STEP: verifying the pod is in kubernetes @ 04/23/23 16:54:51.013
  STEP: verifying pod creation was observed @ 04/23/23 16:54:51.018
  STEP: deleting the pod gracefully @ 04/23/23 16:54:53.031
  STEP: verifying pod deletion was observed @ 04/23/23 16:54:53.046
  Apr 23 16:54:54.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9011" for this suite. @ 04/23/23 16:54:54.792
• [3.934 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/23/23 16:54:54.804
  Apr 23 16:54:54.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 16:54:54.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:54:54.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:54:54.836
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/23/23 16:54:54.84
  Apr 23 16:54:54.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:54:56.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:55:01.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6575" for this suite. @ 04/23/23 16:55:01.981
• [7.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/23/23 16:55:01.99
  Apr 23 16:55:01.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 16:55:01.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:02.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:02.02
  STEP: create the container @ 04/23/23 16:55:02.024
  W0423 16:55:02.046082      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/23/23 16:55:02.046
  STEP: get the container status @ 04/23/23 16:55:06.076
  STEP: the container should be terminated @ 04/23/23 16:55:06.08
  STEP: the termination message should be set @ 04/23/23 16:55:06.08
  Apr 23 16:55:06.080: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/23/23 16:55:06.08
  Apr 23 16:55:06.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6118" for this suite. @ 04/23/23 16:55:06.109
• [4.127 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/23/23 16:55:06.118
  Apr 23 16:55:06.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pod-network-test @ 04/23/23 16:55:06.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:06.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:06.143
  STEP: Performing setup for networking test in namespace pod-network-test-8588 @ 04/23/23 16:55:06.146
  STEP: creating a selector @ 04/23/23 16:55:06.146
  STEP: Creating the service pods in kubernetes @ 04/23/23 16:55:06.147
  Apr 23 16:55:06.147: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/23/23 16:55:28.278
  Apr 23 16:55:30.317: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 23 16:55:30.317: INFO: Going to poll 192.168.21.13 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 16:55:30.321: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.21.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8588 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:55:30.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:55:30.322: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:55:30.322: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8588/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.21.13+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 16:55:31.387: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 23 16:55:31.387: INFO: Going to poll 192.168.6.244 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 16:55:31.391: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.6.244 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8588 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:55:31.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:55:31.392: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:55:31.392: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8588/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.6.244+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 16:55:32.490: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 23 16:55:32.490: INFO: Going to poll 192.168.122.141 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 16:55:32.495: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.122.141 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8588 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 16:55:32.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:55:32.496: INFO: ExecWithOptions: Clientset creation
  Apr 23 16:55:32.496: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8588/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.122.141+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 16:55:33.590: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 23 16:55:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8588" for this suite. @ 04/23/23 16:55:33.595
• [27.486 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/23/23 16:55:33.605
  Apr 23 16:55:33.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 16:55:33.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:33.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:33.651
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9059 @ 04/23/23 16:55:33.66
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/23/23 16:55:33.687
  STEP: creating service externalsvc in namespace services-9059 @ 04/23/23 16:55:33.687
  STEP: creating replication controller externalsvc in namespace services-9059 @ 04/23/23 16:55:33.712
  I0423 16:55:33.727044      21 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9059, replica count: 2
  I0423 16:55:36.778628      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/23/23 16:55:36.783
  Apr 23 16:55:36.800: INFO: Creating new exec pod
  Apr 23 16:55:38.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-9059 exec execpod8dkb7 -- /bin/sh -x -c nslookup clusterip-service.services-9059.svc.cluster.local'
  Apr 23 16:55:39.153: INFO: stderr: "+ nslookup clusterip-service.services-9059.svc.cluster.local\n"
  Apr 23 16:55:39.153: INFO: stdout: "Server:\t\t10.152.183.245\nAddress:\t10.152.183.245#53\n\nclusterip-service.services-9059.svc.cluster.local\tcanonical name = externalsvc.services-9059.svc.cluster.local.\nName:\texternalsvc.services-9059.svc.cluster.local\nAddress: 10.152.183.78\n\n"
  Apr 23 16:55:39.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9059, will wait for the garbage collector to delete the pods @ 04/23/23 16:55:39.159
  Apr 23 16:55:39.221: INFO: Deleting ReplicationController externalsvc took: 8.298577ms
  Apr 23 16:55:39.322: INFO: Terminating ReplicationController externalsvc pods took: 100.466869ms
  Apr 23 16:55:41.750: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-9059" for this suite. @ 04/23/23 16:55:41.773
• [8.177 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/23/23 16:55:41.783
  Apr 23 16:55:41.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-webhook @ 04/23/23 16:55:41.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:41.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:41.888
  STEP: Setting up server cert @ 04/23/23 16:55:41.892
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/23/23 16:55:42.526
  STEP: Deploying the custom resource conversion webhook pod @ 04/23/23 16:55:42.539
  STEP: Wait for the deployment to be ready @ 04/23/23 16:55:42.556
  Apr 23 16:55:42.565: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/23/23 16:55:44.584
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 16:55:44.602
  Apr 23 16:55:45.604: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 23 16:55:45.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Creating a v1 custom resource @ 04/23/23 16:55:48.211
  STEP: Create a v2 custom resource @ 04/23/23 16:55:48.25
  STEP: List CRs in v1 @ 04/23/23 16:55:48.316
  STEP: List CRs in v2 @ 04/23/23 16:55:48.324
  Apr 23 16:55:48.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-3962" for this suite. @ 04/23/23 16:55:48.902
• [7.126 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/23/23 16:55:48.91
  Apr 23 16:55:48.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 16:55:48.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:48.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:48.945
  STEP: Create a ReplicaSet @ 04/23/23 16:55:48.949
  STEP: Verify that the required pods have come up @ 04/23/23 16:55:48.956
  Apr 23 16:55:48.960: INFO: Pod name sample-pod: Found 0 pods out of 3
  Apr 23 16:55:53.967: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/23/23 16:55:53.968
  Apr 23 16:55:53.972: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/23/23 16:55:53.972
  STEP: DeleteCollection of the ReplicaSets @ 04/23/23 16:55:53.977
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/23/23 16:55:53.987
  Apr 23 16:55:53.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5588" for this suite. @ 04/23/23 16:55:54
• [5.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/23/23 16:55:54.024
  Apr 23 16:55:54.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:55:54.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:54.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:54.056
  STEP: starting the proxy server @ 04/23/23 16:55:54.063
  Apr 23 16:55:54.063: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6449 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/23/23 16:55:54.191
  Apr 23 16:55:54.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6449" for this suite. @ 04/23/23 16:55:54.216
• [0.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/23/23 16:55:54.23
  Apr 23 16:55:54.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 16:55:54.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:55:54.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:55:54.264
  STEP: Creating a ReplaceConcurrent cronjob @ 04/23/23 16:55:54.27
  STEP: Ensuring a job is scheduled @ 04/23/23 16:55:54.279
  STEP: Ensuring exactly one is scheduled @ 04/23/23 16:56:00.284
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/23/23 16:56:00.289
  STEP: Ensuring the job is replaced with a new one @ 04/23/23 16:56:00.293
  STEP: Removing cronjob @ 04/23/23 16:57:00.3
  Apr 23 16:57:00.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5109" for this suite. @ 04/23/23 16:57:00.312
• [66.094 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/23/23 16:57:00.325
  Apr 23 16:57:00.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:57:00.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:57:00.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:57:00.366
  STEP: creating a replication controller @ 04/23/23 16:57:00.369
  Apr 23 16:57:00.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 create -f -'
  Apr 23 16:57:00.929: INFO: stderr: ""
  Apr 23 16:57:00.929: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/23/23 16:57:00.929
  Apr 23 16:57:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:57:01.026: INFO: stderr: ""
  Apr 23 16:57:01.026: INFO: stdout: "update-demo-nautilus-dt58n update-demo-nautilus-mjcfx "
  Apr 23 16:57:01.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-dt58n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:57:01.121: INFO: stderr: ""
  Apr 23 16:57:01.121: INFO: stdout: ""
  Apr 23 16:57:01.121: INFO: update-demo-nautilus-dt58n is created but not running
  Apr 23 16:57:06.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:57:06.225: INFO: stderr: ""
  Apr 23 16:57:06.225: INFO: stdout: "update-demo-nautilus-dt58n update-demo-nautilus-mjcfx "
  Apr 23 16:57:06.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-dt58n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:57:06.355: INFO: stderr: ""
  Apr 23 16:57:06.355: INFO: stdout: "true"
  Apr 23 16:57:06.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-dt58n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:57:06.506: INFO: stderr: ""
  Apr 23 16:57:06.506: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:57:06.506: INFO: validating pod update-demo-nautilus-dt58n
  Apr 23 16:57:06.512: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:57:06.512: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:57:06.512: INFO: update-demo-nautilus-dt58n is verified up and running
  Apr 23 16:57:06.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-mjcfx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:57:06.596: INFO: stderr: ""
  Apr 23 16:57:06.596: INFO: stdout: "true"
  Apr 23 16:57:06.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-mjcfx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:57:06.683: INFO: stderr: ""
  Apr 23 16:57:06.683: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:57:06.683: INFO: validating pod update-demo-nautilus-mjcfx
  Apr 23 16:57:06.690: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:57:06.690: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:57:06.690: INFO: update-demo-nautilus-mjcfx is verified up and running
  STEP: scaling down the replication controller @ 04/23/23 16:57:06.69
  Apr 23 16:57:06.691: INFO: scanned /root for discovery docs: <nil>
  Apr 23 16:57:06.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Apr 23 16:57:07.800: INFO: stderr: ""
  Apr 23 16:57:07.800: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/23/23 16:57:07.8
  Apr 23 16:57:07.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:57:07.884: INFO: stderr: ""
  Apr 23 16:57:07.884: INFO: stdout: "update-demo-nautilus-dt58n update-demo-nautilus-mjcfx "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 04/23/23 16:57:07.884
  Apr 23 16:57:12.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:57:12.955: INFO: stderr: ""
  Apr 23 16:57:12.955: INFO: stdout: "update-demo-nautilus-mjcfx "
  Apr 23 16:57:12.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-mjcfx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:57:13.027: INFO: stderr: ""
  Apr 23 16:57:13.027: INFO: stdout: "true"
  Apr 23 16:57:13.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-mjcfx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:57:13.098: INFO: stderr: ""
  Apr 23 16:57:13.098: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:57:13.098: INFO: validating pod update-demo-nautilus-mjcfx
  Apr 23 16:57:13.104: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:57:13.104: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:57:13.104: INFO: update-demo-nautilus-mjcfx is verified up and running
  STEP: scaling up the replication controller @ 04/23/23 16:57:13.104
  Apr 23 16:57:13.107: INFO: scanned /root for discovery docs: <nil>
  Apr 23 16:57:13.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Apr 23 16:57:14.203: INFO: stderr: ""
  Apr 23 16:57:14.203: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/23/23 16:57:14.203
  Apr 23 16:57:14.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:57:14.269: INFO: stderr: ""
  Apr 23 16:57:14.269: INFO: stdout: "update-demo-nautilus-bb8mw update-demo-nautilus-mjcfx "
  Apr 23 16:57:14.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-bb8mw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:57:14.332: INFO: stderr: ""
  Apr 23 16:57:14.332: INFO: stdout: ""
  Apr 23 16:57:14.332: INFO: update-demo-nautilus-bb8mw is created but not running
  Apr 23 16:57:19.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 16:57:19.426: INFO: stderr: ""
  Apr 23 16:57:19.426: INFO: stdout: "update-demo-nautilus-bb8mw update-demo-nautilus-mjcfx "
  Apr 23 16:57:19.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-bb8mw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:57:19.502: INFO: stderr: ""
  Apr 23 16:57:19.502: INFO: stdout: "true"
  Apr 23 16:57:19.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-bb8mw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:57:19.574: INFO: stderr: ""
  Apr 23 16:57:19.574: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:57:19.574: INFO: validating pod update-demo-nautilus-bb8mw
  Apr 23 16:57:19.581: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:57:19.581: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:57:19.581: INFO: update-demo-nautilus-bb8mw is verified up and running
  Apr 23 16:57:19.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-mjcfx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 16:57:19.670: INFO: stderr: ""
  Apr 23 16:57:19.670: INFO: stdout: "true"
  Apr 23 16:57:19.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods update-demo-nautilus-mjcfx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 16:57:19.748: INFO: stderr: ""
  Apr 23 16:57:19.748: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 16:57:19.748: INFO: validating pod update-demo-nautilus-mjcfx
  Apr 23 16:57:19.753: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 16:57:19.753: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 16:57:19.753: INFO: update-demo-nautilus-mjcfx is verified up and running
  STEP: using delete to clean up resources @ 04/23/23 16:57:19.753
  Apr 23 16:57:19.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 delete --grace-period=0 --force -f -'
  Apr 23 16:57:19.832: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 16:57:19.832: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 23 16:57:19.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get rc,svc -l name=update-demo --no-headers'
  Apr 23 16:57:19.945: INFO: stderr: "No resources found in kubectl-6876 namespace.\n"
  Apr 23 16:57:19.945: INFO: stdout: ""
  Apr 23 16:57:19.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6876 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 23 16:57:20.078: INFO: stderr: ""
  Apr 23 16:57:20.078: INFO: stdout: ""
  Apr 23 16:57:20.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6876" for this suite. @ 04/23/23 16:57:20.083
• [19.770 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/23/23 16:57:20.096
  Apr 23 16:57:20.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 16:57:20.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:57:20.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:57:20.129
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-2273 @ 04/23/23 16:57:20.133
  STEP: changing the ExternalName service to type=NodePort @ 04/23/23 16:57:20.14
  STEP: creating replication controller externalname-service in namespace services-2273 @ 04/23/23 16:57:20.187
  I0423 16:57:20.196163      21 runners.go:194] Created replication controller with name: externalname-service, namespace: services-2273, replica count: 2
  I0423 16:57:23.246949      21 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:57:23.246: INFO: Creating new exec pod
  Apr 23 16:57:26.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-2273 exec execpodj4t6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 23 16:57:26.482: INFO: stderr: "+ + echonc -v hostName\n -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 23 16:57:26.482: INFO: stdout: "externalname-service-shwf5"
  Apr 23 16:57:26.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-2273 exec execpodj4t6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.62 80'
  Apr 23 16:57:26.743: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.62 80\nConnection to 10.152.183.62 80 port [tcp/http] succeeded!\n"
  Apr 23 16:57:26.743: INFO: stdout: ""
  Apr 23 16:57:27.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-2273 exec execpodj4t6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.62 80'
  Apr 23 16:57:27.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.62 80\nConnection to 10.152.183.62 80 port [tcp/http] succeeded!\n"
  Apr 23 16:57:27.890: INFO: stdout: ""
  Apr 23 16:57:28.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-2273 exec execpodj4t6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.62 80'
  Apr 23 16:57:28.887: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.62 80\nConnection to 10.152.183.62 80 port [tcp/http] succeeded!\n"
  Apr 23 16:57:28.887: INFO: stdout: "externalname-service-shwf5"
  Apr 23 16:57:28.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-2273 exec execpodj4t6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.70.241 30982'
  Apr 23 16:57:29.023: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.70.241 30982\nConnection to 172.31.70.241 30982 port [tcp/*] succeeded!\n"
  Apr 23 16:57:29.023: INFO: stdout: "externalname-service-shwf5"
  Apr 23 16:57:29.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-2273 exec execpodj4t6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.37.26 30982'
  Apr 23 16:57:29.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.37.26 30982\nConnection to 172.31.37.26 30982 port [tcp/*] succeeded!\n"
  Apr 23 16:57:29.160: INFO: stdout: "externalname-service-mbxqs"
  Apr 23 16:57:29.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 16:57:29.166: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-2273" for this suite. @ 04/23/23 16:57:29.192
• [9.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/23/23 16:57:29.203
  Apr 23 16:57:29.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename init-container @ 04/23/23 16:57:29.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:57:29.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:57:29.246
  STEP: creating the pod @ 04/23/23 16:57:29.25
  Apr 23 16:57:29.250: INFO: PodSpec: initContainers in spec.initContainers
  Apr 23 16:58:17.324: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a6a01e80-4c03-4a43-b48f-e1cd91768bc0", GenerateName:"", Namespace:"init-container-6508", SelfLink:"", UID:"c7208a29-cc26-4c60-8c75-13dfb5be6bd2", ResourceVersion:"9020", Generation:0, CreationTimestamp:time.Date(2023, time.April, 23, 16, 57, 29, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"250340603"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 23, 16, 57, 29, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00070c198), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 23, 16, 58, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00070c1c8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-9knbn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004568220), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9knbn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9knbn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9knbn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00404a370), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-70-241", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005301c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00404a400)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00404a420)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00404a428), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00404a42c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000b6e100), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 23, 16, 57, 29, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 23, 16, 57, 29, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 23, 16, 57, 29, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 23, 16, 57, 29, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.70.241", PodIP:"192.168.6.254", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.6.254"}}, StartTime:time.Date(2023, time.April, 23, 16, 57, 29, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005302a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000530380)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://e04703b49124abeaa4b4eded622f58a935e7ebeaa66bb7334255cd21b461bb8d", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004568320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0045682e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00404a4b4), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 23 16:58:17.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6508" for this suite. @ 04/23/23 16:58:17.331
• [48.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/23/23 16:58:17.339
  Apr 23 16:58:17.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 16:58:17.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:58:17.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:58:17.372
  STEP: creating a Service @ 04/23/23 16:58:17.381
  STEP: watching for the Service to be added @ 04/23/23 16:58:17.392
  Apr 23 16:58:17.394: INFO: Found Service test-service-42ft7 in namespace services-1681 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 23 16:58:17.394: INFO: Service test-service-42ft7 created
  STEP: Getting /status @ 04/23/23 16:58:17.394
  Apr 23 16:58:17.401: INFO: Service test-service-42ft7 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/23/23 16:58:17.401
  STEP: watching for the Service to be patched @ 04/23/23 16:58:17.408
  Apr 23 16:58:17.410: INFO: observed Service test-service-42ft7 in namespace services-1681 with annotations: map[] & LoadBalancer: {[]}
  Apr 23 16:58:17.410: INFO: Found Service test-service-42ft7 in namespace services-1681 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 23 16:58:17.410: INFO: Service test-service-42ft7 has service status patched
  STEP: updating the ServiceStatus @ 04/23/23 16:58:17.41
  Apr 23 16:58:17.426: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/23/23 16:58:17.426
  Apr 23 16:58:17.432: INFO: Observed Service test-service-42ft7 in namespace services-1681 with annotations: map[] & Conditions: {[]}
  Apr 23 16:58:17.432: INFO: Observed event: &Service{ObjectMeta:{test-service-42ft7  services-1681  dac70b82-67aa-4f51-9fa5-cc4f12490600 9030 0 2023-04-23 16:58:17 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-23 16:58:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-23 16:58:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.94,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.94],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 23 16:58:17.432: INFO: Found Service test-service-42ft7 in namespace services-1681 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 23 16:58:17.432: INFO: Service test-service-42ft7 has service status updated
  STEP: patching the service @ 04/23/23 16:58:17.432
  STEP: watching for the Service to be patched @ 04/23/23 16:58:17.446
  Apr 23 16:58:17.448: INFO: observed Service test-service-42ft7 in namespace services-1681 with labels: map[test-service-static:true]
  Apr 23 16:58:17.448: INFO: observed Service test-service-42ft7 in namespace services-1681 with labels: map[test-service-static:true]
  Apr 23 16:58:17.448: INFO: observed Service test-service-42ft7 in namespace services-1681 with labels: map[test-service-static:true]
  Apr 23 16:58:17.448: INFO: Found Service test-service-42ft7 in namespace services-1681 with labels: map[test-service:patched test-service-static:true]
  Apr 23 16:58:17.448: INFO: Service test-service-42ft7 patched
  STEP: deleting the service @ 04/23/23 16:58:17.448
  STEP: watching for the Service to be deleted @ 04/23/23 16:58:17.462
  Apr 23 16:58:17.464: INFO: Observed event: ADDED
  Apr 23 16:58:17.464: INFO: Observed event: MODIFIED
  Apr 23 16:58:17.464: INFO: Observed event: MODIFIED
  Apr 23 16:58:17.464: INFO: Observed event: MODIFIED
  Apr 23 16:58:17.464: INFO: Found Service test-service-42ft7 in namespace services-1681 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 23 16:58:17.464: INFO: Service test-service-42ft7 deleted
  Apr 23 16:58:17.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1681" for this suite. @ 04/23/23 16:58:17.478
• [0.146 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/23/23 16:58:17.487
  Apr 23 16:58:17.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 16:58:17.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:58:17.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:58:17.509
  STEP: validating api versions @ 04/23/23 16:58:17.512
  Apr 23 16:58:17.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-5377 api-versions'
  Apr 23 16:58:17.575: INFO: stderr: ""
  Apr 23 16:58:17.575: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Apr 23 16:58:17.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5377" for this suite. @ 04/23/23 16:58:17.58
• [0.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/23/23 16:58:17.59
  Apr 23 16:58:17.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 16:58:17.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:58:17.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:58:17.611
  STEP: Creating a pod to test downward api env vars @ 04/23/23 16:58:17.613
  STEP: Saw pod success @ 04/23/23 16:58:21.64
  Apr 23 16:58:21.644: INFO: Trying to get logs from node ip-172-31-70-241 pod downward-api-fe839596-33fa-482b-9ee2-63d9041830cf container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 16:58:21.668
  Apr 23 16:58:21.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7494" for this suite. @ 04/23/23 16:58:21.694
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/23/23 16:58:21.704
  Apr 23 16:58:21.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 16:58:21.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:58:21.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:58:21.73
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/23/23 16:58:21.737
  Apr 23 16:58:21.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/23/23 16:58:27.554
  Apr 23 16:58:27.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:58:28.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 16:58:34.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-37" for this suite. @ 04/23/23 16:58:34.446
• [12.753 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/23/23 16:58:34.457
  Apr 23 16:58:34.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 16:58:34.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:58:34.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:58:34.482
  STEP: Updating Namespace "namespaces-8227" @ 04/23/23 16:58:34.494
  Apr 23 16:58:34.504: INFO: Namespace "namespaces-8227" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"cdc5b9e0-b085-436c-af1d-79cea00479c8", "kubernetes.io/metadata.name":"namespaces-8227", "namespaces-8227":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 23 16:58:34.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8227" for this suite. @ 04/23/23 16:58:34.508
• [0.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/23/23 16:58:34.523
  Apr 23 16:58:34.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename init-container @ 04/23/23 16:58:34.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:58:34.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:58:34.55
  STEP: creating the pod @ 04/23/23 16:58:34.553
  Apr 23 16:58:34.553: INFO: PodSpec: initContainers in spec.initContainers
  Apr 23 16:58:38.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-103" for this suite. @ 04/23/23 16:58:38.772
• [4.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 04/23/23 16:58:38.781
  Apr 23 16:58:38.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 16:58:38.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:58:38.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:58:38.807
  STEP: Creating service test in namespace statefulset-3319 @ 04/23/23 16:58:38.811
  STEP: Looking for a node to schedule stateful set and pod @ 04/23/23 16:58:38.817
  STEP: Creating pod with conflicting port in namespace statefulset-3319 @ 04/23/23 16:58:38.837
  STEP: Waiting until pod test-pod will start running in namespace statefulset-3319 @ 04/23/23 16:58:38.849
  STEP: Creating statefulset with conflicting port in namespace statefulset-3319 @ 04/23/23 16:58:40.859
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3319 @ 04/23/23 16:58:40.866
  Apr 23 16:58:40.881: INFO: Observed stateful pod in namespace: statefulset-3319, name: ss-0, uid: 13905fe5-20aa-4650-8fd8-67d83dea7345, status phase: Pending. Waiting for statefulset controller to delete.
  Apr 23 16:58:40.902: INFO: Observed stateful pod in namespace: statefulset-3319, name: ss-0, uid: 13905fe5-20aa-4650-8fd8-67d83dea7345, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 23 16:58:40.916: INFO: Observed stateful pod in namespace: statefulset-3319, name: ss-0, uid: 13905fe5-20aa-4650-8fd8-67d83dea7345, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 23 16:58:40.922: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3319
  STEP: Removing pod with conflicting port in namespace statefulset-3319 @ 04/23/23 16:58:40.922
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3319 and will be in running state @ 04/23/23 16:58:40.938
  Apr 23 16:58:42.948: INFO: Deleting all statefulset in ns statefulset-3319
  Apr 23 16:58:42.952: INFO: Scaling statefulset ss to 0
  Apr 23 16:58:52.974: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 16:58:52.978: INFO: Deleting statefulset ss
  Apr 23 16:58:52.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3319" for this suite. @ 04/23/23 16:58:52.998
• [14.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 04/23/23 16:58:53.009
  Apr 23 16:58:53.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 16:58:53.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:58:53.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:58:53.036
  STEP: Creating service test in namespace statefulset-9579 @ 04/23/23 16:58:53.038
  STEP: Creating stateful set ss in namespace statefulset-9579 @ 04/23/23 16:58:53.044
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9579 @ 04/23/23 16:58:53.056
  Apr 23 16:58:53.064: INFO: Found 0 stateful pods, waiting for 1
  Apr 23 16:59:03.070: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/23/23 16:59:03.07
  Apr 23 16:59:03.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-9579 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 16:59:03.257: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 16:59:03.257: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 16:59:03.257: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 16:59:03.261: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 23 16:59:13.267: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 16:59:13.267: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 16:59:13.287: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Apr 23 16:59:13.287: INFO: ss-0  ip-172-31-70-241  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:58:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:58:53 +0000 UTC  }]
  Apr 23 16:59:13.287: INFO: 
  Apr 23 16:59:13.287: INFO: StatefulSet ss has not reached scale 3, at 1
  Apr 23 16:59:14.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995316238s
  Apr 23 16:59:15.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988691347s
  Apr 23 16:59:16.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983558244s
  Apr 23 16:59:17.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976996578s
  Apr 23 16:59:18.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972053017s
  Apr 23 16:59:19.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966367385s
  Apr 23 16:59:20.326: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961292852s
  Apr 23 16:59:21.331: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956536057s
  Apr 23 16:59:22.338: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.954624ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9579 @ 04/23/23 16:59:23.338
  Apr 23 16:59:23.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-9579 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 16:59:23.497: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 16:59:23.497: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 16:59:23.497: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 16:59:23.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-9579 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 16:59:23.658: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 23 16:59:23.658: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 16:59:23.658: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 16:59:23.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-9579 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 16:59:23.829: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 23 16:59:23.829: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 16:59:23.829: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 16:59:23.838: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Apr 23 16:59:33.844: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 16:59:33.844: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 16:59:33.844: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/23/23 16:59:33.844
  Apr 23 16:59:33.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-9579 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 16:59:34.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 16:59:34.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 16:59:34.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 16:59:34.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-9579 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 16:59:34.167: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 16:59:34.167: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 16:59:34.167: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 16:59:34.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-9579 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 16:59:34.321: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 16:59:34.321: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 16:59:34.321: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 16:59:34.321: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 16:59:34.326: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Apr 23 16:59:44.335: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 16:59:44.335: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 16:59:44.335: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 16:59:44.351: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Apr 23 16:59:44.351: INFO: ss-0  ip-172-31-70-241  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:58:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:58:53 +0000 UTC  }]
  Apr 23 16:59:44.351: INFO: ss-1  ip-172-31-86-26   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:13 +0000 UTC  }]
  Apr 23 16:59:44.351: INFO: ss-2  ip-172-31-37-26   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:13 +0000 UTC  }]
  Apr 23 16:59:44.351: INFO: 
  Apr 23 16:59:44.351: INFO: StatefulSet ss has not reached scale 0, at 3
  Apr 23 16:59:45.356: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Apr 23 16:59:45.356: INFO: ss-0  ip-172-31-70-241  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:58:53 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:58:53 +0000 UTC  }]
  Apr 23 16:59:45.356: INFO: ss-2  ip-172-31-37-26   Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:13 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:34 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 16:59:13 +0000 UTC  }]
  Apr 23 16:59:45.356: INFO: 
  Apr 23 16:59:45.356: INFO: StatefulSet ss has not reached scale 0, at 2
  Apr 23 16:59:46.362: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989210878s
  Apr 23 16:59:47.366: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98454085s
  Apr 23 16:59:48.372: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979503891s
  Apr 23 16:59:49.376: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97460668s
  Apr 23 16:59:50.381: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.970493272s
  Apr 23 16:59:51.391: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.964362681s
  Apr 23 16:59:52.396: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.954806821s
  Apr 23 16:59:53.401: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.994657ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9579 @ 04/23/23 16:59:54.401
  Apr 23 16:59:54.406: INFO: Scaling statefulset ss to 0
  Apr 23 16:59:54.420: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 16:59:54.424: INFO: Deleting all statefulset in ns statefulset-9579
  Apr 23 16:59:54.427: INFO: Scaling statefulset ss to 0
  Apr 23 16:59:54.442: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 16:59:54.446: INFO: Deleting statefulset ss
  Apr 23 16:59:54.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9579" for this suite. @ 04/23/23 16:59:54.481
• [61.480 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/23/23 16:59:54.49
  Apr 23 16:59:54.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 16:59:54.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 16:59:54.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 16:59:54.523
  STEP: creating service in namespace services-5117 @ 04/23/23 16:59:54.526
  STEP: creating service affinity-clusterip in namespace services-5117 @ 04/23/23 16:59:54.526
  STEP: creating replication controller affinity-clusterip in namespace services-5117 @ 04/23/23 16:59:54.538
  I0423 16:59:54.555382      21 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-5117, replica count: 3
  I0423 16:59:57.606793      21 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 16:59:57.614: INFO: Creating new exec pod
  Apr 23 17:00:00.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-5117 exec execpod-affinityvs9sg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Apr 23 17:00:00.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 23 17:00:00.864: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:00:00.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-5117 exec execpod-affinityvs9sg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.179 80'
  Apr 23 17:00:01.058: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.179 80\nConnection to 10.152.183.179 80 port [tcp/http] succeeded!\n"
  Apr 23 17:00:01.058: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:00:01.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-5117 exec execpod-affinityvs9sg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.179:80/ ; done'
  Apr 23 17:00:01.424: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.179:80/\n"
  Apr 23 17:00:01.424: INFO: stdout: "\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh\naffinity-clusterip-fjlwh"
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Received response from host: affinity-clusterip-fjlwh
  Apr 23 17:00:01.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 17:00:01.430: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-5117, will wait for the garbage collector to delete the pods @ 04/23/23 17:00:01.449
  Apr 23 17:00:01.514: INFO: Deleting ReplicationController affinity-clusterip took: 9.497005ms
  Apr 23 17:00:01.614: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.843589ms
  STEP: Destroying namespace "services-5117" for this suite. @ 04/23/23 17:00:04.034
• [9.552 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/23/23 17:00:04.043
  Apr 23 17:00:04.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 17:00:04.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:00:04.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:00:04.07
  Apr 23 17:00:04.101: INFO: created pod pod-service-account-defaultsa
  Apr 23 17:00:04.101: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Apr 23 17:00:04.114: INFO: created pod pod-service-account-mountsa
  Apr 23 17:00:04.114: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 23 17:00:04.128: INFO: created pod pod-service-account-nomountsa
  Apr 23 17:00:04.128: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 23 17:00:04.143: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 23 17:00:04.144: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Apr 23 17:00:04.158: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 23 17:00:04.159: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Apr 23 17:00:04.176: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 23 17:00:04.176: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 23 17:00:04.195: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 23 17:00:04.195: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 23 17:00:04.209: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 23 17:00:04.209: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Apr 23 17:00:04.235: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 23 17:00:04.235: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 23 17:00:04.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8188" for this suite. @ 04/23/23 17:00:04.244
• [0.211 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 04/23/23 17:00:04.254
  Apr 23 17:00:04.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 17:00:04.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:00:04.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:00:04.288
  STEP: Creating simple DaemonSet "daemon-set" @ 04/23/23 17:00:04.315
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 17:00:04.322
  Apr 23 17:00:04.329: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:00:04.329: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:00:04.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:00:04.333: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 17:00:05.339: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:00:05.339: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:00:05.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 17:00:05.344: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 17:00:06.338: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:00:06.339: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:00:06.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 17:00:06.344: INFO: Node ip-172-31-70-241 is running 0 daemon pod, expected 1
  Apr 23 17:00:07.339: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:00:07.340: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:00:07.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 17:00:07.344: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/23/23 17:00:07.348
  STEP: DeleteCollection of the DaemonSets @ 04/23/23 17:00:07.352
  STEP: Verify that ReplicaSets have been deleted @ 04/23/23 17:00:07.364
  Apr 23 17:00:07.387: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9956"},"items":null}

  Apr 23 17:00:07.400: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9957"},"items":[{"metadata":{"name":"daemon-set-6mq9z","generateName":"daemon-set-","namespace":"daemonsets-9506","uid":"42097d46-5292-4611-8d87-a30b0633022a","resourceVersion":"9957","creationTimestamp":"2023-04-23T17:00:04Z","deletionTimestamp":"2023-04-23T17:00:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"faf5f98a-b024-465f-ba52-1a60c2d607ff","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-23T17:00:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf5f98a-b024-465f-ba52-1a60c2d607ff\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-23T17:00:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kxws2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kxws2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-86-26","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-86-26"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:04Z"}],"hostIP":"172.31.86.26","podIP":"192.168.122.151","podIPs":[{"ip":"192.168.122.151"}],"startTime":"2023-04-23T17:00:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-23T17:00:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://63bdae84b5add674155e264d9073c50ccc26bc3b495cf9a7109c7ca13b06145c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-r5452","generateName":"daemon-set-","namespace":"daemonsets-9506","uid":"e3d7a25b-962e-4b6d-a23f-e7f51256284e","resourceVersion":"9955","creationTimestamp":"2023-04-23T17:00:04Z","deletionTimestamp":"2023-04-23T17:00:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"faf5f98a-b024-465f-ba52-1a60c2d607ff","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-23T17:00:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf5f98a-b024-465f-ba52-1a60c2d607ff\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-23T17:00:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wnrpt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wnrpt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-70-241","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-70-241"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:04Z"}],"hostIP":"172.31.70.241","podIP":"192.168.6.203","podIPs":[{"ip":"192.168.6.203"}],"startTime":"2023-04-23T17:00:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-23T17:00:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://c36d2788e9c99e3f8aa42dc08d9dd711d386be996401dc7f01b26e360ba9a48a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v7gjd","generateName":"daemon-set-","namespace":"daemonsets-9506","uid":"8114e9fc-eed2-4c39-8f75-5e13891010ac","resourceVersion":"9954","creationTimestamp":"2023-04-23T17:00:04Z","deletionTimestamp":"2023-04-23T17:00:37Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"faf5f98a-b024-465f-ba52-1a60c2d607ff","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-23T17:00:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faf5f98a-b024-465f-ba52-1a60c2d607ff\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-23T17:00:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nww5n","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nww5n","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-37-26","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-37-26"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-23T17:00:04Z"}],"hostIP":"172.31.37.26","podIP":"192.168.21.20","podIPs":[{"ip":"192.168.21.20"}],"startTime":"2023-04-23T17:00:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-23T17:00:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1584c1daf807f31cb659949364ac2e08dc63b4d3aca40a4ea432ebd6fbf589c5","started":true}],"qosClass":"BestEffort"}}]}

  Apr 23 17:00:07.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9506" for this suite. @ 04/23/23 17:00:07.433
• [3.189 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/23/23 17:00:07.444
  Apr 23 17:00:07.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:00:07.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:00:07.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:00:07.469
  Apr 23 17:00:07.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7231" for this suite. @ 04/23/23 17:00:07.487
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/23/23 17:00:07.504
  Apr 23 17:00:07.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 17:00:07.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:00:07.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:00:07.537
  STEP: apply creating a deployment @ 04/23/23 17:00:07.54
  Apr 23 17:00:07.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7314" for this suite. @ 04/23/23 17:00:07.564
• [0.069 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/23/23 17:00:07.573
  Apr 23 17:00:07.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 17:00:07.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:00:07.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:00:07.6
  STEP: creating the pod with failed condition @ 04/23/23 17:00:07.603
  STEP: updating the pod @ 04/23/23 17:02:07.619
  Apr 23 17:02:08.135: INFO: Successfully updated pod "var-expansion-5d2b2cc2-fa1f-4aff-9f1c-8f13b03f6b33"
  STEP: waiting for pod running @ 04/23/23 17:02:08.135
  STEP: deleting the pod gracefully @ 04/23/23 17:02:10.145
  Apr 23 17:02:10.145: INFO: Deleting pod "var-expansion-5d2b2cc2-fa1f-4aff-9f1c-8f13b03f6b33" in namespace "var-expansion-2726"
  Apr 23 17:02:10.157: INFO: Wait up to 5m0s for pod "var-expansion-5d2b2cc2-fa1f-4aff-9f1c-8f13b03f6b33" to be fully deleted
  Apr 23 17:02:42.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2726" for this suite. @ 04/23/23 17:02:42.264
• [154.699 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 04/23/23 17:02:42.274
  Apr 23 17:02:42.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 17:02:42.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:02:42.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:02:42.305
  STEP: Creating service test in namespace statefulset-7131 @ 04/23/23 17:02:42.309
  STEP: Creating a new StatefulSet @ 04/23/23 17:02:42.316
  Apr 23 17:02:42.337: INFO: Found 0 stateful pods, waiting for 3
  Apr 23 17:02:52.343: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 17:02:52.343: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 17:02:52.343: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/23/23 17:02:52.355
  Apr 23 17:02:52.378: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/23/23 17:02:52.378
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/23/23 17:03:02.398
  STEP: Performing a canary update @ 04/23/23 17:03:02.398
  Apr 23 17:03:02.423: INFO: Updating stateful set ss2
  Apr 23 17:03:02.436: INFO: Waiting for Pod statefulset-7131/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/23/23 17:03:12.446
  Apr 23 17:03:12.534: INFO: Found 2 stateful pods, waiting for 3
  Apr 23 17:03:22.541: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 17:03:22.541: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 17:03:22.541: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/23/23 17:03:22.549
  Apr 23 17:03:22.571: INFO: Updating stateful set ss2
  Apr 23 17:03:22.585: INFO: Waiting for Pod statefulset-7131/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 23 17:03:32.618: INFO: Updating stateful set ss2
  Apr 23 17:03:32.627: INFO: Waiting for StatefulSet statefulset-7131/ss2 to complete update
  Apr 23 17:03:32.627: INFO: Waiting for Pod statefulset-7131/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 23 17:03:42.639: INFO: Deleting all statefulset in ns statefulset-7131
  Apr 23 17:03:42.643: INFO: Scaling statefulset ss2 to 0
  Apr 23 17:03:52.669: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 17:03:52.674: INFO: Deleting statefulset ss2
  Apr 23 17:03:52.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7131" for this suite. @ 04/23/23 17:03:52.698
• [70.435 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/23/23 17:03:52.711
  Apr 23 17:03:52.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename deployment @ 04/23/23 17:03:52.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:03:52.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:03:52.743
  Apr 23 17:03:52.746: INFO: Creating deployment "test-recreate-deployment"
  Apr 23 17:03:52.752: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 23 17:03:52.760: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Apr 23 17:03:54.770: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 23 17:03:54.774: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 23 17:03:54.787: INFO: Updating deployment test-recreate-deployment
  Apr 23 17:03:54.787: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 23 17:03:54.897: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5075  d98dc200-d029-4299-928c-25097fa9d022 10964 2 2023-04-23 17:03:52 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00470fbf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-23 17:03:54 +0000 UTC,LastTransitionTime:2023-04-23 17:03:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-04-23 17:03:54 +0000 UTC,LastTransitionTime:2023-04-23 17:03:52 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 23 17:03:54.901: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-5075  df0c65a1-505b-44a6-90ec-b33bf29d75a8 10961 1 2023-04-23 17:03:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d98dc200-d029-4299-928c-25097fa9d022 0xc00470ffc7 0xc00470ffc8}] [] [{kube-controller-manager Update apps/v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d98dc200-d029-4299-928c-25097fa9d022\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e2e098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 17:03:54.901: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 23 17:03:54.902: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-5075  9fed6347-73f8-440c-b1ae-d6617177404b 10952 2 2023-04-23 17:03:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d98dc200-d029-4299-928c-25097fa9d022 0xc003e2e107 0xc003e2e108}] [] [{kube-controller-manager Update apps/v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d98dc200-d029-4299-928c-25097fa9d022\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e2e1f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 17:03:54.906: INFO: Pod "test-recreate-deployment-54757ffd6c-l7w8s" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-l7w8s test-recreate-deployment-54757ffd6c- deployment-5075  bd6cdf59-aa46-4df3-81f6-b454885256c9 10963 0 2023-04-23 17:03:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c df0c65a1-505b-44a6-90ec-b33bf29d75a8 0xc0032351f7 0xc0032351f8}] [] [{kube-controller-manager Update v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df0c65a1-505b-44a6-90ec-b33bf29d75a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hcs7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hcs7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:03:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:03:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:03:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:03:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:,StartTime:2023-04-23 17:03:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 17:03:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5075" for this suite. @ 04/23/23 17:03:54.911
• [2.208 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/23/23 17:03:54.921
  Apr 23 17:03:54.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename deployment @ 04/23/23 17:03:54.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:03:54.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:03:54.955
  STEP: creating a Deployment @ 04/23/23 17:03:54.964
  Apr 23 17:03:54.964: INFO: Creating simple deployment test-deployment-hzn9m
  Apr 23 17:03:54.978: INFO: deployment "test-deployment-hzn9m" doesn't have the required revision set
  STEP: Getting /status @ 04/23/23 17:03:56.995
  Apr 23 17:03:57.001: INFO: Deployment test-deployment-hzn9m has Conditions: [{Available True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hzn9m-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/23/23 17:03:57.001
  Apr 23 17:03:57.016: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 3, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 3, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 3, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 3, 54, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-hzn9m-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/23/23 17:03:57.016
  Apr 23 17:03:57.018: INFO: Observed &Deployment event: ADDED
  Apr 23 17:03:57.018: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:54 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hzn9m-5994cf9475"}
  Apr 23 17:03:57.019: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.019: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:54 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hzn9m-5994cf9475"}
  Apr 23 17:03:57.019: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-23 17:03:55 +0000 UTC 2023-04-23 17:03:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 23 17:03:57.019: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.019: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-23 17:03:55 +0000 UTC 2023-04-23 17:03:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 23 17:03:57.019: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:55 +0000 UTC 2023-04-23 17:03:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hzn9m-5994cf9475" is progressing.}
  Apr 23 17:03:57.019: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.019: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 23 17:03:57.019: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hzn9m-5994cf9475" has successfully progressed.}
  Apr 23 17:03:57.019: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.019: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 23 17:03:57.019: INFO: Observed Deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hzn9m-5994cf9475" has successfully progressed.}
  Apr 23 17:03:57.019: INFO: Found Deployment test-deployment-hzn9m in namespace deployment-7926 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 17:03:57.019: INFO: Deployment test-deployment-hzn9m has an updated status
  STEP: patching the Statefulset Status @ 04/23/23 17:03:57.019
  Apr 23 17:03:57.019: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 23 17:03:57.034: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/23/23 17:03:57.035
  Apr 23 17:03:57.037: INFO: Observed &Deployment event: ADDED
  Apr 23 17:03:57.037: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:54 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hzn9m-5994cf9475"}
  Apr 23 17:03:57.037: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.037: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:54 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hzn9m-5994cf9475"}
  Apr 23 17:03:57.037: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-23 17:03:55 +0000 UTC 2023-04-23 17:03:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 23 17:03:57.037: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.037: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-23 17:03:55 +0000 UTC 2023-04-23 17:03:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 23 17:03:57.037: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:55 +0000 UTC 2023-04-23 17:03:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hzn9m-5994cf9475" is progressing.}
  Apr 23 17:03:57.038: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.038: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 23 17:03:57.038: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hzn9m-5994cf9475" has successfully progressed.}
  Apr 23 17:03:57.038: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.038: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 23 17:03:57.038: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-23 17:03:56 +0000 UTC 2023-04-23 17:03:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hzn9m-5994cf9475" has successfully progressed.}
  Apr 23 17:03:57.038: INFO: Observed deployment test-deployment-hzn9m in namespace deployment-7926 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 17:03:57.039: INFO: Observed &Deployment event: MODIFIED
  Apr 23 17:03:57.039: INFO: Found deployment test-deployment-hzn9m in namespace deployment-7926 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 23 17:03:57.039: INFO: Deployment test-deployment-hzn9m has a patched status
  Apr 23 17:03:57.042: INFO: Deployment "test-deployment-hzn9m":
  &Deployment{ObjectMeta:{test-deployment-hzn9m  deployment-7926  0dda92cb-db6e-4355-a849-718e363dd53d 11003 1 2023-04-23 17:03:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-23 17:03:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-23 17:03:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003adfb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-hzn9m-5994cf9475",LastUpdateTime:2023-04-23 17:03:57 +0000 UTC,LastTransitionTime:2023-04-23 17:03:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 23 17:03:57.048: INFO: New ReplicaSet "test-deployment-hzn9m-5994cf9475" of Deployment "test-deployment-hzn9m":
  &ReplicaSet{ObjectMeta:{test-deployment-hzn9m-5994cf9475  deployment-7926  816277c5-f6e8-4ab5-a930-faa50d7a0269 10997 1 2023-04-23 17:03:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-hzn9m 0dda92cb-db6e-4355-a849-718e363dd53d 0xc0041585b0 0xc0041585b1}] [] [{kube-controller-manager Update apps/v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dda92cb-db6e-4355-a849-718e363dd53d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:03:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004158678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 17:03:57.054: INFO: Pod "test-deployment-hzn9m-5994cf9475-mmw8z" is available:
  &Pod{ObjectMeta:{test-deployment-hzn9m-5994cf9475-mmw8z test-deployment-hzn9m-5994cf9475- deployment-7926  8395a6af-c506-4b28-909e-3c1ec3e06a7c 10996 0 2023-04-23 17:03:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-hzn9m-5994cf9475 816277c5-f6e8-4ab5-a930-faa50d7a0269 0xc004159a80 0xc004159a81}] [] [{kube-controller-manager Update v1 2023-04-23 17:03:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"816277c5-f6e8-4ab5-a930-faa50d7a0269\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 17:03:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stz8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stz8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:03:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:03:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:03:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:03:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.208,StartTime:2023-04-23 17:03:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 17:03:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d977398d8203fa2000e8f47b9681ad3ea36f81490216246dfe1043c15d1ca56b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.208,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 17:03:57.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7926" for this suite. @ 04/23/23 17:03:57.059
• [2.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/23/23 17:03:57.073
  Apr 23 17:03:57.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:03:57.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:03:57.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:03:57.102
  STEP: creating service nodeport-test with type=NodePort in namespace services-4578 @ 04/23/23 17:03:57.104
  STEP: creating replication controller nodeport-test in namespace services-4578 @ 04/23/23 17:03:57.143
  I0423 17:03:57.151697      21 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-4578, replica count: 2
  I0423 17:04:00.202965      21 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 17:04:00.203: INFO: Creating new exec pod
  Apr 23 17:04:03.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-4578 exec execpodf5z74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 23 17:04:03.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 23 17:04:03.431: INFO: stdout: "nodeport-test-2shjc"
  Apr 23 17:04:03.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-4578 exec execpodf5z74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.207 80'
  Apr 23 17:04:03.584: INFO: stderr: "+ nc -v -t -w 2 10.152.183.207 80\nConnection to 10.152.183.207 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Apr 23 17:04:03.584: INFO: stdout: "nodeport-test-2shjc"
  Apr 23 17:04:03.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-4578 exec execpodf5z74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.86.26 32518'
  Apr 23 17:04:03.762: INFO: stderr: "+ nc -v -t -w 2 172.31.86.26 32518\n+ echo hostName\nConnection to 172.31.86.26 32518 port [tcp/*] succeeded!\n"
  Apr 23 17:04:03.762: INFO: stdout: "nodeport-test-2shjc"
  Apr 23 17:04:03.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-4578 exec execpodf5z74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.37.26 32518'
  Apr 23 17:04:03.908: INFO: stderr: "+ nc -v -t -w 2 172.31.37.26 32518\n+ echo hostName\nConnection to 172.31.37.26 32518 port [tcp/*] succeeded!\n"
  Apr 23 17:04:03.908: INFO: stdout: ""
  Apr 23 17:04:04.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-4578 exec execpodf5z74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.37.26 32518'
  Apr 23 17:04:05.076: INFO: stderr: "+ nc -v -t -w 2 172.31.37.26 32518\n+ echo hostName\nConnection to 172.31.37.26 32518 port [tcp/*] succeeded!\n"
  Apr 23 17:04:05.076: INFO: stdout: "nodeport-test-2shjc"
  Apr 23 17:04:05.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4578" for this suite. @ 04/23/23 17:04:05.083
• [8.020 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/23/23 17:04:05.093
  Apr 23 17:04:05.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 17:04:05.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:04:05.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:04:05.12
  STEP: validating cluster-info @ 04/23/23 17:04:05.123
  Apr 23 17:04:05.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-6519 cluster-info'
  Apr 23 17:04:05.204: INFO: stderr: ""
  Apr 23 17:04:05.204: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 23 17:04:05.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6519" for this suite. @ 04/23/23 17:04:05.21
• [0.126 seconds]
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/23/23 17:04:05.22
  Apr 23 17:04:05.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 17:04:05.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:04:05.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:04:05.248
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/23/23 17:04:05.251
  STEP: When a replication controller with a matching selector is created @ 04/23/23 17:04:07.278
  STEP: Then the orphan pod is adopted @ 04/23/23 17:04:07.286
  Apr 23 17:04:08.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4511" for this suite. @ 04/23/23 17:04:08.3
• [3.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/23/23 17:04:08.31
  Apr 23 17:04:08.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename gc @ 04/23/23 17:04:08.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:04:08.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:04:08.336
  STEP: create the deployment @ 04/23/23 17:04:08.34
  W0423 17:04:08.347482      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/23/23 17:04:08.347
  STEP: delete the deployment @ 04/23/23 17:04:08.863
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/23/23 17:04:08.878
  STEP: Gathering metrics @ 04/23/23 17:04:09.415
  W0423 17:04:09.420196      21 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 23 17:04:09.420: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 17:04:09.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-597" for this suite. @ 04/23/23 17:04:09.426
• [1.124 seconds]
------------------------------
S
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/23/23 17:04:09.435
  Apr 23 17:04:09.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename disruption @ 04/23/23 17:04:09.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:04:09.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:04:09.465
  STEP: creating the pdb @ 04/23/23 17:04:09.472
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:04:09.481
  STEP: updating the pdb @ 04/23/23 17:04:11.489
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:04:11.498
  STEP: patching the pdb @ 04/23/23 17:04:13.506
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:04:13.527
  STEP: Waiting for the pdb to be deleted @ 04/23/23 17:04:13.541
  Apr 23 17:04:13.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-836" for this suite. @ 04/23/23 17:04:13.554
• [4.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/23/23 17:04:13.569
  Apr 23 17:04:13.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename lease-test @ 04/23/23 17:04:13.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:04:13.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:04:13.595
  Apr 23 17:04:13.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-8407" for this suite. @ 04/23/23 17:04:13.691
• [0.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/23/23 17:04:13.704
  Apr 23 17:04:13.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:04:13.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:04:13.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:04:13.738
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/23/23 17:04:13.742
  STEP: Saw pod success @ 04/23/23 17:04:17.779
  Apr 23 17:04:17.783: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-7c49d75b-0beb-4fda-905b-aff16112a6c9 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:04:17.806
  Apr 23 17:04:17.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3366" for this suite. @ 04/23/23 17:04:17.832
• [4.136 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/23/23 17:04:17.842
  Apr 23 17:04:17.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:04:17.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:04:17.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:04:17.87
  STEP: Creating a ResourceQuota with best effort scope @ 04/23/23 17:04:17.872
  STEP: Ensuring ResourceQuota status is calculated @ 04/23/23 17:04:17.88
  STEP: Creating a ResourceQuota with not best effort scope @ 04/23/23 17:04:19.885
  STEP: Ensuring ResourceQuota status is calculated @ 04/23/23 17:04:19.892
  STEP: Creating a best-effort pod @ 04/23/23 17:04:21.898
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/23/23 17:04:21.918
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/23/23 17:04:23.923
  STEP: Deleting the pod @ 04/23/23 17:04:25.93
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 17:04:25.953
  STEP: Creating a not best-effort pod @ 04/23/23 17:04:27.958
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/23/23 17:04:27.971
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/23/23 17:04:29.976
  STEP: Deleting the pod @ 04/23/23 17:04:31.982
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 17:04:31.998
  Apr 23 17:04:34.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9444" for this suite. @ 04/23/23 17:04:34.009
• [16.175 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/23/23 17:04:34.018
  Apr 23 17:04:34.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:04:34.019
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:04:34.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:04:34.041
  STEP: Creating configMap with name configmap-test-upd-cf9d7a02-fa5b-44bf-84d0-ef932e2df5cb @ 04/23/23 17:04:34.053
  STEP: Creating the pod @ 04/23/23 17:04:34.062
  STEP: Updating configmap configmap-test-upd-cf9d7a02-fa5b-44bf-84d0-ef932e2df5cb @ 04/23/23 17:04:36.105
  STEP: waiting to observe update in volume @ 04/23/23 17:04:36.111
  Apr 23 17:05:48.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2163" for this suite. @ 04/23/23 17:05:48.514
• [74.505 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/23/23 17:05:48.524
  Apr 23 17:05:48.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:05:48.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:05:48.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:05:48.554
  STEP: Creating configMap with name configmap-test-volume-64e6de24-9bd4-4dbb-9586-abf8b410d66d @ 04/23/23 17:05:48.568
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:05:48.575
  STEP: Saw pod success @ 04/23/23 17:05:52.617
  Apr 23 17:05:52.620: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-c8e5e8da-ebce-492b-8dca-297415957183 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:05:52.629
  Apr 23 17:05:52.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8671" for this suite. @ 04/23/23 17:05:52.654
• [4.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/23/23 17:05:52.667
  Apr 23 17:05:52.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename job @ 04/23/23 17:05:52.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:05:52.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:05:52.692
  STEP: Creating a job @ 04/23/23 17:05:52.696
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/23/23 17:05:52.701
  STEP: patching /status @ 04/23/23 17:05:54.707
  STEP: updating /status @ 04/23/23 17:05:54.716
  STEP: get /status @ 04/23/23 17:05:54.75
  Apr 23 17:05:54.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8717" for this suite. @ 04/23/23 17:05:54.759
• [2.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/23/23 17:05:54.768
  Apr 23 17:05:54.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename controllerrevisions @ 04/23/23 17:05:54.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:05:54.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:05:54.788
  STEP: Creating DaemonSet "e2e-8mrdd-daemon-set" @ 04/23/23 17:05:54.819
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 17:05:54.827
  Apr 23 17:05:54.832: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:05:54.832: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:05:54.837: INFO: Number of nodes with available pods controlled by daemonset e2e-8mrdd-daemon-set: 0
  Apr 23 17:05:54.837: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 17:05:55.842: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:05:55.842: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:05:55.846: INFO: Number of nodes with available pods controlled by daemonset e2e-8mrdd-daemon-set: 0
  Apr 23 17:05:55.846: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 17:05:56.843: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:05:56.843: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:05:56.849: INFO: Number of nodes with available pods controlled by daemonset e2e-8mrdd-daemon-set: 3
  Apr 23 17:05:56.849: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-8mrdd-daemon-set
  STEP: Confirm DaemonSet "e2e-8mrdd-daemon-set" successfully created with "daemonset-name=e2e-8mrdd-daemon-set" label @ 04/23/23 17:05:56.853
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-8mrdd-daemon-set" @ 04/23/23 17:05:56.861
  Apr 23 17:05:56.865: INFO: Located ControllerRevision: "e2e-8mrdd-daemon-set-ccd5bf45d"
  STEP: Patching ControllerRevision "e2e-8mrdd-daemon-set-ccd5bf45d" @ 04/23/23 17:05:56.868
  Apr 23 17:05:56.875: INFO: e2e-8mrdd-daemon-set-ccd5bf45d has been patched
  STEP: Create a new ControllerRevision @ 04/23/23 17:05:56.875
  Apr 23 17:05:56.885: INFO: Created ControllerRevision: e2e-8mrdd-daemon-set-97b4fbb8d
  STEP: Confirm that there are two ControllerRevisions @ 04/23/23 17:05:56.885
  Apr 23 17:05:56.885: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 17:05:56.889: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-8mrdd-daemon-set-ccd5bf45d" @ 04/23/23 17:05:56.889
  STEP: Confirm that there is only one ControllerRevision @ 04/23/23 17:05:56.897
  Apr 23 17:05:56.897: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 17:05:56.900: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-8mrdd-daemon-set-97b4fbb8d" @ 04/23/23 17:05:56.904
  Apr 23 17:05:56.914: INFO: e2e-8mrdd-daemon-set-97b4fbb8d has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/23/23 17:05:56.914
  W0423 17:05:56.925263      21 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/23/23 17:05:56.925
  Apr 23 17:05:56.925: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 17:05:57.930: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 17:05:57.934: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-8mrdd-daemon-set-97b4fbb8d=updated" @ 04/23/23 17:05:57.934
  STEP: Confirm that there is only one ControllerRevision @ 04/23/23 17:05:57.946
  Apr 23 17:05:57.946: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 23 17:05:57.949: INFO: Found 1 ControllerRevisions
  Apr 23 17:05:57.953: INFO: ControllerRevision "e2e-8mrdd-daemon-set-665948f56f" has revision 3
  STEP: Deleting DaemonSet "e2e-8mrdd-daemon-set" @ 04/23/23 17:05:57.956
  STEP: deleting DaemonSet.extensions e2e-8mrdd-daemon-set in namespace controllerrevisions-9873, will wait for the garbage collector to delete the pods @ 04/23/23 17:05:57.956
  Apr 23 17:05:58.021: INFO: Deleting DaemonSet.extensions e2e-8mrdd-daemon-set took: 9.704078ms
  Apr 23 17:05:58.122: INFO: Terminating DaemonSet.extensions e2e-8mrdd-daemon-set pods took: 101.135978ms
  Apr 23 17:05:59.628: INFO: Number of nodes with available pods controlled by daemonset e2e-8mrdd-daemon-set: 0
  Apr 23 17:05:59.628: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-8mrdd-daemon-set
  Apr 23 17:05:59.631: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11871"},"items":null}

  Apr 23 17:05:59.635: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11871"},"items":null}

  Apr 23 17:05:59.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-9873" for this suite. @ 04/23/23 17:05:59.655
• [4.895 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/23/23 17:05:59.668
  Apr 23 17:05:59.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename dns @ 04/23/23 17:05:59.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:05:59.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:05:59.693
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5319.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5319.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/23/23 17:05:59.697
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5319.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5319.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/23/23 17:05:59.697
  STEP: creating a pod to probe /etc/hosts @ 04/23/23 17:05:59.698
  STEP: submitting the pod to kubernetes @ 04/23/23 17:05:59.698
  STEP: retrieving the pod @ 04/23/23 17:06:01.722
  STEP: looking for the results for each expected name from probers @ 04/23/23 17:06:01.727
  Apr 23 17:06:01.747: INFO: DNS probes using dns-5319/dns-test-41cdb98e-76d5-4930-8bbd-5fc479bd8c09 succeeded

  Apr 23 17:06:01.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 17:06:01.753
  STEP: Destroying namespace "dns-5319" for this suite. @ 04/23/23 17:06:01.771
• [2.113 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/23/23 17:06:01.782
  Apr 23 17:06:01.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:06:01.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:01.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:01.823
  STEP: Creating a ResourceQuota @ 04/23/23 17:06:01.828
  STEP: Getting a ResourceQuota @ 04/23/23 17:06:01.835
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/23/23 17:06:01.839
  STEP: Patching the ResourceQuota @ 04/23/23 17:06:01.848
  STEP: Deleting a Collection of ResourceQuotas @ 04/23/23 17:06:01.86
  STEP: Verifying the deleted ResourceQuota @ 04/23/23 17:06:01.873
  Apr 23 17:06:01.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-570" for this suite. @ 04/23/23 17:06:01.884
• [0.115 seconds]
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/23/23 17:06:01.897
  Apr 23 17:06:01.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 17:06:01.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:01.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:01.944
  Apr 23 17:06:01.950: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/23/23 17:06:02.968
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/23/23 17:06:02.976
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/23/23 17:06:03.987
  Apr 23 17:06:03.999: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/23/23 17:06:03.999
  Apr 23 17:06:05.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9087" for this suite. @ 04/23/23 17:06:05.014
• [3.127 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/23/23 17:06:05.025
  Apr 23 17:06:05.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:06:05.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:05.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:05.049
  STEP: Creating configMap with name configmap-test-upd-ce95fc02-86e9-42e4-93d8-91861454ef19 @ 04/23/23 17:06:05.058
  STEP: Creating the pod @ 04/23/23 17:06:05.063
  STEP: Waiting for pod with text data @ 04/23/23 17:06:07.085
  STEP: Waiting for pod with binary data @ 04/23/23 17:06:07.097
  Apr 23 17:06:07.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-810" for this suite. @ 04/23/23 17:06:07.116
• [2.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/23/23 17:06:07.13
  Apr 23 17:06:07.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:06:07.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:07.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:07.158
  STEP: Counting existing ResourceQuota @ 04/23/23 17:06:07.164
  STEP: Creating a ResourceQuota @ 04/23/23 17:06:12.169
  STEP: Ensuring resource quota status is calculated @ 04/23/23 17:06:12.174
  STEP: Creating a Pod that fits quota @ 04/23/23 17:06:14.179
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/23/23 17:06:14.2
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/23/23 17:06:16.206
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/23/23 17:06:16.209
  STEP: Ensuring a pod cannot update its resource requirements @ 04/23/23 17:06:16.211
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/23/23 17:06:16.218
  STEP: Deleting the pod @ 04/23/23 17:06:18.224
  STEP: Ensuring resource quota status released the pod usage @ 04/23/23 17:06:18.244
  Apr 23 17:06:20.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2370" for this suite. @ 04/23/23 17:06:20.263
• [13.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/23/23 17:06:20.273
  Apr 23 17:06:20.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 17:06:20.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:20.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:20.316
  Apr 23 17:06:20.337: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 23 17:06:25.343: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 17:06:25.343
  STEP: Scaling up "test-rs" replicaset  @ 04/23/23 17:06:25.343
  Apr 23 17:06:25.353: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/23/23 17:06:25.353
  W0423 17:06:25.364674      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 23 17:06:25.367: INFO: observed ReplicaSet test-rs in namespace replicaset-2501 with ReadyReplicas 1, AvailableReplicas 1
  Apr 23 17:06:25.383: INFO: observed ReplicaSet test-rs in namespace replicaset-2501 with ReadyReplicas 1, AvailableReplicas 1
  Apr 23 17:06:25.404: INFO: observed ReplicaSet test-rs in namespace replicaset-2501 with ReadyReplicas 1, AvailableReplicas 1
  Apr 23 17:06:25.425: INFO: observed ReplicaSet test-rs in namespace replicaset-2501 with ReadyReplicas 1, AvailableReplicas 1
  Apr 23 17:06:27.101: INFO: observed ReplicaSet test-rs in namespace replicaset-2501 with ReadyReplicas 2, AvailableReplicas 2
  Apr 23 17:06:27.580: INFO: observed Replicaset test-rs in namespace replicaset-2501 with ReadyReplicas 3 found true
  Apr 23 17:06:27.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2501" for this suite. @ 04/23/23 17:06:27.585
• [7.323 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/23/23 17:06:27.597
  Apr 23 17:06:27.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:06:27.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:27.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:27.62
  STEP: Creating configMap with name configmap-test-volume-map-2a136c0e-33eb-42e6-9948-c7c8c39cd029 @ 04/23/23 17:06:27.643
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:06:27.65
  STEP: Saw pod success @ 04/23/23 17:06:31.702
  Apr 23 17:06:31.706: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-7b3a0947-89c8-4f73-88b8-5de2de3a4cd5 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:06:31.715
  Apr 23 17:06:31.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8243" for this suite. @ 04/23/23 17:06:31.741
• [4.157 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/23/23 17:06:31.754
  Apr 23 17:06:31.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename containers @ 04/23/23 17:06:31.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:31.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:31.781
  STEP: Creating a pod to test override arguments @ 04/23/23 17:06:31.783
  STEP: Saw pod success @ 04/23/23 17:06:35.812
  Apr 23 17:06:35.816: INFO: Trying to get logs from node ip-172-31-70-241 pod client-containers-66e977c8-47b5-4b04-9182-a4794a594758 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:06:35.823
  Apr 23 17:06:35.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8915" for this suite. @ 04/23/23 17:06:35.848
• [4.102 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/23/23 17:06:35.857
  Apr 23 17:06:35.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename events @ 04/23/23 17:06:35.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:35.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:35.879
  STEP: creating a test event @ 04/23/23 17:06:35.883
  STEP: listing events in all namespaces @ 04/23/23 17:06:35.893
  STEP: listing events in test namespace @ 04/23/23 17:06:35.902
  STEP: listing events with field selection filtering on source @ 04/23/23 17:06:35.906
  STEP: listing events with field selection filtering on reportingController @ 04/23/23 17:06:35.909
  STEP: getting the test event @ 04/23/23 17:06:35.913
  STEP: patching the test event @ 04/23/23 17:06:35.916
  STEP: getting the test event @ 04/23/23 17:06:35.927
  STEP: updating the test event @ 04/23/23 17:06:35.931
  STEP: getting the test event @ 04/23/23 17:06:35.94
  STEP: deleting the test event @ 04/23/23 17:06:35.945
  STEP: listing events in all namespaces @ 04/23/23 17:06:35.956
  STEP: listing events in test namespace @ 04/23/23 17:06:35.966
  Apr 23 17:06:35.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4655" for this suite. @ 04/23/23 17:06:35.977
• [0.129 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/23/23 17:06:35.987
  Apr 23 17:06:35.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:06:35.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:36.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:36.02
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:06:36.024
  STEP: Saw pod success @ 04/23/23 17:06:40.054
  Apr 23 17:06:40.059: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-5e3c83a0-f70d-4dee-a7d3-e984b579c24f container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:06:40.067
  Apr 23 17:06:40.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1333" for this suite. @ 04/23/23 17:06:40.174
• [4.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/23/23 17:06:40.188
  Apr 23 17:06:40.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:06:40.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:40.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:40.22
  STEP: Creating projection with secret that has name projected-secret-test-map-b0b9bd53-55d6-40ec-8aa1-e7523a519dbc @ 04/23/23 17:06:40.223
  STEP: Creating a pod to test consume secrets @ 04/23/23 17:06:40.229
  STEP: Saw pod success @ 04/23/23 17:06:44.258
  Apr 23 17:06:44.262: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-secrets-0bda4d03-ce94-41a4-ad44-5c2a75199844 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:06:44.271
  Apr 23 17:06:44.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3497" for this suite. @ 04/23/23 17:06:44.3
• [4.120 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/23/23 17:06:44.31
  Apr 23 17:06:44.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:06:44.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:06:44.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:06:44.333
  STEP: Creating secret with name s-test-opt-del-82db11a8-6dff-450d-b562-66408152c67e @ 04/23/23 17:06:44.342
  STEP: Creating secret with name s-test-opt-upd-440e1767-9c0e-47a2-a6ac-fea69f1e01fd @ 04/23/23 17:06:44.348
  STEP: Creating the pod @ 04/23/23 17:06:44.354
  STEP: Deleting secret s-test-opt-del-82db11a8-6dff-450d-b562-66408152c67e @ 04/23/23 17:06:46.459
  STEP: Updating secret s-test-opt-upd-440e1767-9c0e-47a2-a6ac-fea69f1e01fd @ 04/23/23 17:06:46.469
  STEP: Creating secret with name s-test-opt-create-c78b0bff-331b-4062-a526-94eaa2372d4a @ 04/23/23 17:06:46.477
  STEP: waiting to observe update in volume @ 04/23/23 17:06:46.485
  Apr 23 17:08:13.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7511" for this suite. @ 04/23/23 17:08:13.027
• [88.731 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/23/23 17:08:13.041
  Apr 23 17:08:13.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename disruption @ 04/23/23 17:08:13.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:08:13.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:08:13.076
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:08:13.096
  STEP: Updating PodDisruptionBudget status @ 04/23/23 17:08:15.109
  STEP: Waiting for all pods to be running @ 04/23/23 17:08:15.123
  Apr 23 17:08:15.133: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 04/23/23 17:08:17.138
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:08:17.152
  STEP: Patching PodDisruptionBudget status @ 04/23/23 17:08:17.165
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:08:17.177
  Apr 23 17:08:17.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6450" for this suite. @ 04/23/23 17:08:17.185
• [4.162 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/23/23 17:08:17.204
  Apr 23 17:08:17.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pod-network-test @ 04/23/23 17:08:17.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:08:17.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:08:17.232
  STEP: Performing setup for networking test in namespace pod-network-test-8277 @ 04/23/23 17:08:17.236
  STEP: creating a selector @ 04/23/23 17:08:17.236
  STEP: Creating the service pods in kubernetes @ 04/23/23 17:08:17.236
  Apr 23 17:08:17.236: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/23/23 17:08:39.375
  Apr 23 17:08:41.399: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 23 17:08:41.399: INFO: Breadth first check of 192.168.21.26 on host 172.31.37.26...
  Apr 23 17:08:41.403: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.6.227:9080/dial?request=hostname&protocol=udp&host=192.168.21.26&port=8081&tries=1'] Namespace:pod-network-test-8277 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:08:41.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:08:41.403: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:08:41.403: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8277/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.6.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.21.26%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 17:08:41.478: INFO: Waiting for responses: map[]
  Apr 23 17:08:41.479: INFO: reached 192.168.21.26 after 0/1 tries
  Apr 23 17:08:41.479: INFO: Breadth first check of 192.168.6.225 on host 172.31.70.241...
  Apr 23 17:08:41.484: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.6.227:9080/dial?request=hostname&protocol=udp&host=192.168.6.225&port=8081&tries=1'] Namespace:pod-network-test-8277 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:08:41.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:08:41.485: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:08:41.485: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8277/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.6.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.6.225%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 17:08:41.555: INFO: Waiting for responses: map[]
  Apr 23 17:08:41.555: INFO: reached 192.168.6.225 after 0/1 tries
  Apr 23 17:08:41.555: INFO: Breadth first check of 192.168.122.162 on host 172.31.86.26...
  Apr 23 17:08:41.560: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.6.227:9080/dial?request=hostname&protocol=udp&host=192.168.122.162&port=8081&tries=1'] Namespace:pod-network-test-8277 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:08:41.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:08:41.560: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:08:41.561: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8277/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.6.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.122.162%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 17:08:41.626: INFO: Waiting for responses: map[]
  Apr 23 17:08:41.626: INFO: reached 192.168.122.162 after 0/1 tries
  Apr 23 17:08:41.626: INFO: Going to retry 0 out of 3 pods....
  Apr 23 17:08:41.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8277" for this suite. @ 04/23/23 17:08:41.631
• [24.437 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/23/23 17:08:41.645
  Apr 23 17:08:41.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/23/23 17:08:41.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:08:41.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:08:41.672
  STEP: create the container to handle the HTTPGet hook request. @ 04/23/23 17:08:41.685
  STEP: create the pod with lifecycle hook @ 04/23/23 17:08:43.712
  STEP: delete the pod with lifecycle hook @ 04/23/23 17:08:45.735
  STEP: check prestop hook @ 04/23/23 17:08:47.754
  Apr 23 17:08:47.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9815" for this suite. @ 04/23/23 17:08:47.793
• [6.157 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 04/23/23 17:08:47.802
  Apr 23 17:08:47.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 17:08:47.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:08:47.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:08:47.823
  STEP: Creating simple DaemonSet "daemon-set" @ 04/23/23 17:08:47.856
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 17:08:47.862
  Apr 23 17:08:47.866: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:08:47.866: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:08:47.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:08:47.870: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 17:08:48.874: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:08:48.875: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:08:48.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:08:48.879: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 17:08:49.881: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:08:49.881: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:08:49.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 17:08:49.885: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 04/23/23 17:08:49.889
  Apr 23 17:08:49.894: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/23/23 17:08:49.894
  Apr 23 17:08:49.906: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/23/23 17:08:49.906
  Apr 23 17:08:49.908: INFO: Observed &DaemonSet event: ADDED
  Apr 23 17:08:49.908: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.909: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.909: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.910: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.910: INFO: Found daemon set daemon-set in namespace daemonsets-8218 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 23 17:08:49.910: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/23/23 17:08:49.91
  STEP: watching for the daemon set status to be patched @ 04/23/23 17:08:49.92
  Apr 23 17:08:49.922: INFO: Observed &DaemonSet event: ADDED
  Apr 23 17:08:49.923: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.923: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.923: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.924: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.924: INFO: Observed daemon set daemon-set in namespace daemonsets-8218 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 23 17:08:49.924: INFO: Observed &DaemonSet event: MODIFIED
  Apr 23 17:08:49.924: INFO: Found daemon set daemon-set in namespace daemonsets-8218 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 23 17:08:49.924: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 17:08:49.929
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8218, will wait for the garbage collector to delete the pods @ 04/23/23 17:08:49.929
  Apr 23 17:08:49.992: INFO: Deleting DaemonSet.extensions daemon-set took: 8.001755ms
  Apr 23 17:08:50.093: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.980209ms
  Apr 23 17:08:51.499: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:08:51.499: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 17:08:51.503: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13061"},"items":null}

  Apr 23 17:08:51.507: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13061"},"items":null}

  Apr 23 17:08:51.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8218" for this suite. @ 04/23/23 17:08:51.53
• [3.737 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/23/23 17:08:51.54
  Apr 23 17:08:51.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:08:51.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:08:51.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:08:51.561
  STEP: Creating a pod to test downward api env vars @ 04/23/23 17:08:51.564
  STEP: Saw pod success @ 04/23/23 17:08:55.591
  Apr 23 17:08:55.595: INFO: Trying to get logs from node ip-172-31-70-241 pod downward-api-344db72d-dc8d-465f-ba90-9199d9faf588 container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 17:08:55.605
  Apr 23 17:08:55.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6872" for this suite. @ 04/23/23 17:08:55.632
• [4.103 seconds]
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/23/23 17:08:55.643
  Apr 23 17:08:55.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 17:08:55.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:08:55.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:08:55.694
  STEP: Creating ServiceAccount "e2e-sa-92z92"  @ 04/23/23 17:08:55.704
  Apr 23 17:08:55.715: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-92z92"  @ 04/23/23 17:08:55.715
  Apr 23 17:08:55.727: INFO: AutomountServiceAccountToken: true
  Apr 23 17:08:55.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4398" for this suite. @ 04/23/23 17:08:55.733
• [0.099 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/23/23 17:08:55.743
  Apr 23 17:08:55.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 17:08:55.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:08:55.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:08:55.771
  STEP: Creating pod busybox-3e2a4918-d680-4e97-ba85-4f3773b23de8 in namespace container-probe-1233 @ 04/23/23 17:08:55.775
  Apr 23 17:08:57.798: INFO: Started pod busybox-3e2a4918-d680-4e97-ba85-4f3773b23de8 in namespace container-probe-1233
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 17:08:57.798
  Apr 23 17:08:57.802: INFO: Initial restart count of pod busybox-3e2a4918-d680-4e97-ba85-4f3773b23de8 is 0
  Apr 23 17:12:58.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 17:12:58.504
  STEP: Destroying namespace "container-probe-1233" for this suite. @ 04/23/23 17:12:58.523
• [242.788 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/23/23 17:12:58.533
  Apr 23 17:12:58.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 17:12:58.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:12:58.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:12:58.564
  STEP: Creating a pod to test substitution in container's args @ 04/23/23 17:12:58.568
  STEP: Saw pod success @ 04/23/23 17:13:02.601
  Apr 23 17:13:02.605: INFO: Trying to get logs from node ip-172-31-70-241 pod var-expansion-bcc98446-81c5-4aa7-b6e0-c7313b6806d3 container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 17:13:02.628
  Apr 23 17:13:02.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3053" for this suite. @ 04/23/23 17:13:02.657
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/23/23 17:13:02.672
  Apr 23 17:13:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:13:02.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:13:02.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:13:02.706
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/23/23 17:13:02.709
  STEP: Saw pod success @ 04/23/23 17:13:06.732
  Apr 23 17:13:06.736: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-97e12df2-4173-4a11-9ef6-8ef23a6ab554 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:13:06.744
  Apr 23 17:13:06.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5509" for this suite. @ 04/23/23 17:13:06.768
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/23/23 17:13:06.779
  Apr 23 17:13:06.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 17:13:06.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:13:06.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:13:06.802
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/23/23 17:13:06.806
  Apr 23 17:13:06.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4140 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Apr 23 17:13:06.907: INFO: stderr: ""
  Apr 23 17:13:06.907: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/23/23 17:13:06.907
  Apr 23 17:13:06.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4140 delete pods e2e-test-httpd-pod'
  Apr 23 17:13:09.589: INFO: stderr: ""
  Apr 23 17:13:09.589: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 23 17:13:09.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4140" for this suite. @ 04/23/23 17:13:09.594
• [2.822 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 04/23/23 17:13:09.603
  Apr 23 17:13:09.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 17:13:09.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:13:09.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:13:09.631
  Apr 23 17:13:09.660: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 17:13:09.668
  Apr 23 17:13:09.673: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:09.673: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:09.677: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:13:09.677: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 17:13:10.681: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:10.682: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:10.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 17:13:10.686: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  Apr 23 17:13:11.682: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:11.682: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:11.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 17:13:11.687: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/23/23 17:13:11.707
  STEP: Check that daemon pods images are updated. @ 04/23/23 17:13:11.728
  Apr 23 17:13:11.732: INFO: Wrong image for pod: daemon-set-g8js7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:11.732: INFO: Wrong image for pod: daemon-set-kpqzv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:11.732: INFO: Wrong image for pod: daemon-set-vs55q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:11.736: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:11.736: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:12.742: INFO: Wrong image for pod: daemon-set-g8js7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:12.742: INFO: Wrong image for pod: daemon-set-kpqzv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:12.747: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:12.747: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:13.742: INFO: Wrong image for pod: daemon-set-g8js7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:13.742: INFO: Wrong image for pod: daemon-set-kpqzv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:13.747: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:13.747: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:14.741: INFO: Pod daemon-set-5ntzv is not available
  Apr 23 17:13:14.741: INFO: Wrong image for pod: daemon-set-g8js7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:14.741: INFO: Wrong image for pod: daemon-set-kpqzv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:14.745: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:14.746: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:15.742: INFO: Wrong image for pod: daemon-set-g8js7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:15.746: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:15.747: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:16.742: INFO: Wrong image for pod: daemon-set-g8js7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 23 17:13:16.742: INFO: Pod daemon-set-nsncc is not available
  Apr 23 17:13:16.747: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:16.747: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:17.746: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:17.747: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:18.746: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:18.746: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:19.743: INFO: Pod daemon-set-krvvs is not available
  Apr 23 17:13:19.748: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:19.749: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/23/23 17:13:19.749
  Apr 23 17:13:19.753: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:19.753: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:19.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 17:13:19.757: INFO: Node ip-172-31-86-26 is running 0 daemon pod, expected 1
  Apr 23 17:13:20.763: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:20.763: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:13:20.768: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 17:13:20.768: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 17:13:20.795
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1935, will wait for the garbage collector to delete the pods @ 04/23/23 17:13:20.795
  Apr 23 17:13:20.859: INFO: Deleting DaemonSet.extensions daemon-set took: 8.133783ms
  Apr 23 17:13:20.959: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.387348ms
  Apr 23 17:13:22.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:13:22.665: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 17:13:22.670: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13970"},"items":null}

  Apr 23 17:13:22.677: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13970"},"items":null}

  Apr 23 17:13:22.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1935" for this suite. @ 04/23/23 17:13:22.703
• [13.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/23/23 17:13:22.716
  Apr 23 17:13:22.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 17:13:22.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:13:22.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:13:22.742
  STEP: creating pod @ 04/23/23 17:13:22.746
  Apr 23 17:13:24.781: INFO: Pod pod-hostip-e306ee17-4e0c-4c91-8b26-bcf9328f3dbb has hostIP: 172.31.70.241
  Apr 23 17:13:24.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6606" for this suite. @ 04/23/23 17:13:24.788
• [2.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/23/23 17:13:24.798
  Apr 23 17:13:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 17:13:24.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:13:24.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:13:24.821
  STEP: Creating pod busybox-af0c53dc-6018-4707-8dea-aa5aaf1e4d92 in namespace container-probe-3893 @ 04/23/23 17:13:24.825
  Apr 23 17:13:26.854: INFO: Started pod busybox-af0c53dc-6018-4707-8dea-aa5aaf1e4d92 in namespace container-probe-3893
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 17:13:26.854
  Apr 23 17:13:26.861: INFO: Initial restart count of pod busybox-af0c53dc-6018-4707-8dea-aa5aaf1e4d92 is 0
  Apr 23 17:14:17.015: INFO: Restart count of pod container-probe-3893/busybox-af0c53dc-6018-4707-8dea-aa5aaf1e4d92 is now 1 (50.153951186s elapsed)
  Apr 23 17:14:17.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 17:14:17.02
  STEP: Destroying namespace "container-probe-3893" for this suite. @ 04/23/23 17:14:17.037
• [52.247 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/23/23 17:14:17.046
  Apr 23 17:14:17.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:14:17.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:14:17.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:14:17.082
  STEP: Setting up server cert @ 04/23/23 17:14:17.111
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:14:17.804
  STEP: Deploying the webhook pod @ 04/23/23 17:14:17.819
  STEP: Wait for the deployment to be ready @ 04/23/23 17:14:17.834
  Apr 23 17:14:17.844: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/23/23 17:14:19.86
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:14:19.873
  Apr 23 17:14:20.873: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/23/23 17:14:20.882
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/23/23 17:14:20.882
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/23/23 17:14:20.901
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/23/23 17:14:21.917
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/23/23 17:14:21.918
  STEP: Having no error when timeout is longer than webhook latency @ 04/23/23 17:14:22.958
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/23/23 17:14:22.958
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/23/23 17:14:28.007
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/23/23 17:14:28.007
  Apr 23 17:14:33.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4084" for this suite. @ 04/23/23 17:14:33.178
  STEP: Destroying namespace "webhook-markers-3497" for this suite. @ 04/23/23 17:14:33.186
• [16.151 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/23/23 17:14:33.197
  Apr 23 17:14:33.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:14:33.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:14:33.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:14:33.227
  STEP: Creating configMap with name configmap-test-volume-1b34cff1-001e-4286-9949-c3b1e37432b7 @ 04/23/23 17:14:33.232
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:14:33.238
  STEP: Saw pod success @ 04/23/23 17:14:37.271
  Apr 23 17:14:37.277: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-deae375b-c779-492a-8aa0-e329fdf99d0f container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:14:37.297
  Apr 23 17:14:37.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4555" for this suite. @ 04/23/23 17:14:37.331
• [4.145 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/23/23 17:14:37.347
  Apr 23 17:14:37.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:14:37.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:14:37.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:14:37.371
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/23/23 17:14:37.374
  STEP: Saw pod success @ 04/23/23 17:14:41.402
  Apr 23 17:14:41.406: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-75a96695-2ee8-44a4-9897-3b4f38229f78 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:14:41.422
  Apr 23 17:14:41.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2990" for this suite. @ 04/23/23 17:14:41.454
• [4.115 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/23/23 17:14:41.463
  Apr 23 17:14:41.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-pred @ 04/23/23 17:14:41.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:14:41.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:14:41.484
  Apr 23 17:14:41.487: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 23 17:14:41.501: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 17:14:41.504: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-37-26 before test
  Apr 23 17:14:41.510: INFO: default-http-backend-kubernetes-worker-65fc475d49-65dxk from ingress-nginx-kubernetes-worker started at 2023-04-23 16:24:39 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.510: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Apr 23 17:14:41.510: INFO: nginx-ingress-controller-kubernetes-worker-gbggh from ingress-nginx-kubernetes-worker started at 2023-04-23 16:24:39 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.510: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:14:41.510: INFO: coredns-5c7f76ccb8-sgclp from kube-system started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.510: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 17:14:41.510: INFO: kube-state-metrics-5b95b4459c-4xnvw from kube-system started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.510: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Apr 23 17:14:41.510: INFO: metrics-server-v0.5.2-6cf8c8b69c-zzwzt from kube-system started at 2023-04-23 16:24:07 +0000 UTC (2 container statuses recorded)
  Apr 23 17:14:41.510: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 23 17:14:41.510: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Apr 23 17:14:41.510: INFO: dashboard-metrics-scraper-6b8586b5c9-646rd from kubernetes-dashboard started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.510: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Apr 23 17:14:41.510: INFO: kubernetes-dashboard-6869f4cd5f-64m7l from kubernetes-dashboard started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.510: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Apr 23 17:14:41.510: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-28gbw from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:14:41.510: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:14:41.511: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 17:14:41.511: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-70-241 before test
  Apr 23 17:14:41.518: INFO: nginx-ingress-controller-kubernetes-worker-mfjvm from ingress-nginx-kubernetes-worker started at 2023-04-23 16:30:56 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.518: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:14:41.518: INFO: sonobuoy from sonobuoy started at 2023-04-23 16:37:12 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.518: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 23 17:14:41.518: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-clx9s from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:14:41.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:14:41.518: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 17:14:41.518: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-86-26 before test
  Apr 23 17:14:41.524: INFO: nginx-ingress-controller-kubernetes-worker-httzn from ingress-nginx-kubernetes-worker started at 2023-04-23 16:26:06 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.524: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:14:41.524: INFO: calico-kube-controllers-6c8cb79d47-nmw6q from kube-system started at 2023-04-23 16:31:05 +0000 UTC (1 container statuses recorded)
  Apr 23 17:14:41.524: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Apr 23 17:14:41.524: INFO: sonobuoy-e2e-job-b60cd7a9aecc4153 from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:14:41.524: INFO: 	Container e2e ready: true, restart count 0
  Apr 23 17:14:41.524: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:14:41.524: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-mdknn from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:14:41.524: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:14:41.524: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/23/23 17:14:41.524
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/23/23 17:14:43.555
  STEP: Trying to apply a random label on the found node. @ 04/23/23 17:14:43.575
  STEP: verifying the node has the label kubernetes.io/e2e-6facc81c-6d88-462b-85ed-1919dd891193 95 @ 04/23/23 17:14:43.585
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/23/23 17:14:43.59
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.70.241 on the node which pod4 resides and expect not scheduled @ 04/23/23 17:14:47.615
  STEP: removing the label kubernetes.io/e2e-6facc81c-6d88-462b-85ed-1919dd891193 off the node ip-172-31-70-241 @ 04/23/23 17:19:47.627
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-6facc81c-6d88-462b-85ed-1919dd891193 @ 04/23/23 17:19:47.643
  Apr 23 17:19:47.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3529" for this suite. @ 04/23/23 17:19:47.66
• [306.204 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/23/23 17:19:47.668
  Apr 23 17:19:47.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:19:47.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:19:47.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:19:47.692
  STEP: Setting up server cert @ 04/23/23 17:19:47.721
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:19:47.991
  STEP: Deploying the webhook pod @ 04/23/23 17:19:48.001
  STEP: Wait for the deployment to be ready @ 04/23/23 17:19:48.015
  Apr 23 17:19:48.027: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/23/23 17:19:50.041
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:19:50.054
  Apr 23 17:19:51.056: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 17:19:51.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8468-crds.webhook.example.com via the AdmissionRegistration API @ 04/23/23 17:19:51.572
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/23/23 17:19:51.592
  Apr 23 17:19:53.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1511" for this suite. @ 04/23/23 17:19:54.234
  STEP: Destroying namespace "webhook-markers-5805" for this suite. @ 04/23/23 17:19:54.244
• [6.583 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/23/23 17:19:54.252
  Apr 23 17:19:54.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 17:19:54.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:19:54.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:19:54.274
  STEP: Creating secret with name s-test-opt-del-12c9bbb4-2773-4b89-9ffd-495035d6dafb @ 04/23/23 17:19:54.287
  STEP: Creating secret with name s-test-opt-upd-3d224781-0724-460f-afc4-82e792e3096b @ 04/23/23 17:19:54.292
  STEP: Creating the pod @ 04/23/23 17:19:54.299
  STEP: Deleting secret s-test-opt-del-12c9bbb4-2773-4b89-9ffd-495035d6dafb @ 04/23/23 17:19:56.377
  STEP: Updating secret s-test-opt-upd-3d224781-0724-460f-afc4-82e792e3096b @ 04/23/23 17:19:56.384
  STEP: Creating secret with name s-test-opt-create-75cad687-c7e2-4175-b62c-23ab18e5f3e2 @ 04/23/23 17:19:56.392
  STEP: waiting to observe update in volume @ 04/23/23 17:19:56.405
  Apr 23 17:21:00.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9812" for this suite. @ 04/23/23 17:21:00.794
• [66.550 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/23/23 17:21:00.804
  Apr 23 17:21:00.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:21:00.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:00.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:00.834
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/23/23 17:21:00.838
  STEP: Saw pod success @ 04/23/23 17:21:04.864
  Apr 23 17:21:04.869: INFO: Trying to get logs from node ip-172-31-86-26 pod pod-85fdbe13-3c9c-4a88-bf46-e2a00ed3a0f3 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:21:04.892
  Apr 23 17:21:04.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4928" for this suite. @ 04/23/23 17:21:04.916
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/23/23 17:21:04.928
  Apr 23 17:21:04.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename events @ 04/23/23 17:21:04.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:04.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:04.998
  STEP: Create set of events @ 04/23/23 17:21:05.008
  Apr 23 17:21:05.013: INFO: created test-event-1
  Apr 23 17:21:05.021: INFO: created test-event-2
  Apr 23 17:21:05.030: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/23/23 17:21:05.03
  STEP: delete collection of events @ 04/23/23 17:21:05.038
  Apr 23 17:21:05.038: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/23/23 17:21:05.068
  Apr 23 17:21:05.068: INFO: requesting list of events to confirm quantity
  Apr 23 17:21:05.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5486" for this suite. @ 04/23/23 17:21:05.077
• [0.158 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/23/23 17:21:05.088
  Apr 23 17:21:05.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:21:05.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:05.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:05.113
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:21:05.117
  STEP: Saw pod success @ 04/23/23 17:21:09.15
  Apr 23 17:21:09.154: INFO: Trying to get logs from node ip-172-31-86-26 pod downwardapi-volume-06884b06-4147-44fb-b4a3-fc07437995d2 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:21:09.162
  Apr 23 17:21:09.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3131" for this suite. @ 04/23/23 17:21:09.196
• [4.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/23/23 17:21:09.208
  Apr 23 17:21:09.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:21:09.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:09.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:09.232
  STEP: Creating configMap with name projected-configmap-test-volume-5406359f-669a-4b12-ab3d-23efd6c8f22c @ 04/23/23 17:21:09.235
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:21:09.241
  STEP: Saw pod success @ 04/23/23 17:21:13.264
  Apr 23 17:21:13.268: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-configmaps-e647dc65-d5cd-4a2a-a38f-13b8dfe95c9b container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:21:13.277
  Apr 23 17:21:13.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6064" for this suite. @ 04/23/23 17:21:13.306
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/23/23 17:21:13.319
  Apr 23 17:21:13.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sysctl @ 04/23/23 17:21:13.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:13.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:13.347
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/23/23 17:21:13.35
  Apr 23 17:21:13.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-1189" for this suite. @ 04/23/23 17:21:13.367
• [0.058 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/23/23 17:21:13.377
  Apr 23 17:21:13.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename subpath @ 04/23/23 17:21:13.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:13.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:13.407
  STEP: Setting up data @ 04/23/23 17:21:13.41
  STEP: Creating pod pod-subpath-test-configmap-znpc @ 04/23/23 17:21:13.422
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 17:21:13.422
  STEP: Saw pod success @ 04/23/23 17:21:37.508
  Apr 23 17:21:37.512: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-subpath-test-configmap-znpc container test-container-subpath-configmap-znpc: <nil>
  STEP: delete the pod @ 04/23/23 17:21:37.519
  STEP: Deleting pod pod-subpath-test-configmap-znpc @ 04/23/23 17:21:37.537
  Apr 23 17:21:37.537: INFO: Deleting pod "pod-subpath-test-configmap-znpc" in namespace "subpath-2861"
  Apr 23 17:21:37.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2861" for this suite. @ 04/23/23 17:21:37.545
• [24.175 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/23/23 17:21:37.553
  Apr 23 17:21:37.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 17:21:37.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:37.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:37.574
  STEP: Create set of pods @ 04/23/23 17:21:37.578
  Apr 23 17:21:37.587: INFO: created test-pod-1
  Apr 23 17:21:37.599: INFO: created test-pod-2
  Apr 23 17:21:37.627: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/23/23 17:21:37.627
  STEP: waiting for all pods to be deleted @ 04/23/23 17:21:41.737
  Apr 23 17:21:41.742: INFO: Pod quantity 3 is different from expected quantity 0
  Apr 23 17:21:42.747: INFO: Pod quantity 3 is different from expected quantity 0
  Apr 23 17:21:43.747: INFO: Pod quantity 1 is different from expected quantity 0
  Apr 23 17:21:44.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3547" for this suite. @ 04/23/23 17:21:44.753
• [7.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/23/23 17:21:44.765
  Apr 23 17:21:44.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:21:44.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:44.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:44.785
  STEP: creating service multi-endpoint-test in namespace services-3955 @ 04/23/23 17:21:44.789
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3955 to expose endpoints map[] @ 04/23/23 17:21:44.8
  Apr 23 17:21:44.812: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Apr 23 17:21:45.822: INFO: successfully validated that service multi-endpoint-test in namespace services-3955 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-3955 @ 04/23/23 17:21:45.822
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3955 to expose endpoints map[pod1:[100]] @ 04/23/23 17:21:47.845
  Apr 23 17:21:47.858: INFO: successfully validated that service multi-endpoint-test in namespace services-3955 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-3955 @ 04/23/23 17:21:47.858
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3955 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/23/23 17:21:49.884
  Apr 23 17:21:49.908: INFO: successfully validated that service multi-endpoint-test in namespace services-3955 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/23/23 17:21:49.908
  Apr 23 17:21:49.908: INFO: Creating new exec pod
  Apr 23 17:21:52.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3955 exec execpod4tsfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Apr 23 17:21:53.081: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 23 17:21:53.081: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:21:53.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3955 exec execpod4tsfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.203 80'
  Apr 23 17:21:53.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.203 80\nConnection to 10.152.183.203 80 port [tcp/http] succeeded!\n"
  Apr 23 17:21:53.232: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:21:53.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3955 exec execpod4tsfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Apr 23 17:21:53.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 23 17:21:53.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:21:53.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3955 exec execpod4tsfc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.203 81'
  Apr 23 17:21:53.526: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.203 81\nConnection to 10.152.183.203 81 port [tcp/*] succeeded!\n"
  Apr 23 17:21:53.526: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-3955 @ 04/23/23 17:21:53.526
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3955 to expose endpoints map[pod2:[101]] @ 04/23/23 17:21:53.547
  Apr 23 17:21:53.579: INFO: successfully validated that service multi-endpoint-test in namespace services-3955 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-3955 @ 04/23/23 17:21:53.579
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3955 to expose endpoints map[] @ 04/23/23 17:21:53.6
  Apr 23 17:21:53.611: INFO: successfully validated that service multi-endpoint-test in namespace services-3955 exposes endpoints map[]
  Apr 23 17:21:53.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3955" for this suite. @ 04/23/23 17:21:53.634
• [8.880 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/23/23 17:21:53.645
  Apr 23 17:21:53.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 17:21:53.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:53.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:53.669
  STEP: creating Agnhost RC @ 04/23/23 17:21:53.673
  Apr 23 17:21:53.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-3810 create -f -'
  Apr 23 17:21:54.217: INFO: stderr: ""
  Apr 23 17:21:54.217: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/23/23 17:21:54.217
  Apr 23 17:21:55.224: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:21:55.224: INFO: Found 0 / 1
  Apr 23 17:21:56.224: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:21:56.224: INFO: Found 1 / 1
  Apr 23 17:21:56.224: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/23/23 17:21:56.224
  Apr 23 17:21:56.228: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:21:56.228: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 23 17:21:56.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-3810 patch pod agnhost-primary-bqlpr -p {"metadata":{"annotations":{"x":"y"}}}'
  Apr 23 17:21:56.331: INFO: stderr: ""
  Apr 23 17:21:56.331: INFO: stdout: "pod/agnhost-primary-bqlpr patched\n"
  STEP: checking annotations @ 04/23/23 17:21:56.331
  Apr 23 17:21:56.336: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:21:56.336: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 23 17:21:56.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3810" for this suite. @ 04/23/23 17:21:56.343
• [2.708 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/23/23 17:21:56.357
  Apr 23 17:21:56.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:21:56.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:21:56.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:21:56.43
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:21:56.435
  STEP: Saw pod success @ 04/23/23 17:22:00.465
  Apr 23 17:22:00.469: INFO: Trying to get logs from node ip-172-31-86-26 pod downwardapi-volume-49483207-1b65-4814-9e8d-95784abfa59a container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:22:00.478
  Apr 23 17:22:00.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-607" for this suite. @ 04/23/23 17:22:00.503
• [4.156 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/23/23 17:22:00.515
  Apr 23 17:22:00.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename proxy @ 04/23/23 17:22:00.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:22:00.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:22:00.538
  Apr 23 17:22:00.541: INFO: Creating pod...
  Apr 23 17:22:02.564: INFO: Creating service...
  Apr 23 17:22:02.576: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/pods/agnhost/proxy/some/path/with/DELETE
  Apr 23 17:22:02.592: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 23 17:22:02.592: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/pods/agnhost/proxy/some/path/with/GET
  Apr 23 17:22:02.597: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 23 17:22:02.597: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/pods/agnhost/proxy/some/path/with/HEAD
  Apr 23 17:22:02.604: INFO: http.Client request:HEAD | StatusCode:200
  Apr 23 17:22:02.604: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 23 17:22:02.609: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 23 17:22:02.609: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/pods/agnhost/proxy/some/path/with/PATCH
  Apr 23 17:22:02.613: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 23 17:22:02.613: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/pods/agnhost/proxy/some/path/with/POST
  Apr 23 17:22:02.623: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 23 17:22:02.624: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/pods/agnhost/proxy/some/path/with/PUT
  Apr 23 17:22:02.628: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 23 17:22:02.628: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/services/test-service/proxy/some/path/with/DELETE
  Apr 23 17:22:02.634: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 23 17:22:02.635: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/services/test-service/proxy/some/path/with/GET
  Apr 23 17:22:02.650: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 23 17:22:02.651: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/services/test-service/proxy/some/path/with/HEAD
  Apr 23 17:22:02.659: INFO: http.Client request:HEAD | StatusCode:200
  Apr 23 17:22:02.659: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/services/test-service/proxy/some/path/with/OPTIONS
  Apr 23 17:22:02.667: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 23 17:22:02.667: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/services/test-service/proxy/some/path/with/PATCH
  Apr 23 17:22:02.674: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 23 17:22:02.674: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/services/test-service/proxy/some/path/with/POST
  Apr 23 17:22:02.680: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 23 17:22:02.680: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6599/services/test-service/proxy/some/path/with/PUT
  Apr 23 17:22:02.687: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 23 17:22:02.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6599" for this suite. @ 04/23/23 17:22:02.691
• [2.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/23/23 17:22:02.704
  Apr 23 17:22:02.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:22:02.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:22:02.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:22:02.731
  STEP: Creating configMap with name configmap-test-volume-ded4e0fa-ae33-4640-933d-ff72f645aa75 @ 04/23/23 17:22:02.735
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:22:02.742
  STEP: Saw pod success @ 04/23/23 17:22:06.773
  Apr 23 17:22:06.777: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-d1371203-d4ff-403d-a4a7-203949d187e1 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:22:06.786
  Apr 23 17:22:06.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8518" for this suite. @ 04/23/23 17:22:06.82
• [4.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/23/23 17:22:06.831
  Apr 23 17:22:06.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:22:06.832
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:22:06.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:22:06.852
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/23/23 17:22:06.859
  STEP: Saw pod success @ 04/23/23 17:22:10.893
  Apr 23 17:22:10.898: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-6bf47bd7-d038-4f79-8d95-54b4c698dba1 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:22:10.907
  Apr 23 17:22:10.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1361" for this suite. @ 04/23/23 17:22:10.942
• [4.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/23/23 17:22:10.952
  Apr 23 17:22:10.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:22:10.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:22:10.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:22:10.978
  STEP: Setting up server cert @ 04/23/23 17:22:11.009
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:22:11.782
  STEP: Deploying the webhook pod @ 04/23/23 17:22:11.792
  STEP: Wait for the deployment to be ready @ 04/23/23 17:22:11.807
  Apr 23 17:22:11.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/23/23 17:22:13.834
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:22:13.848
  Apr 23 17:22:14.848: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 17:22:14.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/23/23 17:22:15.372
  STEP: Creating a custom resource that should be denied by the webhook @ 04/23/23 17:22:15.412
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/23/23 17:22:17.452
  STEP: Updating the custom resource with disallowed data should be denied @ 04/23/23 17:22:17.46
  STEP: Deleting the custom resource should be denied @ 04/23/23 17:22:17.471
  STEP: Remove the offending key and value from the custom resource data @ 04/23/23 17:22:17.48
  STEP: Deleting the updated custom resource should be successful @ 04/23/23 17:22:17.492
  Apr 23 17:22:17.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9563" for this suite. @ 04/23/23 17:22:18.09
  STEP: Destroying namespace "webhook-markers-5453" for this suite. @ 04/23/23 17:22:18.102
• [7.157 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/23/23 17:22:18.109
  Apr 23 17:22:18.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename gc @ 04/23/23 17:22:18.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:22:18.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:22:18.132
  STEP: create the rc @ 04/23/23 17:22:18.14
  W0423 17:22:18.147211      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/23/23 17:22:24.152
  STEP: wait for the rc to be deleted @ 04/23/23 17:22:24.163
  Apr 23 17:22:25.184: INFO: 80 pods remaining
  Apr 23 17:22:25.184: INFO: 80 pods has nil DeletionTimestamp
  Apr 23 17:22:25.185: INFO: 
  Apr 23 17:22:26.189: INFO: 71 pods remaining
  Apr 23 17:22:26.200: INFO: 70 pods has nil DeletionTimestamp
  Apr 23 17:22:26.200: INFO: 
  Apr 23 17:22:27.177: INFO: 60 pods remaining
  Apr 23 17:22:27.177: INFO: 60 pods has nil DeletionTimestamp
  Apr 23 17:22:27.177: INFO: 
  Apr 23 17:22:28.175: INFO: 40 pods remaining
  Apr 23 17:22:28.175: INFO: 40 pods has nil DeletionTimestamp
  Apr 23 17:22:28.175: INFO: 
  Apr 23 17:22:29.178: INFO: 31 pods remaining
  Apr 23 17:22:29.179: INFO: 30 pods has nil DeletionTimestamp
  Apr 23 17:22:29.179: INFO: 
  Apr 23 17:22:30.210: INFO: 20 pods remaining
  Apr 23 17:22:30.210: INFO: 20 pods has nil DeletionTimestamp
  Apr 23 17:22:30.210: INFO: 
  STEP: Gathering metrics @ 04/23/23 17:22:31.172
  W0423 17:22:31.179416      21 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 23 17:22:31.182: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 17:22:31.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-628" for this suite. @ 04/23/23 17:22:31.188
• [13.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/23/23 17:22:31.197
  Apr 23 17:22:31.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 17:22:31.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:22:31.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:22:31.221
  STEP: set up a multi version CRD @ 04/23/23 17:22:31.225
  Apr 23 17:22:31.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: mark a version not serverd @ 04/23/23 17:22:35.54
  STEP: check the unserved version gets removed @ 04/23/23 17:22:35.576
  STEP: check the other version is not changed @ 04/23/23 17:22:36.646
  Apr 23 17:22:40.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-692" for this suite. @ 04/23/23 17:22:40.701
• [9.511 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/23/23 17:22:40.709
  Apr 23 17:22:40.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 17:22:40.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:22:40.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:22:40.735
  STEP: creating a ServiceAccount @ 04/23/23 17:22:40.74
  STEP: watching for the ServiceAccount to be added @ 04/23/23 17:22:40.751
  STEP: patching the ServiceAccount @ 04/23/23 17:22:40.754
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/23/23 17:22:40.76
  STEP: deleting the ServiceAccount @ 04/23/23 17:22:40.772
  Apr 23 17:22:40.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3272" for this suite. @ 04/23/23 17:22:40.8
• [0.106 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/23/23 17:22:40.816
  Apr 23 17:22:40.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/23/23 17:22:40.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:22:40.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:22:40.853
  Apr 23 17:22:40.879: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 23 17:23:40.903: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 17:23:40.908: INFO: Starting informer...
  STEP: Starting pods... @ 04/23/23 17:23:40.908
  Apr 23 17:23:41.131: INFO: Pod1 is running on ip-172-31-70-241. Tainting Node
  Apr 23 17:23:43.356: INFO: Pod2 is running on ip-172-31-70-241. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/23/23 17:23:43.356
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/23/23 17:23:43.369
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/23/23 17:23:43.374
  Apr 23 17:23:49.271: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Apr 23 17:24:09.316: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 23 17:24:09.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/23/23 17:24:09.334
  STEP: Destroying namespace "taint-multiple-pods-4430" for this suite. @ 04/23/23 17:24:09.339
• [88.535 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/23/23 17:24:09.351
  Apr 23 17:24:09.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:24:09.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:24:09.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:24:09.406
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-9033 @ 04/23/23 17:24:09.412
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/23/23 17:24:09.44
  STEP: creating service externalsvc in namespace services-9033 @ 04/23/23 17:24:09.44
  STEP: creating replication controller externalsvc in namespace services-9033 @ 04/23/23 17:24:09.463
  I0423 17:24:09.485884      21 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9033, replica count: 2
  I0423 17:24:12.536166      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/23/23 17:24:12.541
  Apr 23 17:24:12.569: INFO: Creating new exec pod
  Apr 23 17:24:14.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-9033 exec execpod2ttnn -- /bin/sh -x -c nslookup nodeport-service.services-9033.svc.cluster.local'
  Apr 23 17:24:14.806: INFO: stderr: "+ nslookup nodeport-service.services-9033.svc.cluster.local\n"
  Apr 23 17:24:14.806: INFO: stdout: "Server:\t\t10.152.183.245\nAddress:\t10.152.183.245#53\n\nnodeport-service.services-9033.svc.cluster.local\tcanonical name = externalsvc.services-9033.svc.cluster.local.\nName:\texternalsvc.services-9033.svc.cluster.local\nAddress: 10.152.183.24\n\n"
  Apr 23 17:24:14.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9033, will wait for the garbage collector to delete the pods @ 04/23/23 17:24:14.816
  Apr 23 17:24:14.881: INFO: Deleting ReplicationController externalsvc took: 8.416537ms
  Apr 23 17:24:14.981: INFO: Terminating ReplicationController externalsvc pods took: 100.409927ms
  Apr 23 17:24:17.410: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-9033" for this suite. @ 04/23/23 17:24:17.425
• [8.082 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/23/23 17:24:17.436
  Apr 23 17:24:17.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pod-network-test @ 04/23/23 17:24:17.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:24:17.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:24:17.461
  STEP: Performing setup for networking test in namespace pod-network-test-9758 @ 04/23/23 17:24:17.467
  STEP: creating a selector @ 04/23/23 17:24:17.467
  STEP: Creating the service pods in kubernetes @ 04/23/23 17:24:17.467
  Apr 23 17:24:17.467: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/23/23 17:24:39.602
  Apr 23 17:24:41.655: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 23 17:24:41.655: INFO: Going to poll 192.168.21.8 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 17:24:41.663: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.21.8:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9758 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:24:41.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:24:41.665: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:24:41.665: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9758/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.21.8%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 17:24:41.746: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 23 17:24:41.746: INFO: Going to poll 192.168.6.233 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 17:24:41.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.6.233:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9758 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:24:41.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:24:41.753: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:24:41.753: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9758/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.6.233%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 17:24:41.835: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 23 17:24:41.835: INFO: Going to poll 192.168.122.142 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 23 17:24:41.844: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.122.142:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9758 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:24:41.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:24:41.844: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:24:41.844: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9758/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.122.142%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 23 17:24:41.941: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 23 17:24:41.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9758" for this suite. @ 04/23/23 17:24:41.95
• [24.524 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/23/23 17:24:41.961
  Apr 23 17:24:41.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename job @ 04/23/23 17:24:41.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:24:41.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:24:41.99
  STEP: Creating a job @ 04/23/23 17:24:41.995
  STEP: Ensuring active pods == parallelism @ 04/23/23 17:24:42.005
  STEP: delete a job @ 04/23/23 17:24:44.011
  STEP: deleting Job.batch foo in namespace job-6472, will wait for the garbage collector to delete the pods @ 04/23/23 17:24:44.012
  Apr 23 17:24:44.075: INFO: Deleting Job.batch foo took: 7.446861ms
  Apr 23 17:24:44.175: INFO: Terminating Job.batch foo pods took: 100.151806ms
  STEP: Ensuring job was deleted @ 04/23/23 17:25:15.775
  Apr 23 17:25:15.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6472" for this suite. @ 04/23/23 17:25:15.79
• [33.836 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/23/23 17:25:15.801
  Apr 23 17:25:15.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:25:15.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:25:15.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:25:15.832
  STEP: Creating a ResourceQuota @ 04/23/23 17:25:15.84
  STEP: Getting a ResourceQuota @ 04/23/23 17:25:15.851
  STEP: Updating a ResourceQuota @ 04/23/23 17:25:15.859
  STEP: Verifying a ResourceQuota was modified @ 04/23/23 17:25:15.866
  STEP: Deleting a ResourceQuota @ 04/23/23 17:25:15.871
  STEP: Verifying the deleted ResourceQuota @ 04/23/23 17:25:15.886
  Apr 23 17:25:15.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4791" for this suite. @ 04/23/23 17:25:15.899
• [0.109 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/23/23 17:25:15.911
  Apr 23 17:25:15.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename security-context @ 04/23/23 17:25:15.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:25:15.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:25:15.944
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/23/23 17:25:15.948
  STEP: Saw pod success @ 04/23/23 17:25:19.978
  Apr 23 17:25:19.983: INFO: Trying to get logs from node ip-172-31-70-241 pod security-context-cd5a4285-74a6-4477-9a16-edda59a3cb93 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:25:20.006
  Apr 23 17:25:20.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-31" for this suite. @ 04/23/23 17:25:20.033
• [4.130 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/23/23 17:25:20.042
  Apr 23 17:25:20.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 17:25:20.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:25:20.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:25:20.069
  STEP: creating Agnhost RC @ 04/23/23 17:25:20.079
  Apr 23 17:25:20.079: INFO: namespace kubectl-2670
  Apr 23 17:25:20.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2670 create -f -'
  Apr 23 17:25:20.423: INFO: stderr: ""
  Apr 23 17:25:20.423: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/23/23 17:25:20.423
  Apr 23 17:25:21.428: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:25:21.429: INFO: Found 0 / 1
  Apr 23 17:25:22.428: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:25:22.428: INFO: Found 1 / 1
  Apr 23 17:25:22.428: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 23 17:25:22.432: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 23 17:25:22.432: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 23 17:25:22.432: INFO: wait on agnhost-primary startup in kubectl-2670 
  Apr 23 17:25:22.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2670 logs agnhost-primary-gjgrz agnhost-primary'
  Apr 23 17:25:22.531: INFO: stderr: ""
  Apr 23 17:25:22.531: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/23/23 17:25:22.531
  Apr 23 17:25:22.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2670 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 23 17:25:22.624: INFO: stderr: ""
  Apr 23 17:25:22.624: INFO: stdout: "service/rm2 exposed\n"
  Apr 23 17:25:22.630: INFO: Service rm2 in namespace kubectl-2670 found.
  STEP: exposing service @ 04/23/23 17:25:24.64
  Apr 23 17:25:24.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2670 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Apr 23 17:25:24.729: INFO: stderr: ""
  Apr 23 17:25:24.729: INFO: stdout: "service/rm3 exposed\n"
  Apr 23 17:25:24.736: INFO: Service rm3 in namespace kubectl-2670 found.
  Apr 23 17:25:26.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2670" for this suite. @ 04/23/23 17:25:26.752
• [6.718 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/23/23 17:25:26.763
  Apr 23 17:25:26.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 17:25:26.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:25:26.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:25:26.79
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/23/23 17:25:26.798
  Apr 23 17:25:26.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:25:28.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:25:33.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9333" for this suite. @ 04/23/23 17:25:33.865
• [7.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/23/23 17:25:33.875
  Apr 23 17:25:33.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename hostport @ 04/23/23 17:25:33.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:25:33.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:25:33.97
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/23/23 17:25:33.979
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.86.26 on the node which pod1 resides and expect scheduled @ 04/23/23 17:25:36.004
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.86.26 but use UDP protocol on the node which pod2 resides @ 04/23/23 17:25:48.052
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/23/23 17:25:52.087
  Apr 23 17:25:52.087: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.86.26 http://127.0.0.1:54323/hostname] Namespace:hostport-596 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:25:52.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:25:52.088: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:25:52.088: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-596/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.86.26+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.86.26, port: 54323 @ 04/23/23 17:25:52.17
  Apr 23 17:25:52.171: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.86.26:54323/hostname] Namespace:hostport-596 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:25:52.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:25:52.171: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:25:52.171: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-596/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.86.26%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.86.26, port: 54323 UDP @ 04/23/23 17:25:52.265
  Apr 23 17:25:52.265: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.86.26 54323] Namespace:hostport-596 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:25:52.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:25:52.266: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:25:52.266: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-596/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.86.26+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Apr 23 17:25:57.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-596" for this suite. @ 04/23/23 17:25:57.353
• [23.486 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/23/23 17:25:57.362
  Apr 23 17:25:57.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 17:25:57.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:25:57.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:25:57.391
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/23/23 17:25:57.395
  Apr 23 17:25:57.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-5574 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 23 17:25:57.476: INFO: stderr: ""
  Apr 23 17:25:57.476: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/23/23 17:25:57.476
  Apr 23 17:25:57.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-5574 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Apr 23 17:25:57.564: INFO: stderr: ""
  Apr 23 17:25:57.564: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/23/23 17:25:57.564
  Apr 23 17:25:57.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-5574 delete pods e2e-test-httpd-pod'
  Apr 23 17:25:59.661: INFO: stderr: ""
  Apr 23 17:25:59.661: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 23 17:25:59.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5574" for this suite. @ 04/23/23 17:25:59.667
• [2.312 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/23/23 17:25:59.675
  Apr 23 17:25:59.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 17:25:59.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:25:59.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:25:59.7
  STEP: creating a ReplicationController @ 04/23/23 17:25:59.709
  STEP: waiting for RC to be added @ 04/23/23 17:25:59.717
  STEP: waiting for available Replicas @ 04/23/23 17:25:59.717
  STEP: patching ReplicationController @ 04/23/23 17:26:00.642
  STEP: waiting for RC to be modified @ 04/23/23 17:26:00.651
  STEP: patching ReplicationController status @ 04/23/23 17:26:00.651
  STEP: waiting for RC to be modified @ 04/23/23 17:26:00.661
  STEP: waiting for available Replicas @ 04/23/23 17:26:00.661
  STEP: fetching ReplicationController status @ 04/23/23 17:26:00.664
  STEP: patching ReplicationController scale @ 04/23/23 17:26:00.671
  STEP: waiting for RC to be modified @ 04/23/23 17:26:00.679
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/23/23 17:26:00.679
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/23/23 17:26:01.881
  STEP: updating ReplicationController status @ 04/23/23 17:26:01.886
  STEP: waiting for RC to be modified @ 04/23/23 17:26:01.893
  STEP: listing all ReplicationControllers @ 04/23/23 17:26:01.893
  STEP: checking that ReplicationController has expected values @ 04/23/23 17:26:01.897
  STEP: deleting ReplicationControllers by collection @ 04/23/23 17:26:01.897
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/23/23 17:26:01.912
  Apr 23 17:26:01.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 17:26:01.958381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-9888" for this suite. @ 04/23/23 17:26:01.962
• [2.298 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/23/23 17:26:01.974
  Apr 23 17:26:01.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:26:01.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:26:01.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:26:02.003
  STEP: creating a ConfigMap @ 04/23/23 17:26:02.009
  STEP: fetching the ConfigMap @ 04/23/23 17:26:02.017
  STEP: patching the ConfigMap @ 04/23/23 17:26:02.024
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/23/23 17:26:02.037
  STEP: deleting the ConfigMap by collection with a label selector @ 04/23/23 17:26:02.042
  STEP: listing all ConfigMaps in test namespace @ 04/23/23 17:26:02.06
  Apr 23 17:26:02.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4759" for this suite. @ 04/23/23 17:26:02.072
• [0.114 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/23/23 17:26:02.089
  Apr 23 17:26:02.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 17:26:02.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:26:02.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:26:02.118
  STEP: create the container @ 04/23/23 17:26:02.125
  W0423 17:26:02.140733      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/23/23 17:26:02.141
  E0423 17:26:02.958676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:03.959188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:04.959294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/23/23 17:26:05.164
  STEP: the container should be terminated @ 04/23/23 17:26:05.169
  STEP: the termination message should be set @ 04/23/23 17:26:05.169
  Apr 23 17:26:05.169: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/23/23 17:26:05.169
  Apr 23 17:26:05.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4996" for this suite. @ 04/23/23 17:26:05.195
• [3.113 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/23/23 17:26:05.203
  Apr 23 17:26:05.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:26:05.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:26:05.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:26:05.237
  STEP: Creating the pod @ 04/23/23 17:26:05.24
  E0423 17:26:05.959439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:06.959861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:26:07.801: INFO: Successfully updated pod "annotationupdate4755733c-f13e-43c7-bc53-35024dfb7ea9"
  E0423 17:26:07.960555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:08.960689      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:26:09.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8250" for this suite. @ 04/23/23 17:26:09.832
• [4.639 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/23/23 17:26:09.843
  Apr 23 17:26:09.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename containers @ 04/23/23 17:26:09.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:26:09.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:26:09.873
  E0423 17:26:09.961245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:10.961325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:26:11.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8153" for this suite. @ 04/23/23 17:26:11.925
• [2.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/23/23 17:26:11.936
  Apr 23 17:26:11.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:26:11.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:26:11.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:26:11.961
  E0423 17:26:11.961355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating configMap with name configmap-test-volume-map-9a2d611f-4c06-48ba-8237-be99479070a5 @ 04/23/23 17:26:11.967
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:26:11.974
  E0423 17:26:12.961733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:13.961836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:14.962255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:15.962483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:26:16.006
  Apr 23 17:26:16.010: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-b3e8c5de-95e2-4a34-8937-1153873b94fc container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:26:16.018
  Apr 23 17:26:16.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4053" for this suite. @ 04/23/23 17:26:16.048
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/23/23 17:26:16.064
  Apr 23 17:26:16.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 17:26:16.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:26:16.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:26:16.101
  Apr 23 17:26:16.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9241" for this suite. @ 04/23/23 17:26:16.128
• [0.074 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/23/23 17:26:16.138
  Apr 23 17:26:16.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/23/23 17:26:16.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:26:16.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:26:16.164
  STEP: create the container to handle the HTTPGet hook request. @ 04/23/23 17:26:16.176
  E0423 17:26:16.963011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:17.964096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/23/23 17:26:18.201
  E0423 17:26:18.964550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:19.965064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/23/23 17:26:20.222
  STEP: delete the pod with lifecycle hook @ 04/23/23 17:26:20.23
  E0423 17:26:20.965177      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:21.965241      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:26:22.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6805" for this suite. @ 04/23/23 17:26:22.255
• [6.125 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/23/23 17:26:22.269
  Apr 23 17:26:22.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 17:26:22.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:26:22.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:26:22.299
  E0423 17:26:22.965374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:23.965687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:24.966294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:25.966373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:26.966569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:27.966563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:28.967456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:29.967568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:30.967669      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:31.968079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:32.968184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:33.969165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:34.969271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:35.969541      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:36.969471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:37.970080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:38.970175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:39.970267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:40.970463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:41.970477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:42.970513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:43.971048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:44.972074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:45.972987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:46.973242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:47.973877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:48.973952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:49.974776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:50.975686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:51.975793      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:52.975891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:53.975998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:54.976034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:55.977140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:56.977723      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:57.978612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:58.979184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:26:59.979305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:00.980158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:01.981029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:02.981756      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:03.982511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:04.982642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:05.983375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:06.983586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:07.983755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:08.984239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:09.984351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:10.984403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:11.984501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:12.984561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:13.984700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:14.984813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:15.985595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:16.986310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:17.987180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:18.987281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:19.988364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:20.989404      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:21.989521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:22.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1951" for this suite. @ 04/23/23 17:27:22.326
• [60.066 seconds]
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/23/23 17:27:22.336
  Apr 23 17:27:22.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:27:22.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:27:22.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:27:22.366
  STEP: Creating the pod @ 04/23/23 17:27:22.369
  E0423 17:27:22.989736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:23.990197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:24.931: INFO: Successfully updated pod "labelsupdate41630c5d-494c-4906-8fa6-0944bb8d5b86"
  E0423 17:27:24.990825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:25.991180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:26.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9827" for this suite. @ 04/23/23 17:27:26.96
• [4.632 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 04/23/23 17:27:26.97
  Apr 23 17:27:26.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 17:27:26.971
  E0423 17:27:26.991555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:27:26.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:27:26.999
  Apr 23 17:27:27.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:27:27.991682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:28.992132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0423 17:27:29.668054      21 warnings.go:70] unknown field "alpha"
  W0423 17:27:29.668077      21 warnings.go:70] unknown field "beta"
  W0423 17:27:29.668084      21 warnings.go:70] unknown field "delta"
  W0423 17:27:29.668090      21 warnings.go:70] unknown field "epsilon"
  W0423 17:27:29.668096      21 warnings.go:70] unknown field "gamma"
  Apr 23 17:27:29.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5269" for this suite. @ 04/23/23 17:27:29.721
• [2.762 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/23/23 17:27:29.732
  Apr 23 17:27:29.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:27:29.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:27:29.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:27:29.763
  STEP: Counting existing ResourceQuota @ 04/23/23 17:27:29.769
  E0423 17:27:29.993241      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:30.994013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:31.994993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:32.995489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:33.995956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 17:27:34.775
  STEP: Ensuring resource quota status is calculated @ 04/23/23 17:27:34.786
  E0423 17:27:34.996680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:35.996796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 04/23/23 17:27:36.792
  STEP: Creating a NodePort Service @ 04/23/23 17:27:36.817
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/23/23 17:27:36.844
  STEP: Ensuring resource quota status captures service creation @ 04/23/23 17:27:36.873
  E0423 17:27:36.997479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:37.998123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 04/23/23 17:27:38.879
  STEP: Ensuring resource quota status released usage @ 04/23/23 17:27:38.951
  E0423 17:27:38.998626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:39.998747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:40.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7023" for this suite. @ 04/23/23 17:27:40.963
• [11.239 seconds]
------------------------------
S
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/23/23 17:27:40.972
  Apr 23 17:27:40.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:27:40.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:27:40.997
  E0423 17:27:41.000291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:27:41.007
  STEP: Creating configMap that has name configmap-test-emptyKey-81a8b143-916e-460c-ae8b-2fea97ba3b13 @ 04/23/23 17:27:41.011
  Apr 23 17:27:41.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3208" for this suite. @ 04/23/23 17:27:41.018
• [0.055 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/23/23 17:27:41.031
  Apr 23 17:27:41.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:27:41.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:27:41.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:27:41.06
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:27:41.065
  E0423 17:27:42.000389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:43.000886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:44.002008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:45.003173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:27:45.1
  Apr 23 17:27:45.105: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-3f86f9ce-99fc-4218-a8cd-0189d843c76e container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:27:45.117
  Apr 23 17:27:45.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1220" for this suite. @ 04/23/23 17:27:45.14
• [4.120 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/23/23 17:27:45.15
  Apr 23 17:27:45.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 17:27:45.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:27:45.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:27:45.184
  Apr 23 17:27:45.191: INFO: Creating ReplicaSet my-hostname-basic-2f4b1ec6-f7c6-4958-9ea4-26bcf7454d5e
  Apr 23 17:27:45.203: INFO: Pod name my-hostname-basic-2f4b1ec6-f7c6-4958-9ea4-26bcf7454d5e: Found 0 pods out of 1
  E0423 17:27:46.004199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:47.004313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:48.004886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:49.005010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:50.005243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:50.207: INFO: Pod name my-hostname-basic-2f4b1ec6-f7c6-4958-9ea4-26bcf7454d5e: Found 1 pods out of 1
  Apr 23 17:27:50.207: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2f4b1ec6-f7c6-4958-9ea4-26bcf7454d5e" is running
  Apr 23 17:27:50.211: INFO: Pod "my-hostname-basic-2f4b1ec6-f7c6-4958-9ea4-26bcf7454d5e-j52g5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 17:27:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 17:27:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 17:27:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 17:27:45 +0000 UTC Reason: Message:}])
  Apr 23 17:27:50.211: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/23/23 17:27:50.211
  Apr 23 17:27:50.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1920" for this suite. @ 04/23/23 17:27:50.23
• [5.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/23/23 17:27:50.24
  Apr 23 17:27:50.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl-logs @ 04/23/23 17:27:50.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:27:50.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:27:50.279
  STEP: creating an pod @ 04/23/23 17:27:50.284
  Apr 23 17:27:50.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-logs-2358 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Apr 23 17:27:50.379: INFO: stderr: ""
  Apr 23 17:27:50.379: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/23/23 17:27:50.379
  Apr 23 17:27:50.379: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0423 17:27:51.006240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:52.006400      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:52.390: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/23/23 17:27:52.39
  Apr 23 17:27:52.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-logs-2358 logs logs-generator logs-generator'
  Apr 23 17:27:52.480: INFO: stderr: ""
  Apr 23 17:27:52.480: INFO: stdout: "I0423 17:27:51.199607       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/2lm6 444\nI0423 17:27:51.399849       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/h68 309\nI0423 17:27:51.600188       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/tsmd 481\nI0423 17:27:51.800477       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/cwz 371\nI0423 17:27:51.999794       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/htf 473\nI0423 17:27:52.200167       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/v2cl 254\nI0423 17:27:52.400491       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/9cpg 322\n"
  STEP: limiting log lines @ 04/23/23 17:27:52.48
  Apr 23 17:27:52.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-logs-2358 logs logs-generator logs-generator --tail=1'
  Apr 23 17:27:52.576: INFO: stderr: ""
  Apr 23 17:27:52.576: INFO: stdout: "I0423 17:27:52.400491       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/9cpg 322\n"
  Apr 23 17:27:52.576: INFO: got output "I0423 17:27:52.400491       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/9cpg 322\n"
  STEP: limiting log bytes @ 04/23/23 17:27:52.576
  Apr 23 17:27:52.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-logs-2358 logs logs-generator logs-generator --limit-bytes=1'
  Apr 23 17:27:52.661: INFO: stderr: ""
  Apr 23 17:27:52.661: INFO: stdout: "I"
  Apr 23 17:27:52.661: INFO: got output "I"
  STEP: exposing timestamps @ 04/23/23 17:27:52.661
  Apr 23 17:27:52.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-logs-2358 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 23 17:27:52.762: INFO: stderr: ""
  Apr 23 17:27:52.762: INFO: stdout: "2023-04-23T17:27:52.599894988Z I0423 17:27:52.599720       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2qq 283\n"
  Apr 23 17:27:52.762: INFO: got output "2023-04-23T17:27:52.599894988Z I0423 17:27:52.599720       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2qq 283\n"
  STEP: restricting to a time range @ 04/23/23 17:27:52.762
  E0423 17:27:53.006630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:54.006737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:55.007347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:55.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-logs-2358 logs logs-generator logs-generator --since=1s'
  Apr 23 17:27:55.345: INFO: stderr: ""
  Apr 23 17:27:55.345: INFO: stdout: "I0423 17:27:54.400525       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/764 268\nI0423 17:27:54.599729       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/nr9 459\nI0423 17:27:54.800052       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/ktgq 425\nI0423 17:27:55.000432       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/j2h6 340\nI0423 17:27:55.199686       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/2gwr 322\n"
  Apr 23 17:27:55.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-logs-2358 logs logs-generator logs-generator --since=24h'
  Apr 23 17:27:55.426: INFO: stderr: ""
  Apr 23 17:27:55.426: INFO: stdout: "I0423 17:27:51.199607       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/2lm6 444\nI0423 17:27:51.399849       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/h68 309\nI0423 17:27:51.600188       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/tsmd 481\nI0423 17:27:51.800477       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/cwz 371\nI0423 17:27:51.999794       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/htf 473\nI0423 17:27:52.200167       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/v2cl 254\nI0423 17:27:52.400491       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/9cpg 322\nI0423 17:27:52.599720       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2qq 283\nI0423 17:27:52.800270       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/k7r 255\nI0423 17:27:53.001231       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/qps6 247\nI0423 17:27:53.200550       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/mxp 468\nI0423 17:27:53.399801       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/gwr 524\nI0423 17:27:53.600119       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/7clh 422\nI0423 17:27:53.800468       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/jw8 211\nI0423 17:27:53.999787       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/vxw 531\nI0423 17:27:54.200143       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/vsh 472\nI0423 17:27:54.400525       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/764 268\nI0423 17:27:54.599729       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/nr9 459\nI0423 17:27:54.800052       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/ktgq 425\nI0423 17:27:55.000432       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/j2h6 340\nI0423 17:27:55.199686       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/2gwr 322\nI0423 17:27:55.400019       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/ms5c 515\n"
  Apr 23 17:27:55.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-logs-2358 delete pod logs-generator'
  E0423 17:27:56.008020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:57.009772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:57.199: INFO: stderr: ""
  Apr 23 17:27:57.199: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 23 17:27:57.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-2358" for this suite. @ 04/23/23 17:27:57.204
• [6.972 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/23/23 17:27:57.214
  Apr 23 17:27:57.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 17:27:57.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:27:57.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:27:57.243
  STEP: Creating pod test-grpc-79d21bde-e4d6-4ccc-bb99-8ba6091322a3 in namespace container-probe-1074 @ 04/23/23 17:27:57.248
  E0423 17:27:58.010418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:27:59.010905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:27:59.271: INFO: Started pod test-grpc-79d21bde-e4d6-4ccc-bb99-8ba6091322a3 in namespace container-probe-1074
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 17:27:59.271
  Apr 23 17:27:59.275: INFO: Initial restart count of pod test-grpc-79d21bde-e4d6-4ccc-bb99-8ba6091322a3 is 0
  E0423 17:28:00.011472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:01.011560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:02.012067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:03.012383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:04.012501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:05.012610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:06.013436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:07.014180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:08.014284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:09.014469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:10.014599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:11.014816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:12.015007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:13.015052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:14.015915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:15.016061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:16.016147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:17.016361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:18.016497      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:19.016721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:20.017836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:21.018241      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:22.018364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:23.018764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:24.018781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:25.019002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:26.019108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:27.020082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:28.021052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:29.021714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:30.021768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:31.021886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:32.022768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:33.023064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:34.023187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:35.024082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:36.024193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:37.024307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:38.024454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:39.024519      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:40.024558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:41.024655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:42.024708      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:43.024796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:44.025648      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:45.025693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:46.026155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:47.026238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:48.026664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:49.026761      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:50.027647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:51.028640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:52.029046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:53.029833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:54.029954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:55.030055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:56.030109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:57.030217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:58.030945      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:28:59.031029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:00.031943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:01.032076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:02.032160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:03.032466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:04.033212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:05.033275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:06.034285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:07.034501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:08.035477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:09.036097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:10.036204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:11.036302      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:12.036966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:13.037066      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:13.496: INFO: Restart count of pod container-probe-1074/test-grpc-79d21bde-e4d6-4ccc-bb99-8ba6091322a3 is now 1 (1m14.220665013s elapsed)
  Apr 23 17:29:13.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 17:29:13.5
  STEP: Destroying namespace "container-probe-1074" for this suite. @ 04/23/23 17:29:13.516
• [76.317 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/23/23 17:29:13.532
  Apr 23 17:29:13.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename subjectreview @ 04/23/23 17:29:13.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:13.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:13.561
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-5" @ 04/23/23 17:29:13.565
  Apr 23 17:29:13.572: INFO: saUsername: "system:serviceaccount:subjectreview-5:e2e"
  Apr 23 17:29:13.572: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-5"}
  Apr 23 17:29:13.573: INFO: saUID: "1f1e0142-bfad-4251-8b24-6cb4b21b71b0"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-5:e2e" @ 04/23/23 17:29:13.573
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-5:e2e" @ 04/23/23 17:29:13.573
  Apr 23 17:29:13.576: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-5:e2e" api 'list' configmaps in "subjectreview-5" namespace @ 04/23/23 17:29:13.576
  Apr 23 17:29:13.579: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-5:e2e" @ 04/23/23 17:29:13.579
  Apr 23 17:29:13.584: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Apr 23 17:29:13.584: INFO: LocalSubjectAccessReview has been verified
  Apr 23 17:29:13.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-5" for this suite. @ 04/23/23 17:29:13.589
• [0.065 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/23/23 17:29:13.597
  Apr 23 17:29:13.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/23/23 17:29:13.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:13.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:13.638
  STEP: create the container to handle the HTTPGet hook request. @ 04/23/23 17:29:13.648
  E0423 17:29:14.037863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:15.037962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/23/23 17:29:15.676
  E0423 17:29:16.038238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:17.038747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/23/23 17:29:17.699
  E0423 17:29:18.039726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:19.039811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/23/23 17:29:19.718
  Apr 23 17:29:19.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7429" for this suite. @ 04/23/23 17:29:19.753
• [6.164 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/23/23 17:29:19.761
  Apr 23 17:29:19.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:29:19.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:19.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:19.787
  STEP: Setting up server cert @ 04/23/23 17:29:19.829
  E0423 17:29:20.040078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:29:20.138
  STEP: Deploying the webhook pod @ 04/23/23 17:29:20.148
  STEP: Wait for the deployment to be ready @ 04/23/23 17:29:20.161
  Apr 23 17:29:20.172: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:29:21.040161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:22.040347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:29:22.185
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:29:22.194
  E0423 17:29:23.041331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:23.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/23/23 17:29:23.198
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/23/23 17:29:23.2
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/23/23 17:29:23.2
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/23/23 17:29:23.2
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/23/23 17:29:23.201
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/23/23 17:29:23.201
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/23/23 17:29:23.202
  Apr 23 17:29:23.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1206" for this suite. @ 04/23/23 17:29:23.268
  STEP: Destroying namespace "webhook-markers-7659" for this suite. @ 04/23/23 17:29:23.275
• [3.521 seconds]
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/23/23 17:29:23.282
  Apr 23 17:29:23.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename security-context-test @ 04/23/23 17:29:23.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:23.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:23.314
  E0423 17:29:24.041582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:25.042284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:26.042545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:27.042746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:27.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5583" for this suite. @ 04/23/23 17:29:27.348
• [4.073 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/23/23 17:29:27.356
  Apr 23 17:29:27.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:29:27.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:27.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:27.385
  STEP: Creating configMap with name projected-configmap-test-volume-map-2234d2ec-ec5c-4730-94e3-56725150b50a @ 04/23/23 17:29:27.389
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:29:27.395
  E0423 17:29:28.043555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:29.043784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:30.044661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:31.044769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:29:31.419
  Apr 23 17:29:31.423: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-configmaps-21f5ba7d-50ad-45de-a012-53d5911dbc55 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:29:31.441
  Apr 23 17:29:31.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2294" for this suite. @ 04/23/23 17:29:31.467
• [4.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/23/23 17:29:31.479
  Apr 23 17:29:31.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename endpointslice @ 04/23/23 17:29:31.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:31.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:31.505
  STEP: getting /apis @ 04/23/23 17:29:31.508
  STEP: getting /apis/discovery.k8s.io @ 04/23/23 17:29:31.513
  STEP: getting /apis/discovery.k8s.iov1 @ 04/23/23 17:29:31.515
  STEP: creating @ 04/23/23 17:29:31.516
  STEP: getting @ 04/23/23 17:29:31.537
  STEP: listing @ 04/23/23 17:29:31.542
  STEP: watching @ 04/23/23 17:29:31.546
  Apr 23 17:29:31.546: INFO: starting watch
  STEP: cluster-wide listing @ 04/23/23 17:29:31.548
  STEP: cluster-wide watching @ 04/23/23 17:29:31.552
  Apr 23 17:29:31.552: INFO: starting watch
  STEP: patching @ 04/23/23 17:29:31.554
  STEP: updating @ 04/23/23 17:29:31.559
  Apr 23 17:29:31.574: INFO: waiting for watch events with expected annotations
  Apr 23 17:29:31.574: INFO: saw patched and updated annotations
  STEP: deleting @ 04/23/23 17:29:31.575
  STEP: deleting a collection @ 04/23/23 17:29:31.593
  Apr 23 17:29:31.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6857" for this suite. @ 04/23/23 17:29:31.621
• [0.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/23/23 17:29:31.634
  Apr 23 17:29:31.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:29:31.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:31.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:31.668
  STEP: creating service endpoint-test2 in namespace services-6895 @ 04/23/23 17:29:31.673
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6895 to expose endpoints map[] @ 04/23/23 17:29:31.697
  Apr 23 17:29:31.703: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0423 17:29:32.045276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:32.716: INFO: successfully validated that service endpoint-test2 in namespace services-6895 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6895 @ 04/23/23 17:29:32.716
  E0423 17:29:33.045382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:34.046135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6895 to expose endpoints map[pod1:[80]] @ 04/23/23 17:29:34.746
  Apr 23 17:29:34.762: INFO: successfully validated that service endpoint-test2 in namespace services-6895 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/23/23 17:29:34.762
  Apr 23 17:29:34.762: INFO: Creating new exec pod
  E0423 17:29:35.046280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:36.047050      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:37.047734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:37.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-6895 exec execpodt9sgw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 23 17:29:37.979: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 23 17:29:37.979: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:29:37.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-6895 exec execpodt9sgw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.77 80'
  E0423 17:29:38.047873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:38.118: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.77 80\nConnection to 10.152.183.77 80 port [tcp/http] succeeded!\n"
  Apr 23 17:29:38.118: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-6895 @ 04/23/23 17:29:38.118
  E0423 17:29:39.048022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:40.048485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6895 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/23/23 17:29:40.139
  Apr 23 17:29:40.156: INFO: successfully validated that service endpoint-test2 in namespace services-6895 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/23/23 17:29:40.156
  E0423 17:29:41.049123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:41.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-6895 exec execpodt9sgw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 23 17:29:41.343: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 23 17:29:41.343: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:29:41.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-6895 exec execpodt9sgw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.77 80'
  Apr 23 17:29:41.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.77 80\nConnection to 10.152.183.77 80 port [tcp/http] succeeded!\n"
  Apr 23 17:29:41.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6895 @ 04/23/23 17:29:41.524
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6895 to expose endpoints map[pod2:[80]] @ 04/23/23 17:29:41.554
  Apr 23 17:29:41.574: INFO: successfully validated that service endpoint-test2 in namespace services-6895 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/23/23 17:29:41.574
  E0423 17:29:42.049553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:42.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-6895 exec execpodt9sgw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 23 17:29:42.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 23 17:29:42.706: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:29:42.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-6895 exec execpodt9sgw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.77 80'
  Apr 23 17:29:42.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.77 80\nConnection to 10.152.183.77 80 port [tcp/http] succeeded!\n"
  Apr 23 17:29:42.849: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-6895 @ 04/23/23 17:29:42.85
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6895 to expose endpoints map[] @ 04/23/23 17:29:42.871
  Apr 23 17:29:42.885: INFO: successfully validated that service endpoint-test2 in namespace services-6895 exposes endpoints map[]
  Apr 23 17:29:42.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6895" for this suite. @ 04/23/23 17:29:42.909
• [11.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/23/23 17:29:42.918
  Apr 23 17:29:42.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename disruption @ 04/23/23 17:29:42.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:42.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:42.947
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:29:42.956
  E0423 17:29:43.050605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:44.050774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/23/23 17:29:44.991
  Apr 23 17:29:44.995: INFO: running pods: 0 < 3
  E0423 17:29:45.051269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:46.051404      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:47.001: INFO: running pods: 1 < 3
  E0423 17:29:47.051429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:48.051581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:49.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-121" for this suite. @ 04/23/23 17:29:49.009
• [6.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/23/23 17:29:49.016
  Apr 23 17:29:49.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-webhook @ 04/23/23 17:29:49.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:49.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:49.041
  STEP: Setting up server cert @ 04/23/23 17:29:49.045
  E0423 17:29:49.052129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/23/23 17:29:49.414
  STEP: Deploying the custom resource conversion webhook pod @ 04/23/23 17:29:49.42
  STEP: Wait for the deployment to be ready @ 04/23/23 17:29:49.432
  Apr 23 17:29:49.441: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0423 17:29:50.053053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:51.053273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:29:51.453
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:29:51.467
  E0423 17:29:52.053794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:52.468: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 23 17:29:52.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:29:53.054662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:54.054920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:55.055092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/23/23 17:29:55.079
  STEP: v2 custom resource should be converted @ 04/23/23 17:29:55.087
  Apr 23 17:29:55.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-3606" for this suite. @ 04/23/23 17:29:55.677
• [6.673 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/23/23 17:29:55.69
  Apr 23 17:29:55.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename init-container @ 04/23/23 17:29:55.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:55.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:55.719
  STEP: creating the pod @ 04/23/23 17:29:55.724
  Apr 23 17:29:55.724: INFO: PodSpec: initContainers in spec.initContainers
  E0423 17:29:56.055868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:57.056410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:58.056510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:29:59.056605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:29:59.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8612" for this suite. @ 04/23/23 17:29:59.389
• [3.710 seconds]
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/23/23 17:29:59.4
  Apr 23 17:29:59.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename svc-latency @ 04/23/23 17:29:59.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:29:59.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:29:59.426
  Apr 23 17:29:59.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7557 @ 04/23/23 17:29:59.43
  I0423 17:29:59.436976      21 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7557, replica count: 1
  E0423 17:30:00.057724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 17:30:00.487407      21 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 17:30:01.057985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 17:30:01.487905      21 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 17:30:01.607: INFO: Created: latency-svc-5zzh7
  Apr 23 17:30:01.625: INFO: Got endpoints: latency-svc-5zzh7 [36.638646ms]
  Apr 23 17:30:01.647: INFO: Created: latency-svc-k8t2t
  Apr 23 17:30:01.657: INFO: Created: latency-svc-vk2hx
  Apr 23 17:30:01.668: INFO: Created: latency-svc-tcvch
  Apr 23 17:30:01.670: INFO: Got endpoints: latency-svc-k8t2t [44.612047ms]
  Apr 23 17:30:01.674: INFO: Created: latency-svc-wx59q
  Apr 23 17:30:01.685: INFO: Created: latency-svc-qv585
  Apr 23 17:30:01.705: INFO: Created: latency-svc-xn79s
  Apr 23 17:30:01.710: INFO: Created: latency-svc-5f8h2
  Apr 23 17:30:01.717: INFO: Got endpoints: latency-svc-tcvch [91.59438ms]
  Apr 23 17:30:01.717: INFO: Got endpoints: latency-svc-vk2hx [91.628013ms]
  Apr 23 17:30:01.729: INFO: Got endpoints: latency-svc-5f8h2 [103.488175ms]
  Apr 23 17:30:01.730: INFO: Got endpoints: latency-svc-wx59q [104.307486ms]
  Apr 23 17:30:01.730: INFO: Got endpoints: latency-svc-xn79s [104.246863ms]
  Apr 23 17:30:01.733: INFO: Created: latency-svc-mv6kz
  Apr 23 17:30:01.747: INFO: Got endpoints: latency-svc-qv585 [120.955719ms]
  Apr 23 17:30:01.747: INFO: Got endpoints: latency-svc-mv6kz [120.757958ms]
  Apr 23 17:30:01.749: INFO: Created: latency-svc-r2b4s
  Apr 23 17:30:01.757: INFO: Got endpoints: latency-svc-r2b4s [130.325ms]
  Apr 23 17:30:01.761: INFO: Created: latency-svc-5x5x8
  Apr 23 17:30:01.767: INFO: Created: latency-svc-vg2x5
  Apr 23 17:30:01.770: INFO: Got endpoints: latency-svc-5x5x8 [143.894333ms]
  Apr 23 17:30:01.781: INFO: Got endpoints: latency-svc-vg2x5 [154.54265ms]
  Apr 23 17:30:01.781: INFO: Created: latency-svc-mwwt8
  Apr 23 17:30:01.787: INFO: Created: latency-svc-jswkr
  Apr 23 17:30:01.794: INFO: Created: latency-svc-f4rnj
  Apr 23 17:30:01.803: INFO: Created: latency-svc-j6dhx
  Apr 23 17:30:01.804: INFO: Got endpoints: latency-svc-mwwt8 [177.235929ms]
  Apr 23 17:30:01.812: INFO: Got endpoints: latency-svc-jswkr [185.505507ms]
  Apr 23 17:30:01.813: INFO: Created: latency-svc-htmjr
  Apr 23 17:30:01.818: INFO: Got endpoints: latency-svc-f4rnj [191.767236ms]
  Apr 23 17:30:01.823: INFO: Got endpoints: latency-svc-j6dhx [196.551335ms]
  Apr 23 17:30:01.825: INFO: Created: latency-svc-wlw8x
  Apr 23 17:30:01.833: INFO: Got endpoints: latency-svc-htmjr [162.41807ms]
  Apr 23 17:30:01.837: INFO: Got endpoints: latency-svc-wlw8x [119.774862ms]
  Apr 23 17:30:01.837: INFO: Created: latency-svc-t4hkk
  Apr 23 17:30:01.847: INFO: Got endpoints: latency-svc-t4hkk [129.235495ms]
  Apr 23 17:30:01.850: INFO: Created: latency-svc-8fdxw
  Apr 23 17:30:01.854: INFO: Created: latency-svc-hg6fn
  Apr 23 17:30:01.856: INFO: Got endpoints: latency-svc-8fdxw [125.948309ms]
  Apr 23 17:30:01.862: INFO: Created: latency-svc-rm9zt
  Apr 23 17:30:01.866: INFO: Got endpoints: latency-svc-hg6fn [136.219767ms]
  Apr 23 17:30:01.871: INFO: Created: latency-svc-4bt2k
  Apr 23 17:30:01.871: INFO: Got endpoints: latency-svc-rm9zt [140.525412ms]
  Apr 23 17:30:01.880: INFO: Created: latency-svc-f6vlh
  Apr 23 17:30:01.882: INFO: Got endpoints: latency-svc-4bt2k [134.593352ms]
  Apr 23 17:30:01.888: INFO: Created: latency-svc-9gsdv
  Apr 23 17:30:01.889: INFO: Got endpoints: latency-svc-f6vlh [141.936919ms]
  Apr 23 17:30:01.895: INFO: Got endpoints: latency-svc-9gsdv [138.748986ms]
  Apr 23 17:30:01.899: INFO: Created: latency-svc-rhqnk
  Apr 23 17:30:01.907: INFO: Created: latency-svc-hc7gd
  Apr 23 17:30:01.908: INFO: Got endpoints: latency-svc-rhqnk [137.767669ms]
  Apr 23 17:30:01.917: INFO: Created: latency-svc-wlqf5
  Apr 23 17:30:01.918: INFO: Got endpoints: latency-svc-hc7gd [137.349907ms]
  Apr 23 17:30:01.926: INFO: Got endpoints: latency-svc-wlqf5 [122.434647ms]
  Apr 23 17:30:01.935: INFO: Created: latency-svc-79dmj
  Apr 23 17:30:01.940: INFO: Got endpoints: latency-svc-79dmj [126.053187ms]
  Apr 23 17:30:02.034: INFO: Created: latency-svc-r6qft
  Apr 23 17:30:02.034: INFO: Created: latency-svc-mztns
  Apr 23 17:30:02.035: INFO: Created: latency-svc-xjxh5
  Apr 23 17:30:02.035: INFO: Created: latency-svc-l86cn
  Apr 23 17:30:02.035: INFO: Created: latency-svc-sbg5b
  Apr 23 17:30:02.038: INFO: Created: latency-svc-msrcr
  Apr 23 17:30:02.038: INFO: Created: latency-svc-gwwnf
  Apr 23 17:30:02.040: INFO: Created: latency-svc-rfql2
  Apr 23 17:30:02.041: INFO: Created: latency-svc-bbg96
  Apr 23 17:30:02.041: INFO: Created: latency-svc-mw64x
  Apr 23 17:30:02.042: INFO: Created: latency-svc-82hf9
  Apr 23 17:30:02.042: INFO: Created: latency-svc-b22dq
  Apr 23 17:30:02.052: INFO: Created: latency-svc-qqhd7
  Apr 23 17:30:02.053: INFO: Created: latency-svc-b42h9
  Apr 23 17:30:02.053: INFO: Created: latency-svc-7wq9z
  Apr 23 17:30:02.053: INFO: Got endpoints: latency-svc-xjxh5 [157.75415ms]
  Apr 23 17:30:02.054: INFO: Got endpoints: latency-svc-r6qft [135.051854ms]
  E0423 17:30:02.059508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:02.059: INFO: Got endpoints: latency-svc-mztns [176.837036ms]
  Apr 23 17:30:02.076: INFO: Created: latency-svc-p4pfj
  Apr 23 17:30:02.076: INFO: Got endpoints: latency-svc-sbg5b [220.230952ms]
  Apr 23 17:30:02.076: INFO: Got endpoints: latency-svc-l86cn [205.066553ms]
  Apr 23 17:30:02.076: INFO: Got endpoints: latency-svc-gwwnf [149.876486ms]
  Apr 23 17:30:02.076: INFO: Got endpoints: latency-svc-rfql2 [136.615576ms]
  Apr 23 17:30:02.076: INFO: Got endpoints: latency-svc-msrcr [243.122953ms]
  Apr 23 17:30:02.098: INFO: Got endpoints: latency-svc-qqhd7 [251.334897ms]
  Apr 23 17:30:02.105: INFO: Got endpoints: latency-svc-b42h9 [282.053691ms]
  Apr 23 17:30:02.110: INFO: Created: latency-svc-hflmx
  Apr 23 17:30:02.278: INFO: Got endpoints: latency-svc-7wq9z [389.28437ms]
  Apr 23 17:30:02.282: INFO: Got endpoints: latency-svc-b22dq [463.851114ms]
  Apr 23 17:30:02.283: INFO: Got endpoints: latency-svc-82hf9 [445.709816ms]
  Apr 23 17:30:02.282: INFO: Got endpoints: latency-svc-bbg96 [374.573271ms]
  Apr 23 17:30:02.291: INFO: Created: latency-svc-f2g59
  Apr 23 17:30:02.298: INFO: Created: latency-svc-sfwd9
  Apr 23 17:30:02.307: INFO: Created: latency-svc-xwpmv
  Apr 23 17:30:02.314: INFO: Got endpoints: latency-svc-mw64x [447.515375ms]
  Apr 23 17:30:02.316: INFO: Created: latency-svc-d9nh7
  Apr 23 17:30:02.321: INFO: Created: latency-svc-xf4ff
  Apr 23 17:30:02.331: INFO: Created: latency-svc-9hhhv
  Apr 23 17:30:02.339: INFO: Created: latency-svc-92sp7
  Apr 23 17:30:02.346: INFO: Created: latency-svc-x6r7j
  Apr 23 17:30:02.363: INFO: Created: latency-svc-z8mm7
  Apr 23 17:30:02.363: INFO: Got endpoints: latency-svc-p4pfj [310.187583ms]
  Apr 23 17:30:02.373: INFO: Created: latency-svc-9jdvc
  Apr 23 17:30:02.389: INFO: Created: latency-svc-gdh68
  Apr 23 17:30:02.399: INFO: Created: latency-svc-pzx96
  Apr 23 17:30:02.407: INFO: Created: latency-svc-q2hcs
  Apr 23 17:30:02.408: INFO: Got endpoints: latency-svc-hflmx [348.753786ms]
  Apr 23 17:30:02.413: INFO: Created: latency-svc-bjfqr
  Apr 23 17:30:02.421: INFO: Created: latency-svc-vsgfp
  Apr 23 17:30:02.461: INFO: Got endpoints: latency-svc-f2g59 [402.043281ms]
  Apr 23 17:30:02.477: INFO: Created: latency-svc-zhvrn
  Apr 23 17:30:02.510: INFO: Got endpoints: latency-svc-sfwd9 [433.822991ms]
  Apr 23 17:30:02.523: INFO: Created: latency-svc-c8dgj
  Apr 23 17:30:02.558: INFO: Got endpoints: latency-svc-xwpmv [481.110383ms]
  Apr 23 17:30:02.570: INFO: Created: latency-svc-2bjdx
  Apr 23 17:30:02.612: INFO: Got endpoints: latency-svc-d9nh7 [535.507469ms]
  Apr 23 17:30:02.629: INFO: Created: latency-svc-5kdx7
  Apr 23 17:30:02.661: INFO: Got endpoints: latency-svc-xf4ff [583.86908ms]
  Apr 23 17:30:02.674: INFO: Created: latency-svc-x5sc6
  Apr 23 17:30:02.709: INFO: Got endpoints: latency-svc-9hhhv [630.975202ms]
  Apr 23 17:30:02.723: INFO: Created: latency-svc-g28vv
  Apr 23 17:30:02.761: INFO: Got endpoints: latency-svc-92sp7 [662.448378ms]
  Apr 23 17:30:02.775: INFO: Created: latency-svc-qm79w
  Apr 23 17:30:02.813: INFO: Got endpoints: latency-svc-x6r7j [706.510838ms]
  Apr 23 17:30:02.823: INFO: Created: latency-svc-k4zwx
  Apr 23 17:30:02.860: INFO: Got endpoints: latency-svc-z8mm7 [581.06831ms]
  Apr 23 17:30:02.871: INFO: Created: latency-svc-587l8
  Apr 23 17:30:02.909: INFO: Got endpoints: latency-svc-9jdvc [624.882294ms]
  Apr 23 17:30:02.922: INFO: Created: latency-svc-zlpw9
  Apr 23 17:30:02.959: INFO: Got endpoints: latency-svc-gdh68 [676.330889ms]
  Apr 23 17:30:02.974: INFO: Created: latency-svc-fxw6g
  Apr 23 17:30:03.013: INFO: Got endpoints: latency-svc-pzx96 [729.235016ms]
  Apr 23 17:30:03.026: INFO: Created: latency-svc-qhf9r
  E0423 17:30:03.060110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:03.060: INFO: Got endpoints: latency-svc-q2hcs [744.654572ms]
  Apr 23 17:30:03.081: INFO: Created: latency-svc-mhbs8
  Apr 23 17:30:03.110: INFO: Got endpoints: latency-svc-bjfqr [746.639295ms]
  Apr 23 17:30:03.122: INFO: Created: latency-svc-6kmfq
  Apr 23 17:30:03.160: INFO: Got endpoints: latency-svc-vsgfp [751.727014ms]
  Apr 23 17:30:03.172: INFO: Created: latency-svc-m4nj9
  Apr 23 17:30:03.209: INFO: Got endpoints: latency-svc-zhvrn [747.51684ms]
  Apr 23 17:30:03.222: INFO: Created: latency-svc-gd9m8
  Apr 23 17:30:03.261: INFO: Got endpoints: latency-svc-c8dgj [750.8435ms]
  Apr 23 17:30:03.274: INFO: Created: latency-svc-cxcb7
  Apr 23 17:30:03.310: INFO: Got endpoints: latency-svc-2bjdx [751.317671ms]
  Apr 23 17:30:03.322: INFO: Created: latency-svc-c97bg
  Apr 23 17:30:03.360: INFO: Got endpoints: latency-svc-5kdx7 [747.863791ms]
  Apr 23 17:30:03.375: INFO: Created: latency-svc-h8ccx
  Apr 23 17:30:03.411: INFO: Got endpoints: latency-svc-x5sc6 [749.435302ms]
  Apr 23 17:30:03.423: INFO: Created: latency-svc-z2cpr
  Apr 23 17:30:03.458: INFO: Got endpoints: latency-svc-g28vv [749.224682ms]
  Apr 23 17:30:03.473: INFO: Created: latency-svc-r844j
  Apr 23 17:30:03.510: INFO: Got endpoints: latency-svc-qm79w [748.865843ms]
  Apr 23 17:30:03.525: INFO: Created: latency-svc-vm7gk
  Apr 23 17:30:03.560: INFO: Got endpoints: latency-svc-k4zwx [747.670299ms]
  Apr 23 17:30:03.571: INFO: Created: latency-svc-gw5q7
  Apr 23 17:30:03.612: INFO: Got endpoints: latency-svc-587l8 [752.934234ms]
  Apr 23 17:30:03.629: INFO: Created: latency-svc-sxxtp
  Apr 23 17:30:03.667: INFO: Got endpoints: latency-svc-zlpw9 [758.283678ms]
  Apr 23 17:30:03.684: INFO: Created: latency-svc-bxb85
  Apr 23 17:30:03.715: INFO: Got endpoints: latency-svc-fxw6g [755.861386ms]
  Apr 23 17:30:03.731: INFO: Created: latency-svc-tp4g7
  Apr 23 17:30:03.759: INFO: Got endpoints: latency-svc-qhf9r [745.817704ms]
  Apr 23 17:30:03.770: INFO: Created: latency-svc-vddlr
  Apr 23 17:30:03.811: INFO: Got endpoints: latency-svc-mhbs8 [750.873356ms]
  Apr 23 17:30:03.824: INFO: Created: latency-svc-h8sk2
  Apr 23 17:30:03.862: INFO: Got endpoints: latency-svc-6kmfq [751.828953ms]
  Apr 23 17:30:03.873: INFO: Created: latency-svc-dzm5k
  Apr 23 17:30:03.910: INFO: Got endpoints: latency-svc-m4nj9 [750.838237ms]
  Apr 23 17:30:03.927: INFO: Created: latency-svc-zxml6
  Apr 23 17:30:03.958: INFO: Got endpoints: latency-svc-gd9m8 [748.870452ms]
  Apr 23 17:30:03.973: INFO: Created: latency-svc-t5pqb
  Apr 23 17:30:04.010: INFO: Got endpoints: latency-svc-cxcb7 [749.121212ms]
  Apr 23 17:30:04.026: INFO: Created: latency-svc-84bvl
  E0423 17:30:04.060558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:04.062: INFO: Got endpoints: latency-svc-c97bg [752.937153ms]
  Apr 23 17:30:04.076: INFO: Created: latency-svc-6mrq4
  Apr 23 17:30:04.110: INFO: Got endpoints: latency-svc-h8ccx [749.676098ms]
  Apr 23 17:30:04.124: INFO: Created: latency-svc-twddc
  Apr 23 17:30:04.168: INFO: Got endpoints: latency-svc-z2cpr [757.226672ms]
  Apr 23 17:30:04.183: INFO: Created: latency-svc-tls55
  Apr 23 17:30:04.209: INFO: Got endpoints: latency-svc-r844j [750.888723ms]
  Apr 23 17:30:04.224: INFO: Created: latency-svc-vncw4
  Apr 23 17:30:04.259: INFO: Got endpoints: latency-svc-vm7gk [749.662388ms]
  Apr 23 17:30:04.272: INFO: Created: latency-svc-j7qj8
  Apr 23 17:30:04.311: INFO: Got endpoints: latency-svc-gw5q7 [750.324496ms]
  Apr 23 17:30:04.325: INFO: Created: latency-svc-v6xq4
  Apr 23 17:30:04.359: INFO: Got endpoints: latency-svc-sxxtp [746.631806ms]
  Apr 23 17:30:04.371: INFO: Created: latency-svc-bzfdt
  Apr 23 17:30:04.408: INFO: Got endpoints: latency-svc-bxb85 [740.411113ms]
  Apr 23 17:30:04.421: INFO: Created: latency-svc-lcssf
  Apr 23 17:30:04.459: INFO: Got endpoints: latency-svc-tp4g7 [743.952151ms]
  Apr 23 17:30:04.476: INFO: Created: latency-svc-8cn8n
  Apr 23 17:30:04.509: INFO: Got endpoints: latency-svc-vddlr [749.933036ms]
  Apr 23 17:30:04.522: INFO: Created: latency-svc-vjhml
  Apr 23 17:30:04.560: INFO: Got endpoints: latency-svc-h8sk2 [749.383225ms]
  Apr 23 17:30:04.577: INFO: Created: latency-svc-h7n98
  Apr 23 17:30:04.609: INFO: Got endpoints: latency-svc-dzm5k [747.279108ms]
  Apr 23 17:30:04.624: INFO: Created: latency-svc-mqqxn
  Apr 23 17:30:04.661: INFO: Got endpoints: latency-svc-zxml6 [750.556038ms]
  Apr 23 17:30:04.673: INFO: Created: latency-svc-qmmht
  Apr 23 17:30:04.709: INFO: Got endpoints: latency-svc-t5pqb [751.658441ms]
  Apr 23 17:30:04.721: INFO: Created: latency-svc-hldmc
  Apr 23 17:30:04.761: INFO: Got endpoints: latency-svc-84bvl [750.32034ms]
  Apr 23 17:30:04.774: INFO: Created: latency-svc-j8j72
  Apr 23 17:30:04.810: INFO: Got endpoints: latency-svc-6mrq4 [747.541632ms]
  Apr 23 17:30:04.826: INFO: Created: latency-svc-9t6wb
  Apr 23 17:30:04.861: INFO: Got endpoints: latency-svc-twddc [751.040868ms]
  Apr 23 17:30:04.872: INFO: Created: latency-svc-pvwvx
  Apr 23 17:30:04.910: INFO: Got endpoints: latency-svc-tls55 [740.835106ms]
  Apr 23 17:30:04.922: INFO: Created: latency-svc-xfzfr
  Apr 23 17:30:04.958: INFO: Got endpoints: latency-svc-vncw4 [748.62495ms]
  Apr 23 17:30:04.976: INFO: Created: latency-svc-n2rgr
  Apr 23 17:30:05.017: INFO: Got endpoints: latency-svc-j7qj8 [757.592668ms]
  Apr 23 17:30:05.032: INFO: Created: latency-svc-lgksr
  E0423 17:30:05.060751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:05.065: INFO: Got endpoints: latency-svc-v6xq4 [753.581952ms]
  Apr 23 17:30:05.078: INFO: Created: latency-svc-8r7mf
  Apr 23 17:30:05.110: INFO: Got endpoints: latency-svc-bzfdt [750.45306ms]
  Apr 23 17:30:05.127: INFO: Created: latency-svc-rmfxn
  Apr 23 17:30:05.162: INFO: Got endpoints: latency-svc-lcssf [753.770646ms]
  Apr 23 17:30:05.176: INFO: Created: latency-svc-xj6rf
  Apr 23 17:30:05.209: INFO: Got endpoints: latency-svc-8cn8n [749.904664ms]
  Apr 23 17:30:05.225: INFO: Created: latency-svc-lw7ms
  Apr 23 17:30:05.261: INFO: Got endpoints: latency-svc-vjhml [752.074504ms]
  Apr 23 17:30:05.273: INFO: Created: latency-svc-8cnhx
  Apr 23 17:30:05.310: INFO: Got endpoints: latency-svc-h7n98 [750.176494ms]
  Apr 23 17:30:05.324: INFO: Created: latency-svc-w8bvf
  Apr 23 17:30:05.361: INFO: Got endpoints: latency-svc-mqqxn [751.653694ms]
  Apr 23 17:30:05.379: INFO: Created: latency-svc-hg2ck
  Apr 23 17:30:05.411: INFO: Got endpoints: latency-svc-qmmht [749.815639ms]
  Apr 23 17:30:05.432: INFO: Created: latency-svc-bksd7
  Apr 23 17:30:05.461: INFO: Got endpoints: latency-svc-hldmc [751.481784ms]
  Apr 23 17:30:05.478: INFO: Created: latency-svc-5dcsh
  Apr 23 17:30:05.508: INFO: Got endpoints: latency-svc-j8j72 [747.152384ms]
  Apr 23 17:30:05.530: INFO: Created: latency-svc-m4fr2
  Apr 23 17:30:05.559: INFO: Got endpoints: latency-svc-9t6wb [748.663009ms]
  Apr 23 17:30:05.572: INFO: Created: latency-svc-gvqzb
  Apr 23 17:30:05.610: INFO: Got endpoints: latency-svc-pvwvx [748.460453ms]
  Apr 23 17:30:05.626: INFO: Created: latency-svc-j54gf
  Apr 23 17:30:05.660: INFO: Got endpoints: latency-svc-xfzfr [749.578229ms]
  Apr 23 17:30:05.672: INFO: Created: latency-svc-kgd6s
  Apr 23 17:30:05.708: INFO: Got endpoints: latency-svc-n2rgr [750.341128ms]
  Apr 23 17:30:05.721: INFO: Created: latency-svc-4bfps
  Apr 23 17:30:05.763: INFO: Got endpoints: latency-svc-lgksr [745.819859ms]
  Apr 23 17:30:05.776: INFO: Created: latency-svc-sc6pp
  Apr 23 17:30:05.811: INFO: Got endpoints: latency-svc-8r7mf [746.028707ms]
  Apr 23 17:30:05.826: INFO: Created: latency-svc-mr2cc
  Apr 23 17:30:05.861: INFO: Got endpoints: latency-svc-rmfxn [751.7365ms]
  Apr 23 17:30:05.876: INFO: Created: latency-svc-n8nq7
  Apr 23 17:30:05.910: INFO: Got endpoints: latency-svc-xj6rf [747.771949ms]
  Apr 23 17:30:05.924: INFO: Created: latency-svc-h4szf
  Apr 23 17:30:05.960: INFO: Got endpoints: latency-svc-lw7ms [750.245805ms]
  Apr 23 17:30:05.975: INFO: Created: latency-svc-pzrmw
  Apr 23 17:30:06.009: INFO: Got endpoints: latency-svc-8cnhx [748.175838ms]
  Apr 23 17:30:06.021: INFO: Created: latency-svc-zww54
  E0423 17:30:06.060966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:06.061: INFO: Got endpoints: latency-svc-w8bvf [750.199617ms]
  Apr 23 17:30:06.077: INFO: Created: latency-svc-fczwj
  Apr 23 17:30:06.111: INFO: Got endpoints: latency-svc-hg2ck [750.246556ms]
  Apr 23 17:30:06.123: INFO: Created: latency-svc-w9cbb
  Apr 23 17:30:06.160: INFO: Got endpoints: latency-svc-bksd7 [748.874405ms]
  Apr 23 17:30:06.171: INFO: Created: latency-svc-s5wf2
  Apr 23 17:30:06.211: INFO: Got endpoints: latency-svc-5dcsh [749.343448ms]
  Apr 23 17:30:06.223: INFO: Created: latency-svc-zcmzn
  Apr 23 17:30:06.259: INFO: Got endpoints: latency-svc-m4fr2 [751.153903ms]
  Apr 23 17:30:06.273: INFO: Created: latency-svc-cmhpv
  Apr 23 17:30:06.309: INFO: Got endpoints: latency-svc-gvqzb [750.033412ms]
  Apr 23 17:30:06.321: INFO: Created: latency-svc-nkwmg
  Apr 23 17:30:06.360: INFO: Got endpoints: latency-svc-j54gf [749.898194ms]
  Apr 23 17:30:06.372: INFO: Created: latency-svc-w898d
  Apr 23 17:30:06.411: INFO: Got endpoints: latency-svc-kgd6s [751.349157ms]
  Apr 23 17:30:06.424: INFO: Created: latency-svc-9qttk
  Apr 23 17:30:06.459: INFO: Got endpoints: latency-svc-4bfps [750.966777ms]
  Apr 23 17:30:06.473: INFO: Created: latency-svc-w5qnb
  Apr 23 17:30:06.509: INFO: Got endpoints: latency-svc-sc6pp [745.739802ms]
  Apr 23 17:30:06.527: INFO: Created: latency-svc-dmcbv
  Apr 23 17:30:06.561: INFO: Got endpoints: latency-svc-mr2cc [750.647339ms]
  Apr 23 17:30:06.575: INFO: Created: latency-svc-hnkrs
  Apr 23 17:30:06.610: INFO: Got endpoints: latency-svc-n8nq7 [747.988379ms]
  Apr 23 17:30:06.625: INFO: Created: latency-svc-d5gk7
  Apr 23 17:30:06.666: INFO: Got endpoints: latency-svc-h4szf [756.036348ms]
  Apr 23 17:30:06.679: INFO: Created: latency-svc-j8st7
  Apr 23 17:30:06.710: INFO: Got endpoints: latency-svc-pzrmw [749.527129ms]
  Apr 23 17:30:06.726: INFO: Created: latency-svc-qdndm
  Apr 23 17:30:06.759: INFO: Got endpoints: latency-svc-zww54 [749.913128ms]
  Apr 23 17:30:06.772: INFO: Created: latency-svc-4vvtw
  Apr 23 17:30:06.812: INFO: Got endpoints: latency-svc-fczwj [751.686885ms]
  Apr 23 17:30:06.828: INFO: Created: latency-svc-g8s2g
  Apr 23 17:30:06.860: INFO: Got endpoints: latency-svc-w9cbb [748.803851ms]
  Apr 23 17:30:06.874: INFO: Created: latency-svc-x4dmw
  Apr 23 17:30:06.909: INFO: Got endpoints: latency-svc-s5wf2 [748.6279ms]
  Apr 23 17:30:06.922: INFO: Created: latency-svc-nnvzr
  Apr 23 17:30:06.958: INFO: Got endpoints: latency-svc-zcmzn [746.765374ms]
  Apr 23 17:30:06.972: INFO: Created: latency-svc-ldhxc
  Apr 23 17:30:07.009: INFO: Got endpoints: latency-svc-cmhpv [749.47258ms]
  Apr 23 17:30:07.023: INFO: Created: latency-svc-zd26t
  Apr 23 17:30:07.060: INFO: Got endpoints: latency-svc-nkwmg [750.82502ms]
  E0423 17:30:07.061412      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:07.073: INFO: Created: latency-svc-64ccw
  Apr 23 17:30:07.110: INFO: Got endpoints: latency-svc-w898d [750.28409ms]
  Apr 23 17:30:07.124: INFO: Created: latency-svc-jljhd
  Apr 23 17:30:07.160: INFO: Got endpoints: latency-svc-9qttk [748.737666ms]
  Apr 23 17:30:07.175: INFO: Created: latency-svc-gkzwh
  Apr 23 17:30:07.209: INFO: Got endpoints: latency-svc-w5qnb [749.349766ms]
  Apr 23 17:30:07.226: INFO: Created: latency-svc-jxnzf
  Apr 23 17:30:07.259: INFO: Got endpoints: latency-svc-dmcbv [749.777699ms]
  Apr 23 17:30:07.271: INFO: Created: latency-svc-mzl55
  Apr 23 17:30:07.310: INFO: Got endpoints: latency-svc-hnkrs [748.246941ms]
  Apr 23 17:30:07.323: INFO: Created: latency-svc-229g7
  Apr 23 17:30:07.360: INFO: Got endpoints: latency-svc-d5gk7 [750.281702ms]
  Apr 23 17:30:07.373: INFO: Created: latency-svc-nqjrz
  Apr 23 17:30:07.411: INFO: Got endpoints: latency-svc-j8st7 [744.832206ms]
  Apr 23 17:30:07.422: INFO: Created: latency-svc-npkjh
  Apr 23 17:30:07.460: INFO: Got endpoints: latency-svc-qdndm [750.207784ms]
  Apr 23 17:30:07.476: INFO: Created: latency-svc-gfzj5
  Apr 23 17:30:07.509: INFO: Got endpoints: latency-svc-4vvtw [749.78539ms]
  Apr 23 17:30:07.524: INFO: Created: latency-svc-dlnlk
  Apr 23 17:30:07.560: INFO: Got endpoints: latency-svc-g8s2g [747.552579ms]
  Apr 23 17:30:07.574: INFO: Created: latency-svc-f2s4m
  Apr 23 17:30:07.611: INFO: Got endpoints: latency-svc-x4dmw [750.649372ms]
  Apr 23 17:30:07.635: INFO: Created: latency-svc-2fdbw
  Apr 23 17:30:07.661: INFO: Got endpoints: latency-svc-nnvzr [752.199268ms]
  Apr 23 17:30:07.674: INFO: Created: latency-svc-55rdl
  Apr 23 17:30:07.714: INFO: Got endpoints: latency-svc-ldhxc [755.129233ms]
  Apr 23 17:30:07.727: INFO: Created: latency-svc-c64tj
  Apr 23 17:30:07.760: INFO: Got endpoints: latency-svc-zd26t [750.201106ms]
  Apr 23 17:30:07.772: INFO: Created: latency-svc-ffbdf
  Apr 23 17:30:07.810: INFO: Got endpoints: latency-svc-64ccw [749.715593ms]
  Apr 23 17:30:07.823: INFO: Created: latency-svc-xt2mh
  Apr 23 17:30:07.860: INFO: Got endpoints: latency-svc-jljhd [749.722109ms]
  Apr 23 17:30:07.871: INFO: Created: latency-svc-ld4gd
  Apr 23 17:30:07.909: INFO: Got endpoints: latency-svc-gkzwh [749.36382ms]
  Apr 23 17:30:07.926: INFO: Created: latency-svc-mb968
  Apr 23 17:30:07.958: INFO: Got endpoints: latency-svc-jxnzf [749.499076ms]
  Apr 23 17:30:07.977: INFO: Created: latency-svc-4ccs9
  Apr 23 17:30:08.010: INFO: Got endpoints: latency-svc-mzl55 [750.935227ms]
  Apr 23 17:30:08.023: INFO: Created: latency-svc-vzqf5
  Apr 23 17:30:08.060: INFO: Got endpoints: latency-svc-229g7 [750.32444ms]
  E0423 17:30:08.061536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:08.075: INFO: Created: latency-svc-tfnwl
  Apr 23 17:30:08.110: INFO: Got endpoints: latency-svc-nqjrz [749.817005ms]
  Apr 23 17:30:08.122: INFO: Created: latency-svc-h8zrj
  Apr 23 17:30:08.158: INFO: Got endpoints: latency-svc-npkjh [747.455126ms]
  Apr 23 17:30:08.173: INFO: Created: latency-svc-n9sx4
  Apr 23 17:30:08.209: INFO: Got endpoints: latency-svc-gfzj5 [748.766342ms]
  Apr 23 17:30:08.224: INFO: Created: latency-svc-pvx9b
  Apr 23 17:30:08.260: INFO: Got endpoints: latency-svc-dlnlk [750.486382ms]
  Apr 23 17:30:08.282: INFO: Created: latency-svc-v59gs
  Apr 23 17:30:08.310: INFO: Got endpoints: latency-svc-f2s4m [750.451153ms]
  Apr 23 17:30:08.321: INFO: Created: latency-svc-dtz5t
  Apr 23 17:30:08.360: INFO: Got endpoints: latency-svc-2fdbw [748.968883ms]
  Apr 23 17:30:08.373: INFO: Created: latency-svc-2xhmz
  Apr 23 17:30:08.409: INFO: Got endpoints: latency-svc-55rdl [747.875089ms]
  Apr 23 17:30:08.421: INFO: Created: latency-svc-tszlm
  Apr 23 17:30:08.458: INFO: Got endpoints: latency-svc-c64tj [744.728016ms]
  Apr 23 17:30:08.473: INFO: Created: latency-svc-v4c8t
  Apr 23 17:30:08.509: INFO: Got endpoints: latency-svc-ffbdf [748.785597ms]
  Apr 23 17:30:08.521: INFO: Created: latency-svc-54fkk
  Apr 23 17:30:08.561: INFO: Got endpoints: latency-svc-xt2mh [750.966961ms]
  Apr 23 17:30:08.573: INFO: Created: latency-svc-pxrp7
  Apr 23 17:30:08.609: INFO: Got endpoints: latency-svc-ld4gd [748.889277ms]
  Apr 23 17:30:08.627: INFO: Created: latency-svc-jhwxw
  Apr 23 17:30:08.659: INFO: Got endpoints: latency-svc-mb968 [749.704219ms]
  Apr 23 17:30:08.673: INFO: Created: latency-svc-mlxnw
  Apr 23 17:30:08.708: INFO: Got endpoints: latency-svc-4ccs9 [749.385733ms]
  Apr 23 17:30:08.720: INFO: Created: latency-svc-d7qw6
  Apr 23 17:30:08.760: INFO: Got endpoints: latency-svc-vzqf5 [749.661036ms]
  Apr 23 17:30:08.774: INFO: Created: latency-svc-wps7d
  Apr 23 17:30:08.810: INFO: Got endpoints: latency-svc-tfnwl [749.824896ms]
  Apr 23 17:30:08.822: INFO: Created: latency-svc-7k2j6
  Apr 23 17:30:08.858: INFO: Got endpoints: latency-svc-h8zrj [748.537219ms]
  Apr 23 17:30:08.871: INFO: Created: latency-svc-cqrk8
  Apr 23 17:30:08.910: INFO: Got endpoints: latency-svc-n9sx4 [752.122036ms]
  Apr 23 17:30:08.928: INFO: Created: latency-svc-hrll2
  Apr 23 17:30:08.959: INFO: Got endpoints: latency-svc-pvx9b [750.117952ms]
  Apr 23 17:30:08.973: INFO: Created: latency-svc-fdbf6
  Apr 23 17:30:09.014: INFO: Got endpoints: latency-svc-v59gs [753.923782ms]
  Apr 23 17:30:09.027: INFO: Created: latency-svc-dk5fx
  E0423 17:30:09.061767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:09.061: INFO: Got endpoints: latency-svc-dtz5t [750.934081ms]
  Apr 23 17:30:09.073: INFO: Created: latency-svc-l6s8q
  Apr 23 17:30:09.109: INFO: Got endpoints: latency-svc-2xhmz [748.609125ms]
  Apr 23 17:30:09.123: INFO: Created: latency-svc-wftj6
  Apr 23 17:30:09.160: INFO: Got endpoints: latency-svc-tszlm [750.670367ms]
  Apr 23 17:30:09.178: INFO: Created: latency-svc-4vjg9
  Apr 23 17:30:09.211: INFO: Got endpoints: latency-svc-v4c8t [752.651643ms]
  Apr 23 17:30:09.228: INFO: Created: latency-svc-mk2rq
  Apr 23 17:30:09.260: INFO: Got endpoints: latency-svc-54fkk [750.919118ms]
  Apr 23 17:30:09.276: INFO: Created: latency-svc-58df8
  Apr 23 17:30:09.311: INFO: Got endpoints: latency-svc-pxrp7 [749.599927ms]
  Apr 23 17:30:09.328: INFO: Created: latency-svc-2tbhc
  Apr 23 17:30:09.361: INFO: Got endpoints: latency-svc-jhwxw [752.007721ms]
  Apr 23 17:30:09.374: INFO: Created: latency-svc-bzds7
  Apr 23 17:30:09.409: INFO: Got endpoints: latency-svc-mlxnw [749.970481ms]
  Apr 23 17:30:09.422: INFO: Created: latency-svc-ksts4
  Apr 23 17:30:09.459: INFO: Got endpoints: latency-svc-d7qw6 [751.119215ms]
  Apr 23 17:30:09.510: INFO: Got endpoints: latency-svc-wps7d [749.848048ms]
  Apr 23 17:30:09.559: INFO: Got endpoints: latency-svc-7k2j6 [749.251575ms]
  Apr 23 17:30:09.701: INFO: Got endpoints: latency-svc-cqrk8 [842.11505ms]
  Apr 23 17:30:09.701: INFO: Got endpoints: latency-svc-hrll2 [790.470788ms]
  Apr 23 17:30:09.713: INFO: Got endpoints: latency-svc-fdbf6 [754.002924ms]
  Apr 23 17:30:09.760: INFO: Got endpoints: latency-svc-dk5fx [745.850785ms]
  Apr 23 17:30:09.812: INFO: Got endpoints: latency-svc-l6s8q [749.944827ms]
  Apr 23 17:30:09.863: INFO: Got endpoints: latency-svc-wftj6 [754.311495ms]
  Apr 23 17:30:09.910: INFO: Got endpoints: latency-svc-4vjg9 [749.849448ms]
  Apr 23 17:30:09.961: INFO: Got endpoints: latency-svc-mk2rq [749.774757ms]
  Apr 23 17:30:10.010: INFO: Got endpoints: latency-svc-58df8 [750.254505ms]
  Apr 23 17:30:10.060: INFO: Got endpoints: latency-svc-2tbhc [749.16311ms]
  E0423 17:30:10.062624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:10.110: INFO: Got endpoints: latency-svc-bzds7 [749.246249ms]
  Apr 23 17:30:10.163: INFO: Got endpoints: latency-svc-ksts4 [753.759751ms]
  Apr 23 17:30:10.163: INFO: Latencies: [44.612047ms 91.59438ms 91.628013ms 103.488175ms 104.246863ms 104.307486ms 119.774862ms 120.757958ms 120.955719ms 122.434647ms 125.948309ms 126.053187ms 129.235495ms 130.325ms 134.593352ms 135.051854ms 136.219767ms 136.615576ms 137.349907ms 137.767669ms 138.748986ms 140.525412ms 141.936919ms 143.894333ms 149.876486ms 154.54265ms 157.75415ms 162.41807ms 176.837036ms 177.235929ms 185.505507ms 191.767236ms 196.551335ms 205.066553ms 220.230952ms 243.122953ms 251.334897ms 282.053691ms 310.187583ms 348.753786ms 374.573271ms 389.28437ms 402.043281ms 433.822991ms 445.709816ms 447.515375ms 463.851114ms 481.110383ms 535.507469ms 581.06831ms 583.86908ms 624.882294ms 630.975202ms 662.448378ms 676.330889ms 706.510838ms 729.235016ms 740.411113ms 740.835106ms 743.952151ms 744.654572ms 744.728016ms 744.832206ms 745.739802ms 745.817704ms 745.819859ms 745.850785ms 746.028707ms 746.631806ms 746.639295ms 746.765374ms 747.152384ms 747.279108ms 747.455126ms 747.51684ms 747.541632ms 747.552579ms 747.670299ms 747.771949ms 747.863791ms 747.875089ms 747.988379ms 748.175838ms 748.246941ms 748.460453ms 748.537219ms 748.609125ms 748.62495ms 748.6279ms 748.663009ms 748.737666ms 748.766342ms 748.785597ms 748.803851ms 748.865843ms 748.870452ms 748.874405ms 748.889277ms 748.968883ms 749.121212ms 749.16311ms 749.224682ms 749.246249ms 749.251575ms 749.343448ms 749.349766ms 749.36382ms 749.383225ms 749.385733ms 749.435302ms 749.47258ms 749.499076ms 749.527129ms 749.578229ms 749.599927ms 749.661036ms 749.662388ms 749.676098ms 749.704219ms 749.715593ms 749.722109ms 749.774757ms 749.777699ms 749.78539ms 749.815639ms 749.817005ms 749.824896ms 749.848048ms 749.849448ms 749.898194ms 749.904664ms 749.913128ms 749.933036ms 749.944827ms 749.970481ms 750.033412ms 750.117952ms 750.176494ms 750.199617ms 750.201106ms 750.207784ms 750.245805ms 750.246556ms 750.254505ms 750.281702ms 750.28409ms 750.32034ms 750.32444ms 750.324496ms 750.341128ms 750.451153ms 750.45306ms 750.486382ms 750.556038ms 750.647339ms 750.649372ms 750.670367ms 750.82502ms 750.838237ms 750.8435ms 750.873356ms 750.888723ms 750.919118ms 750.934081ms 750.935227ms 750.966777ms 750.966961ms 751.040868ms 751.119215ms 751.153903ms 751.317671ms 751.349157ms 751.481784ms 751.653694ms 751.658441ms 751.686885ms 751.727014ms 751.7365ms 751.828953ms 752.007721ms 752.074504ms 752.122036ms 752.199268ms 752.651643ms 752.934234ms 752.937153ms 753.581952ms 753.759751ms 753.770646ms 753.923782ms 754.002924ms 754.311495ms 755.129233ms 755.861386ms 756.036348ms 757.226672ms 757.592668ms 758.283678ms 790.470788ms 842.11505ms]
  Apr 23 17:30:10.164: INFO: 50 %ile: 749.16311ms
  Apr 23 17:30:10.164: INFO: 90 %ile: 752.074504ms
  Apr 23 17:30:10.164: INFO: 99 %ile: 790.470788ms
  Apr 23 17:30:10.164: INFO: Total sample count: 200
  Apr 23 17:30:10.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7557" for this suite. @ 04/23/23 17:30:10.17
• [10.781 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/23/23 17:30:10.182
  Apr 23 17:30:10.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 17:30:10.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:10.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:10.216
  STEP: creating the pod @ 04/23/23 17:30:10.224
  Apr 23 17:30:10.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-9015 create -f -'
  Apr 23 17:30:11.005: INFO: stderr: ""
  Apr 23 17:30:11.005: INFO: stdout: "pod/pause created\n"
  E0423 17:30:11.063138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:12.063749      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/23/23 17:30:13.016
  Apr 23 17:30:13.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-9015 label pods pause testing-label=testing-label-value'
  E0423 17:30:13.064558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:13.095: INFO: stderr: ""
  Apr 23 17:30:13.095: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/23/23 17:30:13.095
  Apr 23 17:30:13.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-9015 get pod pause -L testing-label'
  Apr 23 17:30:13.164: INFO: stderr: ""
  Apr 23 17:30:13.164: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/23/23 17:30:13.164
  Apr 23 17:30:13.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-9015 label pods pause testing-label-'
  Apr 23 17:30:13.247: INFO: stderr: ""
  Apr 23 17:30:13.247: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/23/23 17:30:13.247
  Apr 23 17:30:13.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-9015 get pod pause -L testing-label'
  Apr 23 17:30:13.315: INFO: stderr: ""
  Apr 23 17:30:13.315: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 04/23/23 17:30:13.315
  Apr 23 17:30:13.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-9015 delete --grace-period=0 --force -f -'
  Apr 23 17:30:13.400: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 17:30:13.400: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 23 17:30:13.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-9015 get rc,svc -l name=pause --no-headers'
  Apr 23 17:30:13.473: INFO: stderr: "No resources found in kubectl-9015 namespace.\n"
  Apr 23 17:30:13.473: INFO: stdout: ""
  Apr 23 17:30:13.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-9015 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 23 17:30:13.537: INFO: stderr: ""
  Apr 23 17:30:13.537: INFO: stdout: ""
  Apr 23 17:30:13.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9015" for this suite. @ 04/23/23 17:30:13.543
• [3.368 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/23/23 17:30:13.55
  Apr 23 17:30:13.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:30:13.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:13.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:13.578
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:30:13.581
  E0423 17:30:14.065364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:15.065468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:16.065548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:17.066376      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:30:17.619
  Apr 23 17:30:17.623: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-daac4bec-401a-48d4-92cc-a907c0a5f42a container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:30:17.631
  Apr 23 17:30:17.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7204" for this suite. @ 04/23/23 17:30:17.657
• [4.117 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/23/23 17:30:17.668
  Apr 23 17:30:17.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 17:30:17.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:17.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:17.698
  STEP: Creating a pod to test substitution in container's command @ 04/23/23 17:30:17.702
  E0423 17:30:18.066407      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:19.066618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:20.066829      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:21.067031      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:30:21.729
  Apr 23 17:30:21.733: INFO: Trying to get logs from node ip-172-31-70-241 pod var-expansion-544a399d-7c1f-446f-bce5-9e0d3c6ba4de container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 17:30:21.749
  Apr 23 17:30:21.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1769" for this suite. @ 04/23/23 17:30:21.773
• [4.114 seconds]
------------------------------
S
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/23/23 17:30:21.782
  Apr 23 17:30:21.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename containers @ 04/23/23 17:30:21.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:21.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:21.807
  STEP: Creating a pod to test override command @ 04/23/23 17:30:21.85
  E0423 17:30:22.067617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:23.068625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:24.069658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:25.070107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:30:25.893
  Apr 23 17:30:25.897: INFO: Trying to get logs from node ip-172-31-70-241 pod client-containers-31d579d8-b8e8-4e3c-8275-6c9d896ba0c4 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:30:25.906
  Apr 23 17:30:25.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7685" for this suite. @ 04/23/23 17:30:25.93
• [4.155 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/23/23 17:30:25.937
  Apr 23 17:30:25.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename disruption @ 04/23/23 17:30:25.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:25.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:25.968
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/23/23 17:30:25.973
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:30:25.981
  E0423 17:30:26.070676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:27.070980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/23/23 17:30:27.998
  STEP: Waiting for all pods to be running @ 04/23/23 17:30:27.998
  Apr 23 17:30:28.002: INFO: pods: 0 < 3
  E0423 17:30:28.071032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:29.072083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 04/23/23 17:30:30.01
  STEP: Updating the pdb to allow a pod to be evicted @ 04/23/23 17:30:30.024
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:30:30.041
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/23/23 17:30:30.047
  STEP: Waiting for all pods to be running @ 04/23/23 17:30:30.047
  STEP: Waiting for the pdb to observed all healthy pods @ 04/23/23 17:30:30.052
  E0423 17:30:30.072274      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/23/23 17:30:30.087
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:30:30.108
  E0423 17:30:31.072818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:32.072795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/23/23 17:30:32.118
  Apr 23 17:30:32.130: INFO: running pods: 2 < 3
  E0423 17:30:33.074817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:34.075069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 04/23/23 17:30:34.135
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/23/23 17:30:34.146
  STEP: Waiting for the pdb to be deleted @ 04/23/23 17:30:34.155
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/23/23 17:30:34.159
  STEP: Waiting for all pods to be running @ 04/23/23 17:30:34.159
  Apr 23 17:30:34.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5872" for this suite. @ 04/23/23 17:30:34.191
• [8.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/23/23 17:30:34.206
  Apr 23 17:30:34.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename watch @ 04/23/23 17:30:34.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:34.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:34.245
  STEP: creating a watch on configmaps with a certain label @ 04/23/23 17:30:34.248
  STEP: creating a new configmap @ 04/23/23 17:30:34.25
  STEP: modifying the configmap once @ 04/23/23 17:30:34.257
  STEP: changing the label value of the configmap @ 04/23/23 17:30:34.268
  STEP: Expecting to observe a delete notification for the watched object @ 04/23/23 17:30:34.278
  Apr 23 17:30:34.278: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6436  8cb4a672-84a8-4173-bcdb-2979fe5c94cf 22914 0 2023-04-23 17:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 17:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 17:30:34.279: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6436  8cb4a672-84a8-4173-bcdb-2979fe5c94cf 22915 0 2023-04-23 17:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 17:30:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 17:30:34.279: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6436  8cb4a672-84a8-4173-bcdb-2979fe5c94cf 22916 0 2023-04-23 17:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 17:30:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/23/23 17:30:34.279
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/23/23 17:30:34.289
  E0423 17:30:35.075200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:36.075929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:37.076322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:38.076521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:39.076722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:40.076853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:41.077058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:42.077461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:43.077659      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:44.077872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 04/23/23 17:30:44.289
  STEP: modifying the configmap a third time @ 04/23/23 17:30:44.305
  STEP: deleting the configmap @ 04/23/23 17:30:44.319
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/23/23 17:30:44.327
  Apr 23 17:30:44.327: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6436  8cb4a672-84a8-4173-bcdb-2979fe5c94cf 22990 0 2023-04-23 17:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 17:30:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 17:30:44.327: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6436  8cb4a672-84a8-4173-bcdb-2979fe5c94cf 22991 0 2023-04-23 17:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 17:30:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 17:30:44.327: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6436  8cb4a672-84a8-4173-bcdb-2979fe5c94cf 22992 0 2023-04-23 17:30:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-23 17:30:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 17:30:44.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6436" for this suite. @ 04/23/23 17:30:44.331
• [10.134 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/23/23 17:30:44.34
  Apr 23 17:30:44.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename security-context-test @ 04/23/23 17:30:44.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:44.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:44.372
  E0423 17:30:45.078061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:46.078589      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:47.079456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:48.079800      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:48.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9691" for this suite. @ 04/23/23 17:30:48.409
• [4.076 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/23/23 17:30:48.417
  Apr 23 17:30:48.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 17:30:48.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:48.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:48.439
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/23/23 17:30:48.443
  E0423 17:30:49.080117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:50.081205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 04/23/23 17:30:50.474
  STEP: Then the orphan pod is adopted @ 04/23/23 17:30:50.481
  E0423 17:30:51.081312      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 04/23/23 17:30:51.491
  Apr 23 17:30:51.498: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/23/23 17:30:51.518
  Apr 23 17:30:51.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-312" for this suite. @ 04/23/23 17:30:51.542
• [3.138 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/23/23 17:30:51.556
  Apr 23 17:30:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename gc @ 04/23/23 17:30:51.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:51.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:51.606
  STEP: create the deployment @ 04/23/23 17:30:51.61
  W0423 17:30:51.618510      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/23/23 17:30:51.618
  E0423 17:30:52.081964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 04/23/23 17:30:52.133
  STEP: wait for all rs to be garbage collected @ 04/23/23 17:30:52.139
  STEP: expected 0 rs, got 1 rs @ 04/23/23 17:30:52.149
  STEP: expected 0 pods, got 2 pods @ 04/23/23 17:30:52.154
  STEP: Gathering metrics @ 04/23/23 17:30:52.667
  W0423 17:30:52.671340      21 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 23 17:30:52.671: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 17:30:52.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6698" for this suite. @ 04/23/23 17:30:52.677
• [1.129 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/23/23 17:30:52.685
  Apr 23 17:30:52.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/23/23 17:30:52.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:52.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:52.709
  STEP: create the container to handle the HTTPGet hook request. @ 04/23/23 17:30:52.719
  E0423 17:30:53.081851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:54.082021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/23/23 17:30:54.754
  E0423 17:30:55.082487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:56.082697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/23/23 17:30:56.784
  STEP: delete the pod with lifecycle hook @ 04/23/23 17:30:56.796
  E0423 17:30:57.083523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:30:58.083632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:30:58.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4005" for this suite. @ 04/23/23 17:30:58.83
• [6.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/23/23 17:30:58.841
  Apr 23 17:30:58.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:30:58.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:58.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:58.873
  Apr 23 17:30:58.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7431" for this suite. @ 04/23/23 17:30:58.939
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/23/23 17:30:58.965
  Apr 23 17:30:58.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:30:58.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:30:58.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:30:58.99
  STEP: Creating configMap with name configmap-projected-all-test-volume-a4106864-8da7-4f46-b501-d11ccd251cc4 @ 04/23/23 17:30:58.994
  STEP: Creating secret with name secret-projected-all-test-volume-21f4b5b5-a58e-4666-a5dd-41f7188c9e46 @ 04/23/23 17:30:59.001
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/23/23 17:30:59.006
  E0423 17:30:59.083912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:00.083976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:01.084176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:02.084567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:31:03.033
  Apr 23 17:31:03.038: INFO: Trying to get logs from node ip-172-31-86-26 pod projected-volume-c9b9aa7a-8b2c-44f9-906b-0b82cd27c2d8 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:31:03.063
  Apr 23 17:31:03.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 17:31:03.084909      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "projected-1155" for this suite. @ 04/23/23 17:31:03.088
• [4.132 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/23/23 17:31:03.098
  Apr 23 17:31:03.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 17:31:03.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:31:03.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:31:03.128
  Apr 23 17:31:03.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:31:04.085503      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:05.085830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0423 17:31:05.700177      21 warnings.go:70] unknown field "alpha"
  W0423 17:31:05.700198      21 warnings.go:70] unknown field "beta"
  W0423 17:31:05.700205      21 warnings.go:70] unknown field "delta"
  W0423 17:31:05.700210      21 warnings.go:70] unknown field "epsilon"
  W0423 17:31:05.700216      21 warnings.go:70] unknown field "gamma"
  Apr 23 17:31:05.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8383" for this suite. @ 04/23/23 17:31:05.755
• [2.667 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/23/23 17:31:05.771
  Apr 23 17:31:05.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:31:05.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:31:05.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:31:05.798
  STEP: Creating projection with secret that has name projected-secret-test-8f9585d5-c478-47a3-9010-8d22b94e4091 @ 04/23/23 17:31:05.807
  STEP: Creating a pod to test consume secrets @ 04/23/23 17:31:05.813
  E0423 17:31:06.086569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:07.087664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:08.088613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:09.089515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:31:09.847
  Apr 23 17:31:09.851: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-secrets-abeede7f-352c-4fbd-bce9-1045b5617ac5 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:31:09.859
  Apr 23 17:31:09.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9893" for this suite. @ 04/23/23 17:31:09.89
• [4.135 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/23/23 17:31:09.907
  Apr 23 17:31:09.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename certificates @ 04/23/23 17:31:09.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:31:09.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:31:09.944
  E0423 17:31:10.089639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 04/23/23 17:31:10.585
  STEP: getting /apis/certificates.k8s.io @ 04/23/23 17:31:10.593
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/23/23 17:31:10.596
  STEP: creating @ 04/23/23 17:31:10.598
  STEP: getting @ 04/23/23 17:31:10.623
  STEP: listing @ 04/23/23 17:31:10.626
  STEP: watching @ 04/23/23 17:31:10.631
  Apr 23 17:31:10.631: INFO: starting watch
  STEP: patching @ 04/23/23 17:31:10.633
  STEP: updating @ 04/23/23 17:31:10.646
  Apr 23 17:31:10.655: INFO: waiting for watch events with expected annotations
  Apr 23 17:31:10.655: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/23/23 17:31:10.655
  STEP: patching /approval @ 04/23/23 17:31:10.661
  STEP: updating /approval @ 04/23/23 17:31:10.67
  STEP: getting /status @ 04/23/23 17:31:10.678
  STEP: patching /status @ 04/23/23 17:31:10.682
  STEP: updating /status @ 04/23/23 17:31:10.693
  STEP: deleting @ 04/23/23 17:31:10.704
  STEP: deleting a collection @ 04/23/23 17:31:10.722
  Apr 23 17:31:10.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-3281" for this suite. @ 04/23/23 17:31:10.745
• [0.848 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/23/23 17:31:10.756
  Apr 23 17:31:10.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename deployment @ 04/23/23 17:31:10.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:31:10.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:31:10.783
  Apr 23 17:31:10.786: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 23 17:31:10.797: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0423 17:31:11.090603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:12.091420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:13.091542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:14.091658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:15.092074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:31:15.803: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 17:31:15.803
  Apr 23 17:31:15.803: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 23 17:31:15.810: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 23 17:31:15.819: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0423 17:31:16.092449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:17.092542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:31:17.829: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 23 17:31:17.833: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 23 17:31:17.848: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-40  eae83065-9184-442e-8966-3a62a473c4d9 23472 1 2023-04-23 17:31:15 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-23 17:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f43068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-23 17:31:15 +0000 UTC,LastTransitionTime:2023-04-23 17:31:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-04-23 17:31:17 +0000 UTC,LastTransitionTime:2023-04-23 17:31:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 23 17:31:17.855: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-40  39bdd6d3-74ac-4581-b2e0-9ed7dfdf6129 23462 1 2023-04-23 17:31:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment eae83065-9184-442e-8966-3a62a473c4d9 0xc003f43507 0xc003f43508}] [] [{kube-controller-manager Update apps/v1 2023-04-23 17:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eae83065-9184-442e-8966-3a62a473c4d9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:31:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f435b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 17:31:17.855: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 23 17:31:17.855: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-40  e38d4de4-ba5d-45f7-81e4-7543d20bb5be 23471 2 2023-04-23 17:31:10 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment eae83065-9184-442e-8966-3a62a473c4d9 0xc003f433d7 0xc003f433d8}] [] [{e2e.test Update apps/v1 2023-04-23 17:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eae83065-9184-442e-8966-3a62a473c4d9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:31:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f43498 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 17:31:17.860: INFO: Pod "test-rolling-update-deployment-656d657cd8-7mwjw" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-7mwjw test-rolling-update-deployment-656d657cd8- deployment-40  eb70b307-9b7d-4ae5-a16d-fb014a157002 23461 0 2023-04-23 17:31:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 39bdd6d3-74ac-4581-b2e0-9ed7dfdf6129 0xc003f43a27 0xc003f43a28}] [] [{kube-controller-manager Update v1 2023-04-23 17:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39bdd6d3-74ac-4581-b2e0-9ed7dfdf6129\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 17:31:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmtcj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmtcj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:31:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:31:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.218,StartTime:2023-04-23 17:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 17:31:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://d79e615c1794874647065e2e56cf698c5079c84cfb27d29314ea5d9dcb8b68c3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.218,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 17:31:17.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-40" for this suite. @ 04/23/23 17:31:17.865
• [7.120 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/23/23 17:31:17.878
  Apr 23 17:31:17.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 17:31:17.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:31:17.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:31:17.9
  Apr 23 17:31:17.926: INFO: created pod
  E0423 17:31:18.094332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:19.095038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:20.095336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:21.095513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:31:21.942
  E0423 17:31:22.095900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:23.096076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:24.096433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:25.096503      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:26.096743      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:27.097338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:28.097561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:29.097742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:30.098673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:31.098940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:32.099827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:33.100799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:34.100988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:35.101093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:36.101535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:37.102092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:38.102638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:39.102702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:40.103235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:41.104213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:42.104581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:43.104759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:44.104979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:45.105183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:46.106468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:47.105857      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:48.105967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:49.106294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:50.106488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:51.106822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:31:51.943: INFO: polling logs
  Apr 23 17:31:51.952: INFO: Pod logs: 
  I0423 17:31:18.897938       1 log.go:198] OK: Got token
  I0423 17:31:18.898732       1 log.go:198] validating with in-cluster discovery
  I0423 17:31:18.899149       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0423 17:31:18.899297       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4455:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682271678, NotBefore:1682271078, IssuedAt:1682271078, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4455", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"078cce78-654c-4205-9663-2479be140d16"}}}
  I0423 17:31:18.912089       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0423 17:31:18.919940       1 log.go:198] OK: Validated signature on JWT
  I0423 17:31:18.920020       1 log.go:198] OK: Got valid claims from token!
  I0423 17:31:18.920042       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4455:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682271678, NotBefore:1682271078, IssuedAt:1682271078, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4455", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"078cce78-654c-4205-9663-2479be140d16"}}}

  Apr 23 17:31:51.952: INFO: completed pod
  Apr 23 17:31:51.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4455" for this suite. @ 04/23/23 17:31:51.962
• [34.093 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/23/23 17:31:51.972
  Apr 23 17:31:51.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:31:51.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:31:52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:31:52.005
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:31:52.011
  E0423 17:31:52.107020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:53.107133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:54.107502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:55.108079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:31:56.036
  Apr 23 17:31:56.040: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-075d2a70-d677-4be7-95ce-7d45fb8f390f container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:31:56.05
  Apr 23 17:31:56.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9504" for this suite. @ 04/23/23 17:31:56.095
  E0423 17:31:56.108821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/23/23 17:31:56.114
  Apr 23 17:31:56.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:31:56.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:31:56.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:31:56.151
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/23/23 17:31:56.158
  E0423 17:31:57.108884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:58.108985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:31:59.109082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:00.109204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:32:00.2
  Apr 23 17:32:00.205: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-626e7976-a923-49ac-a3ad-6e1c3d4a5b58 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:32:00.213
  Apr 23 17:32:00.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5367" for this suite. @ 04/23/23 17:32:00.234
• [4.129 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 04/23/23 17:32:00.243
  Apr 23 17:32:00.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename endpointslice @ 04/23/23 17:32:00.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:32:00.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:32:00.269
  E0423 17:32:01.109333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:02.109866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:03.110039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:04.110095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:05.110331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 04/23/23 17:32:05.364
  E0423 17:32:06.110444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:07.110905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:08.111027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:09.112108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:10.112977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 04/23/23 17:32:10.373
  E0423 17:32:11.113689      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:12.114065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:13.114173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:14.114332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:15.115159      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/23/23 17:32:15.385
  E0423 17:32:16.115208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:17.115758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:18.115814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:19.115991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:20.116206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 04/23/23 17:32:20.398
  Apr 23 17:32:20.436: INFO: EndpointSlice for Service endpointslice-6779/example-named-port not found
  E0423 17:32:21.116411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:22.116899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:23.117297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:24.117904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:25.117956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:26.118037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:27.118546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:28.118651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:29.118861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:30.119009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:32:30.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6779" for this suite. @ 04/23/23 17:32:30.453
• [30.221 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/23/23 17:32:30.466
  Apr 23 17:32:30.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 17:32:30.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:32:30.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:32:30.512
  STEP: create the container @ 04/23/23 17:32:30.516
  W0423 17:32:30.538955      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/23/23 17:32:30.539
  E0423 17:32:31.119906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:32.119858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:33.119978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/23/23 17:32:33.56
  STEP: the container should be terminated @ 04/23/23 17:32:33.565
  STEP: the termination message should be set @ 04/23/23 17:32:33.565
  Apr 23 17:32:33.565: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/23/23 17:32:33.565
  Apr 23 17:32:33.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8800" for this suite. @ 04/23/23 17:32:33.59
• [3.132 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/23/23 17:32:33.598
  Apr 23 17:32:33.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/23/23 17:32:33.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:32:33.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:32:33.626
  STEP: Creating 50 configmaps @ 04/23/23 17:32:33.63
  STEP: Creating RC which spawns configmap-volume pods @ 04/23/23 17:32:33.948
  Apr 23 17:32:33.967: INFO: Pod name wrapped-volume-race-deedb5bf-543a-41e7-ba8e-9481f5542faa: Found 0 pods out of 5
  E0423 17:32:34.120344      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:35.120465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:36.120557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:37.120956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:38.121077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:32:38.976: INFO: Pod name wrapped-volume-race-deedb5bf-543a-41e7-ba8e-9481f5542faa: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/23/23 17:32:38.976
  STEP: Creating RC which spawns configmap-volume pods @ 04/23/23 17:32:39.017
  Apr 23 17:32:39.039: INFO: Pod name wrapped-volume-race-a5484169-081d-4721-b0da-3074a7f9416a: Found 0 pods out of 5
  E0423 17:32:39.122649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:40.122747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:41.123013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:42.123918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:43.124021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:32:44.052: INFO: Pod name wrapped-volume-race-a5484169-081d-4721-b0da-3074a7f9416a: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/23/23 17:32:44.052
  STEP: Creating RC which spawns configmap-volume pods @ 04/23/23 17:32:44.086
  E0423 17:32:44.125608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:32:44.137: INFO: Pod name wrapped-volume-race-67fb9ce2-7ccc-47ed-9721-30a373eedaa8: Found 0 pods out of 5
  E0423 17:32:45.125699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:46.126011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:47.127019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:48.127216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:49.127307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:32:49.147: INFO: Pod name wrapped-volume-race-67fb9ce2-7ccc-47ed-9721-30a373eedaa8: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/23/23 17:32:49.147
  Apr 23 17:32:49.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-67fb9ce2-7ccc-47ed-9721-30a373eedaa8 in namespace emptydir-wrapper-2430, will wait for the garbage collector to delete the pods @ 04/23/23 17:32:49.18
  Apr 23 17:32:49.249: INFO: Deleting ReplicationController wrapped-volume-race-67fb9ce2-7ccc-47ed-9721-30a373eedaa8 took: 14.001463ms
  Apr 23 17:32:49.351: INFO: Terminating ReplicationController wrapped-volume-race-67fb9ce2-7ccc-47ed-9721-30a373eedaa8 pods took: 101.318686ms
  E0423 17:32:50.127709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:51.128367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-a5484169-081d-4721-b0da-3074a7f9416a in namespace emptydir-wrapper-2430, will wait for the garbage collector to delete the pods @ 04/23/23 17:32:51.851
  Apr 23 17:32:51.915: INFO: Deleting ReplicationController wrapped-volume-race-a5484169-081d-4721-b0da-3074a7f9416a took: 8.020833ms
  Apr 23 17:32:52.016: INFO: Terminating ReplicationController wrapped-volume-race-a5484169-081d-4721-b0da-3074a7f9416a pods took: 100.93665ms
  E0423 17:32:52.129020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:53.129511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:54.130038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-deedb5bf-543a-41e7-ba8e-9481f5542faa in namespace emptydir-wrapper-2430, will wait for the garbage collector to delete the pods @ 04/23/23 17:32:54.517
  Apr 23 17:32:54.581: INFO: Deleting ReplicationController wrapped-volume-race-deedb5bf-543a-41e7-ba8e-9481f5542faa took: 8.996269ms
  Apr 23 17:32:54.682: INFO: Terminating ReplicationController wrapped-volume-race-deedb5bf-543a-41e7-ba8e-9481f5542faa pods took: 100.949841ms
  E0423 17:32:55.130605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:56.131423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:32:57.131968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 04/23/23 17:32:57.383
  STEP: Destroying namespace "emptydir-wrapper-2430" for this suite. @ 04/23/23 17:32:57.766
• [24.177 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/23/23 17:32:57.776
  Apr 23 17:32:57.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:32:57.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:32:57.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:32:57.81
  STEP: Setting up server cert @ 04/23/23 17:32:57.847
  E0423 17:32:58.132263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:32:58.379
  STEP: Deploying the webhook pod @ 04/23/23 17:32:58.388
  STEP: Wait for the deployment to be ready @ 04/23/23 17:32:58.405
  Apr 23 17:32:58.414: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:32:59.132357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:00.132470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:33:00.429
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:33:00.447
  E0423 17:33:01.132648      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:33:01.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 17:33:01.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9998-crds.webhook.example.com via the AdmissionRegistration API @ 04/23/23 17:33:01.97
  STEP: Creating a custom resource while v1 is storage version @ 04/23/23 17:33:01.988
  E0423 17:33:02.133488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:03.133655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/23/23 17:33:04.059
  STEP: Patching the custom resource while v2 is storage version @ 04/23/23 17:33:04.107
  E0423 17:33:04.134235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:33:04.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6731" for this suite. @ 04/23/23 17:33:04.904
  STEP: Destroying namespace "webhook-markers-6644" for this suite. @ 04/23/23 17:33:04.913
• [7.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/23/23 17:33:04.926
  Apr 23 17:33:04.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-watch @ 04/23/23 17:33:04.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:33:04.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:33:04.954
  Apr 23 17:33:04.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:33:05.135216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:06.135281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:07.135808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 04/23/23 17:33:07.511
  Apr 23 17:33:07.519: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T17:33:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T17:33:07Z]] name:name1 resourceVersion:24581 uid:e5042cf4-5b13-4bdd-9531-c073c7fa065e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0423 17:33:08.135940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:09.136527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:10.136716      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:11.136788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:12.136944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:13.137118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:14.137221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:15.137330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:16.137513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:17.138459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 04/23/23 17:33:17.519
  Apr 23 17:33:17.530: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T17:33:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T17:33:17Z]] name:name2 resourceVersion:24622 uid:6fcd92ab-a19d-4cc5-96a2-0d498d278042] num:map[num1:9223372036854775807 num2:1000000]]}
  E0423 17:33:18.138594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:19.138815      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:20.139018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:21.140122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:22.140680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:23.140792      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:24.141041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:25.141813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:26.142852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:27.142914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 04/23/23 17:33:27.531
  Apr 23 17:33:27.539: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T17:33:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T17:33:27Z]] name:name1 resourceVersion:24642 uid:e5042cf4-5b13-4bdd-9531-c073c7fa065e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0423 17:33:28.143014      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:29.144088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:30.144367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:31.144481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:32.144875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:33.144962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:34.146261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:35.147121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:36.147215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:37.147703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 04/23/23 17:33:37.54
  Apr 23 17:33:37.556: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T17:33:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T17:33:37Z]] name:name2 resourceVersion:24665 uid:6fcd92ab-a19d-4cc5-96a2-0d498d278042] num:map[num1:9223372036854775807 num2:1000000]]}
  E0423 17:33:38.148739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:39.149040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:40.149153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:41.149246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:42.149628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:43.149808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:44.149850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:45.149962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:46.150066      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:47.150992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 04/23/23 17:33:47.556
  Apr 23 17:33:47.569: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T17:33:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T17:33:27Z]] name:name1 resourceVersion:24687 uid:e5042cf4-5b13-4bdd-9531-c073c7fa065e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0423 17:33:48.151157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:49.152088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:50.152198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:51.152430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:52.153015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:53.153120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:54.153149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:55.153242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:56.154186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:57.154766      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 04/23/23 17:33:57.573
  Apr 23 17:33:57.584: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-23T17:33:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-23T17:33:37Z]] name:name2 resourceVersion:24707 uid:6fcd92ab-a19d-4cc5-96a2-0d498d278042] num:map[num1:9223372036854775807 num2:1000000]]}
  E0423 17:33:58.155830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:33:59.156030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:00.156245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:01.156467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:02.157004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:03.157318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:04.157525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:05.157750      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:06.157962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:07.158425      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:34:08.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-3307" for this suite. @ 04/23/23 17:34:08.125
• [63.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/23/23 17:34:08.138
  Apr 23 17:34:08.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename events @ 04/23/23 17:34:08.14
  E0423 17:34:08.158858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:34:08.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:34:08.167
  STEP: creating a test event @ 04/23/23 17:34:08.174
  STEP: listing all events in all namespaces @ 04/23/23 17:34:08.193
  STEP: patching the test event @ 04/23/23 17:34:08.2
  STEP: fetching the test event @ 04/23/23 17:34:08.216
  STEP: updating the test event @ 04/23/23 17:34:08.222
  STEP: getting the test event @ 04/23/23 17:34:08.243
  STEP: deleting the test event @ 04/23/23 17:34:08.252
  STEP: listing all events in all namespaces @ 04/23/23 17:34:08.261
  Apr 23 17:34:08.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8318" for this suite. @ 04/23/23 17:34:08.271
• [0.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/23/23 17:34:08.283
  Apr 23 17:34:08.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 17:34:08.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:34:08.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:34:08.309
  E0423 17:34:09.159094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:10.159449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:34:10.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 17:34:10.345: INFO: Deleting pod "var-expansion-cfa40c1c-0a77-41e2-920a-0fc62f3bba93" in namespace "var-expansion-9742"
  Apr 23 17:34:10.356: INFO: Wait up to 5m0s for pod "var-expansion-cfa40c1c-0a77-41e2-920a-0fc62f3bba93" to be fully deleted
  E0423 17:34:11.159530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:12.159970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-9742" for this suite. @ 04/23/23 17:34:12.367
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/23/23 17:34:12.377
  Apr 23 17:34:12.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename job @ 04/23/23 17:34:12.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:34:12.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:34:12.408
  STEP: Creating a suspended job @ 04/23/23 17:34:12.419
  STEP: Patching the Job @ 04/23/23 17:34:12.427
  STEP: Watching for Job to be patched @ 04/23/23 17:34:12.439
  Apr 23 17:34:12.441: INFO: Event ADDED observed for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 23 17:34:12.441: INFO: Event MODIFIED found for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/23/23 17:34:12.441
  STEP: Watching for Job to be updated @ 04/23/23 17:34:12.454
  Apr 23 17:34:12.456: INFO: Event MODIFIED found for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 17:34:12.456: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/23/23 17:34:12.456
  Apr 23 17:34:12.461: INFO: Job: e2e-dvrfp as labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp]
  STEP: Waiting for job to complete @ 04/23/23 17:34:12.461
  E0423 17:34:13.160952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:14.161356      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:15.161450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:16.161589      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:17.161700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:18.161805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:19.162805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:20.162979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:21.163071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:22.163896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 04/23/23 17:34:22.467
  STEP: Watching for Job to be deleted @ 04/23/23 17:34:22.477
  Apr 23 17:34:22.479: INFO: Event MODIFIED observed for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 17:34:22.479: INFO: Event MODIFIED observed for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 17:34:22.479: INFO: Event MODIFIED observed for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 17:34:22.480: INFO: Event MODIFIED observed for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 17:34:22.480: INFO: Event MODIFIED observed for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 23 17:34:22.480: INFO: Event DELETED found for Job e2e-dvrfp in namespace job-2768 with labels: map[e2e-dvrfp:patched e2e-job-label:e2e-dvrfp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/23/23 17:34:22.48
  Apr 23 17:34:22.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2768" for this suite. @ 04/23/23 17:34:22.492
• [10.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/23/23 17:34:22.522
  Apr 23 17:34:22.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 17:34:22.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:34:22.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:34:22.555
  Apr 23 17:34:22.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:34:23.164581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/23/23 17:34:23.941
  Apr 23 17:34:23.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-7740 --namespace=crd-publish-openapi-7740 create -f -'
  E0423 17:34:24.164666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:34:24.767: INFO: stderr: ""
  Apr 23 17:34:24.767: INFO: stdout: "e2e-test-crd-publish-openapi-2927-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 23 17:34:24.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-7740 --namespace=crd-publish-openapi-7740 delete e2e-test-crd-publish-openapi-2927-crds test-cr'
  Apr 23 17:34:24.871: INFO: stderr: ""
  Apr 23 17:34:24.871: INFO: stdout: "e2e-test-crd-publish-openapi-2927-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 23 17:34:24.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-7740 --namespace=crd-publish-openapi-7740 apply -f -'
  Apr 23 17:34:25.134: INFO: stderr: ""
  Apr 23 17:34:25.134: INFO: stdout: "e2e-test-crd-publish-openapi-2927-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 23 17:34:25.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-7740 --namespace=crd-publish-openapi-7740 delete e2e-test-crd-publish-openapi-2927-crds test-cr'
  E0423 17:34:25.165500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:34:25.208: INFO: stderr: ""
  Apr 23 17:34:25.208: INFO: stdout: "e2e-test-crd-publish-openapi-2927-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/23/23 17:34:25.208
  Apr 23 17:34:25.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-7740 explain e2e-test-crd-publish-openapi-2927-crds'
  Apr 23 17:34:25.406: INFO: stderr: ""
  Apr 23 17:34:25.406: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-2927-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0423 17:34:26.165561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:34:26.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7740" for this suite. @ 04/23/23 17:34:26.781
• [4.268 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/23/23 17:34:26.792
  Apr 23 17:34:26.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename subpath @ 04/23/23 17:34:26.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:34:26.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:34:26.871
  STEP: Setting up data @ 04/23/23 17:34:26.874
  STEP: Creating pod pod-subpath-test-downwardapi-ncx5 @ 04/23/23 17:34:26.887
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 17:34:26.887
  E0423 17:34:27.165842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:28.166235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:29.167287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:30.167431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:31.168087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:32.168501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:33.169149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:34.169255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:35.170143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:36.171002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:37.171148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:38.171598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:39.172657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:40.172905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:41.173580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:42.173900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:43.174307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:44.174349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:45.175104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:46.175227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:47.176239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:48.176461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:49.177545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:50.177851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:34:50.972
  Apr 23 17:34:50.975: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-subpath-test-downwardapi-ncx5 container test-container-subpath-downwardapi-ncx5: <nil>
  STEP: delete the pod @ 04/23/23 17:34:50.992
  STEP: Deleting pod pod-subpath-test-downwardapi-ncx5 @ 04/23/23 17:34:51.017
  Apr 23 17:34:51.017: INFO: Deleting pod "pod-subpath-test-downwardapi-ncx5" in namespace "subpath-6273"
  Apr 23 17:34:51.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6273" for this suite. @ 04/23/23 17:34:51.031
• [24.253 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/23/23 17:34:51.045
  Apr 23 17:34:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:34:51.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:34:51.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:34:51.073
  STEP: Creating configMap with name configmap-test-volume-map-04f99114-9d40-4a64-a09d-affb288e8269 @ 04/23/23 17:34:51.077
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:34:51.082
  E0423 17:34:51.177887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:52.177991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:53.179143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:54.179189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:34:55.114
  Apr 23 17:34:55.118: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-64ea6a0d-a494-4835-bcf5-807284427d7b container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:34:55.128
  Apr 23 17:34:55.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1730" for this suite. @ 04/23/23 17:34:55.152
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/23/23 17:34:55.161
  Apr 23 17:34:55.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 17:34:55.162
  E0423 17:34:55.180076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:34:55.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:34:55.191
  STEP: Creating secret with name secret-test-map-a2b5ece6-b30c-49c2-b218-1fa30d682757 @ 04/23/23 17:34:55.204
  STEP: Creating a pod to test consume secrets @ 04/23/23 17:34:55.215
  E0423 17:34:56.180320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:57.180751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:58.181722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:34:59.181954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:34:59.245
  Apr 23 17:34:59.249: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-secrets-c643d179-fb06-4138-9078-dacc772c09e5 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:34:59.286
  Apr 23 17:34:59.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1968" for this suite. @ 04/23/23 17:34:59.375
• [4.364 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/23/23 17:34:59.527
  Apr 23 17:34:59.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 17:34:59.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:34:59.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:34:59.612
  STEP: Creating projection with secret that has name secret-emptykey-test-d600d2ba-c256-4ef3-885d-44bfcfb283d6 @ 04/23/23 17:34:59.614
  Apr 23 17:34:59.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5614" for this suite. @ 04/23/23 17:34:59.622
• [0.187 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/23/23 17:34:59.715
  Apr 23 17:34:59.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 17:34:59.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:35:00.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:35:00.017
  Apr 23 17:35:00.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: creating the pod @ 04/23/23 17:35:00.027
  STEP: submitting the pod to kubernetes @ 04/23/23 17:35:00.028
  E0423 17:35:00.182230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:01.183184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:02.184122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:35:02.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3595" for this suite. @ 04/23/23 17:35:02.264
• [2.596 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/23/23 17:35:02.312
  Apr 23 17:35:02.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:35:02.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:35:02.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:35:02.484
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:35:02.489
  E0423 17:35:03.184050      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:04.184153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:05.184263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:06.184378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:35:06.516
  Apr 23 17:35:06.521: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-79d52e77-d40c-4bdb-9c41-a94741b51966 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:35:06.531
  Apr 23 17:35:06.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4619" for this suite. @ 04/23/23 17:35:06.556
• [4.259 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/23/23 17:35:06.571
  Apr 23 17:35:06.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:35:06.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:35:06.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:35:06.6
  STEP: creating service in namespace services-9069 @ 04/23/23 17:35:06.605
  STEP: creating service affinity-clusterip-transition in namespace services-9069 @ 04/23/23 17:35:06.605
  STEP: creating replication controller affinity-clusterip-transition in namespace services-9069 @ 04/23/23 17:35:06.616
  I0423 17:35:06.625713      21 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-9069, replica count: 3
  E0423 17:35:07.184981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:08.185033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:09.185928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 17:35:09.677214      21 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 17:35:09.688: INFO: Creating new exec pod
  E0423 17:35:10.186027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:11.186326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:12.186613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:35:12.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-9069 exec execpod-affinityg8m7b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Apr 23 17:35:12.881: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 23 17:35:12.881: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:35:12.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-9069 exec execpod-affinityg8m7b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.238 80'
  Apr 23 17:35:13.031: INFO: stderr: "+ + nc -v -t -w 2 10.152.183.238 80\nConnection to 10.152.183.238 80 port [tcp/http] succeeded!\necho hostName\n"
  Apr 23 17:35:13.031: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:35:13.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-9069 exec execpod-affinityg8m7b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.238:80/ ; done'
  E0423 17:35:13.187140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:35:13.293: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n"
  Apr 23 17:35:13.293: INFO: stdout: "\naffinity-clusterip-transition-rp5l2\naffinity-clusterip-transition-rp5l2\naffinity-clusterip-transition-rp5l2\naffinity-clusterip-transition-rp5l2\naffinity-clusterip-transition-kd7j4\naffinity-clusterip-transition-rp5l2\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kd7j4\naffinity-clusterip-transition-rp5l2\naffinity-clusterip-transition-rp5l2\naffinity-clusterip-transition-kd7j4\naffinity-clusterip-transition-kd7j4\naffinity-clusterip-transition-rp5l2\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-rp5l2"
  Apr 23 17:35:13.293: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.293: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.293: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-kd7j4
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-kd7j4
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-kd7j4
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-kd7j4
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.294: INFO: Received response from host: affinity-clusterip-transition-rp5l2
  Apr 23 17:35:13.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-9069 exec execpod-affinityg8m7b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.238:80/ ; done'
  Apr 23 17:35:13.564: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.238:80/\n"
  Apr 23 17:35:13.564: INFO: stdout: "\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8\naffinity-clusterip-transition-kmzv8"
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Received response from host: affinity-clusterip-transition-kmzv8
  Apr 23 17:35:13.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 17:35:13.569: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9069, will wait for the garbage collector to delete the pods @ 04/23/23 17:35:13.585
  Apr 23 17:35:13.649: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.309724ms
  Apr 23 17:35:13.750: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.599906ms
  E0423 17:35:14.187849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:15.188164      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:16.188223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9069" for this suite. @ 04/23/23 17:35:16.372
• [9.812 seconds]
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/23/23 17:35:16.383
  Apr 23 17:35:16.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename dns @ 04/23/23 17:35:16.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:35:16.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:35:16.421
  STEP: Creating a test headless service @ 04/23/23 17:35:16.433
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-885.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-885.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/23/23 17:35:16.447
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-885.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-885.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/23/23 17:35:16.447
  STEP: creating a pod to probe DNS @ 04/23/23 17:35:16.447
  STEP: submitting the pod to kubernetes @ 04/23/23 17:35:16.447
  E0423 17:35:17.188306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:18.188652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 17:35:18.471
  STEP: looking for the results for each expected name from probers @ 04/23/23 17:35:18.476
  Apr 23 17:35:18.497: INFO: DNS probes using dns-885/dns-test-1e299a21-86e6-4226-952b-485571ec221f succeeded

  Apr 23 17:35:18.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 17:35:18.501
  STEP: deleting the test headless service @ 04/23/23 17:35:18.524
  STEP: Destroying namespace "dns-885" for this suite. @ 04/23/23 17:35:18.537
• [2.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/23/23 17:35:18.553
  Apr 23 17:35:18.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:35:18.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:35:18.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:35:18.598
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:35:18.602
  E0423 17:35:19.189410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:20.189775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:21.189890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:22.190966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:35:22.634
  Apr 23 17:35:22.644: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-2923ba9a-0fa8-4e26-b225-20e90d3a5445 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:35:22.658
  Apr 23 17:35:22.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5070" for this suite. @ 04/23/23 17:35:22.683
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/23/23 17:35:22.692
  Apr 23 17:35:22.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:35:22.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:35:22.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:35:22.721
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:35:22.724
  E0423 17:35:23.191636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:24.191749      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:25.191885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:26.191993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:35:26.755
  Apr 23 17:35:26.759: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-ba203f93-cabc-4d2f-919b-2bba6c97b629 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:35:26.767
  Apr 23 17:35:26.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5417" for this suite. @ 04/23/23 17:35:26.792
• [4.113 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/23/23 17:35:26.805
  Apr 23 17:35:26.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-preemption @ 04/23/23 17:35:26.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:35:26.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:35:26.838
  Apr 23 17:35:26.859: INFO: Waiting up to 1m0s for all nodes to be ready
  E0423 17:35:27.192541      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:28.192681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:29.192956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:30.193107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:31.193197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:32.194058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:33.194232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:34.195001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:35.195577      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:36.196082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:37.196178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:38.196287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:39.196938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:40.197048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:41.197553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:42.197947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:43.198864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:44.199002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:45.199921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:46.200483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:47.200992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:48.201096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:49.201994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:50.202104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:51.202210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:52.203215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:53.204244      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:54.204386      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:55.204645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:56.205072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:57.205948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:58.206156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:35:59.207006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:00.208103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:01.208206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:02.208760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:03.209247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:04.209479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:05.210367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:06.210839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:07.210903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:08.211012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:09.212100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:10.212820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:11.212841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:12.213179      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:13.214040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:14.214157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:15.214832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:16.215023      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:17.215320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:18.216247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:19.217252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:20.218298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:21.218831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:22.218948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:23.219027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:24.220069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:25.220626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:26.220846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:36:26.880: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/23/23 17:36:26.896
  Apr 23 17:36:26.936: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 23 17:36:26.949: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 23 17:36:26.975: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 23 17:36:26.999: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 23 17:36:27.039: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 23 17:36:27.050: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/23/23 17:36:27.05
  E0423 17:36:27.221823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:28.222043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/23/23 17:36:29.078
  E0423 17:36:29.222769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:30.223047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:31.223568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:32.224008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:36:33.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-5118" for this suite. @ 04/23/23 17:36:33.184
• [66.386 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/23/23 17:36:33.192
  Apr 23 17:36:33.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:36:33.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:36:33.221
  E0423 17:36:33.223987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:36:33.225
  STEP: Setting up server cert @ 04/23/23 17:36:33.262
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:36:33.909
  STEP: Deploying the webhook pod @ 04/23/23 17:36:33.919
  STEP: Wait for the deployment to be ready @ 04/23/23 17:36:33.935
  Apr 23 17:36:33.947: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:36:34.224577      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:35.224670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:36:35.964
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:36:35.976
  E0423 17:36:36.225526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:36:36.977: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/23/23 17:36:36.982
  STEP: create a configmap that should be updated by the webhook @ 04/23/23 17:36:36.999
  Apr 23 17:36:37.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6121" for this suite. @ 04/23/23 17:36:37.077
  STEP: Destroying namespace "webhook-markers-7674" for this suite. @ 04/23/23 17:36:37.086
• [3.905 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/23/23 17:36:37.097
  Apr 23 17:36:37.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-preemption @ 04/23/23 17:36:37.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:36:37.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:36:37.128
  Apr 23 17:36:37.154: INFO: Waiting up to 1m0s for all nodes to be ready
  E0423 17:36:37.226566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:38.226873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:39.227436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:40.228161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:41.228556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:42.229359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:43.229695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:44.230090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:45.230637      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:46.230727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:47.231685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:48.231797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:49.232539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:50.232913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:51.233509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:52.234499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:53.235058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:54.236082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:55.236782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:56.237549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:57.238464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:58.238590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:36:59.238620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:00.239036      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:01.239435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:02.239768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:03.240406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:04.241041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:05.242329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:06.242897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:07.243260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:08.244318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:09.244670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:10.245392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:11.246323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:12.246658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:13.247149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:14.247289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:15.248324      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:16.248521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:17.248841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:18.249073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:19.249238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:20.249309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:21.249633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:22.249959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:23.250726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:24.251019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:25.251787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:26.251895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:27.252611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:28.253548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:29.253930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:30.254042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:31.254097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:32.254442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:33.255483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:34.255586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:35.255954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:36.256038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:37:37.180: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/23/23 17:37:37.185
  Apr 23 17:37:37.206: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 23 17:37:37.225: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 23 17:37:37.248: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  E0423 17:37:37.256915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:37:37.267: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 23 17:37:37.300: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 23 17:37:37.308: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/23/23 17:37:37.308
  E0423 17:37:38.257012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:39.257512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:40.258131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:41.258571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/23/23 17:37:41.354
  E0423 17:37:42.259019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:43.260148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:44.261406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:45.261497      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:37:45.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7939" for this suite. @ 04/23/23 17:37:45.497
• [68.410 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/23/23 17:37:45.508
  Apr 23 17:37:45.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:37:45.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:37:45.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:37:45.534
  STEP: Creating configMap with name projected-configmap-test-volume-map-bd10a09e-2b9a-4fff-a995-1b3349045f48 @ 04/23/23 17:37:45.538
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:37:45.545
  E0423 17:37:46.261641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:47.262612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:48.263049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:49.264054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:37:49.585
  Apr 23 17:37:49.589: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-configmaps-4cd34179-6d07-43b1-8f28-f6303c84c3db container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:37:49.612
  Apr 23 17:37:49.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-420" for this suite. @ 04/23/23 17:37:49.642
• [4.155 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/23/23 17:37:49.664
  Apr 23 17:37:49.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 17:37:49.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:37:49.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:37:49.712
  STEP: creating the pod @ 04/23/23 17:37:49.725
  STEP: waiting for pod running @ 04/23/23 17:37:49.737
  E0423 17:37:50.264220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:51.265055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 04/23/23 17:37:51.763
  Apr 23 17:37:51.770: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-826 PodName:var-expansion-63fb707d-5680-41d8-beee-4be61f65268f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:37:51.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:37:51.770: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:37:51.770: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-826/pods/var-expansion-63fb707d-5680-41d8-beee-4be61f65268f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/23/23 17:37:51.83
  Apr 23 17:37:51.834: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-826 PodName:var-expansion-63fb707d-5680-41d8-beee-4be61f65268f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:37:51.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:37:51.834: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:37:51.834: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-826/pods/var-expansion-63fb707d-5680-41d8-beee-4be61f65268f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/23/23 17:37:51.9
  E0423 17:37:52.265991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:37:52.422: INFO: Successfully updated pod "var-expansion-63fb707d-5680-41d8-beee-4be61f65268f"
  STEP: waiting for annotated pod running @ 04/23/23 17:37:52.422
  STEP: deleting the pod gracefully @ 04/23/23 17:37:52.426
  Apr 23 17:37:52.426: INFO: Deleting pod "var-expansion-63fb707d-5680-41d8-beee-4be61f65268f" in namespace "var-expansion-826"
  Apr 23 17:37:52.440: INFO: Wait up to 5m0s for pod "var-expansion-63fb707d-5680-41d8-beee-4be61f65268f" to be fully deleted
  E0423 17:37:53.266120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:54.267200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:55.267919      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:56.268148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:57.268752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:58.268877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:37:59.269068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:00.269475      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:01.269604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:02.270364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:03.271034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:04.275354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:05.272211      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:06.272418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:07.273448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:08.273570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:09.273666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:10.273791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:11.274102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:12.274456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:13.275442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:14.275572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:15.275667      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:16.276100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:17.276529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:18.276762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:19.276865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:20.276966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:21.277076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:22.278083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:23.279100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:24.279070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:24.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-826" for this suite. @ 04/23/23 17:38:24.544
• [34.890 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/23/23 17:38:24.555
  Apr 23 17:38:24.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 17:38:24.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:38:24.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:38:24.587
  STEP: fetching the /apis discovery document @ 04/23/23 17:38:24.593
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/23/23 17:38:24.596
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/23/23 17:38:24.596
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/23/23 17:38:24.596
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/23/23 17:38:24.597
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/23/23 17:38:24.597
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/23/23 17:38:24.599
  Apr 23 17:38:24.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2198" for this suite. @ 04/23/23 17:38:24.604
• [0.058 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/23/23 17:38:24.615
  Apr 23 17:38:24.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:38:24.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:38:24.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:38:24.676
  STEP: Creating projection with secret that has name projected-secret-test-map-d21eb0a2-70d2-430d-8563-4f9535a26d45 @ 04/23/23 17:38:24.68
  STEP: Creating a pod to test consume secrets @ 04/23/23 17:38:24.688
  E0423 17:38:25.279198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:26.279365      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:27.280359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:28.280558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:38:28.719
  Apr 23 17:38:28.726: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-secrets-1aafe04a-efe9-477d-8e2f-2ff3e25d3d90 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:38:28.735
  Apr 23 17:38:28.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3872" for this suite. @ 04/23/23 17:38:28.779
• [4.174 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 04/23/23 17:38:28.789
  Apr 23 17:38:28.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename aggregator @ 04/23/23 17:38:28.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:38:28.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:38:28.818
  Apr 23 17:38:28.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Registering the sample API server. @ 04/23/23 17:38:28.825
  Apr 23 17:38:29.110: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 23 17:38:29.157: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0423 17:38:29.281627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:30.281746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:31.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:31.282338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:32.282746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:33.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:33.283270      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:34.283473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:35.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:35.283536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:36.283598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:37.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:37.284436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:38.284549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:39.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:39.284961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:40.285176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:41.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:41.285252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:42.286222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:43.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:43.286804      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:44.286918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:45.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:45.287041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:46.288103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:47.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:47.288416      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:48.288532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:49.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:49.288708      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:50.288816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:51.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 17:38:51.289315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:52.289692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:53.290697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:38:53.387: INFO: Waited 115.07945ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/23/23 17:38:53.44
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/23/23 17:38:53.444
  STEP: List APIServices @ 04/23/23 17:38:53.453
  Apr 23 17:38:53.461: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/23/23 17:38:53.461
  Apr 23 17:38:53.479: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/23/23 17:38:53.479
  Apr 23 17:38:53.497: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.April, 23, 17, 38, 53, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/23/23 17:38:53.497
  Apr 23 17:38:53.503: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-04-23 17:38:53 +0000 UTC Passed all checks passed}
  Apr 23 17:38:53.503: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 17:38:53.503: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/23/23 17:38:53.503
  Apr 23 17:38:53.519: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-699067324" @ 04/23/23 17:38:53.52
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/23/23 17:38:53.543
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/23/23 17:38:53.555
  STEP: Patch APIService Status @ 04/23/23 17:38:53.56
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/23/23 17:38:53.571
  Apr 23 17:38:53.575: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-04-23 17:38:53 +0000 UTC Passed all checks passed}
  Apr 23 17:38:53.575: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 17:38:53.575: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 23 17:38:53.575: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/23/23 17:38:53.575
  STEP: Confirm that the generated APIService has been deleted @ 04/23/23 17:38:53.583
  Apr 23 17:38:53.583: INFO: Requesting list of APIServices to confirm quantity
  Apr 23 17:38:53.589: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 23 17:38:53.589: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 23 17:38:53.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-2534" for this suite. @ 04/23/23 17:38:53.737
• [24.956 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/23/23 17:38:53.75
  Apr 23 17:38:53.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:38:53.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:38:53.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:38:53.776
  STEP: Creating resourceQuota "e2e-rq-status-xtfn6" @ 04/23/23 17:38:53.785
  Apr 23 17:38:53.794: INFO: Resource quota "e2e-rq-status-xtfn6" reports spec: hard cpu limit of 500m
  Apr 23 17:38:53.794: INFO: Resource quota "e2e-rq-status-xtfn6" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-xtfn6" /status @ 04/23/23 17:38:53.794
  STEP: Confirm /status for "e2e-rq-status-xtfn6" resourceQuota via watch @ 04/23/23 17:38:53.805
  Apr 23 17:38:53.807: INFO: observed resourceQuota "e2e-rq-status-xtfn6" in namespace "resourcequota-3817" with hard status: v1.ResourceList(nil)
  Apr 23 17:38:53.807: INFO: Found resourceQuota "e2e-rq-status-xtfn6" in namespace "resourcequota-3817" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 23 17:38:53.807: INFO: ResourceQuota "e2e-rq-status-xtfn6" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/23/23 17:38:53.811
  Apr 23 17:38:53.818: INFO: Resource quota "e2e-rq-status-xtfn6" reports spec: hard cpu limit of 1
  Apr 23 17:38:53.818: INFO: Resource quota "e2e-rq-status-xtfn6" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-xtfn6" /status @ 04/23/23 17:38:53.818
  STEP: Confirm /status for "e2e-rq-status-xtfn6" resourceQuota via watch @ 04/23/23 17:38:53.827
  Apr 23 17:38:53.829: INFO: observed resourceQuota "e2e-rq-status-xtfn6" in namespace "resourcequota-3817" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 23 17:38:53.829: INFO: Found resourceQuota "e2e-rq-status-xtfn6" in namespace "resourcequota-3817" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 23 17:38:53.829: INFO: ResourceQuota "e2e-rq-status-xtfn6" /status was patched
  STEP: Get "e2e-rq-status-xtfn6" /status @ 04/23/23 17:38:53.829
  Apr 23 17:38:53.833: INFO: Resourcequota "e2e-rq-status-xtfn6" reports status: hard cpu of 1
  Apr 23 17:38:53.833: INFO: Resourcequota "e2e-rq-status-xtfn6" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-xtfn6" /status before checking Spec is unchanged @ 04/23/23 17:38:53.838
  Apr 23 17:38:53.845: INFO: Resourcequota "e2e-rq-status-xtfn6" reports status: hard cpu of 2
  Apr 23 17:38:53.845: INFO: Resourcequota "e2e-rq-status-xtfn6" reports status: hard memory of 2Gi
  Apr 23 17:38:53.846: INFO: Found resourceQuota "e2e-rq-status-xtfn6" in namespace "resourcequota-3817" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0423 17:38:54.291033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:55.291148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:56.292099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:57.292582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:58.293032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:38:59.293870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:00.293957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:01.294101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:02.294291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:03.294727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:04.294769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:05.294859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:06.295041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:07.295586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:08.295694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:09.295816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:10.295917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:11.296187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:12.296474      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:13.296552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:14.296900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:15.296969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:16.297089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:17.298065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:18.298177      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:19.299095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:20.300209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:21.300588      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:22.300998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:23.301112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:24.302177      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:25.302281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:26.302376      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:27.302774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:28.302934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:29.303726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:30.303819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:31.303935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:32.304049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:33.304941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:33.855: INFO: ResourceQuota "e2e-rq-status-xtfn6" Spec was unchanged and /status reset
  Apr 23 17:39:33.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3817" for this suite. @ 04/23/23 17:39:33.859
• [40.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 04/23/23 17:39:33.869
  Apr 23 17:39:33.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 17:39:33.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:39:33.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:39:33.894
  STEP: Creating service test in namespace statefulset-7509 @ 04/23/23 17:39:33.897
  STEP: Creating statefulset ss in namespace statefulset-7509 @ 04/23/23 17:39:33.906
  Apr 23 17:39:33.918: INFO: Found 0 stateful pods, waiting for 1
  E0423 17:39:34.306079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:35.306304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:36.306507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:37.306622      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:38.306921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:39.307042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:40.307111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:41.307219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:42.307666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:43.308095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:43.924: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/23/23 17:39:43.932
  STEP: Getting /status @ 04/23/23 17:39:43.95
  Apr 23 17:39:43.957: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/23/23 17:39:43.957
  Apr 23 17:39:43.968: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/23/23 17:39:43.968
  Apr 23 17:39:43.970: INFO: Observed &StatefulSet event: ADDED
  Apr 23 17:39:43.970: INFO: Found Statefulset ss in namespace statefulset-7509 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 23 17:39:43.970: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/23/23 17:39:43.97
  Apr 23 17:39:43.970: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 23 17:39:43.979: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/23/23 17:39:43.98
  Apr 23 17:39:43.982: INFO: Observed &StatefulSet event: ADDED
  Apr 23 17:39:43.982: INFO: Deleting all statefulset in ns statefulset-7509
  Apr 23 17:39:43.987: INFO: Scaling statefulset ss to 0
  E0423 17:39:44.308922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:45.309479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:46.309574      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:47.310627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:48.311362      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:49.311483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:50.311768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:51.312133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:52.312227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:39:53.312349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:54.010: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 17:39:54.014: INFO: Deleting statefulset ss
  Apr 23 17:39:54.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7509" for this suite. @ 04/23/23 17:39:54.034
• [20.173 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/23/23 17:39:54.042
  Apr 23 17:39:54.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename deployment @ 04/23/23 17:39:54.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:39:54.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:39:54.075
  STEP: creating a Deployment @ 04/23/23 17:39:54.083
  STEP: waiting for Deployment to be created @ 04/23/23 17:39:54.089
  STEP: waiting for all Replicas to be Ready @ 04/23/23 17:39:54.091
  Apr 23 17:39:54.092: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 17:39:54.093: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 17:39:54.110: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 17:39:54.110: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 17:39:54.140: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 17:39:54.140: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 17:39:54.167: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 23 17:39:54.167: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0423 17:39:54.312795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:55.206: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 23 17:39:55.206: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0423 17:39:55.313801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:55.645: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/23/23 17:39:55.646
  W0423 17:39:55.664747      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 23 17:39:55.666: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/23/23 17:39:55.667
  Apr 23 17:39:55.670: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0
  Apr 23 17:39:55.670: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0
  Apr 23 17:39:55.670: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0
  Apr 23 17:39:55.670: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0
  Apr 23 17:39:55.670: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0
  Apr 23 17:39:55.670: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0
  Apr 23 17:39:55.670: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0
  Apr 23 17:39:55.670: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 0
  Apr 23 17:39:55.671: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:55.671: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:55.671: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:55.671: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:55.671: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:55.671: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:55.681: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:55.681: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:55.704: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:55.704: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:55.720: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:55.720: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:55.729: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:55.729: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  E0423 17:39:56.313875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:57.247: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:57.247: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:57.274: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  STEP: listing Deployments @ 04/23/23 17:39:57.274
  Apr 23 17:39:57.280: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/23/23 17:39:57.28
  Apr 23 17:39:57.293: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/23/23 17:39:57.293
  Apr 23 17:39:57.303: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 17:39:57.311: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0423 17:39:57.314099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:57.335: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 17:39:57.366: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 17:39:57.387: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 17:39:58.239: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 17:39:58.284: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 17:39:58.293: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 23 17:39:58.303: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0423 17:39:58.314462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:58.319: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0423 17:39:59.315091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:39:59.683: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/23/23 17:39:59.717
  STEP: fetching the DeploymentStatus @ 04/23/23 17:39:59.725
  Apr 23 17:39:59.731: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:59.731: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:59.731: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:59.731: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:59.732: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 1
  Apr 23 17:39:59.732: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:59.732: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:59.732: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:59.732: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:59.732: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 2
  Apr 23 17:39:59.732: INFO: observed Deployment test-deployment in namespace deployment-3227 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/23/23 17:39:59.732
  Apr 23 17:39:59.745: INFO: observed event type MODIFIED
  Apr 23 17:39:59.745: INFO: observed event type MODIFIED
  Apr 23 17:39:59.745: INFO: observed event type MODIFIED
  Apr 23 17:39:59.745: INFO: observed event type MODIFIED
  Apr 23 17:39:59.745: INFO: observed event type MODIFIED
  Apr 23 17:39:59.746: INFO: observed event type MODIFIED
  Apr 23 17:39:59.746: INFO: observed event type MODIFIED
  Apr 23 17:39:59.746: INFO: observed event type MODIFIED
  Apr 23 17:39:59.746: INFO: observed event type MODIFIED
  Apr 23 17:39:59.746: INFO: observed event type MODIFIED
  Apr 23 17:39:59.746: INFO: observed event type MODIFIED
  Apr 23 17:39:59.746: INFO: observed event type MODIFIED
  Apr 23 17:39:59.753: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 23 17:39:59.761: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-3227  9e86f145-d897-4f83-b6fb-65a1718ac032 27026 4 2023-04-23 17:39:55 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 276ab04e-54b0-4b36-ab82-2dfe074c4bf5 0xc0050b2c67 0xc0050b2c68}] [] [{kube-controller-manager Update apps/v1 2023-04-23 17:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"276ab04e-54b0-4b36-ab82-2dfe074c4bf5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:39:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050b2cf0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 23 17:39:59.766: INFO: pod: "test-deployment-5b5dcbcd95-nkjjc":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-nkjjc test-deployment-5b5dcbcd95- deployment-3227  bb204cbd-7879-4e3c-a0ec-43c8cfbb84c6 27020 0 2023-04-23 17:39:55 +0000 UTC 2023-04-23 17:40:00 +0000 UTC 0xc008806a08 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 9e86f145-d897-4f83-b6fb-65a1718ac032 0xc008806a37 0xc008806a38}] [] [{kube-controller-manager Update v1 2023-04-23 17:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e86f145-d897-4f83-b6fb-65a1718ac032\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 17:39:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xx7dh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xx7dh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.254,StartTime:2023-04-23 17:39:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 17:39:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://fe19c57649add3d2f8cf1ffaa490d4447a5a5c56c42adfd2faed2737641eb148,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.254,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 23 17:39:59.766: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-3227  960527a7-ac09-4e0c-9920-e9a3d68b105f 27015 2 2023-04-23 17:39:57 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 276ab04e-54b0-4b36-ab82-2dfe074c4bf5 0xc0050b2d57 0xc0050b2d58}] [] [{kube-controller-manager Update apps/v1 2023-04-23 17:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"276ab04e-54b0-4b36-ab82-2dfe074c4bf5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 17:39:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050b2de0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Apr 23 17:39:59.771: INFO: pod: "test-deployment-6fc78d85c6-6bm8p":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-6bm8p test-deployment-6fc78d85c6- deployment-3227  2fcbffa5-1e03-4bb8-850a-18352225ffd2 26957 0 2023-04-23 17:39:57 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 960527a7-ac09-4e0c-9920-e9a3d68b105f 0xc0050b3087 0xc0050b3088}] [] [{kube-controller-manager Update v1 2023-04-23 17:39:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"960527a7-ac09-4e0c-9920-e9a3d68b105f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 17:39:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9cbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9cbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.196,StartTime:2023-04-23 17:39:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 17:39:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://84fa869ec92ee664aafa2cec2250383c8ecbc120c45bc92b50797fd0d4ab3505,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.196,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 23 17:39:59.771: INFO: pod: "test-deployment-6fc78d85c6-x5dgt":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-x5dgt test-deployment-6fc78d85c6- deployment-3227  df1a617d-1e5e-4aad-82b5-d24d48eef3dc 27014 0 2023-04-23 17:39:58 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 960527a7-ac09-4e0c-9920-e9a3d68b105f 0xc0050b3277 0xc0050b3278}] [] [{kube-controller-manager Update v1 2023-04-23 17:39:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"960527a7-ac09-4e0c-9920-e9a3d68b105f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 17:39:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bp4xp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bp4xp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 17:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.26,PodIP:192.168.122.166,StartTime:2023-04-23 17:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 17:39:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://68816ed74821f907df2f87d568ef0a9ac49d09c4e395362ba3d76b9f9863a86d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.166,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 23 17:39:59.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3227" for this suite. @ 04/23/23 17:39:59.781
• [5.749 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/23/23 17:39:59.794
  Apr 23 17:39:59.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:39:59.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:39:59.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:39:59.83
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:39:59.834
  E0423 17:40:00.315596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:01.316095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:02.316336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:03.317329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:40:03.86
  Apr 23 17:40:03.864: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-00e5755e-0dda-42e2-9454-763f5765e39a container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:40:03.882
  Apr 23 17:40:03.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2432" for this suite. @ 04/23/23 17:40:03.917
• [4.133 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/23/23 17:40:03.929
  Apr 23 17:40:03.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:40:03.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:40:03.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:40:03.962
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:40:03.966
  E0423 17:40:04.317440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:05.317539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:06.318377      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:07.318546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:40:08.001
  Apr 23 17:40:08.005: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-97a5f4af-9f1e-4248-b717-c209f64f8dd6 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:40:08.013
  Apr 23 17:40:08.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9661" for this suite. @ 04/23/23 17:40:08.042
• [4.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/23/23 17:40:08.052
  Apr 23 17:40:08.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 17:40:08.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:40:08.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:40:08.082
  Apr 23 17:40:08.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:40:08.319481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:09.320093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/23/23 17:40:09.79
  Apr 23 17:40:09.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 create -f -'
  E0423 17:40:10.320197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:10.357: INFO: stderr: ""
  Apr 23 17:40:10.357: INFO: stdout: "e2e-test-crd-publish-openapi-9913-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 23 17:40:10.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 delete e2e-test-crd-publish-openapi-9913-crds test-foo'
  Apr 23 17:40:10.433: INFO: stderr: ""
  Apr 23 17:40:10.433: INFO: stdout: "e2e-test-crd-publish-openapi-9913-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 23 17:40:10.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 apply -f -'
  Apr 23 17:40:11.067: INFO: stderr: ""
  Apr 23 17:40:11.067: INFO: stdout: "e2e-test-crd-publish-openapi-9913-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 23 17:40:11.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 delete e2e-test-crd-publish-openapi-9913-crds test-foo'
  Apr 23 17:40:11.142: INFO: stderr: ""
  Apr 23 17:40:11.142: INFO: stdout: "e2e-test-crd-publish-openapi-9913-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/23/23 17:40:11.142
  Apr 23 17:40:11.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 create -f -'
  E0423 17:40:11.321012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:11.355: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/23/23 17:40:11.355
  Apr 23 17:40:11.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 create -f -'
  Apr 23 17:40:11.569: INFO: rc: 1
  Apr 23 17:40:11.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 apply -f -'
  Apr 23 17:40:11.790: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/23/23 17:40:11.79
  Apr 23 17:40:11.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 create -f -'
  Apr 23 17:40:11.995: INFO: rc: 1
  Apr 23 17:40:11.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 --namespace=crd-publish-openapi-8778 apply -f -'
  Apr 23 17:40:12.216: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/23/23 17:40:12.216
  Apr 23 17:40:12.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 explain e2e-test-crd-publish-openapi-9913-crds'
  E0423 17:40:12.321930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:12.428: INFO: stderr: ""
  Apr 23 17:40:12.428: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9913-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/23/23 17:40:12.429
  Apr 23 17:40:12.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 explain e2e-test-crd-publish-openapi-9913-crds.metadata'
  Apr 23 17:40:12.628: INFO: stderr: ""
  Apr 23 17:40:12.628: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9913-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 23 17:40:12.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 explain e2e-test-crd-publish-openapi-9913-crds.spec'
  Apr 23 17:40:12.832: INFO: stderr: ""
  Apr 23 17:40:12.832: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9913-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 23 17:40:12.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 explain e2e-test-crd-publish-openapi-9913-crds.spec.bars'
  Apr 23 17:40:13.037: INFO: stderr: ""
  Apr 23 17:40:13.037: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9913-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/23/23 17:40:13.037
  Apr 23 17:40:13.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8778 explain e2e-test-crd-publish-openapi-9913-crds.spec.bars2'
  Apr 23 17:40:13.237: INFO: rc: 1
  E0423 17:40:13.322938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:14.323311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:14.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8778" for this suite. @ 04/23/23 17:40:14.769
• [6.724 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/23/23 17:40:14.779
  Apr 23 17:40:14.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 17:40:14.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:40:14.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:40:14.806
  STEP: creating all guestbook components @ 04/23/23 17:40:14.809
  Apr 23 17:40:14.809: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 23 17:40:14.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 create -f -'
  E0423 17:40:15.323983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:15.456: INFO: stderr: ""
  Apr 23 17:40:15.456: INFO: stdout: "service/agnhost-replica created\n"
  Apr 23 17:40:15.456: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 23 17:40:15.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 create -f -'
  Apr 23 17:40:15.731: INFO: stderr: ""
  Apr 23 17:40:15.731: INFO: stdout: "service/agnhost-primary created\n"
  Apr 23 17:40:15.731: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 23 17:40:15.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 create -f -'
  Apr 23 17:40:16.267: INFO: stderr: ""
  Apr 23 17:40:16.267: INFO: stdout: "service/frontend created\n"
  Apr 23 17:40:16.267: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 23 17:40:16.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 create -f -'
  E0423 17:40:16.324534      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:16.532: INFO: stderr: ""
  Apr 23 17:40:16.532: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 23 17:40:16.532: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 23 17:40:16.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 create -f -'
  Apr 23 17:40:16.763: INFO: stderr: ""
  Apr 23 17:40:16.763: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 23 17:40:16.763: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 23 17:40:16.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 create -f -'
  Apr 23 17:40:17.044: INFO: stderr: ""
  Apr 23 17:40:17.044: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/23/23 17:40:17.044
  Apr 23 17:40:17.044: INFO: Waiting for all frontend pods to be Running.
  E0423 17:40:17.325146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:18.325957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:19.326112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:20.326661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:21.326905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:22.095: INFO: Waiting for frontend to serve content.
  Apr 23 17:40:22.109: INFO: Trying to add a new entry to the guestbook.
  Apr 23 17:40:22.125: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/23/23 17:40:22.137
  Apr 23 17:40:22.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 delete --grace-period=0 --force -f -'
  Apr 23 17:40:22.249: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 17:40:22.249: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 17:40:22.249
  Apr 23 17:40:22.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 delete --grace-period=0 --force -f -'
  E0423 17:40:22.327005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:22.383: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 17:40:22.383: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 17:40:22.384
  Apr 23 17:40:22.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 delete --grace-period=0 --force -f -'
  Apr 23 17:40:22.473: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 17:40:22.473: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 17:40:22.473
  Apr 23 17:40:22.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 delete --grace-period=0 --force -f -'
  Apr 23 17:40:22.547: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 17:40:22.547: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 17:40:22.548
  Apr 23 17:40:22.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 delete --grace-period=0 --force -f -'
  Apr 23 17:40:22.640: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 17:40:22.640: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/23/23 17:40:22.64
  Apr 23 17:40:22.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-2142 delete --grace-period=0 --force -f -'
  Apr 23 17:40:22.756: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 17:40:22.757: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 23 17:40:22.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2142" for this suite. @ 04/23/23 17:40:22.767
• [8.007 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/23/23 17:40:22.786
  Apr 23 17:40:22.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 17:40:22.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:40:22.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:40:22.829
  STEP: Creating a pod to test service account token:  @ 04/23/23 17:40:22.833
  E0423 17:40:23.327628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:24.328094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:25.328226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:26.328391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:40:26.88
  Apr 23 17:40:26.884: INFO: Trying to get logs from node ip-172-31-70-241 pod test-pod-0897f45f-f206-4922-8404-a535cc862349 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:40:26.898
  Apr 23 17:40:26.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6199" for this suite. @ 04/23/23 17:40:26.927
• [4.147 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/23/23 17:40:26.935
  Apr 23 17:40:26.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename init-container @ 04/23/23 17:40:26.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:40:26.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:40:26.963
  STEP: creating the pod @ 04/23/23 17:40:26.968
  Apr 23 17:40:26.968: INFO: PodSpec: initContainers in spec.initContainers
  E0423 17:40:27.328555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:28.328793      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:29.328879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:30.329015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:31.329114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:40:31.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-998" for this suite. @ 04/23/23 17:40:31.82
• [4.894 seconds]
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/23/23 17:40:31.829
  Apr 23 17:40:31.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 17:40:31.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:40:31.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:40:31.859
  STEP: Creating a suspended cronjob @ 04/23/23 17:40:31.873
  STEP: Ensuring no jobs are scheduled @ 04/23/23 17:40:31.88
  E0423 17:40:32.330346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:33.330059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:34.331091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:35.332106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:36.332215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:37.333090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:38.333641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:39.333768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:40.333874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:41.333973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:42.334949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:43.335040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:44.335811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:45.335926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:46.336973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:47.337127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:48.337169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:49.337390      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:50.337948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:51.338069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:52.339027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:53.339141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:54.339234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:55.339342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:56.339430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:57.339574      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:58.340067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:40:59.340932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:00.341562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:01.341674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:02.341984      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:03.342192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:04.343003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:05.343047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:06.343752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:07.344210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:08.344314      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:09.345054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:10.345100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:11.345325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:12.346408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:13.346620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:14.346758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:15.346847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:16.347044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:17.348063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:18.348663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:19.348992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:20.349916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:21.350036      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:22.350095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:23.350236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:24.350319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:25.350422      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:26.350939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:27.351602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:28.352106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:29.352367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:30.353335      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:31.353476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:32.354305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:33.354545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:34.355145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:35.355305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:36.356095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:37.356166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:38.356282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:39.356411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:40.357366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:41.357471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:42.357611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:43.357677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:44.357992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:45.358088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:46.359048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:47.360104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:48.360196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:49.360442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:50.361001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:51.361111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:52.361301      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:53.361313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:54.361937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:55.362146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:56.362495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:57.362639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:58.363002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:41:59.364087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:00.364471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:01.364572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:02.364751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:03.364938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:04.365361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:05.365607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:06.366653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:07.367663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:08.368101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:09.368431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:10.369162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:11.369269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:12.369965      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:13.370139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:14.370278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:15.370589      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:16.371420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:17.371629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:18.371687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:19.372094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:20.372891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:21.372997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:22.374138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:23.374180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:24.375247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:25.376086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:26.376126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:27.376675      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:28.377505      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:29.377607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:30.377776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:31.378015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:32.378657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:33.378972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:34.379029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:35.380080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:36.380192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:37.380286      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:38.381125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:39.381226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:40.382172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:41.382688      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:42.383604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:43.383717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:44.384204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:45.384642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:46.384771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:47.385730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:48.385891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:49.386188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:50.386506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:51.386789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:52.387029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:53.387055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:54.388070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:55.388133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:56.388815      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:57.389054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:58.389126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:42:59.389441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:00.390151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:01.390848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:02.391839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:03.392425      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:04.393621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:05.393623      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:06.394703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:07.395230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:08.395848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:09.395947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:10.396769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:11.396887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:12.397297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:13.397395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:14.397504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:15.397796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:16.397906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:17.398664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:18.399477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:19.400236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:20.400716      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:21.400973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:22.401013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:23.401212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:24.402124      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:25.403140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:26.404086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:27.405171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:28.405284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:29.405492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:30.405765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:31.406492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:32.406767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:33.407439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:34.408113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:35.408297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:36.409400      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:37.409755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:38.409855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:39.410040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:40.410344      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:41.410446      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:42.411173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:43.412083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:44.412998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:45.413102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:46.413873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:47.414011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:48.414998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:49.415028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:50.416096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:51.417064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:52.417746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:53.418217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:54.418455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:55.418646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:56.419521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:57.419650      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:58.420665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:43:59.420854      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:00.421565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:01.422005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:02.422132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:03.422316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:04.423249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:05.423398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:06.423416      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:07.423807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:08.423870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:09.424010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:10.424511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:11.424715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:12.424952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:13.425179      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:14.425449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:15.425560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:16.426371      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:17.426461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:18.426581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:19.426790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:20.427774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:21.427960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:22.428512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:23.429393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:24.429631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:25.429696      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:26.430771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:27.431734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:28.431982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:29.432082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:30.433220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:31.433268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:32.433433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:33.433550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:34.434454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:35.434607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:36.435049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:37.435198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:38.435548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:39.435662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:40.435776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:41.436155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:42.436917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:43.437022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:44.437250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:45.437584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:46.437886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:47.438441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:48.439223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:49.439335      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:50.440072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:51.440183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:52.440864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:53.440977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:54.441242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:55.441350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:56.442226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:57.442862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:58.442842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:44:59.443006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:00.443866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:01.444651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:02.445300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:03.445328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:04.446297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:05.446408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:06.450365      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:07.450382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:08.451173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:09.451303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:10.452089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:11.452119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:12.452174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:13.452403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:14.453035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:15.453205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:16.453293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:17.453711      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:18.454087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:19.454322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:20.454354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:21.454457      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:22.455207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:23.455385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:24.455498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:25.455639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:26.455798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:27.456476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:28.457269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:29.457567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:30.457599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:31.457706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/23/23 17:45:31.89
  STEP: Removing cronjob @ 04/23/23 17:45:31.896
  Apr 23 17:45:31.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2650" for this suite. @ 04/23/23 17:45:31.911
• [300.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/23/23 17:45:31.925
  Apr 23 17:45:31.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/23/23 17:45:31.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:45:31.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:45:31.964
  E0423 17:45:32.458740      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:33.459021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:34.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/23/23 17:45:34.026
  STEP: Cleaning up the configmap @ 04/23/23 17:45:34.036
  STEP: Cleaning up the pod @ 04/23/23 17:45:34.047
  STEP: Destroying namespace "emptydir-wrapper-5357" for this suite. @ 04/23/23 17:45:34.084
• [2.175 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/23/23 17:45:34.1
  Apr 23 17:45:34.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 17:45:34.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:45:34.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:45:34.137
  Apr 23 17:45:34.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8363" for this suite. @ 04/23/23 17:45:34.206
• [0.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/23/23 17:45:34.229
  Apr 23 17:45:34.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename podtemplate @ 04/23/23 17:45:34.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:45:34.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:45:34.28
  STEP: Create a pod template @ 04/23/23 17:45:34.293
  STEP: Replace a pod template @ 04/23/23 17:45:34.31
  Apr 23 17:45:34.326: INFO: Found updated podtemplate annotation: "true"

  Apr 23 17:45:34.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6547" for this suite. @ 04/23/23 17:45:34.332
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/23/23 17:45:34.343
  Apr 23 17:45:34.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:45:34.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:45:34.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:45:34.376
  STEP: Setting up server cert @ 04/23/23 17:45:34.418
  E0423 17:45:34.459617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:45:34.932
  STEP: Deploying the webhook pod @ 04/23/23 17:45:34.947
  STEP: Wait for the deployment to be ready @ 04/23/23 17:45:34.969
  Apr 23 17:45:34.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:45:35.460170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:36.460868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:45:37.007
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:45:37.023
  E0423 17:45:37.461901      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:38.024: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/23/23 17:45:38.029
  STEP: create a pod that should be updated by the webhook @ 04/23/23 17:45:38.054
  Apr 23 17:45:38.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7308" for this suite. @ 04/23/23 17:45:38.193
  STEP: Destroying namespace "webhook-markers-6041" for this suite. @ 04/23/23 17:45:38.202
• [3.872 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 04/23/23 17:45:38.216
  Apr 23 17:45:38.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 17:45:38.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:45:38.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:45:38.251
  STEP: Creating simple DaemonSet "daemon-set" @ 04/23/23 17:45:38.295
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/23/23 17:45:38.304
  Apr 23 17:45:38.320: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:38.320: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:38.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:45:38.329: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  E0423 17:45:38.462373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:39.334: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:39.334: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:39.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 17:45:39.338: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  E0423 17:45:39.462695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:40.335: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:40.335: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:40.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 17:45:40.340: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/23/23 17:45:40.344
  Apr 23 17:45:40.371: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:40.371: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:40.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 17:45:40.380: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  E0423 17:45:40.462786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:41.386: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:41.386: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:41.391: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 17:45:41.391: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  E0423 17:45:41.463435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:42.397: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:42.397: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:42.404: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 23 17:45:42.404: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  E0423 17:45:42.463845      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:43.387: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:43.387: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 17:45:43.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 17:45:43.392: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 17:45:43.395
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7629, will wait for the garbage collector to delete the pods @ 04/23/23 17:45:43.395
  Apr 23 17:45:43.460: INFO: Deleting DaemonSet.extensions daemon-set took: 9.178483ms
  E0423 17:45:43.464122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:43.561: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.141794ms
  E0423 17:45:44.464993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:45.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:45:45.167: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 17:45:45.173: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28464"},"items":null}

  Apr 23 17:45:45.180: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28464"},"items":null}

  Apr 23 17:45:45.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7629" for this suite. @ 04/23/23 17:45:45.224
• [7.016 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/23/23 17:45:45.234
  Apr 23 17:45:45.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:45:45.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:45:45.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:45:45.27
  STEP: Counting existing ResourceQuota @ 04/23/23 17:45:45.28
  E0423 17:45:45.465127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:46.468238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:47.468974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:48.469059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:49.469744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 17:45:50.285
  STEP: Ensuring resource quota status is calculated @ 04/23/23 17:45:50.293
  E0423 17:45:50.470364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:51.470399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 04/23/23 17:45:52.298
  STEP: Ensuring resource quota status captures replicaset creation @ 04/23/23 17:45:52.319
  E0423 17:45:52.470519      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:53.471015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 04/23/23 17:45:54.327
  STEP: Ensuring resource quota status released usage @ 04/23/23 17:45:54.342
  E0423 17:45:54.471071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:55.472119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:56.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8640" for this suite. @ 04/23/23 17:45:56.353
• [11.130 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/23/23 17:45:56.364
  Apr 23 17:45:56.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 17:45:56.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:45:56.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:45:56.409
  Apr 23 17:45:56.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:45:56.472953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:57.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 17:45:57.472988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "custom-resource-definition-7842" for this suite. @ 04/23/23 17:45:57.473
• [1.122 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/23/23 17:45:57.487
  Apr 23 17:45:57.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 17:45:57.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:45:57.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:45:57.554
  STEP: Creating pod test-grpc-72056375-5f34-41be-986c-887d4215f6df in namespace container-probe-9355 @ 04/23/23 17:45:57.567
  E0423 17:45:58.473378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:45:59.473538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:45:59.626: INFO: Started pod test-grpc-72056375-5f34-41be-986c-887d4215f6df in namespace container-probe-9355
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 17:45:59.626
  Apr 23 17:45:59.631: INFO: Initial restart count of pod test-grpc-72056375-5f34-41be-986c-887d4215f6df is 0
  E0423 17:46:00.473665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:01.474467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:02.475126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:03.475453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:04.475541      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:05.476104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:06.476608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:07.477569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:08.477665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:09.477868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:10.478208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:11.478433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:12.479050      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:13.480119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:14.480234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:15.480967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:16.480432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:17.480771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:18.480872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:19.480980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:20.481081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:21.481340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:22.481522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:23.481733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:24.482664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:25.482772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:26.483035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:27.484053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:28.484174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:29.484265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:30.484686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:31.484928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:32.485890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:33.486413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:34.486521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:35.486769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:36.491625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:37.492338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:38.493399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:39.494449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:40.494550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:41.494658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:42.495621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:43.495710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:44.495831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:45.495953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:46.496046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:47.496147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:48.496188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:49.496394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:50.496512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:51.496638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:52.496752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:53.496852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:54.497937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:55.498145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:56.498245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:57.499173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:58.500021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:46:59.500222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:00.500541      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:01.500855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:02.500970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:03.501158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:04.501273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:05.501397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:06.502472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:07.502668      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:08.503288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:09.503384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:10.503572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:11.503595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:12.504635      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:13.504872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:14.504981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:15.505099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:16.505197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:17.506286      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:18.506418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:19.506557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:20.506745      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:21.506830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:22.507119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:23.507222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:24.507340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:25.508378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:26.508521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:27.508762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:28.508879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:29.509124      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:30.509671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:31.509867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:32.510222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:33.510335      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:34.510461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:35.510565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:36.510670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:37.511024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:38.511715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:39.511804      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:40.512706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:41.512838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:42.513713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:43.513833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:44.514065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:45.514289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:46.514949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:47.515033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:48.515144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:49.515243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:50.516088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:51.516237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:52.516563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:53.516559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:54.517264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:55.517349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:56.517513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:57.517749      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:58.517871      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:47:59.518000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:00.518740      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:01.518851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:02.519009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:03.519108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:04.520077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:05.520260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:06.520378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:07.520783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:08.521092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:09.521291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:10.522094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:11.522295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:12.522325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:13.522436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:14.523440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:15.523545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:16.524214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:17.524686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:18.525612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:19.525813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:20.526738      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:21.526931      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:22.527638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:23.528090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:24.528603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:25.528965      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:26.529086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:27.529166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:28.529655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:29.529832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:30.530849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:31.531006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:32.531535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:33.532144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:34.532460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:35.532682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:36.533113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:37.533209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:38.533662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:39.533759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:40.534856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:41.535018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:42.535093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:43.535132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:44.536115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:45.536240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:46.536333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:47.536805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:48.537662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:49.537872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:50.538350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:51.538531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:52.538798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:53.539736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:54.539811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:55.540020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:56.540840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:57.541208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:58.541833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:48:59.541879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:00.542714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:01.542823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:02.543708      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:03.543886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:04.543947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:05.544058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:06.544167      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:07.544595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:08.544857      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:09.545161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:10.546227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:11.546339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:12.546375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:13.546686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:14.547270      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:15.547390      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:16.548359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:17.548759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:18.549130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:19.549339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:20.549663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:21.550744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:22.551203      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:23.551319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:24.551640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:25.552088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:26.552573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:27.552807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:28.553096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:29.553201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:30.553558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:31.553647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:32.553760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:33.554294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:34.554779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:35.554924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:36.555029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:37.556079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:38.556619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:39.557468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:40.558469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:41.558660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:42.559599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:43.559699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:44.560077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:45.560185      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:46.561231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:47.561489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:48.561534      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:49.562018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:50.562594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:51.563138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:52.563840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:53.564130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:54.565025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:55.565110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:56.565317      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:57.565345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:58.565453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:49:59.565640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:50:00.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 17:50:00.305
  STEP: Destroying namespace "container-probe-9355" for this suite. @ 04/23/23 17:50:00.322
• [242.844 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/23/23 17:50:00.332
  Apr 23 17:50:00.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 17:50:00.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:50:00.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:50:00.368
  STEP: Creating pod liveness-5889b9f2-7157-4638-9467-33360677fd65 in namespace container-probe-6091 @ 04/23/23 17:50:00.372
  E0423 17:50:00.566408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:01.566780      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:50:02.394: INFO: Started pod liveness-5889b9f2-7157-4638-9467-33360677fd65 in namespace container-probe-6091
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 17:50:02.394
  Apr 23 17:50:02.398: INFO: Initial restart count of pod liveness-5889b9f2-7157-4638-9467-33360677fd65 is 0
  E0423 17:50:02.567751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:03.568117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:04.568220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:05.568350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:06.569163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:07.569611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:08.569740      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:09.569840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:10.570338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:11.570538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:12.570660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:13.570916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:14.571352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:15.572091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:16.572374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:17.572853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:18.573055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:19.573178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:20.574213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:21.574332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:22.574934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:23.575121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:24.575893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:25.576034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:26.576939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:27.577259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:28.577683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:29.577910      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:30.577970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:31.578063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:32.578582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:33.578809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:34.579096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:35.579205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:36.579278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:37.579899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:38.580192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:39.580451      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:40.581360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:41.581656      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:42.581830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:43.581934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:44.582932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:45.583105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:46.584089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:47.584197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:48.584923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:49.585020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:50.586059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:51.586168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:52.587034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:53.588115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:54.589038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:55.589144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:56.589728      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:57.590418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:58.591111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:50:59.591220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:00.592082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:01.592425      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:02.593204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:03.593300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:04.594214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:05.594316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:06.595161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:07.596092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:08.596802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:09.596927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:10.597468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:11.597672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:12.597889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:13.598019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:14.598378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:15.599042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:16.599043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:17.599952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:18.600041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:19.601249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:20.601349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:21.601452      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:22.602429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:23.603155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:24.603190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:25.603268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:26.604097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:27.604292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:28.604735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:29.604817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:30.604929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:31.605037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:32.605299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:33.605434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:34.605677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:35.605770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:36.606158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:37.606308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:38.606530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:39.606638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:40.606769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:41.607031      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:42.607134      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:43.607229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:44.607330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:45.607911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:46.608125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:47.608454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:48.608700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:49.608801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:50.608905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:51.609566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:52.610028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:53.610313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:54.610439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:55.610518      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:56.610685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:57.610792      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:58.611009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:51:59.611135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:00.611250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:01.611361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:02.612079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:03.612200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:04.612370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:05.612488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:06.613119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:07.613977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:08.614200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:09.614300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:10.614374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:11.614505      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:12.614580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:13.614702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:14.614946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:15.615124      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:16.615430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:17.615736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:18.615877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:19.615970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:20.616059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:21.616383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:22.617029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:23.617145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:24.617237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:25.617418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:26.617682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:27.617901      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:28.618159      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:29.618295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:30.618395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:31.618753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:32.619690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:33.619825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:34.619936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:35.620039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:36.620200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:37.620915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:38.621019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:39.621919      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:40.622466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:41.623316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:42.624264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:43.625214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:44.625293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:45.625408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:46.625605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:47.626002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:48.626111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:49.626213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:50.626330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:51.627393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:52.628091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:53.628207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:54.628933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:55.629065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:56.629138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:57.629699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:58.629827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:52:59.629933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:00.630046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:01.630144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:02.630512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:03.631126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:04.631233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:05.632111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:06.632317      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:07.633152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:08.633327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:09.634310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:10.634426      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:11.635265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:12.636090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:13.637140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:14.637676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:15.637759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:16.637855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:17.637896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:18.638010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:19.638115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:20.638231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:21.638329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:22.638898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:23.639024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:24.640098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:25.640209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:26.640416      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:27.640738      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:28.640904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:29.640979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:30.641062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:31.641666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:32.642002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:33.642875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:34.643029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:35.643386      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:36.643481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:37.643873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:38.644374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:39.644500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:40.644592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:41.644678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:42.645633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:43.645764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:44.645977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:45.646925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:46.647067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:47.648098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:48.648302      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:49.649192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:50.649446      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:51.649549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:52.649886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:53.650303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:54.650122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:55.650275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:56.650467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:57.650491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:58.650640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:53:59.650747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:00.650930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:01.651032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:02.652206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:03.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 17:54:03.075
  STEP: Destroying namespace "container-probe-6091" for this suite. @ 04/23/23 17:54:03.089
• [242.767 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/23/23 17:54:03.1
  Apr 23 17:54:03.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 17:54:03.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:03.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:03.133
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:54:03.137
  E0423 17:54:03.652384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:04.652499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:05.653496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:06.653603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:54:07.169
  Apr 23 17:54:07.173: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-bddf0f3e-b890-4f8f-9a2c-0ce0f82289d2 container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:54:07.196
  Apr 23 17:54:07.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2698" for this suite. @ 04/23/23 17:54:07.22
• [4.128 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/23/23 17:54:07.229
  Apr 23 17:54:07.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 17:54:07.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:07.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:07.259
  STEP: Given a ReplicationController is created @ 04/23/23 17:54:07.265
  STEP: When the matched label of one of its pods change @ 04/23/23 17:54:07.273
  Apr 23 17:54:07.278: INFO: Pod name pod-release: Found 0 pods out of 1
  E0423 17:54:07.654595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:08.654814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:09.655191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:10.656107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:11.656403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:12.283: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/23/23 17:54:12.294
  E0423 17:54:12.656995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:13.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9933" for this suite. @ 04/23/23 17:54:13.309
• [6.087 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/23/23 17:54:13.317
  Apr 23 17:54:13.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-pred @ 04/23/23 17:54:13.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:13.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:13.35
  Apr 23 17:54:13.358: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 23 17:54:13.373: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 17:54:13.379: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-37-26 before test
  Apr 23 17:54:13.386: INFO: default-http-backend-kubernetes-worker-65fc475d49-65dxk from ingress-nginx-kubernetes-worker started at 2023-04-23 16:24:39 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.386: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Apr 23 17:54:13.386: INFO: nginx-ingress-controller-kubernetes-worker-gbggh from ingress-nginx-kubernetes-worker started at 2023-04-23 16:24:39 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.386: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:54:13.386: INFO: coredns-5c7f76ccb8-sgclp from kube-system started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.386: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 17:54:13.386: INFO: kube-state-metrics-5b95b4459c-4xnvw from kube-system started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.386: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Apr 23 17:54:13.386: INFO: metrics-server-v0.5.2-6cf8c8b69c-zzwzt from kube-system started at 2023-04-23 16:24:07 +0000 UTC (2 container statuses recorded)
  Apr 23 17:54:13.386: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 23 17:54:13.386: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Apr 23 17:54:13.386: INFO: dashboard-metrics-scraper-6b8586b5c9-646rd from kubernetes-dashboard started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.387: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Apr 23 17:54:13.387: INFO: kubernetes-dashboard-6869f4cd5f-64m7l from kubernetes-dashboard started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.387: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Apr 23 17:54:13.387: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-28gbw from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:54:13.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:54:13.387: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 17:54:13.387: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-70-241 before test
  Apr 23 17:54:13.393: INFO: nginx-ingress-controller-kubernetes-worker-4cgst from ingress-nginx-kubernetes-worker started at 2023-04-23 17:24:09 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.393: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:54:13.393: INFO: pod-release-hf6fm from replication-controller-9933 started at 2023-04-23 17:54:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.393: INFO: 	Container pod-release ready: true, restart count 0
  Apr 23 17:54:13.393: INFO: pod-release-qwfx8 from replication-controller-9933 started at 2023-04-23 17:54:12 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.393: INFO: 	Container pod-release ready: true, restart count 0
  Apr 23 17:54:13.393: INFO: sonobuoy from sonobuoy started at 2023-04-23 16:37:12 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.393: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 23 17:54:13.393: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-clx9s from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:54:13.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:54:13.393: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 17:54:13.393: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-86-26 before test
  Apr 23 17:54:13.409: INFO: nginx-ingress-controller-kubernetes-worker-httzn from ingress-nginx-kubernetes-worker started at 2023-04-23 16:26:06 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.409: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:54:13.409: INFO: calico-kube-controllers-6c8cb79d47-nmw6q from kube-system started at 2023-04-23 16:31:05 +0000 UTC (1 container statuses recorded)
  Apr 23 17:54:13.409: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Apr 23 17:54:13.409: INFO: sonobuoy-e2e-job-b60cd7a9aecc4153 from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:54:13.410: INFO: 	Container e2e ready: true, restart count 0
  Apr 23 17:54:13.410: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:54:13.410: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-mdknn from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:54:13.410: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:54:13.410: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/23/23 17:54:13.41
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.1758a1e2229dbc61], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 04/23/23 17:54:13.449
  E0423 17:54:13.657327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:14.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7111" for this suite. @ 04/23/23 17:54:14.45
• [1.146 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/23/23 17:54:14.463
  Apr 23 17:54:14.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replicaset @ 04/23/23 17:54:14.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:14.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:14.494
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/23/23 17:54:14.498
  Apr 23 17:54:14.511: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0423 17:54:14.657989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:15.658165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:16.658838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:17.659024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:18.659308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:19.517: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 17:54:19.517
  STEP: getting scale subresource @ 04/23/23 17:54:19.517
  STEP: updating a scale subresource @ 04/23/23 17:54:19.53
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/23/23 17:54:19.536
  STEP: Patch a scale subresource @ 04/23/23 17:54:19.543
  Apr 23 17:54:19.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1539" for this suite. @ 04/23/23 17:54:19.566
• [5.121 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/23/23 17:54:19.585
  Apr 23 17:54:19.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:54:19.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:19.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:19.626
  STEP: Creating the pod @ 04/23/23 17:54:19.629
  E0423 17:54:19.659230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:20.659685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:21.659788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:22.190: INFO: Successfully updated pod "annotationupdate9d2a937a-7cdc-40ed-a36a-ccec86ce5d59"
  E0423 17:54:22.659853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:23.659963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:24.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5108" for this suite. @ 04/23/23 17:54:24.218
• [4.646 seconds]
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/23/23 17:54:24.231
  Apr 23 17:54:24.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename discovery @ 04/23/23 17:54:24.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:24.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:24.262
  STEP: Setting up server cert @ 04/23/23 17:54:24.277
  E0423 17:54:24.661009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:25.300: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 23 17:54:25.302: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 23 17:54:25.302: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 23 17:54:25.302: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 23 17:54:25.302: INFO: Checking APIGroup: apps
  Apr 23 17:54:25.305: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 23 17:54:25.305: INFO: Versions found [{apps/v1 v1}]
  Apr 23 17:54:25.305: INFO: apps/v1 matches apps/v1
  Apr 23 17:54:25.305: INFO: Checking APIGroup: events.k8s.io
  Apr 23 17:54:25.306: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 23 17:54:25.306: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 23 17:54:25.306: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 23 17:54:25.306: INFO: Checking APIGroup: authentication.k8s.io
  Apr 23 17:54:25.307: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 23 17:54:25.307: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 23 17:54:25.307: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 23 17:54:25.307: INFO: Checking APIGroup: authorization.k8s.io
  Apr 23 17:54:25.308: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 23 17:54:25.308: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 23 17:54:25.308: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 23 17:54:25.308: INFO: Checking APIGroup: autoscaling
  Apr 23 17:54:25.310: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 23 17:54:25.310: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 23 17:54:25.310: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 23 17:54:25.310: INFO: Checking APIGroup: batch
  Apr 23 17:54:25.311: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 23 17:54:25.311: INFO: Versions found [{batch/v1 v1}]
  Apr 23 17:54:25.311: INFO: batch/v1 matches batch/v1
  Apr 23 17:54:25.311: INFO: Checking APIGroup: certificates.k8s.io
  Apr 23 17:54:25.312: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 23 17:54:25.312: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 23 17:54:25.312: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 23 17:54:25.312: INFO: Checking APIGroup: networking.k8s.io
  Apr 23 17:54:25.314: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 23 17:54:25.314: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 23 17:54:25.314: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 23 17:54:25.314: INFO: Checking APIGroup: policy
  Apr 23 17:54:25.315: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 23 17:54:25.315: INFO: Versions found [{policy/v1 v1}]
  Apr 23 17:54:25.315: INFO: policy/v1 matches policy/v1
  Apr 23 17:54:25.315: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 23 17:54:25.316: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 23 17:54:25.316: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 23 17:54:25.316: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 23 17:54:25.316: INFO: Checking APIGroup: storage.k8s.io
  Apr 23 17:54:25.317: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 23 17:54:25.317: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 23 17:54:25.317: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 23 17:54:25.317: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 23 17:54:25.318: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 23 17:54:25.318: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 23 17:54:25.318: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 23 17:54:25.318: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 23 17:54:25.320: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 23 17:54:25.320: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 23 17:54:25.320: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 23 17:54:25.320: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 23 17:54:25.321: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 23 17:54:25.321: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 23 17:54:25.321: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 23 17:54:25.321: INFO: Checking APIGroup: coordination.k8s.io
  Apr 23 17:54:25.322: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 23 17:54:25.322: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 23 17:54:25.322: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 23 17:54:25.322: INFO: Checking APIGroup: node.k8s.io
  Apr 23 17:54:25.324: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 23 17:54:25.324: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 23 17:54:25.324: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 23 17:54:25.324: INFO: Checking APIGroup: discovery.k8s.io
  Apr 23 17:54:25.325: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 23 17:54:25.325: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 23 17:54:25.325: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 23 17:54:25.325: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 23 17:54:25.327: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 23 17:54:25.327: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 23 17:54:25.327: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 23 17:54:25.327: INFO: Checking APIGroup: metrics.k8s.io
  Apr 23 17:54:25.328: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Apr 23 17:54:25.328: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Apr 23 17:54:25.328: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Apr 23 17:54:25.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-6613" for this suite. @ 04/23/23 17:54:25.336
• [1.112 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/23/23 17:54:25.345
  Apr 23 17:54:25.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename gc @ 04/23/23 17:54:25.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:25.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:25.375
  STEP: create the rc1 @ 04/23/23 17:54:25.383
  STEP: create the rc2 @ 04/23/23 17:54:25.39
  E0423 17:54:25.661611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:26.662313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:27.663369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:28.664243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:29.664778      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:30.665686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/23/23 17:54:31.405
  E0423 17:54:31.671623      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/23/23 17:54:32.1
  STEP: wait for the rc to be deleted @ 04/23/23 17:54:32.11
  E0423 17:54:32.672971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:33.673822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:34.673929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:35.674226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:36.675076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:37.159: INFO: 74 pods remaining
  Apr 23 17:54:37.165: INFO: 74 pods has nil DeletionTimestamp
  Apr 23 17:54:37.165: INFO: 
  E0423 17:54:37.675147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:38.675373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:39.675493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:40.675604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:41.675704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/23/23 17:54:42.126
  W0423 17:54:42.131849      21 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 23 17:54:42.131: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 17:54:42.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-229jf" in namespace "gc-9620"
  Apr 23 17:54:42.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-286nr" in namespace "gc-9620"
  Apr 23 17:54:42.174: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hf6b" in namespace "gc-9620"
  Apr 23 17:54:42.195: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mkbf" in namespace "gc-9620"
  Apr 23 17:54:42.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mr6g" in namespace "gc-9620"
  Apr 23 17:54:42.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pbdk" in namespace "gc-9620"
  Apr 23 17:54:42.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gdvg" in namespace "gc-9620"
  Apr 23 17:54:42.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lqbf" in namespace "gc-9620"
  Apr 23 17:54:42.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qh6t" in namespace "gc-9620"
  Apr 23 17:54:42.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rt5s" in namespace "gc-9620"
  Apr 23 17:54:42.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-4szxj" in namespace "gc-9620"
  Apr 23 17:54:42.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wqxc" in namespace "gc-9620"
  Apr 23 17:54:42.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-5772d" in namespace "gc-9620"
  Apr 23 17:54:42.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-58l26" in namespace "gc-9620"
  Apr 23 17:54:42.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-649jt" in namespace "gc-9620"
  Apr 23 17:54:42.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-6w992" in namespace "gc-9620"
  Apr 23 17:54:42.414: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zjnh" in namespace "gc-9620"
  Apr 23 17:54:42.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dcqb" in namespace "gc-9620"
  Apr 23 17:54:42.452: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dvcb" in namespace "gc-9620"
  Apr 23 17:54:42.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xv5c" in namespace "gc-9620"
  Apr 23 17:54:42.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-97dwr" in namespace "gc-9620"
  Apr 23 17:54:42.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-98gkn" in namespace "gc-9620"
  Apr 23 17:54:42.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kr98" in namespace "gc-9620"
  Apr 23 17:54:42.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-b64sf" in namespace "gc-9620"
  Apr 23 17:54:42.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl5gk" in namespace "gc-9620"
  Apr 23 17:54:42.573: INFO: Deleting pod "simpletest-rc-to-be-deleted-blhlh" in namespace "gc-9620"
  Apr 23 17:54:42.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpfzl" in namespace "gc-9620"
  Apr 23 17:54:42.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb6hv" in namespace "gc-9620"
  Apr 23 17:54:42.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9fts" in namespace "gc-9620"
  Apr 23 17:54:42.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlxwv" in namespace "gc-9620"
  E0423 17:54:42.676590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:42.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr2p8" in namespace "gc-9620"
  Apr 23 17:54:42.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4cj8" in namespace "gc-9620"
  Apr 23 17:54:42.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-fb9br" in namespace "gc-9620"
  Apr 23 17:54:42.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnj78" in namespace "gc-9620"
  Apr 23 17:54:42.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-frjsn" in namespace "gc-9620"
  Apr 23 17:54:42.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-frph5" in namespace "gc-9620"
  Apr 23 17:54:42.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx5v2" in namespace "gc-9620"
  Apr 23 17:54:42.801: INFO: Deleting pod "simpletest-rc-to-be-deleted-fz5pt" in namespace "gc-9620"
  Apr 23 17:54:42.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2br4" in namespace "gc-9620"
  Apr 23 17:54:42.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2fbl" in namespace "gc-9620"
  Apr 23 17:54:42.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-gg8kz" in namespace "gc-9620"
  Apr 23 17:54:42.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggllv" in namespace "gc-9620"
  Apr 23 17:54:42.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghtvv" in namespace "gc-9620"
  Apr 23 17:54:42.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-gj4zp" in namespace "gc-9620"
  Apr 23 17:54:43.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-gltvf" in namespace "gc-9620"
  Apr 23 17:54:43.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-gztmj" in namespace "gc-9620"
  Apr 23 17:54:43.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-htc5k" in namespace "gc-9620"
  Apr 23 17:54:43.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzrgn" in namespace "gc-9620"
  Apr 23 17:54:43.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9t76" in namespace "gc-9620"
  Apr 23 17:54:43.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-jf8b5" in namespace "gc-9620"
  Apr 23 17:54:43.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9620" for this suite. @ 04/23/23 17:54:43.095
• [17.759 seconds]
------------------------------
SS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/23/23 17:54:43.104
  Apr 23 17:54:43.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename containers @ 04/23/23 17:54:43.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:43.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:43.132
  STEP: Creating a pod to test override all @ 04/23/23 17:54:43.136
  E0423 17:54:43.676896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:44.677024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:45.677415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:46.677714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:47.678429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:48.678997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:49.679822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:50.679962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:54:51.183
  Apr 23 17:54:51.186: INFO: Trying to get logs from node ip-172-31-70-241 pod client-containers-8fca5c9b-d099-4ba7-a804-e2bb71caad02 container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:54:51.194
  Apr 23 17:54:51.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5716" for this suite. @ 04/23/23 17:54:51.217
• [8.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/23/23 17:54:51.236
  Apr 23 17:54:51.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:54:51.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:54:51.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:54:51.257
  STEP: Setting up server cert @ 04/23/23 17:54:51.299
  E0423 17:54:51.680223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:54:51.758
  STEP: Deploying the webhook pod @ 04/23/23 17:54:51.766
  STEP: Wait for the deployment to be ready @ 04/23/23 17:54:51.779
  Apr 23 17:54:51.787: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:54:52.681297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:53.681429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:54:53.8
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:54:53.819
  E0423 17:54:54.681552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:54:54.819: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/23/23 17:54:54.824
  STEP: create a pod that should be denied by the webhook @ 04/23/23 17:54:54.845
  STEP: create a pod that causes the webhook to hang @ 04/23/23 17:54:54.864
  E0423 17:54:55.682151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:56.682706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:57.683186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:58.683338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:54:59.683453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:00.683542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:01.684108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:02.684654      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:03.684791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:04.684901      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 04/23/23 17:55:04.885
  STEP: create a configmap that should be admitted by the webhook @ 04/23/23 17:55:04.898
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/23/23 17:55:04.911
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/23/23 17:55:04.923
  STEP: create a namespace that bypass the webhook @ 04/23/23 17:55:04.93
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/23/23 17:55:04.957
  Apr 23 17:55:04.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8810" for this suite. @ 04/23/23 17:55:05.039
  STEP: Destroying namespace "webhook-markers-8194" for this suite. @ 04/23/23 17:55:05.05
  STEP: Destroying namespace "exempted-namespace-3424" for this suite. @ 04/23/23 17:55:05.06
• [13.831 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/23/23 17:55:05.067
  Apr 23 17:55:05.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:55:05.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:05.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:05.106
  STEP: Creating configMap with name configmap-test-volume-2ef24864-07b7-4f89-a1bc-f89ea408b2a5 @ 04/23/23 17:55:05.112
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:55:05.117
  E0423 17:55:05.685011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:06.685151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:07.685314      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:08.685505      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:55:09.141
  Apr 23 17:55:09.146: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-3f90cc00-efb3-42ba-8c9d-921c1c115e03 container configmap-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:55:09.156
  Apr 23 17:55:09.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5483" for this suite. @ 04/23/23 17:55:09.192
• [4.134 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/23/23 17:55:09.202
  Apr 23 17:55:09.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:55:09.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:09.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:09.233
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:55:09.237
  E0423 17:55:09.686461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:10.686564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:11.687317      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:12.688105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:55:13.303
  Apr 23 17:55:13.313: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-23047f00-207e-472d-a651-fcac63b529af container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:55:13.325
  Apr 23 17:55:13.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5430" for this suite. @ 04/23/23 17:55:13.37
• [4.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/23/23 17:55:13.39
  Apr 23 17:55:13.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename svcaccounts @ 04/23/23 17:55:13.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:13.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:13.439
  E0423 17:55:13.688674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:14.688746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/23/23 17:55:15.493
  Apr 23 17:55:15.493: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4055 pod-service-account-29eae062-3802-4747-9440-4ddeae67a968 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  E0423 17:55:15.688803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/23/23 17:55:15.715
  Apr 23 17:55:15.715: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4055 pod-service-account-29eae062-3802-4747-9440-4ddeae67a968 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 04/23/23 17:55:15.918
  Apr 23 17:55:15.918: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4055 pod-service-account-29eae062-3802-4747-9440-4ddeae67a968 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Apr 23 17:55:16.136: INFO: Got root ca configmap in namespace "svcaccounts-4055"
  Apr 23 17:55:16.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4055" for this suite. @ 04/23/23 17:55:16.146
• [2.766 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/23/23 17:55:16.157
  Apr 23 17:55:16.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename disruption @ 04/23/23 17:55:16.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:16.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:16.188
  STEP: Creating a kubernetes client @ 04/23/23 17:55:16.193
  Apr 23 17:55:16.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename disruption-2 @ 04/23/23 17:55:16.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:16.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:16.233
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:55:16.247
  E0423 17:55:16.688882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:17.688876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:55:18.263
  E0423 17:55:18.689012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:19.689190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/23/23 17:55:20.28
  E0423 17:55:20.690215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:21.690328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 04/23/23 17:55:22.289
  STEP: listing a collection of PDBs in namespace disruption-3152 @ 04/23/23 17:55:22.293
  STEP: deleting a collection of PDBs @ 04/23/23 17:55:22.297
  STEP: Waiting for the PDB collection to be deleted @ 04/23/23 17:55:22.313
  Apr 23 17:55:22.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 17:55:22.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-986" for this suite. @ 04/23/23 17:55:22.329
  STEP: Destroying namespace "disruption-3152" for this suite. @ 04/23/23 17:55:22.339
• [6.190 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/23/23 17:55:22.348
  Apr 23 17:55:22.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:55:22.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:22.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:22.374
  STEP: Creating a pod to test downward API volume plugin @ 04/23/23 17:55:22.38
  E0423 17:55:22.690694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:23.690946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:24.691028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:25.691162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:55:26.412
  Apr 23 17:55:26.418: INFO: Trying to get logs from node ip-172-31-70-241 pod downwardapi-volume-9160ae74-f41d-4bdc-b4f9-0a6845f9bd5d container client-container: <nil>
  STEP: delete the pod @ 04/23/23 17:55:26.427
  Apr 23 17:55:26.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5957" for this suite. @ 04/23/23 17:55:26.46
• [4.124 seconds]
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/23/23 17:55:26.474
  Apr 23 17:55:26.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pod-network-test @ 04/23/23 17:55:26.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:26.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:26.506
  STEP: Performing setup for networking test in namespace pod-network-test-3937 @ 04/23/23 17:55:26.513
  STEP: creating a selector @ 04/23/23 17:55:26.513
  STEP: Creating the service pods in kubernetes @ 04/23/23 17:55:26.513
  Apr 23 17:55:26.513: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0423 17:55:26.691499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:27.691598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:28.692397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:29.692501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:30.693146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:31.693333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:32.694136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:33.694219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:34.694995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:35.695044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:36.695453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:37.695979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:38.696057      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:39.696165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:40.696369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:41.696481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:42.696693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:43.696828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:44.697476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:45.697675      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:46.698458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:47.699017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/23/23 17:55:48.653
  E0423 17:55:48.699720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:49.700114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:55:50.675: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 23 17:55:50.675: INFO: Breadth first check of 192.168.21.55 on host 172.31.37.26...
  Apr 23 17:55:50.679: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.6.251:9080/dial?request=hostname&protocol=http&host=192.168.21.55&port=8083&tries=1'] Namespace:pod-network-test-3937 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:55:50.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:55:50.680: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:55:50.680: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3937/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.6.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.21.55%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0423 17:55:50.700593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:55:50.805: INFO: Waiting for responses: map[]
  Apr 23 17:55:50.805: INFO: reached 192.168.21.55 after 0/1 tries
  Apr 23 17:55:50.805: INFO: Breadth first check of 192.168.6.253 on host 172.31.70.241...
  Apr 23 17:55:50.813: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.6.251:9080/dial?request=hostname&protocol=http&host=192.168.6.253&port=8083&tries=1'] Namespace:pod-network-test-3937 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:55:50.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:55:50.814: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:55:50.814: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3937/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.6.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.6.253%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 17:55:50.898: INFO: Waiting for responses: map[]
  Apr 23 17:55:50.898: INFO: reached 192.168.6.253 after 0/1 tries
  Apr 23 17:55:50.898: INFO: Breadth first check of 192.168.122.142 on host 172.31.86.26...
  Apr 23 17:55:50.902: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.6.251:9080/dial?request=hostname&protocol=http&host=192.168.122.142&port=8083&tries=1'] Namespace:pod-network-test-3937 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 17:55:50.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 17:55:50.903: INFO: ExecWithOptions: Clientset creation
  Apr 23 17:55:50.903: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3937/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.6.251%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.122.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 23 17:55:50.997: INFO: Waiting for responses: map[]
  Apr 23 17:55:50.997: INFO: reached 192.168.122.142 after 0/1 tries
  Apr 23 17:55:50.997: INFO: Going to retry 0 out of 3 pods....
  Apr 23 17:55:50.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3937" for this suite. @ 04/23/23 17:55:51.006
• [24.544 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/23/23 17:55:51.027
  Apr 23 17:55:51.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:55:51.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:51.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:51.056
  STEP: Setting up server cert @ 04/23/23 17:55:51.092
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:55:51.402
  STEP: Deploying the webhook pod @ 04/23/23 17:55:51.416
  STEP: Wait for the deployment to be ready @ 04/23/23 17:55:51.433
  Apr 23 17:55:51.447: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:55:51.701438      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:52.701614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:55:53.459
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:55:53.47
  E0423 17:55:53.701743      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:55:54.470: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/23/23 17:55:54.475
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 17:55:54.495
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/23/23 17:55:54.506
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 17:55:54.52
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/23/23 17:55:54.534
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 17:55:54.549
  Apr 23 17:55:54.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9030" for this suite. @ 04/23/23 17:55:54.626
  STEP: Destroying namespace "webhook-markers-4971" for this suite. @ 04/23/23 17:55:54.637
• [3.621 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/23/23 17:55:54.652
  Apr 23 17:55:54.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename proxy @ 04/23/23 17:55:54.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:55:54.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:55:54.679
  STEP: starting an echo server on multiple ports @ 04/23/23 17:55:54.701
  STEP: creating replication controller proxy-service-d2jgx in namespace proxy-1812 @ 04/23/23 17:55:54.701
  E0423 17:55:54.701976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 17:55:54.712172      21 runners.go:194] Created replication controller with name: proxy-service-d2jgx, namespace: proxy-1812, replica count: 1
  E0423 17:55:55.701921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 17:55:55.763234      21 runners.go:194] proxy-service-d2jgx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 17:55:56.702996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 17:55:56.764351      21 runners.go:194] proxy-service-d2jgx Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 17:55:56.771: INFO: setup took 2.085322224s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/23/23 17:55:56.771
  Apr 23 17:55:56.787: INFO: (0) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 15.3822ms)
  Apr 23 17:55:56.788: INFO: (0) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 15.96999ms)
  Apr 23 17:55:56.788: INFO: (0) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 16.524275ms)
  Apr 23 17:55:56.788: INFO: (0) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 16.564122ms)
  Apr 23 17:55:56.789: INFO: (0) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 17.683943ms)
  Apr 23 17:55:56.789: INFO: (0) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 17.563792ms)
  Apr 23 17:55:56.790: INFO: (0) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 18.572433ms)
  Apr 23 17:55:56.791: INFO: (0) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 18.79824ms)
  Apr 23 17:55:56.791: INFO: (0) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 19.168418ms)
  Apr 23 17:55:56.791: INFO: (0) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 19.826134ms)
  Apr 23 17:55:56.791: INFO: (0) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 19.467747ms)
  Apr 23 17:55:56.803: INFO: (0) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 31.187595ms)
  Apr 23 17:55:56.809: INFO: (0) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 37.621745ms)
  Apr 23 17:55:56.809: INFO: (0) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 37.515658ms)
  Apr 23 17:55:56.812: INFO: (0) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 39.999076ms)
  Apr 23 17:55:56.812: INFO: (0) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 40.800616ms)
  Apr 23 17:55:56.820: INFO: (1) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 7.372532ms)
  Apr 23 17:55:56.823: INFO: (1) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 10.600257ms)
  Apr 23 17:55:56.823: INFO: (1) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 10.754549ms)
  Apr 23 17:55:56.824: INFO: (1) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 11.436878ms)
  Apr 23 17:55:56.825: INFO: (1) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 12.509263ms)
  Apr 23 17:55:56.825: INFO: (1) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 12.635703ms)
  Apr 23 17:55:56.827: INFO: (1) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 14.149038ms)
  Apr 23 17:55:56.827: INFO: (1) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 14.568143ms)
  Apr 23 17:55:56.827: INFO: (1) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 14.305272ms)
  Apr 23 17:55:56.828: INFO: (1) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 14.964798ms)
  Apr 23 17:55:56.828: INFO: (1) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 14.712848ms)
  Apr 23 17:55:56.828: INFO: (1) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 15.273939ms)
  Apr 23 17:55:56.829: INFO: (1) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 16.292128ms)
  Apr 23 17:55:56.830: INFO: (1) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 17.735809ms)
  Apr 23 17:55:56.830: INFO: (1) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 16.862369ms)
  Apr 23 17:55:56.830: INFO: (1) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 17.16519ms)
  Apr 23 17:55:56.842: INFO: (2) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 10.471043ms)
  Apr 23 17:55:56.843: INFO: (2) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 12.397463ms)
  Apr 23 17:55:56.843: INFO: (2) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 12.2041ms)
  Apr 23 17:55:56.845: INFO: (2) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 13.864974ms)
  Apr 23 17:55:56.845: INFO: (2) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 13.918536ms)
  Apr 23 17:55:56.845: INFO: (2) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 13.604778ms)
  Apr 23 17:55:56.845: INFO: (2) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 14.479469ms)
  Apr 23 17:55:56.845: INFO: (2) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 13.979083ms)
  Apr 23 17:55:56.845: INFO: (2) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 14.905779ms)
  Apr 23 17:55:56.846: INFO: (2) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 15.268377ms)
  Apr 23 17:55:56.846: INFO: (2) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 14.802428ms)
  Apr 23 17:55:56.847: INFO: (2) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 15.787224ms)
  Apr 23 17:55:56.847: INFO: (2) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 16.25764ms)
  Apr 23 17:55:56.847: INFO: (2) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 16.06591ms)
  Apr 23 17:55:56.848: INFO: (2) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 16.417123ms)
  Apr 23 17:55:56.849: INFO: (2) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 17.810506ms)
  Apr 23 17:55:56.865: INFO: (3) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 15.937736ms)
  Apr 23 17:55:56.867: INFO: (3) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 17.649313ms)
  Apr 23 17:55:56.869: INFO: (3) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 19.414872ms)
  Apr 23 17:55:56.869: INFO: (3) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 19.797229ms)
  Apr 23 17:55:56.870: INFO: (3) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 20.552668ms)
  Apr 23 17:55:56.870: INFO: (3) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 20.32982ms)
  Apr 23 17:55:56.870: INFO: (3) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 20.508672ms)
  Apr 23 17:55:56.871: INFO: (3) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 22.031522ms)
  Apr 23 17:55:56.871: INFO: (3) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 21.72974ms)
  Apr 23 17:55:56.872: INFO: (3) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 22.679123ms)
  Apr 23 17:55:56.872: INFO: (3) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 23.096022ms)
  Apr 23 17:55:56.873: INFO: (3) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 23.653477ms)
  Apr 23 17:55:56.873: INFO: (3) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 23.275031ms)
  Apr 23 17:55:56.873: INFO: (3) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 23.605378ms)
  Apr 23 17:55:56.873: INFO: (3) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 23.561758ms)
  Apr 23 17:55:56.876: INFO: (3) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 26.496425ms)
  Apr 23 17:55:56.890: INFO: (4) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 13.774319ms)
  Apr 23 17:55:56.892: INFO: (4) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 15.454289ms)
  Apr 23 17:55:56.893: INFO: (4) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 15.726572ms)
  Apr 23 17:55:56.893: INFO: (4) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 15.794132ms)
  Apr 23 17:55:56.896: INFO: (4) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 18.228448ms)
  Apr 23 17:55:56.899: INFO: (4) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 21.57444ms)
  Apr 23 17:55:56.901: INFO: (4) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 22.815945ms)
  Apr 23 17:55:56.902: INFO: (4) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 24.973651ms)
  Apr 23 17:55:56.902: INFO: (4) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 24.880686ms)
  Apr 23 17:55:56.902: INFO: (4) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 25.594229ms)
  Apr 23 17:55:56.903: INFO: (4) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 24.974724ms)
  Apr 23 17:55:56.903: INFO: (4) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 25.852769ms)
  Apr 23 17:55:56.903: INFO: (4) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 26.909424ms)
  Apr 23 17:55:56.904: INFO: (4) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 26.110955ms)
  Apr 23 17:55:56.904: INFO: (4) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 26.25876ms)
  Apr 23 17:55:56.904: INFO: (4) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 26.636222ms)
  Apr 23 17:55:56.915: INFO: (5) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 10.624038ms)
  Apr 23 17:55:56.915: INFO: (5) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 10.921763ms)
  Apr 23 17:55:56.919: INFO: (5) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 13.777032ms)
  Apr 23 17:55:56.920: INFO: (5) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 14.332063ms)
  Apr 23 17:55:56.920: INFO: (5) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 14.512322ms)
  Apr 23 17:55:56.921: INFO: (5) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 15.090453ms)
  Apr 23 17:55:56.922: INFO: (5) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 16.679819ms)
  Apr 23 17:55:56.923: INFO: (5) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 16.947014ms)
  Apr 23 17:55:56.924: INFO: (5) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 17.991973ms)
  Apr 23 17:55:56.924: INFO: (5) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 19.164217ms)
  Apr 23 17:55:56.924: INFO: (5) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 18.842825ms)
  Apr 23 17:55:56.924: INFO: (5) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 19.845364ms)
  Apr 23 17:55:56.925: INFO: (5) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 20.123953ms)
  Apr 23 17:55:56.925: INFO: (5) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 20.426306ms)
  Apr 23 17:55:56.925: INFO: (5) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 20.388437ms)
  Apr 23 17:55:56.925: INFO: (5) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 20.154202ms)
  Apr 23 17:55:56.935: INFO: (6) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 9.530229ms)
  Apr 23 17:55:56.940: INFO: (6) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 13.727361ms)
  Apr 23 17:55:56.942: INFO: (6) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 16.198508ms)
  Apr 23 17:55:56.945: INFO: (6) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 18.876882ms)
  Apr 23 17:55:56.945: INFO: (6) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 18.68595ms)
  Apr 23 17:55:56.946: INFO: (6) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 20.07169ms)
  Apr 23 17:55:56.946: INFO: (6) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 19.872775ms)
  Apr 23 17:55:56.947: INFO: (6) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 20.832288ms)
  Apr 23 17:55:56.947: INFO: (6) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 20.687987ms)
  Apr 23 17:55:56.947: INFO: (6) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 21.333158ms)
  Apr 23 17:55:56.947: INFO: (6) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 21.432748ms)
  Apr 23 17:55:56.949: INFO: (6) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 23.432289ms)
  Apr 23 17:55:56.949: INFO: (6) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 23.047375ms)
  Apr 23 17:55:56.949: INFO: (6) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 22.707647ms)
  Apr 23 17:55:56.950: INFO: (6) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 23.582195ms)
  Apr 23 17:55:56.950: INFO: (6) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 24.781764ms)
  Apr 23 17:55:56.957: INFO: (7) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 6.453956ms)
  Apr 23 17:55:56.961: INFO: (7) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 10.690012ms)
  Apr 23 17:55:56.964: INFO: (7) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 13.76202ms)
  Apr 23 17:55:56.965: INFO: (7) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 14.014953ms)
  Apr 23 17:55:56.965: INFO: (7) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 14.099303ms)
  Apr 23 17:55:56.965: INFO: (7) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 14.331545ms)
  Apr 23 17:55:56.965: INFO: (7) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 14.439557ms)
  Apr 23 17:55:56.965: INFO: (7) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 14.319557ms)
  Apr 23 17:55:56.966: INFO: (7) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 15.667991ms)
  Apr 23 17:55:56.966: INFO: (7) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 15.302321ms)
  Apr 23 17:55:56.967: INFO: (7) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 17.197826ms)
  Apr 23 17:55:56.967: INFO: (7) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 16.853724ms)
  Apr 23 17:55:56.968: INFO: (7) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 16.734097ms)
  Apr 23 17:55:56.968: INFO: (7) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 17.107268ms)
  Apr 23 17:55:56.969: INFO: (7) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 18.384278ms)
  Apr 23 17:55:56.970: INFO: (7) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 19.264187ms)
  Apr 23 17:55:56.982: INFO: (8) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 11.983664ms)
  Apr 23 17:55:56.982: INFO: (8) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 11.860467ms)
  Apr 23 17:55:56.983: INFO: (8) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 13.253338ms)
  Apr 23 17:55:56.985: INFO: (8) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 14.19253ms)
  Apr 23 17:55:56.985: INFO: (8) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 14.844472ms)
  Apr 23 17:55:56.986: INFO: (8) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 15.582866ms)
  Apr 23 17:55:56.986: INFO: (8) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 15.727311ms)
  Apr 23 17:55:56.986: INFO: (8) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 16.174779ms)
  Apr 23 17:55:56.986: INFO: (8) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 15.923311ms)
  Apr 23 17:55:56.987: INFO: (8) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 16.44778ms)
  Apr 23 17:55:56.987: INFO: (8) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 16.725869ms)
  Apr 23 17:55:56.987: INFO: (8) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 17.290017ms)
  Apr 23 17:55:56.988: INFO: (8) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 17.466738ms)
  Apr 23 17:55:56.988: INFO: (8) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 17.992348ms)
  Apr 23 17:55:56.989: INFO: (8) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 19.346958ms)
  Apr 23 17:55:56.991: INFO: (8) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 20.528318ms)
  Apr 23 17:55:56.998: INFO: (9) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 6.898073ms)
  Apr 23 17:55:57.002: INFO: (9) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 10.284462ms)
  Apr 23 17:55:57.003: INFO: (9) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 12.291587ms)
  Apr 23 17:55:57.003: INFO: (9) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 11.992684ms)
  Apr 23 17:55:57.005: INFO: (9) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 13.612246ms)
  Apr 23 17:55:57.005: INFO: (9) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 13.283759ms)
  Apr 23 17:55:57.005: INFO: (9) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 14.010833ms)
  Apr 23 17:55:57.006: INFO: (9) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 14.1509ms)
  Apr 23 17:55:57.006: INFO: (9) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 14.141642ms)
  Apr 23 17:55:57.006: INFO: (9) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 14.983762ms)
  Apr 23 17:55:57.006: INFO: (9) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 15.222652ms)
  Apr 23 17:55:57.007: INFO: (9) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 16.081183ms)
  Apr 23 17:55:57.008: INFO: (9) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 16.795832ms)
  Apr 23 17:55:57.008: INFO: (9) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 16.712735ms)
  Apr 23 17:55:57.009: INFO: (9) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 17.351165ms)
  Apr 23 17:55:57.009: INFO: (9) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 17.996312ms)
  Apr 23 17:55:57.017: INFO: (10) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 7.369527ms)
  Apr 23 17:55:57.020: INFO: (10) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 10.248246ms)
  Apr 23 17:55:57.021: INFO: (10) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 10.708021ms)
  Apr 23 17:55:57.021: INFO: (10) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 10.867275ms)
  Apr 23 17:55:57.022: INFO: (10) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 12.181688ms)
  Apr 23 17:55:57.022: INFO: (10) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 12.1408ms)
  Apr 23 17:55:57.023: INFO: (10) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 12.92604ms)
  Apr 23 17:55:57.023: INFO: (10) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 12.702092ms)
  Apr 23 17:55:57.023: INFO: (10) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 13.17251ms)
  Apr 23 17:55:57.023: INFO: (10) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 13.214925ms)
  Apr 23 17:55:57.024: INFO: (10) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 13.94682ms)
  Apr 23 17:55:57.024: INFO: (10) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 14.645106ms)
  Apr 23 17:55:57.026: INFO: (10) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 15.611241ms)
  Apr 23 17:55:57.026: INFO: (10) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 15.927462ms)
  Apr 23 17:55:57.027: INFO: (10) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 17.322199ms)
  Apr 23 17:55:57.027: INFO: (10) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 17.441062ms)
  Apr 23 17:55:57.035: INFO: (11) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 8.222924ms)
  Apr 23 17:55:57.037: INFO: (11) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 9.363821ms)
  Apr 23 17:55:57.037: INFO: (11) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 9.909241ms)
  Apr 23 17:55:57.040: INFO: (11) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 12.752665ms)
  Apr 23 17:55:57.042: INFO: (11) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 14.473702ms)
  Apr 23 17:55:57.043: INFO: (11) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 15.225403ms)
  Apr 23 17:55:57.043: INFO: (11) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 15.392497ms)
  Apr 23 17:55:57.044: INFO: (11) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 16.06537ms)
  Apr 23 17:55:57.044: INFO: (11) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 15.920173ms)
  Apr 23 17:55:57.045: INFO: (11) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 16.981282ms)
  Apr 23 17:55:57.045: INFO: (11) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 17.367562ms)
  Apr 23 17:55:57.045: INFO: (11) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 17.686663ms)
  Apr 23 17:55:57.046: INFO: (11) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 18.08623ms)
  Apr 23 17:55:57.047: INFO: (11) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 19.752358ms)
  Apr 23 17:55:57.049: INFO: (11) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 21.141098ms)
  Apr 23 17:55:57.049: INFO: (11) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 21.935318ms)
  Apr 23 17:55:57.065: INFO: (12) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 15.060399ms)
  Apr 23 17:55:57.065: INFO: (12) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 15.42224ms)
  Apr 23 17:55:57.065: INFO: (12) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 15.087135ms)
  Apr 23 17:55:57.065: INFO: (12) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 15.126461ms)
  Apr 23 17:55:57.065: INFO: (12) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 15.352875ms)
  Apr 23 17:55:57.067: INFO: (12) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 17.439528ms)
  Apr 23 17:55:57.067: INFO: (12) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 17.285284ms)
  Apr 23 17:55:57.067: INFO: (12) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 17.909966ms)
  Apr 23 17:55:57.068: INFO: (12) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 17.707724ms)
  Apr 23 17:55:57.068: INFO: (12) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 18.155621ms)
  Apr 23 17:55:57.068: INFO: (12) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 18.435274ms)
  Apr 23 17:55:57.068: INFO: (12) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 18.432184ms)
  Apr 23 17:55:57.068: INFO: (12) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 18.403388ms)
  Apr 23 17:55:57.069: INFO: (12) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 19.409144ms)
  Apr 23 17:55:57.069: INFO: (12) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 19.478109ms)
  Apr 23 17:55:57.069: INFO: (12) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 19.567977ms)
  Apr 23 17:55:57.079: INFO: (13) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 9.546769ms)
  Apr 23 17:55:57.083: INFO: (13) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 12.95046ms)
  Apr 23 17:55:57.085: INFO: (13) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 14.788873ms)
  Apr 23 17:55:57.085: INFO: (13) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 14.929334ms)
  Apr 23 17:55:57.085: INFO: (13) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 15.312774ms)
  Apr 23 17:55:57.085: INFO: (13) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 15.445098ms)
  Apr 23 17:55:57.086: INFO: (13) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 16.080211ms)
  Apr 23 17:55:57.086: INFO: (13) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 16.627622ms)
  Apr 23 17:55:57.089: INFO: (13) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 19.370368ms)
  Apr 23 17:55:57.089: INFO: (13) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 19.306471ms)
  Apr 23 17:55:57.090: INFO: (13) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 20.343741ms)
  Apr 23 17:55:57.091: INFO: (13) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 20.917831ms)
  Apr 23 17:55:57.091: INFO: (13) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 20.592804ms)
  Apr 23 17:55:57.091: INFO: (13) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 21.017817ms)
  Apr 23 17:55:57.091: INFO: (13) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 21.40441ms)
  Apr 23 17:55:57.091: INFO: (13) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 21.246126ms)
  Apr 23 17:55:57.098: INFO: (14) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 6.7715ms)
  Apr 23 17:55:57.099: INFO: (14) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 7.786281ms)
  Apr 23 17:55:57.100: INFO: (14) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 7.957784ms)
  Apr 23 17:55:57.100: INFO: (14) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 8.04928ms)
  Apr 23 17:55:57.101: INFO: (14) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 9.206864ms)
  Apr 23 17:55:57.101: INFO: (14) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 9.711034ms)
  Apr 23 17:55:57.103: INFO: (14) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 10.839499ms)
  Apr 23 17:55:57.103: INFO: (14) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 10.373845ms)
  Apr 23 17:55:57.103: INFO: (14) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 10.975905ms)
  Apr 23 17:55:57.103: INFO: (14) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 11.092201ms)
  Apr 23 17:55:57.104: INFO: (14) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 11.819015ms)
  Apr 23 17:55:57.104: INFO: (14) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 11.430301ms)
  Apr 23 17:55:57.104: INFO: (14) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 12.853632ms)
  Apr 23 17:55:57.106: INFO: (14) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 14.087135ms)
  Apr 23 17:55:57.106: INFO: (14) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 14.508102ms)
  Apr 23 17:55:57.106: INFO: (14) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 14.560495ms)
  Apr 23 17:55:57.117: INFO: (15) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 10.472746ms)
  Apr 23 17:55:57.117: INFO: (15) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 10.54722ms)
  Apr 23 17:55:57.117: INFO: (15) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 10.839747ms)
  Apr 23 17:55:57.118: INFO: (15) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 10.862299ms)
  Apr 23 17:55:57.119: INFO: (15) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 11.421933ms)
  Apr 23 17:55:57.120: INFO: (15) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 12.785952ms)
  Apr 23 17:55:57.121: INFO: (15) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 14.292092ms)
  Apr 23 17:55:57.122: INFO: (15) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 14.703331ms)
  Apr 23 17:55:57.124: INFO: (15) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 16.670899ms)
  Apr 23 17:55:57.124: INFO: (15) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 16.668319ms)
  Apr 23 17:55:57.124: INFO: (15) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 16.824216ms)
  Apr 23 17:55:57.125: INFO: (15) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 17.30748ms)
  Apr 23 17:55:57.125: INFO: (15) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 17.197971ms)
  Apr 23 17:55:57.125: INFO: (15) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 18.320894ms)
  Apr 23 17:55:57.125: INFO: (15) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 18.122228ms)
  Apr 23 17:55:57.126: INFO: (15) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 18.900548ms)
  Apr 23 17:55:57.132: INFO: (16) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 5.36162ms)
  Apr 23 17:55:57.133: INFO: (16) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 5.959696ms)
  Apr 23 17:55:57.133: INFO: (16) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 6.586303ms)
  Apr 23 17:55:57.134: INFO: (16) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 7.700021ms)
  Apr 23 17:55:57.135: INFO: (16) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 8.656312ms)
  Apr 23 17:55:57.136: INFO: (16) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 8.690426ms)
  Apr 23 17:55:57.137: INFO: (16) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 9.895617ms)
  Apr 23 17:55:57.139: INFO: (16) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 11.752385ms)
  Apr 23 17:55:57.139: INFO: (16) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 12.146001ms)
  Apr 23 17:55:57.139: INFO: (16) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 12.369299ms)
  Apr 23 17:55:57.139: INFO: (16) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 12.391182ms)
  Apr 23 17:55:57.140: INFO: (16) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 13.058327ms)
  Apr 23 17:55:57.140: INFO: (16) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 13.467503ms)
  Apr 23 17:55:57.141: INFO: (16) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 14.290939ms)
  Apr 23 17:55:57.141: INFO: (16) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 14.0647ms)
  Apr 23 17:55:57.141: INFO: (16) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 14.222097ms)
  Apr 23 17:55:57.149: INFO: (17) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 6.897411ms)
  Apr 23 17:55:57.150: INFO: (17) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 8.344318ms)
  Apr 23 17:55:57.151: INFO: (17) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 9.032159ms)
  Apr 23 17:55:57.151: INFO: (17) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 9.236854ms)
  Apr 23 17:55:57.152: INFO: (17) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 9.729297ms)
  Apr 23 17:55:57.152: INFO: (17) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 9.701641ms)
  Apr 23 17:55:57.152: INFO: (17) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 9.7343ms)
  Apr 23 17:55:57.154: INFO: (17) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 12.17908ms)
  Apr 23 17:55:57.154: INFO: (17) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 12.105965ms)
  Apr 23 17:55:57.156: INFO: (17) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 13.148337ms)
  Apr 23 17:55:57.157: INFO: (17) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 14.325845ms)
  Apr 23 17:55:57.157: INFO: (17) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 14.547238ms)
  Apr 23 17:55:57.158: INFO: (17) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 16.349566ms)
  Apr 23 17:55:57.159: INFO: (17) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 16.585659ms)
  Apr 23 17:55:57.159: INFO: (17) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 16.228954ms)
  Apr 23 17:55:57.159: INFO: (17) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 16.429722ms)
  Apr 23 17:55:57.166: INFO: (18) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 7.08581ms)
  Apr 23 17:55:57.167: INFO: (18) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 7.980291ms)
  Apr 23 17:55:57.168: INFO: (18) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 8.741191ms)
  Apr 23 17:55:57.168: INFO: (18) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 8.295165ms)
  Apr 23 17:55:57.170: INFO: (18) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 10.299236ms)
  Apr 23 17:55:57.173: INFO: (18) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 14.131094ms)
  Apr 23 17:55:57.173: INFO: (18) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 13.920304ms)
  Apr 23 17:55:57.174: INFO: (18) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 14.551823ms)
  Apr 23 17:55:57.174: INFO: (18) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 14.497898ms)
  Apr 23 17:55:57.176: INFO: (18) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 16.614272ms)
  Apr 23 17:55:57.176: INFO: (18) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 17.672232ms)
  Apr 23 17:55:57.177: INFO: (18) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 17.757701ms)
  Apr 23 17:55:57.177: INFO: (18) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 17.656636ms)
  Apr 23 17:55:57.177: INFO: (18) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 18.316807ms)
  Apr 23 17:55:57.178: INFO: (18) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 18.248933ms)
  Apr 23 17:55:57.178: INFO: (18) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 19.192612ms)
  Apr 23 17:55:57.186: INFO: (19) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:443/proxy/tlsrewritem... (200; 7.23644ms)
  Apr 23 17:55:57.186: INFO: (19) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:162/proxy/: bar (200; 7.768913ms)
  Apr 23 17:55:57.186: INFO: (19) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f/proxy/rewriteme">test</a> (200; 6.669905ms)
  Apr 23 17:55:57.190: INFO: (19) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:1080/proxy/rewriteme">test<... (200; 10.959694ms)
  Apr 23 17:55:57.191: INFO: (19) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:462/proxy/: tls qux (200; 11.945502ms)
  Apr 23 17:55:57.192: INFO: (19) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:162/proxy/: bar (200; 12.68099ms)
  Apr 23 17:55:57.192: INFO: (19) /api/v1/namespaces/proxy-1812/pods/proxy-service-d2jgx-h976f:160/proxy/: foo (200; 13.237727ms)
  Apr 23 17:55:57.193: INFO: (19) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:160/proxy/: foo (200; 13.497238ms)
  Apr 23 17:55:57.193: INFO: (19) /api/v1/namespaces/proxy-1812/pods/https:proxy-service-d2jgx-h976f:460/proxy/: tls baz (200; 13.749667ms)
  Apr 23 17:55:57.193: INFO: (19) /api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/: <a href="/api/v1/namespaces/proxy-1812/pods/http:proxy-service-d2jgx-h976f:1080/proxy/rewriteme">... (200; 13.568913ms)
  Apr 23 17:55:57.194: INFO: (19) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname2/proxy/: tls qux (200; 15.081235ms)
  Apr 23 17:55:57.194: INFO: (19) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname2/proxy/: bar (200; 15.161704ms)
  Apr 23 17:55:57.195: INFO: (19) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname2/proxy/: bar (200; 15.298097ms)
  Apr 23 17:55:57.195: INFO: (19) /api/v1/namespaces/proxy-1812/services/https:proxy-service-d2jgx:tlsportname1/proxy/: tls baz (200; 16.046059ms)
  Apr 23 17:55:57.195: INFO: (19) /api/v1/namespaces/proxy-1812/services/http:proxy-service-d2jgx:portname1/proxy/: foo (200; 15.623783ms)
  Apr 23 17:55:57.195: INFO: (19) /api/v1/namespaces/proxy-1812/services/proxy-service-d2jgx:portname1/proxy/: foo (200; 15.935351ms)
  Apr 23 17:55:57.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-d2jgx in namespace proxy-1812, will wait for the garbage collector to delete the pods @ 04/23/23 17:55:57.2
  Apr 23 17:55:57.268: INFO: Deleting ReplicationController proxy-service-d2jgx took: 12.355263ms
  Apr 23 17:55:57.368: INFO: Terminating ReplicationController proxy-service-d2jgx pods took: 100.423707ms
  E0423 17:55:57.703273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:58.703653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:55:59.704073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-1812" for this suite. @ 04/23/23 17:56:00.369
• [5.726 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 04/23/23 17:56:00.379
  Apr 23 17:56:00.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 17:56:00.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:56:00.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:56:00.403
  Apr 23 17:56:00.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:56:00.704645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:01.704786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:02.705226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:56:03.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6190" for this suite. @ 04/23/23 17:56:03.033
• [2.664 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/23/23 17:56:03.045
  Apr 23 17:56:03.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:56:03.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:56:03.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:56:03.088
  STEP: Creating configMap with name projected-configmap-test-volume-e368a092-508d-4013-a7c9-441e344fb8dd @ 04/23/23 17:56:03.097
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:56:03.106
  E0423 17:56:03.706000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:04.706116      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:05.706228      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:06.706597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:56:07.146
  Apr 23 17:56:07.154: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-configmaps-a25ac285-3226-476a-bca9-ff951f2253aa container agnhost-container: <nil>
  STEP: delete the pod @ 04/23/23 17:56:07.163
  Apr 23 17:56:07.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5676" for this suite. @ 04/23/23 17:56:07.194
• [4.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/23/23 17:56:07.207
  Apr 23 17:56:07.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 17:56:07.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:56:07.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:56:07.235
  Apr 23 17:56:07.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: creating the pod @ 04/23/23 17:56:07.241
  STEP: submitting the pod to kubernetes @ 04/23/23 17:56:07.241
  E0423 17:56:07.707306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:08.708099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:56:09.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9877" for this suite. @ 04/23/23 17:56:09.298
• [2.101 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/23/23 17:56:09.308
  Apr 23 17:56:09.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename security-context-test @ 04/23/23 17:56:09.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:56:09.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:56:09.337
  E0423 17:56:09.708707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:10.708796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:11.709929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:12.710236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:56:13.384: INFO: Got logs for pod "busybox-privileged-false-725cab3e-e224-4d42-8e48-a4bb205b37c6": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 23 17:56:13.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4047" for this suite. @ 04/23/23 17:56:13.389
• [4.091 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/23/23 17:56:13.399
  Apr 23 17:56:13.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:56:13.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:56:13.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:56:13.435
  STEP: Setting up server cert @ 04/23/23 17:56:13.475
  E0423 17:56:13.710292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:56:13.737
  STEP: Deploying the webhook pod @ 04/23/23 17:56:13.744
  STEP: Wait for the deployment to be ready @ 04/23/23 17:56:13.764
  Apr 23 17:56:13.776: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:56:14.710402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:15.710521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:56:15.79
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:56:15.805
  E0423 17:56:16.710605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:56:16.809: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/23/23 17:56:16.892
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 17:56:16.942
  STEP: Deleting the collection of validation webhooks @ 04/23/23 17:56:16.986
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/23/23 17:56:17.053
  Apr 23 17:56:17.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-44" for this suite. @ 04/23/23 17:56:17.133
  STEP: Destroying namespace "webhook-markers-2461" for this suite. @ 04/23/23 17:56:17.144
• [3.754 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/23/23 17:56:17.156
  Apr 23 17:56:17.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 17:56:17.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:56:17.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:56:17.234
  STEP: Counting existing ResourceQuota @ 04/23/23 17:56:17.24
  E0423 17:56:17.711564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:18.712552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:19.712723      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:20.713026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:21.713940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 17:56:22.245
  STEP: Ensuring resource quota status is calculated @ 04/23/23 17:56:22.252
  E0423 17:56:22.714866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:23.715088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:56:24.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5325" for this suite. @ 04/23/23 17:56:24.263
• [7.115 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/23/23 17:56:24.272
  Apr 23 17:56:24.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:56:24.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:56:24.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:56:24.307
  STEP: Setting up server cert @ 04/23/23 17:56:24.35
  E0423 17:56:24.715703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:56:24.852
  STEP: Deploying the webhook pod @ 04/23/23 17:56:24.861
  STEP: Wait for the deployment to be ready @ 04/23/23 17:56:24.879
  Apr 23 17:56:24.887: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:56:25.716709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:26.717606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:56:26.901
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:56:26.917
  E0423 17:56:27.718024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:56:27.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/23/23 17:56:28.009
  STEP: Creating a configMap that should be mutated @ 04/23/23 17:56:28.025
  STEP: Deleting the collection of validation webhooks @ 04/23/23 17:56:28.065
  STEP: Creating a configMap that should not be mutated @ 04/23/23 17:56:28.137
  Apr 23 17:56:28.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4067" for this suite. @ 04/23/23 17:56:28.205
  STEP: Destroying namespace "webhook-markers-4502" for this suite. @ 04/23/23 17:56:28.213
• [3.954 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/23/23 17:56:28.226
  Apr 23 17:56:28.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:56:28.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:56:28.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:56:28.256
  STEP: Creating configMap with name cm-test-opt-del-d77a6b08-1dc0-4ac2-ae67-94a8381d16f2 @ 04/23/23 17:56:28.269
  STEP: Creating configMap with name cm-test-opt-upd-5b883578-227a-4453-8308-76a0a11980b6 @ 04/23/23 17:56:28.278
  STEP: Creating the pod @ 04/23/23 17:56:28.288
  E0423 17:56:28.718062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:29.718163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-d77a6b08-1dc0-4ac2-ae67-94a8381d16f2 @ 04/23/23 17:56:30.344
  STEP: Updating configmap cm-test-opt-upd-5b883578-227a-4453-8308-76a0a11980b6 @ 04/23/23 17:56:30.354
  STEP: Creating configMap with name cm-test-opt-create-47793274-0b60-4b79-9633-f5c2ec027a18 @ 04/23/23 17:56:30.36
  STEP: waiting to observe update in volume @ 04/23/23 17:56:30.366
  E0423 17:56:30.719004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:31.719126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:32.720078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:33.720207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:34.720307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:35.720408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:36.720883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:37.721353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:38.721865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:39.721992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:40.722933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:41.723280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:42.723732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:43.723830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:44.723926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:45.724037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:46.724581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:47.725445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:48.726240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:49.726379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:50.727026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:51.728108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:52.729124      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:53.729224      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:54.730009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:55.730136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:56.733610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:57.733985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:58.734461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:56:59.734698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:00.735712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:01.735825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:02.736858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:03.737069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:04.737819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:05.737915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:06.738073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:07.738572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:08.739126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:09.739224      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:10.739760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:11.740120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:12.740589      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:13.740821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:14.741324      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:15.741445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:16.742165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:17.743122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:18.743834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:19.744699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:20.745391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:21.745615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:22.746012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:23.746243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:24.747098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:25.747204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:26.748217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:27.748480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:28.749444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:29.749553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:30.749826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:31.750061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:32.750506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:33.750579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:34.750743      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:35.750981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:36.751706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:37.752009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:38.752169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:39.752289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:40.752602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:41.752688      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:42.753641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:43.754592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:44.754674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:45.755055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:46.755110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:47.755201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:48.755329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:57:48.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-955" for this suite. @ 04/23/23 17:57:48.821
• [80.603 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/23/23 17:57:48.83
  Apr 23 17:57:48.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 17:57:48.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:57:48.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:57:48.858
  STEP: creating the pod @ 04/23/23 17:57:48.861
  STEP: submitting the pod to kubernetes @ 04/23/23 17:57:48.861
  W0423 17:57:48.872947      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0423 17:57:49.756080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:50.756271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/23/23 17:57:50.898
  STEP: updating the pod @ 04/23/23 17:57:50.902
  Apr 23 17:57:51.449: INFO: Successfully updated pod "pod-update-activedeadlineseconds-177b7445-237f-40c9-bdbd-29873baa3b9e"
  E0423 17:57:51.757199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:52.757602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:53.758538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:54.758635      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:57:55.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6825" for this suite. @ 04/23/23 17:57:55.486
• [6.667 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/23/23 17:57:55.497
  Apr 23 17:57:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 17:57:55.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:57:55.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:57:55.522
  STEP: Creating secret with name secret-test-eca06d2e-76a8-468c-8041-0d0b171fd424 @ 04/23/23 17:57:55.524
  STEP: Creating a pod to test consume secrets @ 04/23/23 17:57:55.531
  E0423 17:57:55.759607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:56.761143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:57.761222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:57:58.761747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:57:59.556
  Apr 23 17:57:59.561: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-secrets-b7c7d480-1ad5-4b00-8086-9b1dad9d4c80 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:57:59.568
  Apr 23 17:57:59.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3111" for this suite. @ 04/23/23 17:57:59.594
• [4.106 seconds]
------------------------------
S
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/23/23 17:57:59.603
  Apr 23 17:57:59.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:57:59.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:57:59.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:57:59.638
  STEP: creating a collection of services @ 04/23/23 17:57:59.652
  Apr 23 17:57:59.652: INFO: Creating e2e-svc-a-4nz95
  Apr 23 17:57:59.673: INFO: Creating e2e-svc-b-cs8f5
  Apr 23 17:57:59.697: INFO: Creating e2e-svc-c-2vkfg
  STEP: deleting service collection @ 04/23/23 17:57:59.722
  E0423 17:57:59.762286      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:57:59.771: INFO: Collection of services has been deleted
  Apr 23 17:57:59.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7185" for this suite. @ 04/23/23 17:57:59.779
• [0.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/23/23 17:57:59.79
  Apr 23 17:57:59.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 17:57:59.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:57:59.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:57:59.829
  Apr 23 17:57:59.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:58:00.762409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/23/23 17:58:01.435
  Apr 23 17:58:01.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-124 --namespace=crd-publish-openapi-124 create -f -'
  E0423 17:58:01.762873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:02.124: INFO: stderr: ""
  Apr 23 17:58:02.124: INFO: stdout: "e2e-test-crd-publish-openapi-7372-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 23 17:58:02.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-124 --namespace=crd-publish-openapi-124 delete e2e-test-crd-publish-openapi-7372-crds test-cr'
  Apr 23 17:58:02.210: INFO: stderr: ""
  Apr 23 17:58:02.210: INFO: stdout: "e2e-test-crd-publish-openapi-7372-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 23 17:58:02.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-124 --namespace=crd-publish-openapi-124 apply -f -'
  Apr 23 17:58:02.452: INFO: stderr: ""
  Apr 23 17:58:02.452: INFO: stdout: "e2e-test-crd-publish-openapi-7372-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 23 17:58:02.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-124 --namespace=crd-publish-openapi-124 delete e2e-test-crd-publish-openapi-7372-crds test-cr'
  Apr 23 17:58:02.527: INFO: stderr: ""
  Apr 23 17:58:02.527: INFO: stdout: "e2e-test-crd-publish-openapi-7372-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/23/23 17:58:02.527
  Apr 23 17:58:02.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-124 explain e2e-test-crd-publish-openapi-7372-crds'
  E0423 17:58:02.763718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:03.043: INFO: stderr: ""
  Apr 23 17:58:03.043: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-7372-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0423 17:58:03.764103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:04.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-124" for this suite. @ 04/23/23 17:58:04.387
• [4.605 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/23/23 17:58:04.396
  Apr 23 17:58:04.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 17:58:04.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:04.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:04.426
  STEP: Creating ReplicationController "e2e-rc-bnhxg" @ 04/23/23 17:58:04.428
  Apr 23 17:58:04.434: INFO: Get Replication Controller "e2e-rc-bnhxg" to confirm replicas
  E0423 17:58:04.764944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:05.440: INFO: Get Replication Controller "e2e-rc-bnhxg" to confirm replicas
  Apr 23 17:58:05.445: INFO: Found 1 replicas for "e2e-rc-bnhxg" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-bnhxg" @ 04/23/23 17:58:05.445
  STEP: Updating a scale subresource @ 04/23/23 17:58:05.449
  STEP: Verifying replicas where modified for replication controller "e2e-rc-bnhxg" @ 04/23/23 17:58:05.457
  Apr 23 17:58:05.457: INFO: Get Replication Controller "e2e-rc-bnhxg" to confirm replicas
  E0423 17:58:05.765845      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:06.462: INFO: Get Replication Controller "e2e-rc-bnhxg" to confirm replicas
  Apr 23 17:58:06.467: INFO: Found 2 replicas for "e2e-rc-bnhxg" replication controller
  Apr 23 17:58:06.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3131" for this suite. @ 04/23/23 17:58:06.473
• [2.092 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/23/23 17:58:06.488
  Apr 23 17:58:06.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:58:06.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:06.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:06.514
  STEP: creating an Endpoint @ 04/23/23 17:58:06.523
  STEP: waiting for available Endpoint @ 04/23/23 17:58:06.528
  STEP: listing all Endpoints @ 04/23/23 17:58:06.53
  STEP: updating the Endpoint @ 04/23/23 17:58:06.544
  STEP: fetching the Endpoint @ 04/23/23 17:58:06.552
  STEP: patching the Endpoint @ 04/23/23 17:58:06.555
  STEP: fetching the Endpoint @ 04/23/23 17:58:06.566
  STEP: deleting the Endpoint by Collection @ 04/23/23 17:58:06.572
  STEP: waiting for Endpoint deletion @ 04/23/23 17:58:06.589
  STEP: fetching the Endpoint @ 04/23/23 17:58:06.59
  Apr 23 17:58:06.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4638" for this suite. @ 04/23/23 17:58:06.601
• [0.128 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/23/23 17:58:06.617
  Apr 23 17:58:06.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 17:58:06.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:06.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:06.652
  STEP: creating a Pod with a static label @ 04/23/23 17:58:06.667
  STEP: watching for Pod to be ready @ 04/23/23 17:58:06.681
  Apr 23 17:58:06.686: INFO: observed Pod pod-test in namespace pods-9361 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 23 17:58:06.687: INFO: observed Pod pod-test in namespace pods-9361 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:06 +0000 UTC  }]
  Apr 23 17:58:06.708: INFO: observed Pod pod-test in namespace pods-9361 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:06 +0000 UTC  }]
  E0423 17:58:06.766716      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:07.667: INFO: Found Pod pod-test in namespace pods-9361 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-23 17:58:06 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/23/23 17:58:07.671
  STEP: getting the Pod and ensuring that it's patched @ 04/23/23 17:58:07.682
  STEP: replacing the Pod's status Ready condition to False @ 04/23/23 17:58:07.686
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/23/23 17:58:07.703
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/23/23 17:58:07.703
  STEP: watching for the Pod to be deleted @ 04/23/23 17:58:07.726
  Apr 23 17:58:07.728: INFO: observed event type MODIFIED
  E0423 17:58:07.767629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:08.768061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:09.681: INFO: observed event type MODIFIED
  E0423 17:58:09.769034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:10.179: INFO: observed event type MODIFIED
  Apr 23 17:58:10.685: INFO: observed event type MODIFIED
  Apr 23 17:58:10.705: INFO: observed event type MODIFIED
  Apr 23 17:58:10.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9361" for this suite. @ 04/23/23 17:58:10.724
• [4.114 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/23/23 17:58:10.732
  Apr 23 17:58:10.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 17:58:10.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:10.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:10.757
  STEP: creating service in namespace services-8280 @ 04/23/23 17:58:10.761
  STEP: creating service affinity-nodeport in namespace services-8280 @ 04/23/23 17:58:10.761
  E0423 17:58:10.770025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating replication controller affinity-nodeport in namespace services-8280 @ 04/23/23 17:58:10.777
  I0423 17:58:10.788673      21 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-8280, replica count: 3
  E0423 17:58:11.770726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:12.771030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:13.771449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 17:58:13.839821      21 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 17:58:13.852: INFO: Creating new exec pod
  E0423 17:58:14.771632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:15.771671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:16.772076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:16.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-8280 exec execpod-affinityg7xpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Apr 23 17:58:17.027: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 23 17:58:17.027: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:58:17.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-8280 exec execpod-affinityg7xpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.236 80'
  Apr 23 17:58:17.185: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.236 80\nConnection to 10.152.183.236 80 port [tcp/http] succeeded!\n"
  Apr 23 17:58:17.185: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:58:17.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-8280 exec execpod-affinityg7xpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.37.26 31230'
  Apr 23 17:58:17.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.37.26 31230\nConnection to 172.31.37.26 31230 port [tcp/*] succeeded!\n"
  Apr 23 17:58:17.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:58:17.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-8280 exec execpod-affinityg7xpp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.70.241 31230'
  Apr 23 17:58:17.482: INFO: stderr: "+ echo hostName+ nc -v -t -w 2 172.31.70.241 31230\nConnection to 172.31.70.241 31230 port [tcp/*] succeeded!\n\n"
  Apr 23 17:58:17.482: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 17:58:17.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-8280 exec execpod-affinityg7xpp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.37.26:31230/ ; done'
  Apr 23 17:58:17.755: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:31230/\n"
  Apr 23 17:58:17.755: INFO: stdout: "\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b\naffinity-nodeport-7sq2b"
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Received response from host: affinity-nodeport-7sq2b
  Apr 23 17:58:17.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 17:58:17.761: INFO: Cleaning up the exec pod
  E0423 17:58:17.772986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController affinity-nodeport in namespace services-8280, will wait for the garbage collector to delete the pods @ 04/23/23 17:58:17.779
  Apr 23 17:58:17.841: INFO: Deleting ReplicationController affinity-nodeport took: 7.693476ms
  Apr 23 17:58:17.942: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.556328ms
  E0423 17:58:18.773922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:19.774551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8280" for this suite. @ 04/23/23 17:58:19.982
• [9.262 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/23/23 17:58:19.994
  Apr 23 17:58:19.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename ingress @ 04/23/23 17:58:19.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:20.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:20.031
  STEP: getting /apis @ 04/23/23 17:58:20.035
  STEP: getting /apis/networking.k8s.io @ 04/23/23 17:58:20.039
  STEP: getting /apis/networking.k8s.iov1 @ 04/23/23 17:58:20.041
  STEP: creating @ 04/23/23 17:58:20.042
  STEP: getting @ 04/23/23 17:58:20.08
  STEP: listing @ 04/23/23 17:58:20.087
  STEP: watching @ 04/23/23 17:58:20.092
  Apr 23 17:58:20.092: INFO: starting watch
  STEP: cluster-wide listing @ 04/23/23 17:58:20.096
  STEP: cluster-wide watching @ 04/23/23 17:58:20.1
  Apr 23 17:58:20.100: INFO: starting watch
  STEP: patching @ 04/23/23 17:58:20.102
  STEP: updating @ 04/23/23 17:58:20.111
  Apr 23 17:58:20.130: INFO: waiting for watch events with expected annotations
  Apr 23 17:58:20.130: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/23/23 17:58:20.13
  STEP: updating /status @ 04/23/23 17:58:20.142
  STEP: get /status @ 04/23/23 17:58:20.219
  STEP: deleting @ 04/23/23 17:58:20.226
  STEP: deleting a collection @ 04/23/23 17:58:20.252
  Apr 23 17:58:20.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-4570" for this suite. @ 04/23/23 17:58:20.28
• [0.299 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/23/23 17:58:20.296
  Apr 23 17:58:20.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 17:58:20.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:20.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:20.344
  STEP: creating a secret @ 04/23/23 17:58:20.348
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/23/23 17:58:20.355
  STEP: patching the secret @ 04/23/23 17:58:20.359
  STEP: deleting the secret using a LabelSelector @ 04/23/23 17:58:20.376
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/23/23 17:58:20.387
  Apr 23 17:58:20.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9082" for this suite. @ 04/23/23 17:58:20.398
• [0.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 04/23/23 17:58:20.41
  Apr 23 17:58:20.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 17:58:20.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:20.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:20.442
  Apr 23 17:58:20.471: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/23/23 17:58:20.477
  Apr 23 17:58:20.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:58:20.485: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/23/23 17:58:20.485
  Apr 23 17:58:20.526: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:58:20.526: INFO: Node ip-172-31-86-26 is running 0 daemon pod, expected 1
  E0423 17:58:20.775657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:21.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:58:21.531: INFO: Node ip-172-31-86-26 is running 0 daemon pod, expected 1
  E0423 17:58:21.776413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:22.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 17:58:22.532: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/23/23 17:58:22.537
  Apr 23 17:58:22.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 17:58:22.563: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0423 17:58:22.777043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:23.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:58:23.569: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/23/23 17:58:23.569
  Apr 23 17:58:23.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:58:23.593: INFO: Node ip-172-31-86-26 is running 0 daemon pod, expected 1
  E0423 17:58:23.777966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:24.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:58:24.600: INFO: Node ip-172-31-86-26 is running 0 daemon pod, expected 1
  E0423 17:58:24.778594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:25.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 23 17:58:25.603: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 17:58:25.613
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3503, will wait for the garbage collector to delete the pods @ 04/23/23 17:58:25.613
  Apr 23 17:58:25.683: INFO: Deleting DaemonSet.extensions daemon-set took: 14.912629ms
  E0423 17:58:25.778760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:25.784: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.29672ms
  E0423 17:58:26.779330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:26.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 17:58:26.989: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 17:58:26.994: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34248"},"items":null}

  Apr 23 17:58:27.002: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34248"},"items":null}

  Apr 23 17:58:27.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3503" for this suite. @ 04/23/23 17:58:27.047
• [6.647 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/23/23 17:58:27.058
  Apr 23 17:58:27.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename dns @ 04/23/23 17:58:27.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:27.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:27.088
  STEP: Creating a test headless service @ 04/23/23 17:58:27.091
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7416.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7416.svc.cluster.local;sleep 1; done
   @ 04/23/23 17:58:27.097
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7416.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7416.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7416.svc.cluster.local;sleep 1; done
   @ 04/23/23 17:58:27.097
  STEP: creating a pod to probe DNS @ 04/23/23 17:58:27.097
  STEP: submitting the pod to kubernetes @ 04/23/23 17:58:27.097
  E0423 17:58:27.779981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:28.780083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 17:58:29.131
  STEP: looking for the results for each expected name from probers @ 04/23/23 17:58:29.135
  Apr 23 17:58:29.146: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local from pod dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5: the server could not find the requested resource (get pods dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5)
  Apr 23 17:58:29.152: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local from pod dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5: the server could not find the requested resource (get pods dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5)
  Apr 23 17:58:29.157: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7416.svc.cluster.local from pod dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5: the server could not find the requested resource (get pods dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5)
  Apr 23 17:58:29.163: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7416.svc.cluster.local from pod dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5: the server could not find the requested resource (get pods dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5)
  Apr 23 17:58:29.169: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local from pod dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5: the server could not find the requested resource (get pods dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5)
  Apr 23 17:58:29.175: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local from pod dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5: the server could not find the requested resource (get pods dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5)
  Apr 23 17:58:29.180: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7416.svc.cluster.local from pod dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5: the server could not find the requested resource (get pods dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5)
  Apr 23 17:58:29.185: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7416.svc.cluster.local from pod dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5: the server could not find the requested resource (get pods dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5)
  Apr 23 17:58:29.185: INFO: Lookups using dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7416.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7416.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7416.svc.cluster.local jessie_udp@dns-test-service-2.dns-7416.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7416.svc.cluster.local]

  E0423 17:58:29.780464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:30.781463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:31.781586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:32.781681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:33.781803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:34.225: INFO: DNS probes using dns-7416/dns-test-017477e7-d45c-4586-bc0e-a8805652fcf5 succeeded

  Apr 23 17:58:34.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 17:58:34.23
  STEP: deleting the test headless service @ 04/23/23 17:58:34.252
  STEP: Destroying namespace "dns-7416" for this suite. @ 04/23/23 17:58:34.271
• [7.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/23/23 17:58:34.282
  Apr 23 17:58:34.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename podtemplate @ 04/23/23 17:58:34.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:34.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:34.353
  STEP: Create set of pod templates @ 04/23/23 17:58:34.356
  Apr 23 17:58:34.362: INFO: created test-podtemplate-1
  Apr 23 17:58:34.368: INFO: created test-podtemplate-2
  Apr 23 17:58:34.375: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/23/23 17:58:34.375
  STEP: delete collection of pod templates @ 04/23/23 17:58:34.379
  Apr 23 17:58:34.379: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/23/23 17:58:34.401
  Apr 23 17:58:34.401: INFO: requesting list of pod templates to confirm quantity
  Apr 23 17:58:34.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3975" for this suite. @ 04/23/23 17:58:34.409
• [0.136 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/23/23 17:58:34.418
  Apr 23 17:58:34.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 17:58:34.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:34.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:34.441
  Apr 23 17:58:34.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:58:34.782363      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:35.782841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:36.783346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:37.783790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:38.784283      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:39.785226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:40.785898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:40.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4833" for this suite. @ 04/23/23 17:58:40.849
• [6.440 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/23/23 17:58:40.86
  Apr 23 17:58:40.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 17:58:40.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:40.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:40.898
  STEP: Creating secret with name secret-test-08952382-8584-416d-a747-0c121a43b591 @ 04/23/23 17:58:40.9
  STEP: Creating a pod to test consume secrets @ 04/23/23 17:58:40.905
  E0423 17:58:41.786905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:42.787011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:43.787132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:44.787240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:58:44.935
  Apr 23 17:58:44.939: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-secrets-ca226fad-843f-427c-a14b-b4ca02c1f37f container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:58:44.955
  Apr 23 17:58:44.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-380" for this suite. @ 04/23/23 17:58:44.979
• [4.126 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/23/23 17:58:44.987
  Apr 23 17:58:44.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 17:58:44.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:45.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:45.015
  STEP: Setting up server cert @ 04/23/23 17:58:45.044
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 17:58:45.644
  STEP: Deploying the webhook pod @ 04/23/23 17:58:45.653
  STEP: Wait for the deployment to be ready @ 04/23/23 17:58:45.669
  Apr 23 17:58:45.679: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 17:58:45.787864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:46.787969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 17:58:47.693
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 17:58:47.709
  E0423 17:58:47.787999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:48.710: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/23/23 17:58:48.717
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/23/23 17:58:48.736
  STEP: Creating a dummy validating-webhook-configuration object @ 04/23/23 17:58:48.754
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/23/23 17:58:48.78
  E0423 17:58:48.788911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/23/23 17:58:48.79
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/23/23 17:58:48.803
  Apr 23 17:58:48.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5313" for this suite. @ 04/23/23 17:58:48.902
  STEP: Destroying namespace "webhook-markers-2754" for this suite. @ 04/23/23 17:58:48.909
• [3.929 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/23/23 17:58:48.919
  Apr 23 17:58:48.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename limitrange @ 04/23/23 17:58:48.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:48.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:48.945
  STEP: Creating LimitRange "e2e-limitrange-rjmlp" in namespace "limitrange-9516" @ 04/23/23 17:58:48.95
  STEP: Creating another limitRange in another namespace @ 04/23/23 17:58:48.957
  Apr 23 17:58:48.981: INFO: Namespace "e2e-limitrange-rjmlp-4365" created
  Apr 23 17:58:48.981: INFO: Creating LimitRange "e2e-limitrange-rjmlp" in namespace "e2e-limitrange-rjmlp-4365"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-rjmlp" @ 04/23/23 17:58:48.992
  Apr 23 17:58:48.998: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-rjmlp" in "limitrange-9516" namespace @ 04/23/23 17:58:48.998
  Apr 23 17:58:49.008: INFO: LimitRange "e2e-limitrange-rjmlp" has been patched
  STEP: Delete LimitRange "e2e-limitrange-rjmlp" by Collection with labelSelector: "e2e-limitrange-rjmlp=patched" @ 04/23/23 17:58:49.009
  STEP: Confirm that the limitRange "e2e-limitrange-rjmlp" has been deleted @ 04/23/23 17:58:49.021
  Apr 23 17:58:49.021: INFO: Requesting list of LimitRange to confirm quantity
  Apr 23 17:58:49.024: INFO: Found 0 LimitRange with label "e2e-limitrange-rjmlp=patched"
  Apr 23 17:58:49.025: INFO: LimitRange "e2e-limitrange-rjmlp" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-rjmlp" @ 04/23/23 17:58:49.025
  Apr 23 17:58:49.030: INFO: Found 1 limitRange
  Apr 23 17:58:49.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9516" for this suite. @ 04/23/23 17:58:49.035
  STEP: Destroying namespace "e2e-limitrange-rjmlp-4365" for this suite. @ 04/23/23 17:58:49.043
• [0.131 seconds]
------------------------------
SS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/23/23 17:58:49.05
  Apr 23 17:58:49.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/23/23 17:58:49.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:49.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:49.079
  STEP: getting /apis @ 04/23/23 17:58:49.082
  STEP: getting /apis/storage.k8s.io @ 04/23/23 17:58:49.085
  STEP: getting /apis/storage.k8s.io/v1 @ 04/23/23 17:58:49.086
  STEP: creating @ 04/23/23 17:58:49.088
  STEP: watching @ 04/23/23 17:58:49.106
  Apr 23 17:58:49.106: INFO: starting watch
  STEP: getting @ 04/23/23 17:58:49.113
  STEP: listing in namespace @ 04/23/23 17:58:49.116
  STEP: listing across namespaces @ 04/23/23 17:58:49.119
  STEP: patching @ 04/23/23 17:58:49.123
  STEP: updating @ 04/23/23 17:58:49.132
  Apr 23 17:58:49.138: INFO: waiting for watch events with expected annotations in namespace
  Apr 23 17:58:49.138: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/23/23 17:58:49.138
  STEP: deleting a collection @ 04/23/23 17:58:49.152
  Apr 23 17:58:49.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-7365" for this suite. @ 04/23/23 17:58:49.177
• [0.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/23/23 17:58:49.188
  Apr 23 17:58:49.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 17:58:49.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:49.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:49.21
  STEP: Creating namespace "e2e-ns-6qslw" @ 04/23/23 17:58:49.214
  Apr 23 17:58:49.238: INFO: Namespace "e2e-ns-6qslw-3009" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-6qslw-3009" @ 04/23/23 17:58:49.238
  Apr 23 17:58:49.249: INFO: Namespace "e2e-ns-6qslw-3009" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-6qslw-3009" @ 04/23/23 17:58:49.249
  Apr 23 17:58:49.258: INFO: Namespace "e2e-ns-6qslw-3009" has []v1.FinalizerName{"kubernetes"}
  Apr 23 17:58:49.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5513" for this suite. @ 04/23/23 17:58:49.27
  STEP: Destroying namespace "e2e-ns-6qslw-3009" for this suite. @ 04/23/23 17:58:49.278
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/23/23 17:58:49.285
  Apr 23 17:58:49.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 17:58:49.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:49.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:49.316
  E0423 17:58:49.789236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:50.790151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 17:58:51.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2690" for this suite. @ 04/23/23 17:58:51.356
• [2.079 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/23/23 17:58:51.364
  Apr 23 17:58:51.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:58:51.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:51.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:51.389
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/23/23 17:58:51.392
  E0423 17:58:51.790825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:52.791080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:53.791210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:54.791321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:58:55.418
  Apr 23 17:58:55.422: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-cf5f4571-d367-4956-bed7-fdeb06085182 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:58:55.429
  Apr 23 17:58:55.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8075" for this suite. @ 04/23/23 17:58:55.449
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/23/23 17:58:55.466
  Apr 23 17:58:55.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-pred @ 04/23/23 17:58:55.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:55.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:55.487
  Apr 23 17:58:55.490: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 23 17:58:55.500: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 17:58:55.504: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-37-26 before test
  Apr 23 17:58:55.510: INFO: default-http-backend-kubernetes-worker-65fc475d49-65dxk from ingress-nginx-kubernetes-worker started at 2023-04-23 16:24:39 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.511: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: nginx-ingress-controller-kubernetes-worker-gbggh from ingress-nginx-kubernetes-worker started at 2023-04-23 16:24:39 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.511: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: coredns-5c7f76ccb8-sgclp from kube-system started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.511: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: kube-state-metrics-5b95b4459c-4xnvw from kube-system started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.511: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: metrics-server-v0.5.2-6cf8c8b69c-zzwzt from kube-system started at 2023-04-23 16:24:07 +0000 UTC (2 container statuses recorded)
  Apr 23 17:58:55.511: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: dashboard-metrics-scraper-6b8586b5c9-646rd from kubernetes-dashboard started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.511: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: kubernetes-dashboard-6869f4cd5f-64m7l from kubernetes-dashboard started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.511: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-28gbw from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:58:55.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 17:58:55.511: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-70-241 before test
  Apr 23 17:58:55.517: INFO: nginx-ingress-controller-kubernetes-worker-4cgst from ingress-nginx-kubernetes-worker started at 2023-04-23 17:24:09 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.517: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:58:55.517: INFO: test-runtimeclass-runtimeclass-2690-preconfigured-handler-mn8m4 from runtimeclass-2690 started at 2023-04-23 17:58:49 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.517: INFO: 	Container test ready: false, restart count 0
  Apr 23 17:58:55.517: INFO: sonobuoy from sonobuoy started at 2023-04-23 16:37:12 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.517: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 23 17:58:55.517: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-clx9s from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:58:55.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:58:55.517: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 17:58:55.517: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-86-26 before test
  Apr 23 17:58:55.524: INFO: nginx-ingress-controller-kubernetes-worker-httzn from ingress-nginx-kubernetes-worker started at 2023-04-23 16:26:06 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.524: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 17:58:55.524: INFO: calico-kube-controllers-6c8cb79d47-nmw6q from kube-system started at 2023-04-23 16:31:05 +0000 UTC (1 container statuses recorded)
  Apr 23 17:58:55.524: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Apr 23 17:58:55.524: INFO: sonobuoy-e2e-job-b60cd7a9aecc4153 from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:58:55.524: INFO: 	Container e2e ready: true, restart count 0
  Apr 23 17:58:55.524: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:58:55.524: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-mdknn from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 17:58:55.524: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 17:58:55.524: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-37-26 @ 04/23/23 17:58:55.539
  STEP: verifying the node has the label node ip-172-31-70-241 @ 04/23/23 17:58:55.554
  STEP: verifying the node has the label node ip-172-31-86-26 @ 04/23/23 17:58:55.577
  Apr 23 17:58:55.590: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-65dxk requesting resource cpu=10m on Node ip-172-31-37-26
  Apr 23 17:58:55.590: INFO: Pod nginx-ingress-controller-kubernetes-worker-4cgst requesting resource cpu=0m on Node ip-172-31-70-241
  Apr 23 17:58:55.590: INFO: Pod nginx-ingress-controller-kubernetes-worker-gbggh requesting resource cpu=0m on Node ip-172-31-37-26
  Apr 23 17:58:55.590: INFO: Pod nginx-ingress-controller-kubernetes-worker-httzn requesting resource cpu=0m on Node ip-172-31-86-26
  Apr 23 17:58:55.591: INFO: Pod calico-kube-controllers-6c8cb79d47-nmw6q requesting resource cpu=0m on Node ip-172-31-86-26
  Apr 23 17:58:55.591: INFO: Pod coredns-5c7f76ccb8-sgclp requesting resource cpu=100m on Node ip-172-31-37-26
  Apr 23 17:58:55.591: INFO: Pod kube-state-metrics-5b95b4459c-4xnvw requesting resource cpu=0m on Node ip-172-31-37-26
  Apr 23 17:58:55.591: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-zzwzt requesting resource cpu=5m on Node ip-172-31-37-26
  Apr 23 17:58:55.591: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-646rd requesting resource cpu=0m on Node ip-172-31-37-26
  Apr 23 17:58:55.591: INFO: Pod kubernetes-dashboard-6869f4cd5f-64m7l requesting resource cpu=0m on Node ip-172-31-37-26
  Apr 23 17:58:55.591: INFO: Pod test-runtimeclass-runtimeclass-2690-preconfigured-handler-mn8m4 requesting resource cpu=0m on Node ip-172-31-70-241
  Apr 23 17:58:55.591: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-70-241
  Apr 23 17:58:55.591: INFO: Pod sonobuoy-e2e-job-b60cd7a9aecc4153 requesting resource cpu=0m on Node ip-172-31-86-26
  Apr 23 17:58:55.591: INFO: Pod sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-28gbw requesting resource cpu=0m on Node ip-172-31-37-26
  Apr 23 17:58:55.591: INFO: Pod sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-clx9s requesting resource cpu=0m on Node ip-172-31-70-241
  Apr 23 17:58:55.591: INFO: Pod sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-mdknn requesting resource cpu=0m on Node ip-172-31-86-26
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/23/23 17:58:55.592
  Apr 23 17:58:55.592: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-86-26
  Apr 23 17:58:55.603: INFO: Creating a pod which consumes cpu=1319m on Node ip-172-31-37-26
  Apr 23 17:58:55.616: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-70-241
  E0423 17:58:55.792101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:58:56.792273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/23/23 17:58:57.648
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0e9d82bb-167f-4890-b565-f62684f9d879.1758a223d6d998f6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9440/filler-pod-0e9d82bb-167f-4890-b565-f62684f9d879 to ip-172-31-70-241] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0e9d82bb-167f-4890-b565-f62684f9d879.1758a224005d9187], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0e9d82bb-167f-4890-b565-f62684f9d879.1758a224016aadb4], Reason = [Created], Message = [Created container filler-pod-0e9d82bb-167f-4890-b565-f62684f9d879] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0e9d82bb-167f-4890-b565-f62684f9d879.1758a22405bb2f66], Reason = [Started], Message = [Started container filler-pod-0e9d82bb-167f-4890-b565-f62684f9d879] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6a690206-6595-4ac7-866b-deda467f22ee.1758a223d582602f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9440/filler-pod-6a690206-6595-4ac7-866b-deda467f22ee to ip-172-31-86-26] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6a690206-6595-4ac7-866b-deda467f22ee.1758a2240ee9edfd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6a690206-6595-4ac7-866b-deda467f22ee.1758a2241007d53b], Reason = [Created], Message = [Created container filler-pod-6a690206-6595-4ac7-866b-deda467f22ee] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6a690206-6595-4ac7-866b-deda467f22ee.1758a22414938858], Reason = [Started], Message = [Started container filler-pod-6a690206-6595-4ac7-866b-deda467f22ee] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8282c8b1-4a82-42ce-a75f-24651c5c6fac.1758a223d6253b5f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9440/filler-pod-8282c8b1-4a82-42ce-a75f-24651c5c6fac to ip-172-31-37-26] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8282c8b1-4a82-42ce-a75f-24651c5c6fac.1758a2240ad25720], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8282c8b1-4a82-42ce-a75f-24651c5c6fac.1758a2240beb859f], Reason = [Created], Message = [Created container filler-pod-8282c8b1-4a82-42ce-a75f-24651c5c6fac] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8282c8b1-4a82-42ce-a75f-24651c5c6fac.1758a22412775472], Reason = [Started], Message = [Started container filler-pod-8282c8b1-4a82-42ce-a75f-24651c5c6fac] @ 04/23/23 17:58:57.653
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.1758a2244fab9272], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 04/23/23 17:58:57.673
  E0423 17:58:57.792572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-37-26 @ 04/23/23 17:58:58.672
  STEP: verifying the node doesn't have the label node @ 04/23/23 17:58:58.691
  STEP: removing the label node off the node ip-172-31-70-241 @ 04/23/23 17:58:58.695
  STEP: verifying the node doesn't have the label node @ 04/23/23 17:58:58.723
  STEP: removing the label node off the node ip-172-31-86-26 @ 04/23/23 17:58:58.729
  STEP: verifying the node doesn't have the label node @ 04/23/23 17:58:58.752
  Apr 23 17:58:58.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9440" for this suite. @ 04/23/23 17:58:58.777
• [3.325 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/23/23 17:58:58.793
  Apr 23 17:58:58.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 17:58:58.794936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename subpath @ 04/23/23 17:58:58.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:58:58.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:58:58.856
  STEP: Setting up data @ 04/23/23 17:58:58.864
  STEP: Creating pod pod-subpath-test-projected-cptw @ 04/23/23 17:58:58.881
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 17:58:58.881
  E0423 17:58:59.795138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:00.795288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:01.795366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:02.796074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:03.796479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:04.796577      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:05.796624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:06.796748      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:07.797247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:08.797720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:09.797915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:10.797935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:11.798974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:12.799019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:13.799134      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:14.799218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:15.799586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:16.799670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:17.799916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:18.799847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:19.800035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:20.800131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:21.800255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:22.800586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:59:23.01
  Apr 23 17:59:23.014: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-subpath-test-projected-cptw container test-container-subpath-projected-cptw: <nil>
  STEP: delete the pod @ 04/23/23 17:59:23.028
  STEP: Deleting pod pod-subpath-test-projected-cptw @ 04/23/23 17:59:23.05
  Apr 23 17:59:23.050: INFO: Deleting pod "pod-subpath-test-projected-cptw" in namespace "subpath-9947"
  Apr 23 17:59:23.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9947" for this suite. @ 04/23/23 17:59:23.059
• [24.276 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/23/23 17:59:23.07
  Apr 23 17:59:23.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 17:59:23.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:59:23.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:59:23.102
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/23/23 17:59:23.106
  E0423 17:59:23.801527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:24.802025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:25.803029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:26.803046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:59:27.139
  Apr 23 17:59:27.144: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-6a1558b8-7a42-4039-b492-c5569a97f5ea container test-container: <nil>
  STEP: delete the pod @ 04/23/23 17:59:27.153
  Apr 23 17:59:27.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4822" for this suite. @ 04/23/23 17:59:27.179
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/23/23 17:59:27.187
  Apr 23 17:59:27.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 17:59:27.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:59:27.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:59:27.212
  STEP: Creating configMap with name projected-configmap-test-volume-9308e9b9-e75c-41c6-89d2-7e089ede86d4 @ 04/23/23 17:59:27.215
  STEP: Creating a pod to test consume configMaps @ 04/23/23 17:59:27.221
  E0423 17:59:27.803176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:28.803248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:29.804149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:30.804463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 17:59:31.248
  Apr 23 17:59:31.252: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-configmaps-1c7fe8d9-6d5a-43e5-ae1d-b9a0f1b55103 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 17:59:31.26
  Apr 23 17:59:31.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4100" for this suite. @ 04/23/23 17:59:31.285
• [4.106 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/23/23 17:59:31.294
  Apr 23 17:59:31.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 17:59:31.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 17:59:31.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 17:59:31.368
  STEP: Creating configMap with name cm-test-opt-del-42224740-3fa8-4300-94ce-ac079df28510 @ 04/23/23 17:59:31.378
  STEP: Creating configMap with name cm-test-opt-upd-afa75e99-4693-4f7b-b1b2-c2559f06040f @ 04/23/23 17:59:31.384
  STEP: Creating the pod @ 04/23/23 17:59:31.39
  E0423 17:59:31.805144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:32.805346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-42224740-3fa8-4300-94ce-ac079df28510 @ 04/23/23 17:59:33.445
  STEP: Updating configmap cm-test-opt-upd-afa75e99-4693-4f7b-b1b2-c2559f06040f @ 04/23/23 17:59:33.453
  STEP: Creating configMap with name cm-test-opt-create-0fb04ed8-ff4a-4836-a4a5-d37f56f642cd @ 04/23/23 17:59:33.459
  STEP: waiting to observe update in volume @ 04/23/23 17:59:33.465
  E0423 17:59:33.806123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:34.806250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:35.806718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:36.806827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:37.807034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:38.807141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:39.807232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:40.807352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:41.808004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:42.808347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:43.809147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:44.809529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:45.810448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:46.810569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:47.810994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:48.811014      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:49.811923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:50.812178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:51.812715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:52.812978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:53.813825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:54.814168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:55.814960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:56.815083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:57.815415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:58.815506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 17:59:59.816085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:00.816338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:01.817112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:02.818101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:03.819040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:04.819139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:05.820084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:06.820172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:07.820588      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:08.820699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:09.821637      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:10.821703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:11.822504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:12.822624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:13.822738      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:14.823036      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:15.823810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:16.824843      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:17.825265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:18.825374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:19.826089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:20.826200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:21.827239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:22.827599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:23.827688      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:24.828125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:25.828515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:26.828924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:27.829269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:28.829395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:29.830424      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:30.830770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:31.831090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:32.831435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:33.834558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:34.834650      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:35.835195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:36.835309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:37.835979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:38.836220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:39.837254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:40.837732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:41.837584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:42.837607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:43.838105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:44.838235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:45.838362      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:46.838463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:47.839039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:00:47.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-389" for this suite. @ 04/23/23 18:00:47.904
• [76.619 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/23/23 18:00:47.922
  Apr 23 18:00:47.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 18:00:47.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:00:47.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:00:47.942
  E0423 18:00:48.840099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:49.840210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:00:49.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 18:00:49.977: INFO: Deleting pod "var-expansion-1d1d347d-de62-4cf4-a20b-55df56fe241c" in namespace "var-expansion-9449"
  Apr 23 18:00:49.985: INFO: Wait up to 5m0s for pod "var-expansion-1d1d347d-de62-4cf4-a20b-55df56fe241c" to be fully deleted
  E0423 18:00:50.840324      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:51.840572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-9449" for this suite. @ 04/23/23 18:00:51.996
• [4.082 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/23/23 18:00:52.005
  Apr 23 18:00:52.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 18:00:52.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:00:52.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:00:52.038
  STEP: Creating secret with name projected-secret-test-73904709-5dd2-400e-9047-92de4481e457 @ 04/23/23 18:00:52.041
  STEP: Creating a pod to test consume secrets @ 04/23/23 18:00:52.047
  E0423 18:00:52.840703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:53.840797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:54.840904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:55.841003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:00:56.076
  Apr 23 18:00:56.080: INFO: Trying to get logs from node ip-172-31-86-26 pod pod-projected-secrets-6231e168-a1e7-4715-90c2-ea95c14cc72d container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 18:00:56.129
  Apr 23 18:00:56.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2312" for this suite. @ 04/23/23 18:00:56.156
• [4.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/23/23 18:00:56.18
  Apr 23 18:00:56.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 18:00:56.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:00:56.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:00:56.209
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/23/23 18:00:56.213
  E0423 18:00:56.841127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:57.841515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:58.841622      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:00:59.841767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:01:00.241
  Apr 23 18:01:00.247: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-98b09f3f-774c-419f-95d8-940e4abe0285 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 18:01:00.256
  Apr 23 18:01:00.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9703" for this suite. @ 04/23/23 18:01:00.284
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/23/23 18:01:00.296
  Apr 23 18:01:00.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 18:01:00.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:01:00.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:01:00.333
  Apr 23 18:01:00.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-859" for this suite. @ 04/23/23 18:01:00.376
• [0.092 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/23/23 18:01:00.388
  Apr 23 18:01:00.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename ingressclass @ 04/23/23 18:01:00.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:01:00.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:01:00.463
  STEP: getting /apis @ 04/23/23 18:01:00.468
  STEP: getting /apis/networking.k8s.io @ 04/23/23 18:01:00.472
  STEP: getting /apis/networking.k8s.iov1 @ 04/23/23 18:01:00.473
  STEP: creating @ 04/23/23 18:01:00.475
  STEP: getting @ 04/23/23 18:01:00.491
  STEP: listing @ 04/23/23 18:01:00.495
  STEP: watching @ 04/23/23 18:01:00.499
  Apr 23 18:01:00.499: INFO: starting watch
  STEP: patching @ 04/23/23 18:01:00.5
  STEP: updating @ 04/23/23 18:01:00.505
  Apr 23 18:01:00.511: INFO: waiting for watch events with expected annotations
  Apr 23 18:01:00.511: INFO: saw patched and updated annotations
  STEP: deleting @ 04/23/23 18:01:00.511
  STEP: deleting a collection @ 04/23/23 18:01:00.526
  Apr 23 18:01:00.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-426" for this suite. @ 04/23/23 18:01:00.562
• [0.183 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/23/23 18:01:00.576
  Apr 23 18:01:00.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename configmap @ 04/23/23 18:01:00.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:01:00.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:01:00.603
  STEP: Creating configMap configmap-1169/configmap-test-02436078-9bc1-4687-a670-bf211613c9d4 @ 04/23/23 18:01:00.607
  STEP: Creating a pod to test consume configMaps @ 04/23/23 18:01:00.612
  E0423 18:01:00.842157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:01.842271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:02.842355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:03.842683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:01:04.637
  Apr 23 18:01:04.640: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-configmaps-3784b2f3-7011-472c-9e07-bb3bd314252f container env-test: <nil>
  STEP: delete the pod @ 04/23/23 18:01:04.649
  Apr 23 18:01:04.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1169" for this suite. @ 04/23/23 18:01:04.673
• [4.107 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/23/23 18:01:04.683
  Apr 23 18:01:04.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename dns @ 04/23/23 18:01:04.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:01:04.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:01:04.707
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/23/23 18:01:04.711
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/23/23 18:01:04.711
  STEP: creating a pod to probe DNS @ 04/23/23 18:01:04.711
  STEP: submitting the pod to kubernetes @ 04/23/23 18:01:04.712
  E0423 18:01:04.843165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:05.843101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 18:01:06.731
  STEP: looking for the results for each expected name from probers @ 04/23/23 18:01:06.735
  Apr 23 18:01:06.755: INFO: DNS probes using dns-283/dns-test-6384a9c9-8b78-4775-ae57-3ce8cb3dbb88 succeeded

  Apr 23 18:01:06.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 18:01:06.762
  STEP: Destroying namespace "dns-283" for this suite. @ 04/23/23 18:01:06.78
• [2.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/23/23 18:01:06.788
  Apr 23 18:01:06.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename dns @ 04/23/23 18:01:06.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:01:06.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:01:06.813
  STEP: Creating a test headless service @ 04/23/23 18:01:06.817
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1629.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1629.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1629.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1629.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1629.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1629.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1629.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1629.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1629.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1629.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 20.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.20_udp@PTR;check="$$(dig +tcp +noall +answer +search 20.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.20_tcp@PTR;sleep 1; done
   @ 04/23/23 18:01:06.837
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1629.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1629.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1629.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1629.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1629.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1629.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1629.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1629.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1629.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1629.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 20.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.20_udp@PTR;check="$$(dig +tcp +noall +answer +search 20.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.20_tcp@PTR;sleep 1; done
   @ 04/23/23 18:01:06.838
  STEP: creating a pod to probe DNS @ 04/23/23 18:01:06.838
  STEP: submitting the pod to kubernetes @ 04/23/23 18:01:06.838
  E0423 18:01:06.843671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:07.843701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:08.843796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 18:01:08.875
  STEP: looking for the results for each expected name from probers @ 04/23/23 18:01:08.88
  Apr 23 18:01:08.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-1629.svc.cluster.local from pod dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa: the server could not find the requested resource (get pods dns-test-20858524-8e46-4ce1-a08f-07809181aeaa)
  Apr 23 18:01:08.894: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1629.svc.cluster.local from pod dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa: the server could not find the requested resource (get pods dns-test-20858524-8e46-4ce1-a08f-07809181aeaa)
  Apr 23 18:01:08.899: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local from pod dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa: the server could not find the requested resource (get pods dns-test-20858524-8e46-4ce1-a08f-07809181aeaa)
  Apr 23 18:01:08.903: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local from pod dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa: the server could not find the requested resource (get pods dns-test-20858524-8e46-4ce1-a08f-07809181aeaa)
  Apr 23 18:01:08.938: INFO: Unable to read jessie_udp@dns-test-service.dns-1629.svc.cluster.local from pod dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa: the server could not find the requested resource (get pods dns-test-20858524-8e46-4ce1-a08f-07809181aeaa)
  Apr 23 18:01:08.942: INFO: Unable to read jessie_tcp@dns-test-service.dns-1629.svc.cluster.local from pod dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa: the server could not find the requested resource (get pods dns-test-20858524-8e46-4ce1-a08f-07809181aeaa)
  Apr 23 18:01:08.947: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local from pod dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa: the server could not find the requested resource (get pods dns-test-20858524-8e46-4ce1-a08f-07809181aeaa)
  Apr 23 18:01:08.955: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local from pod dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa: the server could not find the requested resource (get pods dns-test-20858524-8e46-4ce1-a08f-07809181aeaa)
  Apr 23 18:01:08.980: INFO: Lookups using dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa failed for: [wheezy_udp@dns-test-service.dns-1629.svc.cluster.local wheezy_tcp@dns-test-service.dns-1629.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local jessie_udp@dns-test-service.dns-1629.svc.cluster.local jessie_tcp@dns-test-service.dns-1629.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1629.svc.cluster.local]

  E0423 18:01:09.844663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:10.844887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:11.844988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:12.845402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:13.845518      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:01:14.060: INFO: DNS probes using dns-1629/dns-test-20858524-8e46-4ce1-a08f-07809181aeaa succeeded

  Apr 23 18:01:14.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 18:01:14.065
  STEP: deleting the test service @ 04/23/23 18:01:14.092
  STEP: deleting the test headless service @ 04/23/23 18:01:14.127
  STEP: Destroying namespace "dns-1629" for this suite. @ 04/23/23 18:01:14.144
• [7.367 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/23/23 18:01:14.156
  Apr 23 18:01:14.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 18:01:14.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:01:14.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:01:14.18
  STEP: Setting up server cert @ 04/23/23 18:01:14.208
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 18:01:14.661
  STEP: Deploying the webhook pod @ 04/23/23 18:01:14.673
  STEP: Wait for the deployment to be ready @ 04/23/23 18:01:14.69
  Apr 23 18:01:14.702: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 18:01:14.846303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:15.846459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 18:01:16.716
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 18:01:16.745
  E0423 18:01:16.846617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:01:17.745: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 23 18:01:17.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 18:01:17.847253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8247-crds.webhook.example.com via the AdmissionRegistration API @ 04/23/23 18:01:18.277
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/23/23 18:01:18.296
  E0423 18:01:18.848230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:19.848625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:01:20.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 18:01:20.849147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4560" for this suite. @ 04/23/23 18:01:20.948
  STEP: Destroying namespace "webhook-markers-6940" for this suite. @ 04/23/23 18:01:20.955
• [6.807 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/23/23 18:01:20.969
  Apr 23 18:01:20.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 18:01:20.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:01:20.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:01:20.995
  STEP: Setting up server cert @ 04/23/23 18:01:21.029
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 18:01:21.345
  STEP: Deploying the webhook pod @ 04/23/23 18:01:21.352
  STEP: Wait for the deployment to be ready @ 04/23/23 18:01:21.372
  Apr 23 18:01:21.390: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 18:01:21.849923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:22.850289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 18:01:23.406
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 18:01:23.432
  E0423 18:01:23.851154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:01:24.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/23/23 18:01:24.437
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/23/23 18:01:24.454
  Apr 23 18:01:24.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 18:01:24.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1992" for this suite. @ 04/23/23 18:01:24.539
  STEP: Destroying namespace "webhook-markers-8865" for this suite. @ 04/23/23 18:01:24.55
• [3.589 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/23/23 18:01:24.559
  Apr 23 18:01:24.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 18:01:24.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:01:24.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:01:24.59
  STEP: Creating pod liveness-4e25485f-b21b-42df-add4-a66656ee805e in namespace container-probe-4808 @ 04/23/23 18:01:24.593
  E0423 18:01:24.852094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:25.852342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:01:26.614: INFO: Started pod liveness-4e25485f-b21b-42df-add4-a66656ee805e in namespace container-probe-4808
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 18:01:26.614
  Apr 23 18:01:26.619: INFO: Initial restart count of pod liveness-4e25485f-b21b-42df-add4-a66656ee805e is 0
  E0423 18:01:26.852576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:27.852735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:28.853133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:29.853248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:30.853665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:31.853776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:32.854560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:33.854672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:34.855734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:35.855852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:36.855966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:37.856496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:38.857255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:39.857363      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:40.858481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:41.858712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:42.859108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:43.859213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:44.860080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:45.860844      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:01:46.678: INFO: Restart count of pod container-probe-4808/liveness-4e25485f-b21b-42df-add4-a66656ee805e is now 1 (20.059432824s elapsed)
  E0423 18:01:46.861886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:47.862262      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:48.862612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:49.862729      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:50.863110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:51.863195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:52.864251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:53.864325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:54.864700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:55.865013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:56.865498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:57.866042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:58.866504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:01:59.866641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:00.867400      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:01.867492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:02.868111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:03.868233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:04.869206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:05.869569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:02:06.730: INFO: Restart count of pod container-probe-4808/liveness-4e25485f-b21b-42df-add4-a66656ee805e is now 2 (40.111559327s elapsed)
  E0423 18:02:06.870082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:07.870529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:08.871165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:09.872194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:10.872592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:11.873590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:12.874395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:13.874530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:14.875394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:15.875585      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:16.876340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:17.876865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:18.877299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:19.877526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:20.878106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:21.878294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:22.879098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:23.880115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:24.880121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:25.880408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:02:26.796: INFO: Restart count of pod container-probe-4808/liveness-4e25485f-b21b-42df-add4-a66656ee805e is now 3 (1m0.177687276s elapsed)
  E0423 18:02:26.880947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:27.881080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:28.882173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:29.882278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:30.883060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:31.883243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:32.883811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:33.883916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:34.884927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:35.885121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:36.885682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:37.886268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:38.886940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:39.887025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:40.887303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:41.887411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:42.888214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:43.888402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:44.888639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:45.888852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:02:46.858: INFO: Restart count of pod container-probe-4808/liveness-4e25485f-b21b-42df-add4-a66656ee805e is now 4 (1m20.239000908s elapsed)
  E0423 18:02:46.889259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:47.889862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:48.890678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:49.890930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:50.891542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:51.891646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:52.892422      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:53.892530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:54.893015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:55.893321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:56.894207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:57.894326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:58.894358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:02:59.894470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:00.894890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:01.895607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:02.895638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:03.895779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:04.895849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:05.895971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:06.896048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:07.896169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:08.896273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:09.896393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:10.896484      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:11.896927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:12.897008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:13.897190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:14.897271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:15.897382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:16.897501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:17.897591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:18.897715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:19.897870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:20.898074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:21.898207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:22.898240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:23.898628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:24.899190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:25.899298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:26.899412      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:27.899978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:28.900091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:29.900946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:30.901042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:31.901650      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:32.901731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:33.901955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:34.902069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:35.902094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:36.902211      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:37.903054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:38.904093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:39.904211      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:40.904545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:41.904653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:42.905255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:43.905372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:44.905621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:45.905811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:46.906043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:47.906074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:48.906194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:49.906308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:50.906693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:51.907476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:52.908083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:53.908230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:54.908337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:55.909290      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:56.909415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:57.909526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:03:58.909858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:03:59.074: INFO: Restart count of pod container-probe-4808/liveness-4e25485f-b21b-42df-add4-a66656ee805e is now 5 (2m32.455154123s elapsed)
  Apr 23 18:03:59.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 18:03:59.079
  STEP: Destroying namespace "container-probe-4808" for this suite. @ 04/23/23 18:03:59.096
• [154.547 seconds]
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/23/23 18:03:59.107
  Apr 23 18:03:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename proxy @ 04/23/23 18:03:59.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:03:59.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:03:59.137
  Apr 23 18:03:59.150: INFO: Creating pod...
  E0423 18:03:59.910403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:00.910819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:01.179: INFO: Creating service...
  Apr 23 18:04:01.189: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/pods/agnhost/proxy?method=DELETE
  Apr 23 18:04:01.208: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 23 18:04:01.209: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/pods/agnhost/proxy?method=OPTIONS
  Apr 23 18:04:01.219: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 23 18:04:01.220: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/pods/agnhost/proxy?method=PATCH
  Apr 23 18:04:01.225: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 23 18:04:01.226: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/pods/agnhost/proxy?method=POST
  Apr 23 18:04:01.232: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 23 18:04:01.232: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/pods/agnhost/proxy?method=PUT
  Apr 23 18:04:01.238: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 23 18:04:01.238: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 23 18:04:01.246: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 23 18:04:01.246: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 23 18:04:01.253: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 23 18:04:01.253: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 23 18:04:01.260: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 23 18:04:01.260: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/services/e2e-proxy-test-service/proxy?method=POST
  Apr 23 18:04:01.267: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 23 18:04:01.267: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 23 18:04:01.273: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 23 18:04:01.273: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/pods/agnhost/proxy?method=GET
  Apr 23 18:04:01.277: INFO: http.Client request:GET StatusCode:301
  Apr 23 18:04:01.277: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/services/e2e-proxy-test-service/proxy?method=GET
  Apr 23 18:04:01.283: INFO: http.Client request:GET StatusCode:301
  Apr 23 18:04:01.283: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/pods/agnhost/proxy?method=HEAD
  Apr 23 18:04:01.286: INFO: http.Client request:HEAD StatusCode:301
  Apr 23 18:04:01.286: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1216/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 23 18:04:01.292: INFO: http.Client request:HEAD StatusCode:301
  Apr 23 18:04:01.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-1216" for this suite. @ 04/23/23 18:04:01.298
• [2.199 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/23/23 18:04:01.307
  Apr 23 18:04:01.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename limitrange @ 04/23/23 18:04:01.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:01.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:01.331
  STEP: Creating a LimitRange @ 04/23/23 18:04:01.335
  STEP: Setting up watch @ 04/23/23 18:04:01.335
  STEP: Submitting a LimitRange @ 04/23/23 18:04:01.44
  STEP: Verifying LimitRange creation was observed @ 04/23/23 18:04:01.446
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/23/23 18:04:01.447
  Apr 23 18:04:01.451: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 23 18:04:01.451: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/23/23 18:04:01.451
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/23/23 18:04:01.461
  Apr 23 18:04:01.467: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 23 18:04:01.467: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/23/23 18:04:01.467
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/23/23 18:04:01.483
  Apr 23 18:04:01.488: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 23 18:04:01.488: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/23/23 18:04:01.488
  STEP: Failing to create a Pod with more than max resources @ 04/23/23 18:04:01.491
  STEP: Updating a LimitRange @ 04/23/23 18:04:01.495
  STEP: Verifying LimitRange updating is effective @ 04/23/23 18:04:01.501
  E0423 18:04:01.911604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:02.911896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 04/23/23 18:04:03.508
  STEP: Failing to create a Pod with more than max resources @ 04/23/23 18:04:03.515
  STEP: Deleting a LimitRange @ 04/23/23 18:04:03.521
  STEP: Verifying the LimitRange was deleted @ 04/23/23 18:04:03.537
  E0423 18:04:03.912717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:04.913346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:05.913896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:06.913990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:07.914542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:08.542: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/23/23 18:04:08.542
  Apr 23 18:04:08.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6853" for this suite. @ 04/23/23 18:04:08.559
• [7.263 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/23/23 18:04:08.571
  Apr 23 18:04:08.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename dns @ 04/23/23 18:04:08.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:08.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:08.594
  STEP: Creating a test headless service @ 04/23/23 18:04:08.598
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5066 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5066;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5066 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5066;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5066.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5066.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5066.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5066.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5066.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5066.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5066.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5066.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5066.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5066.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5066.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5066.svc;check="$$(dig +notcp +noall +answer +search 232.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.232_tcp@PTR;sleep 1; done
   @ 04/23/23 18:04:08.616
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5066 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5066;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5066 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5066;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5066.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5066.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5066.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5066.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5066.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5066.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5066.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5066.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5066.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5066.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5066.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5066.svc;check="$$(dig +notcp +noall +answer +search 232.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.232_tcp@PTR;sleep 1; done
   @ 04/23/23 18:04:08.616
  STEP: creating a pod to probe DNS @ 04/23/23 18:04:08.616
  STEP: submitting the pod to kubernetes @ 04/23/23 18:04:08.616
  E0423 18:04:08.914608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:09.920273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/23/23 18:04:10.643
  STEP: looking for the results for each expected name from probers @ 04/23/23 18:04:10.648
  Apr 23 18:04:10.654: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.658: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.666: INFO: Unable to read wheezy_udp@dns-test-service.dns-5066 from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.671: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5066 from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.676: INFO: Unable to read wheezy_udp@dns-test-service.dns-5066.svc from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.682: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5066.svc from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.718: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.722: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.727: INFO: Unable to read jessie_udp@dns-test-service.dns-5066 from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.732: INFO: Unable to read jessie_tcp@dns-test-service.dns-5066 from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.737: INFO: Unable to read jessie_udp@dns-test-service.dns-5066.svc from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.742: INFO: Unable to read jessie_tcp@dns-test-service.dns-5066.svc from pod dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e: the server could not find the requested resource (get pods dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e)
  Apr 23 18:04:10.772: INFO: Lookups using dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5066 wheezy_tcp@dns-test-service.dns-5066 wheezy_udp@dns-test-service.dns-5066.svc wheezy_tcp@dns-test-service.dns-5066.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5066 jessie_tcp@dns-test-service.dns-5066 jessie_udp@dns-test-service.dns-5066.svc jessie_tcp@dns-test-service.dns-5066.svc]

  E0423 18:04:10.920321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:11.920492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:12.920816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:13.921640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:14.921779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:15.900: INFO: DNS probes using dns-5066/dns-test-12b42d48-819b-4cd1-97f8-6c5ccec1102e succeeded

  Apr 23 18:04:15.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 18:04:15.904
  E0423 18:04:15.922647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the test service @ 04/23/23 18:04:15.939
  STEP: deleting the test headless service @ 04/23/23 18:04:15.974
  STEP: Destroying namespace "dns-5066" for this suite. @ 04/23/23 18:04:15.988
• [7.435 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/23/23 18:04:16.012
  Apr 23 18:04:16.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename deployment @ 04/23/23 18:04:16.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:16.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:16.044
  Apr 23 18:04:16.059: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0423 18:04:16.922757      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:17.923202      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:18.923265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:19.923379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:20.923420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:21.066: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 18:04:21.066
  Apr 23 18:04:21.066: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/23/23 18:04:21.082
  Apr 23 18:04:21.096: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1345  4664768d-b925-4901-a827-e04f1d46bd62 36295 1 2023-04-23 18:04:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-23 18:04:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00404bc18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 23 18:04:21.106: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Apr 23 18:04:21.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1345" for this suite. @ 04/23/23 18:04:21.124
• [5.130 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/23/23 18:04:21.143
  Apr 23 18:04:21.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename job @ 04/23/23 18:04:21.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:21.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:21.182
  STEP: Creating a job @ 04/23/23 18:04:21.185
  STEP: Ensuring job reaches completions @ 04/23/23 18:04:21.203
  E0423 18:04:21.923726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:22.923806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:23.924142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:24.924997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:25.925153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:26.929141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:27.929674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:28.929982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:29.929872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:30.930136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:31.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8126" for this suite. @ 04/23/23 18:04:31.213
• [10.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/23/23 18:04:31.222
  Apr 23 18:04:31.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename conformance-tests @ 04/23/23 18:04:31.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:31.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:31.251
  STEP: Getting node addresses @ 04/23/23 18:04:31.258
  Apr 23 18:04:31.259: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 23 18:04:31.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-2551" for this suite. @ 04/23/23 18:04:31.27
• [0.057 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/23/23 18:04:31.28
  Apr 23 18:04:31.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 18:04:31.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:31.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:31.303
  Apr 23 18:04:31.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6734" for this suite. @ 04/23/23 18:04:31.365
• [0.094 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/23/23 18:04:31.375
  Apr 23 18:04:31.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 18:04:31.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:31.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:31.403
  E0423 18:04:31.930244      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:32.931070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:33.931181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:34.931301      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:35.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9546" for this suite. @ 04/23/23 18:04:35.448
• [4.081 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/23/23 18:04:35.456
  Apr 23 18:04:35.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename deployment @ 04/23/23 18:04:35.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:35.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:35.492
  Apr 23 18:04:35.508: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0423 18:04:35.932150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:36.932221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:37.932701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:38.932823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:39.932921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:40.514: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/23/23 18:04:40.514
  Apr 23 18:04:40.514: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0423 18:04:40.933062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:41.933193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:42.519: INFO: Creating deployment "test-rollover-deployment"
  Apr 23 18:04:42.531: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0423 18:04:42.933381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:43.934330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:44.541: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 23 18:04:44.549: INFO: Ensure that both replica sets have 1 created replica
  Apr 23 18:04:44.559: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 23 18:04:44.582: INFO: Updating deployment test-rollover-deployment
  Apr 23 18:04:44.582: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0423 18:04:44.935119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:45.935178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:46.594: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Apr 23 18:04:46.604: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 23 18:04:46.613: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 18:04:46.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 18:04:46.936100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:47.936346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:48.623: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 18:04:48.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 18:04:48.936381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:49.936591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:50.621: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 18:04:50.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 18:04:50.936697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:51.936808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:52.622: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 18:04:52.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 18:04:52.937802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:53.937922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:54.621: INFO: all replica sets need to contain the pod-template-hash label
  Apr 23 18:04:54.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 23, 18, 4, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 23, 18, 4, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0423 18:04:54.938027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:55.938125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:04:56.624: INFO: 
  Apr 23 18:04:56.624: INFO: Ensure that both old replica sets have no replicas
  Apr 23 18:04:56.636: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6250  bb4e1480-3dab-4969-9a17-6b3097f2d66d 36712 2 2023-04-23 18:04:42 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-23 18:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 18:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047f58e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-23 18:04:42 +0000 UTC,LastTransitionTime:2023-04-23 18:04:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-04-23 18:04:55 +0000 UTC,LastTransitionTime:2023-04-23 18:04:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 23 18:04:56.641: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-6250  750437c5-4137-4d79-b7e1-9c7cd947ae67 36702 2 2023-04-23 18:04:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bb4e1480-3dab-4969-9a17-6b3097f2d66d 0xc0047f5d87 0xc0047f5d88}] [] [{kube-controller-manager Update apps/v1 2023-04-23 18:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb4e1480-3dab-4969-9a17-6b3097f2d66d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 18:04:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047f5e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 18:04:56.641: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 23 18:04:56.642: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6250  34b0515d-636d-4121-9b0b-1dd602143fdb 36711 2 2023-04-23 18:04:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bb4e1480-3dab-4969-9a17-6b3097f2d66d 0xc0047f5c57 0xc0047f5c58}] [] [{e2e.test Update apps/v1 2023-04-23 18:04:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 18:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb4e1480-3dab-4969-9a17-6b3097f2d66d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-23 18:04:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0047f5d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 18:04:56.642: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-6250  e0283e5f-fa92-4323-97f6-a301363e43fb 36661 2 2023-04-23 18:04:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bb4e1480-3dab-4969-9a17-6b3097f2d66d 0xc0047f5ea7 0xc0047f5ea8}] [] [{kube-controller-manager Update apps/v1 2023-04-23 18:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb4e1480-3dab-4969-9a17-6b3097f2d66d\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 18:04:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047f5f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 18:04:56.647: INFO: Pod "test-rollover-deployment-57777854c9-9tqw5" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-9tqw5 test-rollover-deployment-57777854c9- deployment-6250  69396faa-a35b-41b2-9eae-5abf8534c15d 36679 0 2023-04-23 18:04:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 750437c5-4137-4d79-b7e1-9c7cd947ae67 0xc003f424b7 0xc003f424b8}] [] [{kube-controller-manager Update v1 2023-04-23 18:04:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"750437c5-4137-4d79-b7e1-9c7cd947ae67\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:04:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d29vz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d29vz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:04:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:04:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:04:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:04:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.238,StartTime:2023-04-23 18:04:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:04:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f42c5c31986028b35ffa314494799fbc4d3c8c71dd9ccbf988be8cb4b71e845b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.238,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:04:56.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6250" for this suite. @ 04/23/23 18:04:56.654
• [21.205 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/23/23 18:04:56.663
  Apr 23 18:04:56.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 18:04:56.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:04:56.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:04:56.697
  STEP: Creating projection with secret that has name projected-secret-test-65ff1f1a-430b-4824-8ef8-6bb6bb9e91ac @ 04/23/23 18:04:56.701
  STEP: Creating a pod to test consume secrets @ 04/23/23 18:04:56.712
  E0423 18:04:56.938300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:57.938732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:58.939755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:04:59.940421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:05:00.753
  Apr 23 18:05:00.760: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-projected-secrets-9e442d3e-8dfa-4082-8f7d-8fb394a7e46d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 18:05:00.792
  Apr 23 18:05:00.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8989" for this suite. @ 04/23/23 18:05:00.819
• [4.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/23/23 18:05:00.829
  Apr 23 18:05:00.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 18:05:00.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:05:00.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:05:00.852
  STEP: creating a replication controller @ 04/23/23 18:05:00.861
  Apr 23 18:05:00.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 create -f -'
  E0423 18:05:00.940595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:05:01.220: INFO: stderr: ""
  Apr 23 18:05:01.220: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/23/23 18:05:01.22
  Apr 23 18:05:01.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 18:05:01.299: INFO: stderr: ""
  Apr 23 18:05:01.299: INFO: stdout: "update-demo-nautilus-bw2d5 update-demo-nautilus-jz5w7 "
  Apr 23 18:05:01.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get pods update-demo-nautilus-bw2d5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 18:05:01.367: INFO: stderr: ""
  Apr 23 18:05:01.367: INFO: stdout: ""
  Apr 23 18:05:01.367: INFO: update-demo-nautilus-bw2d5 is created but not running
  E0423 18:05:01.941193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:02.941561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:03.941751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:04.941954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:05.942060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:05:06.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 23 18:05:06.471: INFO: stderr: ""
  Apr 23 18:05:06.471: INFO: stdout: "update-demo-nautilus-bw2d5 update-demo-nautilus-jz5w7 "
  Apr 23 18:05:06.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get pods update-demo-nautilus-bw2d5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 18:05:06.577: INFO: stderr: ""
  Apr 23 18:05:06.577: INFO: stdout: "true"
  Apr 23 18:05:06.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get pods update-demo-nautilus-bw2d5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 18:05:06.672: INFO: stderr: ""
  Apr 23 18:05:06.672: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 18:05:06.672: INFO: validating pod update-demo-nautilus-bw2d5
  Apr 23 18:05:06.679: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 18:05:06.680: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 18:05:06.680: INFO: update-demo-nautilus-bw2d5 is verified up and running
  Apr 23 18:05:06.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get pods update-demo-nautilus-jz5w7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 23 18:05:06.765: INFO: stderr: ""
  Apr 23 18:05:06.765: INFO: stdout: "true"
  Apr 23 18:05:06.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get pods update-demo-nautilus-jz5w7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 23 18:05:06.846: INFO: stderr: ""
  Apr 23 18:05:06.846: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 23 18:05:06.846: INFO: validating pod update-demo-nautilus-jz5w7
  Apr 23 18:05:06.853: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 23 18:05:06.853: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 23 18:05:06.853: INFO: update-demo-nautilus-jz5w7 is verified up and running
  STEP: using delete to clean up resources @ 04/23/23 18:05:06.854
  Apr 23 18:05:06.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 delete --grace-period=0 --force -f -'
  E0423 18:05:06.943100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:05:06.961: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 23 18:05:06.961: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 23 18:05:06.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get rc,svc -l name=update-demo --no-headers'
  Apr 23 18:05:07.101: INFO: stderr: "No resources found in kubectl-4901 namespace.\n"
  Apr 23 18:05:07.101: INFO: stdout: ""
  Apr 23 18:05:07.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-4901 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 23 18:05:07.207: INFO: stderr: ""
  Apr 23 18:05:07.207: INFO: stdout: ""
  Apr 23 18:05:07.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4901" for this suite. @ 04/23/23 18:05:07.218
• [6.398 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/23/23 18:05:07.228
  Apr 23 18:05:07.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename taint-single-pod @ 04/23/23 18:05:07.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:05:07.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:05:07.251
  Apr 23 18:05:07.256: INFO: Waiting up to 1m0s for all nodes to be ready
  E0423 18:05:07.944164      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:08.944275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:09.944789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:10.944869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:11.945220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:12.945572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:13.945691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:14.945800      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:15.945895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:16.946462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:17.946662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:18.947334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:19.947444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:20.948370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:21.948407      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:22.948440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:23.949399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:24.949584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:25.949874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:26.949982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:27.950087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:28.950199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:29.950315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:30.950338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:31.950467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:32.950834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:33.951696      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:34.951798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:35.952096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:36.952223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:37.952733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:38.952963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:39.953456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:40.953495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:41.954469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:42.954810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:43.955046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:44.956095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:45.957062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:46.957420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:47.958296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:48.958524      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:49.958642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:50.959002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:51.960087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:52.960428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:53.960537      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:54.960625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:55.961448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:56.961736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:57.962445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:58.962754      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:05:59.962986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:00.963703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:01.964410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:02.964511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:03.964630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:04.964753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:05.965746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:06.965828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:06:07.277: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 18:06:07.282: INFO: Starting informer...
  STEP: Starting pod... @ 04/23/23 18:06:07.282
  Apr 23 18:06:07.508: INFO: Pod is running on ip-172-31-70-241. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/23/23 18:06:07.508
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/23/23 18:06:07.52
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/23/23 18:06:07.532
  Apr 23 18:06:07.533: INFO: Pod wasn't evicted. Proceeding
  Apr 23 18:06:07.533: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/23/23 18:06:07.552
  STEP: Waiting some time to make sure that toleration time passed. @ 04/23/23 18:06:07.565
  E0423 18:06:07.966489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:08.966643      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:09.967033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:10.967142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:11.968157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:12.968759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:13.968850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:14.969289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:15.969913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:16.970042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:17.970285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:18.970396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:19.970646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:20.970753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:21.970838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:22.971044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:23.972125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:24.972350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:25.972638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:26.973032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:27.973809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:28.974128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:29.974359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:30.975371      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:31.975455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:32.976123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:33.976218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:34.976410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:35.976496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:36.976603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:37.976716      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:38.976927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:39.977530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:40.977970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:41.978113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:42.978599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:43.978705      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:44.978969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:45.979037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:46.979141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:47.979561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:48.979642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:49.980109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:50.980300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:51.980968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:52.981166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:53.982358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:54.982432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:55.983414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:56.983524      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:57.983628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:58.984089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:06:59.984187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:00.984374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:01.984482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:02.984590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:03.984721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:04.984831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:05.985018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:06.985977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:07.986837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:08.987035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:09.988105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:10.988343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:11.988466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:12.989149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:13.989402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:14.989734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:15.989956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:16.990317      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:17.990462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:18.991012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:19.992115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:20.992257      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:21.992881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:07:22.566: INFO: Pod wasn't evicted. Test successful
  Apr 23 18:07:22.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-7607" for this suite. @ 04/23/23 18:07:22.571
• [135.351 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/23/23 18:07:22.582
  Apr 23 18:07:22.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename podtemplate @ 04/23/23 18:07:22.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:07:22.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:07:22.605
  Apr 23 18:07:22.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4285" for this suite. @ 04/23/23 18:07:22.651
• [0.078 seconds]
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/23/23 18:07:22.66
  Apr 23 18:07:22.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 18:07:22.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:07:22.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:07:22.687
  STEP: Creating a pod to test downward api env vars @ 04/23/23 18:07:22.69
  E0423 18:07:22.993385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:23.994228      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:24.994893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:25.995028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:07:26.72
  Apr 23 18:07:26.724: INFO: Trying to get logs from node ip-172-31-70-241 pod downward-api-638a15cb-911c-428e-a210-86176994f815 container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 18:07:26.742
  Apr 23 18:07:26.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9554" for this suite. @ 04/23/23 18:07:26.8
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/23/23 18:07:26.812
  Apr 23 18:07:26.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename deployment @ 04/23/23 18:07:26.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:07:26.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:07:26.842
  Apr 23 18:07:26.846: INFO: Creating deployment "webserver-deployment"
  Apr 23 18:07:26.853: INFO: Waiting for observed generation 1
  E0423 18:07:26.996183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:27.997928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:07:28.868: INFO: Waiting for all required pods to come up
  Apr 23 18:07:28.873: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/23/23 18:07:28.873
  E0423 18:07:28.998346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:29.998487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:07:30.888: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 23 18:07:30.895: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Apr 23 18:07:30.907: INFO: Updating deployment webserver-deployment
  Apr 23 18:07:30.907: INFO: Waiting for observed generation 2
  E0423 18:07:30.999150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:31.999250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:07:32.919: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 23 18:07:32.923: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 23 18:07:32.927: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 23 18:07:32.939: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 23 18:07:32.939: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 23 18:07:32.943: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 23 18:07:32.950: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 23 18:07:32.950: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 23 18:07:32.964: INFO: Updating deployment webserver-deployment
  Apr 23 18:07:32.964: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Apr 23 18:07:32.974: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Apr 23 18:07:32.986: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0423 18:07:33.000375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:07:33.033: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-5984  0acac649-0179-48b5-a68b-639939064b2a 37536 3 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e62018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-04-23 18:07:31 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-23 18:07:32 +0000 UTC,LastTransitionTime:2023-04-23 18:07:32 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Apr 23 18:07:33.059: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-5984  3b64fb3f-04c5-4703-ba21-b5e8920206f1 37527 3 2023-04-23 18:07:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 0acac649-0179-48b5-a68b-639939064b2a 0xc003e624d7 0xc003e624d8}] [] [{kube-controller-manager Update apps/v1 2023-04-23 18:07:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0acac649-0179-48b5-a68b-639939064b2a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e62578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 18:07:33.060: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 23 18:07:33.060: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-5984  caae711d-0883-4efd-8eb0-11a72f2e76f0 37525 3 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 0acac649-0179-48b5-a68b-639939064b2a 0xc003e623e7 0xc003e623e8}] [] [{kube-controller-manager Update apps/v1 2023-04-23 18:07:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0acac649-0179-48b5-a68b-639939064b2a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e62478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Apr 23 18:07:33.088: INFO: Pod "webserver-deployment-67bd4bf6dc-4kkgw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4kkgw webserver-deployment-67bd4bf6dc- deployment-5984  cbe42247-9abe-4934-942b-01bf40e0b546 37566 0 2023-04-23 18:07:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003564c77 0xc003564c78}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lvhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lvhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:,StartTime:2023-04-23 18:07:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.088: INFO: Pod "webserver-deployment-67bd4bf6dc-69rmg" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-69rmg webserver-deployment-67bd4bf6dc- deployment-5984  345dde3b-ee08-4e51-a4ca-93ebd78ab72b 37395 0 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003564e67 0xc003564e68}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqpl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqpl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-37-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.37.26,PodIP:192.168.21.54,StartTime:2023-04-23 18:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:07:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://54b9660e0222a6c62f82b8367b78691c4b066239a1fc7dcd8add1a113e4a5693,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.54,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.088: INFO: Pod "webserver-deployment-67bd4bf6dc-76fns" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-76fns webserver-deployment-67bd4bf6dc- deployment-5984  d586363a-eae2-46b6-8eba-61acd474cd03 37557 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565057 0xc003565058}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxmkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxmkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.088: INFO: Pod "webserver-deployment-67bd4bf6dc-8m2m2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8m2m2 webserver-deployment-67bd4bf6dc- deployment-5984  050fc9e7-08cb-45cf-8933-78ce5a7c6e67 37562 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565197 0xc003565198}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcq9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcq9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.088: INFO: Pod "webserver-deployment-67bd4bf6dc-8q2cq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8q2cq webserver-deployment-67bd4bf6dc- deployment-5984  ff37f600-d901-4820-9ac8-e9568a5a1e41 37537 0 2023-04-23 18:07:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc0035652e7 0xc0035652e8}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q75zq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q75zq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.088: INFO: Pod "webserver-deployment-67bd4bf6dc-9jt9x" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9jt9x webserver-deployment-67bd4bf6dc- deployment-5984  8ef422a4-7f01-4c8a-8d0d-e05f826b1239 37559 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565460 0xc003565461}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmr55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmr55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.089: INFO: Pod "webserver-deployment-67bd4bf6dc-dsvdk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dsvdk webserver-deployment-67bd4bf6dc- deployment-5984  7101d689-d564-4b0f-9db3-4a79e9140bc7 37549 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565617 0xc003565618}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hjr92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hjr92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-37-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.089: INFO: Pod "webserver-deployment-67bd4bf6dc-gztpt" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gztpt webserver-deployment-67bd4bf6dc- deployment-5984  429fa344-1a22-4cda-a6d6-e26c0171d6e0 37412 0 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565790 0xc003565791}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5szbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5szbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.237,StartTime:2023-04-23 18:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:07:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://de58351fa9f24eb23f1dff66ba01334f8de429ad7c0e256c20e82dae0ad6a030,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.237,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.089: INFO: Pod "webserver-deployment-67bd4bf6dc-hndt5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hndt5 webserver-deployment-67bd4bf6dc- deployment-5984  7c562da1-a65a-4acf-8cdc-c547a299140b 37335 0 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565977 0xc003565978}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94s8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94s8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.26,PodIP:192.168.122.153,StartTime:2023-04-23 18:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:07:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://31bed982a45268be3fee7d8ef2f6fbf39f10d723a318625f0ccc1536ae40495a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.153,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.089: INFO: Pod "webserver-deployment-67bd4bf6dc-jqxg2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jqxg2 webserver-deployment-67bd4bf6dc- deployment-5984  a798f812-3eee-4816-95a4-2c546846a46f 37561 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565b67 0xc003565b68}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7prs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7prs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.089: INFO: Pod "webserver-deployment-67bd4bf6dc-jwktp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jwktp webserver-deployment-67bd4bf6dc- deployment-5984  e817088d-304c-4000-8637-2963ed3a98cb 37400 0 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565ca7 0xc003565ca8}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9bqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9bqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.239,StartTime:2023-04-23 18:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:07:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f2f4a21dc62bf4b63dc49aa6fe46c74d56b4d8d49944e8cfe02188ea8bc1a557,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.239,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.090: INFO: Pod "webserver-deployment-67bd4bf6dc-kn7zb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kn7zb webserver-deployment-67bd4bf6dc- deployment-5984  e9a21478-9356-432e-ba1d-320d49aef795 37558 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565e97 0xc003565e98}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzk65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzk65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.090: INFO: Pod "webserver-deployment-67bd4bf6dc-ldcjs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ldcjs webserver-deployment-67bd4bf6dc- deployment-5984  e31c1359-2124-4f7b-ae1b-0bd6fb30f679 37554 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003565fd7 0xc003565fd8}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp4hv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp4hv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.090: INFO: Pod "webserver-deployment-67bd4bf6dc-nf6x7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nf6x7 webserver-deployment-67bd4bf6dc- deployment-5984  262cfe48-8fc1-4dad-b4e4-b1f1b87ef025 37388 0 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003d78240 0xc003d78241}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wncjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wncjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-37-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.37.26,PodIP:192.168.21.59,StartTime:2023-04-23 18:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:07:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ae7a5cc5723fb525c56d1d1c229939bbea81584f62d0e7fe7cd16f43c6920c21,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.59,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.090: INFO: Pod "webserver-deployment-67bd4bf6dc-nk54w" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nk54w webserver-deployment-67bd4bf6dc- deployment-5984  052a69e5-5845-4d87-baf6-1458338bc824 37380 0 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003d78447 0xc003d78448}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwpkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwpkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.26,PodIP:192.168.122.156,StartTime:2023-04-23 18:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:07:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://551da12e3201a5bd451bae1322f613d8df1d5c1df88d2bee715a1f7304868279,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.156,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.090: INFO: Pod "webserver-deployment-67bd4bf6dc-qdwch" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qdwch webserver-deployment-67bd4bf6dc- deployment-5984  2a6f447c-8edb-443b-8d3c-efc6ded1c103 37383 0 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003d78637 0xc003d78638}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5bxnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bxnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.26,PodIP:192.168.122.159,StartTime:2023-04-23 18:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:07:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d26fb87081176ae41c136a1d634be9b3b76d016c2798f1bfd375745a96538964,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.159,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.091: INFO: Pod "webserver-deployment-67bd4bf6dc-r8wss" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-r8wss webserver-deployment-67bd4bf6dc- deployment-5984  00c0cb65-b17d-4a26-8a86-f22b3a03b2d9 37391 0 2023-04-23 18:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003d78827 0xc003d78828}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2q7gk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2q7gk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-37-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.37.26,PodIP:192.168.21.57,StartTime:2023-04-23 18:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-23 18:07:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1bc9e19814e0bca9f9d996fe6c9c907985a5045dd0a99f6e03f619b5ac84902f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.57,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.091: INFO: Pod "webserver-deployment-67bd4bf6dc-scsf2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-scsf2 webserver-deployment-67bd4bf6dc- deployment-5984  36b4ca9d-38c4-4cbb-9509-ca33819ebecf 37540 0 2023-04-23 18:07:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003d78a17 0xc003d78a18}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcc5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcc5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.091: INFO: Pod "webserver-deployment-67bd4bf6dc-w6zkr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w6zkr webserver-deployment-67bd4bf6dc- deployment-5984  55586cec-e4f0-4611-8bbf-284b385b8ea9 37550 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003d78b80 0xc003d78b81}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dt7jj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dt7jj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-37-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.091: INFO: Pod "webserver-deployment-67bd4bf6dc-zb4sr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zb4sr webserver-deployment-67bd4bf6dc- deployment-5984  0c8a8d75-ec61-4438-93a6-343be6a759d0 37552 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc caae711d-0883-4efd-8eb0-11a72f2e76f0 0xc003d79990 0xc003d79991}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"caae711d-0883-4efd-8eb0-11a72f2e76f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6h8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6h8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.091: INFO: Pod "webserver-deployment-7b75d79cf5-5gvrt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5gvrt webserver-deployment-7b75d79cf5- deployment-5984  6133977d-6e07-43bc-84bd-19e9e2793fbd 37565 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3b64fb3f-04c5-4703-ba21-b5e8920206f1 0xc003d79b10 0xc003d79b11}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b64fb3f-04c5-4703-ba21-b5e8920206f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shmfx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shmfx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.092: INFO: Pod "webserver-deployment-7b75d79cf5-9pmv6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9pmv6 webserver-deployment-7b75d79cf5- deployment-5984  72d39a3b-d9f9-438b-b877-0b66cd8f2e8e 37497 0 2023-04-23 18:07:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3b64fb3f-04c5-4703-ba21-b5e8920206f1 0xc003d79db0 0xc003d79db1}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b64fb3f-04c5-4703-ba21-b5e8920206f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gv2dz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gv2dz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-37-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.37.26,PodIP:192.168.21.61,StartTime:2023-04-23 18:07:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.61,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.092: INFO: Pod "webserver-deployment-7b75d79cf5-bmwcb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bmwcb webserver-deployment-7b75d79cf5- deployment-5984  65833a3f-8f7f-4790-84f7-39a0cf1dfe70 37466 0 2023-04-23 18:07:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3b64fb3f-04c5-4703-ba21-b5e8920206f1 0xc0053ce157 0xc0053ce158}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b64fb3f-04c5-4703-ba21-b5e8920206f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nr5zt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nr5zt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:,StartTime:2023-04-23 18:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.092: INFO: Pod "webserver-deployment-7b75d79cf5-d7blr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-d7blr webserver-deployment-7b75d79cf5- deployment-5984  25c8ecd5-ebe4-470a-b374-7ed8fd7fbd74 37538 0 2023-04-23 18:07:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3b64fb3f-04c5-4703-ba21-b5e8920206f1 0xc0053ce347 0xc0053ce348}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b64fb3f-04c5-4703-ba21-b5e8920206f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jgn8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jgn8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.26,PodIP:192.168.122.155,StartTime:2023-04-23 18:07:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.155,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.093: INFO: Pod "webserver-deployment-7b75d79cf5-dgkwl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dgkwl webserver-deployment-7b75d79cf5- deployment-5984  099d9ead-029e-40b1-8368-45a2d3679497 37514 0 2023-04-23 18:07:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3b64fb3f-04c5-4703-ba21-b5e8920206f1 0xc0053ce567 0xc0053ce568}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b64fb3f-04c5-4703-ba21-b5e8920206f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.6.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blhch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blhch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.70.241,PodIP:192.168.6.227,StartTime:2023-04-23 18:07:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.6.227,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.093: INFO: Pod "webserver-deployment-7b75d79cf5-qwxp6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qwxp6 webserver-deployment-7b75d79cf5- deployment-5984  a20ab9d7-620d-457e-a70d-3ad02b5f7277 37563 0 2023-04-23 18:07:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3b64fb3f-04c5-4703-ba21-b5e8920206f1 0xc0053ce797 0xc0053ce798}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b64fb3f-04c5-4703-ba21-b5e8920206f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftjl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftjl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-37-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.37.26,PodIP:,StartTime:2023-04-23 18:07:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.093: INFO: Pod "webserver-deployment-7b75d79cf5-rhfpq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-rhfpq webserver-deployment-7b75d79cf5- deployment-5984  2508b2e8-3fb1-4d7e-baeb-7ebe236257b8 37564 0 2023-04-23 18:07:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3b64fb3f-04c5-4703-ba21-b5e8920206f1 0xc0053ce9a7 0xc0053ce9a8}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b64fb3f-04c5-4703-ba21-b5e8920206f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcwsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcwsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-86-26,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.86.26,PodIP:192.168.122.158,StartTime:2023-04-23 18:07:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.158,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.093: INFO: Pod "webserver-deployment-7b75d79cf5-s6b6s" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-s6b6s webserver-deployment-7b75d79cf5- deployment-5984  299ebe2a-89ad-40c2-99df-e8dc6f1d0b96 37555 0 2023-04-23 18:07:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3b64fb3f-04c5-4703-ba21-b5e8920206f1 0xc0053cebd7 0xc0053cebd8}] [] [{kube-controller-manager Update v1 2023-04-23 18:07:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b64fb3f-04c5-4703-ba21-b5e8920206f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhmsm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhmsm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-70-241,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-23 18:07:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 23 18:07:33.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5984" for this suite. @ 04/23/23 18:07:33.122
• [6.335 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/23/23 18:07:33.148
  Apr 23 18:07:33.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename var-expansion @ 04/23/23 18:07:33.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:07:33.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:07:33.194
  STEP: Creating a pod to test env composition @ 04/23/23 18:07:33.205
  E0423 18:07:34.003822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:35.003968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:36.004129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:37.004194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:07:37.241
  Apr 23 18:07:37.246: INFO: Trying to get logs from node ip-172-31-70-241 pod var-expansion-5113e3b5-b707-42d3-80b2-6b48350bc32f container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 18:07:37.256
  Apr 23 18:07:37.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9580" for this suite. @ 04/23/23 18:07:37.292
• [4.159 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/23/23 18:07:37.307
  Apr 23 18:07:37.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename gc @ 04/23/23 18:07:37.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:07:37.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:07:37.335
  STEP: create the rc @ 04/23/23 18:07:37.34
  W0423 18:07:37.351214      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0423 18:07:38.004954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:39.005089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:40.006939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:41.007091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:42.007186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/23/23 18:07:42.361
  STEP: wait for all pods to be garbage collected @ 04/23/23 18:07:42.384
  E0423 18:07:43.008106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:44.009064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:45.009173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:46.009381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:47.009487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/23/23 18:07:47.393
  W0423 18:07:47.397046      21 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 23 18:07:47.397: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 18:07:47.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7413" for this suite. @ 04/23/23 18:07:47.402
• [10.102 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/23/23 18:07:47.411
  Apr 23 18:07:47.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-runtime @ 04/23/23 18:07:47.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:07:47.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:07:47.441
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/23/23 18:07:47.461
  E0423 18:07:48.009557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:49.009658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:50.010612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:51.011035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:52.011210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:53.011375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:54.012335      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:55.012388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:56.013233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:57.013444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:58.014457      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:07:59.014564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:00.015458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:01.016527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:02.017482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:03.018183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/23/23 18:08:03.559
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/23/23 18:08:03.563
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/23/23 18:08:03.571
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/23/23 18:08:03.571
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/23/23 18:08:03.6
  E0423 18:08:04.018305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:05.018420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:06.018554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/23/23 18:08:06.625
  E0423 18:08:07.018663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:08.019508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/23/23 18:08:08.641
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/23/23 18:08:08.65
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/23/23 18:08:08.65
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/23/23 18:08:08.675
  E0423 18:08:09.020088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/23/23 18:08:09.686
  E0423 18:08:10.020215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:11.020699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:12.021683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/23/23 18:08:12.708
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/23/23 18:08:12.717
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/23/23 18:08:12.717
  Apr 23 18:08:12.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8213" for this suite. @ 04/23/23 18:08:12.858
• [25.456 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 04/23/23 18:08:12.867
  Apr 23 18:08:12.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 18:08:12.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:08:12.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:08:12.89
  STEP: Creating service test in namespace statefulset-1265 @ 04/23/23 18:08:12.895
  Apr 23 18:08:12.916: INFO: Found 0 stateful pods, waiting for 1
  E0423 18:08:13.021713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:14.026828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:15.022104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:16.022194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:17.022346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:18.022586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:19.022719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:20.022983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:21.023047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:22.023143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:08:22.927: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/23/23 18:08:22.935
  W0423 18:08:22.945397      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 23 18:08:22.953: INFO: Found 1 stateful pods, waiting for 2
  E0423 18:08:23.023908      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:24.023912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:25.024709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:26.024922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:27.025109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:28.025184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:29.025415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:30.025530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:31.026428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:32.026620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:08:32.960: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 18:08:32.960: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/23/23 18:08:32.968
  STEP: Delete all of the StatefulSets @ 04/23/23 18:08:32.972
  STEP: Verify that StatefulSets have been deleted @ 04/23/23 18:08:32.982
  Apr 23 18:08:32.989: INFO: Deleting all statefulset in ns statefulset-1265
  Apr 23 18:08:33.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1265" for this suite. @ 04/23/23 18:08:33.014
  E0423 18:08:33.027017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
• [20.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/23/23 18:08:33.033
  Apr 23 18:08:33.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 18:08:33.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:08:33.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:08:33.059
  STEP: Create a pod @ 04/23/23 18:08:33.064
  E0423 18:08:34.027104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:35.028067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 04/23/23 18:08:35.089
  Apr 23 18:08:35.098: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 23 18:08:35.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9617" for this suite. @ 04/23/23 18:08:35.104
• [2.084 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/23/23 18:08:35.118
  Apr 23 18:08:35.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-pred @ 04/23/23 18:08:35.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:08:35.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:08:35.145
  Apr 23 18:08:35.154: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 23 18:08:35.166: INFO: Waiting for terminating namespaces to be deleted...
  Apr 23 18:08:35.170: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-37-26 before test
  Apr 23 18:08:35.179: INFO: default-http-backend-kubernetes-worker-65fc475d49-65dxk from ingress-nginx-kubernetes-worker started at 2023-04-23 16:24:39 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.180: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Apr 23 18:08:35.180: INFO: nginx-ingress-controller-kubernetes-worker-gbggh from ingress-nginx-kubernetes-worker started at 2023-04-23 16:24:39 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.180: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: coredns-5c7f76ccb8-sgclp from kube-system started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.181: INFO: 	Container coredns ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: kube-state-metrics-5b95b4459c-4xnvw from kube-system started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.181: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: metrics-server-v0.5.2-6cf8c8b69c-zzwzt from kube-system started at 2023-04-23 16:24:07 +0000 UTC (2 container statuses recorded)
  Apr 23 18:08:35.181: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: dashboard-metrics-scraper-6b8586b5c9-646rd from kubernetes-dashboard started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.181: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: kubernetes-dashboard-6869f4cd5f-64m7l from kubernetes-dashboard started at 2023-04-23 16:24:07 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.181: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-28gbw from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 18:08:35.181: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 18:08:35.181: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-70-241 before test
  Apr 23 18:08:35.188: INFO: nginx-ingress-controller-kubernetes-worker-lbblq from ingress-nginx-kubernetes-worker started at 2023-04-23 18:06:19 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.188: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 18:08:35.188: INFO: pod-v2rhn from pods-9617 started at 2023-04-23 18:08:33 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.188: INFO: 	Container agnhost ready: true, restart count 0
  Apr 23 18:08:35.188: INFO: sonobuoy from sonobuoy started at 2023-04-23 16:37:12 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.188: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 23 18:08:35.188: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-clx9s from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 18:08:35.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 18:08:35.188: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 23 18:08:35.188: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-86-26 before test
  Apr 23 18:08:35.195: INFO: nginx-ingress-controller-kubernetes-worker-httzn from ingress-nginx-kubernetes-worker started at 2023-04-23 16:26:06 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.195: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Apr 23 18:08:35.195: INFO: calico-kube-controllers-6c8cb79d47-nmw6q from kube-system started at 2023-04-23 16:31:05 +0000 UTC (1 container statuses recorded)
  Apr 23 18:08:35.195: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Apr 23 18:08:35.195: INFO: sonobuoy-e2e-job-b60cd7a9aecc4153 from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 18:08:35.195: INFO: 	Container e2e ready: true, restart count 0
  Apr 23 18:08:35.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 18:08:35.195: INFO: sonobuoy-systemd-logs-daemon-set-4880b8b46b374b98-mdknn from sonobuoy started at 2023-04-23 16:37:15 +0000 UTC (2 container statuses recorded)
  Apr 23 18:08:35.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 23 18:08:35.195: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/23/23 18:08:35.195
  E0423 18:08:36.028212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:37.028361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/23/23 18:08:37.229
  STEP: Trying to apply a random label on the found node. @ 04/23/23 18:08:37.247
  STEP: verifying the node has the label kubernetes.io/e2e-c0b6d2db-f376-42f3-bddb-0242c668db99 42 @ 04/23/23 18:08:37.26
  STEP: Trying to relaunch the pod, now with labels. @ 04/23/23 18:08:37.267
  E0423 18:08:38.028527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:39.028606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-c0b6d2db-f376-42f3-bddb-0242c668db99 off the node ip-172-31-70-241 @ 04/23/23 18:08:39.293
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-c0b6d2db-f376-42f3-bddb-0242c668db99 @ 04/23/23 18:08:39.307
  Apr 23 18:08:39.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-285" for this suite. @ 04/23/23 18:08:39.32
• [4.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/23/23 18:08:39.334
  Apr 23 18:08:39.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 18:08:39.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:08:39.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:08:39.356
  E0423 18:08:40.029584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:41.029970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:42.030680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:43.031024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:44.031922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:45.032027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:46.032099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:47.032217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:48.032353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:49.032453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:50.033047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:51.033105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:52.033323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:53.033687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:54.033790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:55.034210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:56.034479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:57.034715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:58.034948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:08:59.035029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:00.036096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:01.037151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:09:01.442: INFO: Container started at 2023-04-23 18:08:40 +0000 UTC, pod became ready at 2023-04-23 18:08:59 +0000 UTC
  Apr 23 18:09:01.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-7414" for this suite. @ 04/23/23 18:09:01.449
• [22.131 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/23/23 18:09:01.465
  Apr 23 18:09:01.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 18:09:01.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:09:01.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:09:01.519
  STEP: Creating a pod to test downward api env vars @ 04/23/23 18:09:01.524
  E0423 18:09:02.037276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:03.037697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:04.038322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:05.038442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:09:05.557
  Apr 23 18:09:05.561: INFO: Trying to get logs from node ip-172-31-70-241 pod downward-api-ddb42a23-e696-4c9c-ac14-ea78fe06932b container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 18:09:05.569
  Apr 23 18:09:05.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5008" for this suite. @ 04/23/23 18:09:05.617
• [4.161 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/23/23 18:09:05.627
  Apr 23 18:09:05.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename gc @ 04/23/23 18:09:05.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:09:05.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:09:05.651
  STEP: create the rc @ 04/23/23 18:09:05.669
  W0423 18:09:05.677959      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0423 18:09:06.039551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:07.039778      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:08.039936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:09.040068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:10.040841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:11.040961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/23/23 18:09:11.684
  STEP: wait for the rc to be deleted @ 04/23/23 18:09:11.697
  E0423 18:09:12.042213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:13.042599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:14.042711      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:15.044108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:16.044213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/23/23 18:09:16.717
  E0423 18:09:17.045183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:18.045218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:19.045883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:20.046013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:21.046068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:22.046399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:23.046408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:24.046635      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:25.046864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:26.047007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:27.047133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:28.048068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:29.048398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:30.049013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:31.049216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:32.049434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:33.049741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:34.049929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:35.050108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:36.050454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:37.050503      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:38.050731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:39.050953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:40.051154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:41.052105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:42.052877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:43.053160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:44.053315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:45.053394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:46.054326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/23/23 18:09:46.731
  W0423 18:09:46.737450      21 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 23 18:09:46.737: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 23 18:09:46.739: INFO: Deleting pod "simpletest.rc-27mnq" in namespace "gc-8807"
  Apr 23 18:09:46.756: INFO: Deleting pod "simpletest.rc-2kgv5" in namespace "gc-8807"
  Apr 23 18:09:46.778: INFO: Deleting pod "simpletest.rc-2mk66" in namespace "gc-8807"
  Apr 23 18:09:46.794: INFO: Deleting pod "simpletest.rc-2p9bw" in namespace "gc-8807"
  Apr 23 18:09:46.823: INFO: Deleting pod "simpletest.rc-2xqhj" in namespace "gc-8807"
  Apr 23 18:09:46.855: INFO: Deleting pod "simpletest.rc-464ft" in namespace "gc-8807"
  Apr 23 18:09:46.873: INFO: Deleting pod "simpletest.rc-48jqq" in namespace "gc-8807"
  Apr 23 18:09:46.898: INFO: Deleting pod "simpletest.rc-4hf94" in namespace "gc-8807"
  Apr 23 18:09:46.928: INFO: Deleting pod "simpletest.rc-4v2n9" in namespace "gc-8807"
  Apr 23 18:09:46.952: INFO: Deleting pod "simpletest.rc-582ww" in namespace "gc-8807"
  Apr 23 18:09:46.976: INFO: Deleting pod "simpletest.rc-5g9hl" in namespace "gc-8807"
  Apr 23 18:09:47.001: INFO: Deleting pod "simpletest.rc-5m7p2" in namespace "gc-8807"
  Apr 23 18:09:47.025: INFO: Deleting pod "simpletest.rc-5mv8b" in namespace "gc-8807"
  Apr 23 18:09:47.041: INFO: Deleting pod "simpletest.rc-5r5pk" in namespace "gc-8807"
  E0423 18:09:47.055049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:09:47.058: INFO: Deleting pod "simpletest.rc-5t5km" in namespace "gc-8807"
  Apr 23 18:09:47.082: INFO: Deleting pod "simpletest.rc-75844" in namespace "gc-8807"
  Apr 23 18:09:47.098: INFO: Deleting pod "simpletest.rc-7m9dj" in namespace "gc-8807"
  Apr 23 18:09:47.113: INFO: Deleting pod "simpletest.rc-875cx" in namespace "gc-8807"
  Apr 23 18:09:47.130: INFO: Deleting pod "simpletest.rc-99v78" in namespace "gc-8807"
  Apr 23 18:09:47.155: INFO: Deleting pod "simpletest.rc-9nxkl" in namespace "gc-8807"
  Apr 23 18:09:47.169: INFO: Deleting pod "simpletest.rc-9sbn4" in namespace "gc-8807"
  Apr 23 18:09:47.191: INFO: Deleting pod "simpletest.rc-9zxj4" in namespace "gc-8807"
  Apr 23 18:09:47.206: INFO: Deleting pod "simpletest.rc-b5xnj" in namespace "gc-8807"
  Apr 23 18:09:47.221: INFO: Deleting pod "simpletest.rc-bccqq" in namespace "gc-8807"
  Apr 23 18:09:47.238: INFO: Deleting pod "simpletest.rc-bcsfl" in namespace "gc-8807"
  Apr 23 18:09:47.256: INFO: Deleting pod "simpletest.rc-bdwh5" in namespace "gc-8807"
  Apr 23 18:09:47.271: INFO: Deleting pod "simpletest.rc-bmvbd" in namespace "gc-8807"
  Apr 23 18:09:47.287: INFO: Deleting pod "simpletest.rc-btllb" in namespace "gc-8807"
  Apr 23 18:09:47.300: INFO: Deleting pod "simpletest.rc-c95mb" in namespace "gc-8807"
  Apr 23 18:09:47.317: INFO: Deleting pod "simpletest.rc-cg2md" in namespace "gc-8807"
  Apr 23 18:09:47.333: INFO: Deleting pod "simpletest.rc-cm65l" in namespace "gc-8807"
  Apr 23 18:09:47.348: INFO: Deleting pod "simpletest.rc-cqz8h" in namespace "gc-8807"
  Apr 23 18:09:47.364: INFO: Deleting pod "simpletest.rc-ct59g" in namespace "gc-8807"
  Apr 23 18:09:47.376: INFO: Deleting pod "simpletest.rc-d9ktc" in namespace "gc-8807"
  Apr 23 18:09:47.391: INFO: Deleting pod "simpletest.rc-dhhdl" in namespace "gc-8807"
  Apr 23 18:09:47.406: INFO: Deleting pod "simpletest.rc-dkxxn" in namespace "gc-8807"
  Apr 23 18:09:47.419: INFO: Deleting pod "simpletest.rc-dmb7c" in namespace "gc-8807"
  Apr 23 18:09:47.434: INFO: Deleting pod "simpletest.rc-dvltb" in namespace "gc-8807"
  Apr 23 18:09:47.452: INFO: Deleting pod "simpletest.rc-g4xtl" in namespace "gc-8807"
  Apr 23 18:09:47.467: INFO: Deleting pod "simpletest.rc-gcbts" in namespace "gc-8807"
  Apr 23 18:09:47.479: INFO: Deleting pod "simpletest.rc-glhsq" in namespace "gc-8807"
  Apr 23 18:09:47.496: INFO: Deleting pod "simpletest.rc-glkks" in namespace "gc-8807"
  Apr 23 18:09:47.513: INFO: Deleting pod "simpletest.rc-gt7jq" in namespace "gc-8807"
  Apr 23 18:09:47.526: INFO: Deleting pod "simpletest.rc-gtfbw" in namespace "gc-8807"
  Apr 23 18:09:47.541: INFO: Deleting pod "simpletest.rc-h4m5t" in namespace "gc-8807"
  Apr 23 18:09:47.557: INFO: Deleting pod "simpletest.rc-hmnmt" in namespace "gc-8807"
  Apr 23 18:09:47.572: INFO: Deleting pod "simpletest.rc-hpd4t" in namespace "gc-8807"
  Apr 23 18:09:47.587: INFO: Deleting pod "simpletest.rc-j4kn7" in namespace "gc-8807"
  Apr 23 18:09:47.601: INFO: Deleting pod "simpletest.rc-jhkgr" in namespace "gc-8807"
  Apr 23 18:09:47.616: INFO: Deleting pod "simpletest.rc-jrmx4" in namespace "gc-8807"
  Apr 23 18:09:47.630: INFO: Deleting pod "simpletest.rc-jzn47" in namespace "gc-8807"
  Apr 23 18:09:47.644: INFO: Deleting pod "simpletest.rc-krhdg" in namespace "gc-8807"
  Apr 23 18:09:47.659: INFO: Deleting pod "simpletest.rc-ksfhd" in namespace "gc-8807"
  Apr 23 18:09:47.676: INFO: Deleting pod "simpletest.rc-kt2dw" in namespace "gc-8807"
  Apr 23 18:09:47.689: INFO: Deleting pod "simpletest.rc-l5fhs" in namespace "gc-8807"
  Apr 23 18:09:47.707: INFO: Deleting pod "simpletest.rc-l5rwh" in namespace "gc-8807"
  Apr 23 18:09:47.725: INFO: Deleting pod "simpletest.rc-l7t9p" in namespace "gc-8807"
  Apr 23 18:09:47.745: INFO: Deleting pod "simpletest.rc-lhtm4" in namespace "gc-8807"
  Apr 23 18:09:47.759: INFO: Deleting pod "simpletest.rc-mnmw2" in namespace "gc-8807"
  Apr 23 18:09:47.776: INFO: Deleting pod "simpletest.rc-mthk2" in namespace "gc-8807"
  Apr 23 18:09:47.788: INFO: Deleting pod "simpletest.rc-nfmvc" in namespace "gc-8807"
  Apr 23 18:09:47.808: INFO: Deleting pod "simpletest.rc-phlj4" in namespace "gc-8807"
  Apr 23 18:09:47.824: INFO: Deleting pod "simpletest.rc-pj6fw" in namespace "gc-8807"
  Apr 23 18:09:47.839: INFO: Deleting pod "simpletest.rc-qbtjm" in namespace "gc-8807"
  Apr 23 18:09:47.853: INFO: Deleting pod "simpletest.rc-qmqrg" in namespace "gc-8807"
  Apr 23 18:09:47.867: INFO: Deleting pod "simpletest.rc-qz4qh" in namespace "gc-8807"
  Apr 23 18:09:47.881: INFO: Deleting pod "simpletest.rc-rb5z4" in namespace "gc-8807"
  Apr 23 18:09:47.895: INFO: Deleting pod "simpletest.rc-rfnjw" in namespace "gc-8807"
  Apr 23 18:09:47.911: INFO: Deleting pod "simpletest.rc-rktfg" in namespace "gc-8807"
  Apr 23 18:09:47.924: INFO: Deleting pod "simpletest.rc-rm5hv" in namespace "gc-8807"
  Apr 23 18:09:47.940: INFO: Deleting pod "simpletest.rc-rrnwf" in namespace "gc-8807"
  Apr 23 18:09:47.958: INFO: Deleting pod "simpletest.rc-rs2lk" in namespace "gc-8807"
  Apr 23 18:09:47.974: INFO: Deleting pod "simpletest.rc-rw9wv" in namespace "gc-8807"
  Apr 23 18:09:47.992: INFO: Deleting pod "simpletest.rc-s69w6" in namespace "gc-8807"
  Apr 23 18:09:48.035: INFO: Deleting pod "simpletest.rc-sfszp" in namespace "gc-8807"
  E0423 18:09:48.055515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:09:48.086: INFO: Deleting pod "simpletest.rc-sqnms" in namespace "gc-8807"
  Apr 23 18:09:48.136: INFO: Deleting pod "simpletest.rc-sxs52" in namespace "gc-8807"
  Apr 23 18:09:48.184: INFO: Deleting pod "simpletest.rc-thzst" in namespace "gc-8807"
  Apr 23 18:09:48.436: INFO: Deleting pod "simpletest.rc-tkkj5" in namespace "gc-8807"
  Apr 23 18:09:48.457: INFO: Deleting pod "simpletest.rc-ttgfs" in namespace "gc-8807"
  Apr 23 18:09:48.479: INFO: Deleting pod "simpletest.rc-vbrnq" in namespace "gc-8807"
  Apr 23 18:09:48.495: INFO: Deleting pod "simpletest.rc-vj688" in namespace "gc-8807"
  Apr 23 18:09:48.507: INFO: Deleting pod "simpletest.rc-vkw8q" in namespace "gc-8807"
  Apr 23 18:09:48.523: INFO: Deleting pod "simpletest.rc-vmlzj" in namespace "gc-8807"
  Apr 23 18:09:48.541: INFO: Deleting pod "simpletest.rc-vv7nx" in namespace "gc-8807"
  Apr 23 18:09:48.582: INFO: Deleting pod "simpletest.rc-w4lp5" in namespace "gc-8807"
  Apr 23 18:09:48.637: INFO: Deleting pod "simpletest.rc-w5gbp" in namespace "gc-8807"
  Apr 23 18:09:48.686: INFO: Deleting pod "simpletest.rc-wlvxh" in namespace "gc-8807"
  Apr 23 18:09:48.733: INFO: Deleting pod "simpletest.rc-wrs7d" in namespace "gc-8807"
  Apr 23 18:09:48.783: INFO: Deleting pod "simpletest.rc-wv4kp" in namespace "gc-8807"
  Apr 23 18:09:48.831: INFO: Deleting pod "simpletest.rc-ww6cx" in namespace "gc-8807"
  Apr 23 18:09:48.884: INFO: Deleting pod "simpletest.rc-wxzr6" in namespace "gc-8807"
  Apr 23 18:09:48.934: INFO: Deleting pod "simpletest.rc-x5zcd" in namespace "gc-8807"
  Apr 23 18:09:48.983: INFO: Deleting pod "simpletest.rc-xrl5v" in namespace "gc-8807"
  Apr 23 18:09:49.034: INFO: Deleting pod "simpletest.rc-xx8qv" in namespace "gc-8807"
  E0423 18:09:49.056562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:09:49.082: INFO: Deleting pod "simpletest.rc-z9fq7" in namespace "gc-8807"
  Apr 23 18:09:49.137: INFO: Deleting pod "simpletest.rc-zd98v" in namespace "gc-8807"
  Apr 23 18:09:49.188: INFO: Deleting pod "simpletest.rc-zmswj" in namespace "gc-8807"
  Apr 23 18:09:49.232: INFO: Deleting pod "simpletest.rc-znf9w" in namespace "gc-8807"
  Apr 23 18:09:49.285: INFO: Deleting pod "simpletest.rc-zrdkj" in namespace "gc-8807"
  Apr 23 18:09:49.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8807" for this suite. @ 04/23/23 18:09:49.373
• [43.800 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 04/23/23 18:09:49.429
  Apr 23 18:09:49.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 18:09:49.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:09:49.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:09:49.447
  Apr 23 18:09:49.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 18:09:50.056744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:51.057126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0423 18:09:52.012132      21 warnings.go:70] unknown field "alpha"
  W0423 18:09:52.012154      21 warnings.go:70] unknown field "beta"
  W0423 18:09:52.012160      21 warnings.go:70] unknown field "delta"
  W0423 18:09:52.012166      21 warnings.go:70] unknown field "epsilon"
  W0423 18:09:52.012171      21 warnings.go:70] unknown field "gamma"
  Apr 23 18:09:52.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0423 18:09:52.058679      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "field-validation-9056" for this suite. @ 04/23/23 18:09:52.059
• [2.638 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/23/23 18:09:52.069
  Apr 23 18:09:52.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 18:09:52.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:09:52.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:09:52.093
  STEP: Creating the pod @ 04/23/23 18:09:52.097
  E0423 18:09:53.059097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:54.060119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:55.060524      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:56.069230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:57.069996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:58.070052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:09:59.070986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:00.071111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:00.665: INFO: Successfully updated pod "labelsupdate5c44d030-cddd-43e8-8ba6-a8aecfbf976a"
  E0423 18:10:01.071451      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:02.071878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:02.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1937" for this suite. @ 04/23/23 18:10:02.695
• [10.634 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/23/23 18:10:02.703
  Apr 23 18:10:02.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 18:10:02.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:02.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:02.727
  STEP: creating the pod @ 04/23/23 18:10:02.731
  STEP: submitting the pod to kubernetes @ 04/23/23 18:10:02.731
  E0423 18:10:03.072692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:04.073092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/23/23 18:10:04.756
  STEP: updating the pod @ 04/23/23 18:10:04.761
  E0423 18:10:05.073198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:05.279: INFO: Successfully updated pod "pod-update-dd15c311-ac13-4529-8d14-9f2c97b8b19f"
  STEP: verifying the updated pod is in kubernetes @ 04/23/23 18:10:05.288
  Apr 23 18:10:05.299: INFO: Pod update OK
  Apr 23 18:10:05.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9693" for this suite. @ 04/23/23 18:10:05.305
• [2.610 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/23/23 18:10:05.317
  Apr 23 18:10:05.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubelet-test @ 04/23/23 18:10:05.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:05.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:05.347
  E0423 18:10:06.074002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:07.074600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:07.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-542" for this suite. @ 04/23/23 18:10:07.409
• [2.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/23/23 18:10:07.42
  Apr 23 18:10:07.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 18:10:07.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:07.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:07.447
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/23/23 18:10:07.45
  E0423 18:10:08.075588      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:09.076104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:10.076917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:11.077154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:10:11.48
  Apr 23 18:10:11.485: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-c1a84ea4-e08d-4a0b-83e8-42a02b8da6b9 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 18:10:11.493
  Apr 23 18:10:11.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4936" for this suite. @ 04/23/23 18:10:11.514
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/23/23 18:10:11.526
  Apr 23 18:10:11.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename kubectl @ 04/23/23 18:10:11.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:11.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:11.548
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/23/23 18:10:11.554
  Apr 23 18:10:11.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-5026 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 23 18:10:11.636: INFO: stderr: ""
  Apr 23 18:10:11.636: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/23/23 18:10:11.636
  E0423 18:10:12.078298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:13.078356      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:14.078847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:15.079139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:16.080106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/23/23 18:10:16.687
  Apr 23 18:10:16.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-5026 get pod e2e-test-httpd-pod -o json'
  Apr 23 18:10:16.761: INFO: stderr: ""
  Apr 23 18:10:16.761: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-04-23T18:10:11Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5026\",\n        \"resourceVersion\": \"41242\",\n        \"uid\": \"a3dfac5b-63f7-48b0-9313-2ea632500f2b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-562kq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-70-241\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-562kq\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-23T18:10:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-23T18:10:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-23T18:10:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-23T18:10:11Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ae753449e19b89266f9ac68596caa70e70379d52c7b0c0c424669c0478aca2c8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-23T18:10:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.70.241\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.6.232\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.6.232\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-23T18:10:11Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/23/23 18:10:16.761
  Apr 23 18:10:16.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-5026 replace -f -'
  E0423 18:10:17.080398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:17.484: INFO: stderr: ""
  Apr 23 18:10:17.484: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/23/23 18:10:17.484
  Apr 23 18:10:17.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=kubectl-5026 delete pods e2e-test-httpd-pod'
  E0423 18:10:18.080499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:19.080583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:19.634: INFO: stderr: ""
  Apr 23 18:10:19.634: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 23 18:10:19.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5026" for this suite. @ 04/23/23 18:10:19.639
• [8.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/23/23 18:10:19.649
  Apr 23 18:10:19.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename container-probe @ 04/23/23 18:10:19.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:19.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:19.672
  STEP: Creating pod liveness-4cefbd47-bac2-4226-87c8-3f7a76c19170 in namespace container-probe-8605 @ 04/23/23 18:10:19.678
  E0423 18:10:20.080684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:21.080788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:21.718: INFO: Started pod liveness-4cefbd47-bac2-4226-87c8-3f7a76c19170 in namespace container-probe-8605
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/23/23 18:10:21.718
  Apr 23 18:10:21.723: INFO: Initial restart count of pod liveness-4cefbd47-bac2-4226-87c8-3f7a76c19170 is 0
  E0423 18:10:22.081486      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:23.081790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:24.081893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:25.081999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:26.083123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:27.083087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:28.084147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:29.084304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:30.085092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:31.085260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:32.085865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:33.085967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:34.086938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:35.087116      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:36.088184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:37.088974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:38.089830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:39.089913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:40.090567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:41.090658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:41.784: INFO: Restart count of pod container-probe-8605/liveness-4cefbd47-bac2-4226-87c8-3f7a76c19170 is now 1 (20.060581626s elapsed)
  Apr 23 18:10:41.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/23/23 18:10:41.788
  STEP: Destroying namespace "container-probe-8605" for this suite. @ 04/23/23 18:10:41.804
• [22.166 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/23/23 18:10:41.816
  Apr 23 18:10:41.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename dns @ 04/23/23 18:10:41.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:41.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:41.845
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/23/23 18:10:41.849
  Apr 23 18:10:41.859: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3720  4cf56694-dedb-4694-b7ea-79717215b6d0 41377 0 2023-04-23 18:10:41 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-23 18:10:41 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klhcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klhcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0423 18:10:42.090896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:43.091069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/23/23 18:10:43.872
  Apr 23 18:10:43.872: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3720 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 18:10:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 18:10:43.873: INFO: ExecWithOptions: Clientset creation
  Apr 23 18:10:43.873: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-3720/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 04/23/23 18:10:43.989
  Apr 23 18:10:43.989: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3720 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 18:10:43.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 18:10:43.990: INFO: ExecWithOptions: Clientset creation
  Apr 23 18:10:43.990: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-3720/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0423 18:10:44.092009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:44.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 18:10:44.109: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-3720" for this suite. @ 04/23/23 18:10:44.129
• [2.320 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/23/23 18:10:44.136
  Apr 23 18:10:44.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 18:10:44.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:44.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:44.159
  STEP: fetching services @ 04/23/23 18:10:44.162
  Apr 23 18:10:44.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1691" for this suite. @ 04/23/23 18:10:44.17
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/23/23 18:10:44.179
  Apr 23 18:10:44.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename namespaces @ 04/23/23 18:10:44.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:44.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:44.212
  STEP: Read namespace status @ 04/23/23 18:10:44.219
  Apr 23 18:10:44.223: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/23/23 18:10:44.224
  Apr 23 18:10:44.231: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/23/23 18:10:44.231
  Apr 23 18:10:44.247: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 23 18:10:44.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6126" for this suite. @ 04/23/23 18:10:44.252
• [0.082 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/23/23 18:10:44.262
  Apr 23 18:10:44.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename services @ 04/23/23 18:10:44.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:44.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:44.29
  STEP: creating service in namespace services-3633 @ 04/23/23 18:10:44.294
  STEP: creating service affinity-nodeport-transition in namespace services-3633 @ 04/23/23 18:10:44.294
  STEP: creating replication controller affinity-nodeport-transition in namespace services-3633 @ 04/23/23 18:10:44.312
  I0423 18:10:44.322372      21 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-3633, replica count: 3
  E0423 18:10:45.092603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:46.093736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:47.094056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 18:10:47.373455      21 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0423 18:10:48.094139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:49.094232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:50.094388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0423 18:10:50.373997      21 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 23 18:10:50.385: INFO: Creating new exec pod
  E0423 18:10:51.094491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:52.095092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:53.096050      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:53.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3633 exec execpod-affinitywwm6p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 23 18:10:53.555: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 23 18:10:53.555: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 18:10:53.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3633 exec execpod-affinitywwm6p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.138 80'
  Apr 23 18:10:53.716: INFO: stderr: "+ nc -v -t -w 2 10.152.183.138 80\nConnection to 10.152.183.138 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Apr 23 18:10:53.716: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 18:10:53.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3633 exec execpod-affinitywwm6p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.37.26 30730'
  Apr 23 18:10:53.870: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.37.26 30730\nConnection to 172.31.37.26 30730 port [tcp/*] succeeded!\n"
  Apr 23 18:10:53.870: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 23 18:10:53.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3633 exec execpod-affinitywwm6p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.86.26 30730'
  Apr 23 18:10:54.086: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.86.26 30730\nConnection to 172.31.86.26 30730 port [tcp/*] succeeded!\n"
  Apr 23 18:10:54.086: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  E0423 18:10:54.096206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:10:54.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3633 exec execpod-affinitywwm6p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.37.26:30730/ ; done'
  Apr 23 18:10:54.376: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n"
  Apr 23 18:10:54.376: INFO: stdout: "\naffinity-nodeport-transition-ng2rk\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-xwvp5\naffinity-nodeport-transition-ng2rk\naffinity-nodeport-transition-xwvp5\naffinity-nodeport-transition-xwvp5\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-xwvp5\naffinity-nodeport-transition-ng2rk\naffinity-nodeport-transition-ng2rk\naffinity-nodeport-transition-xwvp5\naffinity-nodeport-transition-ng2rk\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-xwvp5\naffinity-nodeport-transition-ng2rk\naffinity-nodeport-transition-ng2rk"
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-ng2rk
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-xwvp5
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-ng2rk
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-xwvp5
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-xwvp5
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-xwvp5
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-ng2rk
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-ng2rk
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-xwvp5
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-ng2rk
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-xwvp5
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-ng2rk
  Apr 23 18:10:54.376: INFO: Received response from host: affinity-nodeport-transition-ng2rk
  Apr 23 18:10:54.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=services-3633 exec execpod-affinitywwm6p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.37.26:30730/ ; done'
  Apr 23 18:10:54.672: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.37.26:30730/\n"
  Apr 23 18:10:54.672: INFO: stdout: "\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26\naffinity-nodeport-transition-6qv26"
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Received response from host: affinity-nodeport-transition-6qv26
  Apr 23 18:10:54.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 18:10:54.678: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3633, will wait for the garbage collector to delete the pods @ 04/23/23 18:10:54.691
  Apr 23 18:10:54.757: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.806311ms
  Apr 23 18:10:54.858: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.76707ms
  E0423 18:10:55.096642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:56.097107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:57.097763      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-3633" for this suite. @ 04/23/23 18:10:57.4
• [13.148 seconds]
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/23/23 18:10:57.41
  Apr 23 18:10:57.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/23/23 18:10:57.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:10:57.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:10:57.439
  STEP: creating a target pod @ 04/23/23 18:10:57.444
  E0423 18:10:58.097822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:10:59.098510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 04/23/23 18:10:59.468
  E0423 18:11:00.098858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:01.099040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 04/23/23 18:11:01.495
  Apr 23 18:11:01.495: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5839 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 18:11:01.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 18:11:01.495: INFO: ExecWithOptions: Clientset creation
  Apr 23 18:11:01.495: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-5839/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Apr 23 18:11:01.576: INFO: Exec stderr: ""
  Apr 23 18:11:01.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-5839" for this suite. @ 04/23/23 18:11:01.59
• [4.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/23/23 18:11:01.6
  Apr 23 18:11:01.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 18:11:01.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:11:01.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:11:01.622
  STEP: Creating secret with name secret-test-0471835e-04b9-4f52-8d45-13eba1b4220f @ 04/23/23 18:11:01.649
  STEP: Creating a pod to test consume secrets @ 04/23/23 18:11:01.656
  E0423 18:11:02.100010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:03.100282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:04.101032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:05.101379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:11:05.689
  Apr 23 18:11:05.693: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-secrets-e5494e17-6d76-4405-a7ee-d62c4ec11149 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 18:11:05.702
  Apr 23 18:11:05.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7614" for this suite. @ 04/23/23 18:11:05.731
  STEP: Destroying namespace "secret-namespace-1050" for this suite. @ 04/23/23 18:11:05.738
• [4.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/23/23 18:11:05.754
  Apr 23 18:11:05.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename runtimeclass @ 04/23/23 18:11:05.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:11:05.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:11:05.779
  STEP: getting /apis @ 04/23/23 18:11:05.783
  STEP: getting /apis/node.k8s.io @ 04/23/23 18:11:05.79
  STEP: getting /apis/node.k8s.io/v1 @ 04/23/23 18:11:05.792
  STEP: creating @ 04/23/23 18:11:05.794
  STEP: watching @ 04/23/23 18:11:05.818
  Apr 23 18:11:05.818: INFO: starting watch
  STEP: getting @ 04/23/23 18:11:05.827
  STEP: listing @ 04/23/23 18:11:05.831
  STEP: patching @ 04/23/23 18:11:05.835
  STEP: updating @ 04/23/23 18:11:05.844
  Apr 23 18:11:05.852: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/23/23 18:11:05.852
  STEP: deleting a collection @ 04/23/23 18:11:05.869
  Apr 23 18:11:05.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-462" for this suite. @ 04/23/23 18:11:05.895
• [0.149 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/23/23 18:11:05.903
  Apr 23 18:11:05.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename projected @ 04/23/23 18:11:05.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:11:05.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:11:05.927
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-785f0dd5-565d-462b-94cb-ba2d121ad9ac @ 04/23/23 18:11:05.935
  STEP: Creating the pod @ 04/23/23 18:11:05.942
  E0423 18:11:06.103841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:07.104008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-785f0dd5-565d-462b-94cb-ba2d121ad9ac @ 04/23/23 18:11:07.984
  STEP: waiting to observe update in volume @ 04/23/23 18:11:07.991
  E0423 18:11:08.104859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:09.104960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:10.105998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:11.106208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:12.107204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:13.108103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:14.108906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:15.109139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:16.110200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:17.110995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:18.111810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:19.112352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:20.113135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:21.113236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:22.113790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:23.113876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:24.114727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:25.115019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:26.115521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:27.116408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:28.116942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:29.117144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:30.117619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:31.117749      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:32.118215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:33.118370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:34.118419      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:35.118607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:36.119549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:37.120005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:38.120139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:39.120237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:40.120350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:41.120453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:42.120875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:43.120998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:44.121104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:45.121406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:46.121631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:47.121990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:48.122092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:49.122214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:50.122390      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:51.123041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:52.123476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:53.123591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:54.124459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:55.124701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:56.125100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:57.125698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:58.125801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:11:59.126340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:00.126602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:01.127141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:02.127093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:03.128100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:04.128319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:05.129118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:06.129414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:07.129955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:08.130804      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:09.131838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:10.132049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:11.132113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:12.132520      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:13.133465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:14.134108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:15.134227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:16.134457      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:17.135102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:18.135184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:19.136100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:20.136197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:21.136320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:22.136925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:23.136966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:24.137176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:25.137289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:26.137372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:27.137460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:28.137694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:29.138252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:30.138490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:31.138591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:32.138846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:33.139048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:34.140108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:12:34.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4415" for this suite. @ 04/23/23 18:12:34.479
• [88.585 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/23/23 18:12:34.49
  Apr 23 18:12:34.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename watch @ 04/23/23 18:12:34.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:12:34.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:12:34.535
  STEP: creating a watch on configmaps with label A @ 04/23/23 18:12:34.542
  STEP: creating a watch on configmaps with label B @ 04/23/23 18:12:34.544
  STEP: creating a watch on configmaps with label A or B @ 04/23/23 18:12:34.546
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/23/23 18:12:34.548
  Apr 23 18:12:34.557: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976  84b789f9-f6b7-420a-ba47-d3a9ed5613f0 41957 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 18:12:34.558: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976  84b789f9-f6b7-420a-ba47-d3a9ed5613f0 41957 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/23/23 18:12:34.558
  Apr 23 18:12:34.569: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976  84b789f9-f6b7-420a-ba47-d3a9ed5613f0 41958 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 18:12:34.569: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976  84b789f9-f6b7-420a-ba47-d3a9ed5613f0 41958 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/23/23 18:12:34.569
  Apr 23 18:12:34.580: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976  84b789f9-f6b7-420a-ba47-d3a9ed5613f0 41959 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 18:12:34.580: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976  84b789f9-f6b7-420a-ba47-d3a9ed5613f0 41959 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/23/23 18:12:34.58
  Apr 23 18:12:34.593: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976  84b789f9-f6b7-420a-ba47-d3a9ed5613f0 41960 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 18:12:34.593: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6976  84b789f9-f6b7-420a-ba47-d3a9ed5613f0 41960 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/23/23 18:12:34.594
  Apr 23 18:12:34.600: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6976  93d515b0-910e-45a2-a315-3514d1598eba 41961 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 18:12:34.600: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6976  93d515b0-910e-45a2-a315-3514d1598eba 41961 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0423 18:12:35.141012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:36.141374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:37.141471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:38.141717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:39.141835      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:40.142128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:41.142333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:42.142972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:43.143014      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:44.143140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/23/23 18:12:44.601
  Apr 23 18:12:44.611: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6976  93d515b0-910e-45a2-a315-3514d1598eba 41997 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 23 18:12:44.611: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6976  93d515b0-910e-45a2-a315-3514d1598eba 41997 0 2023-04-23 18:12:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-23 18:12:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0423 18:12:45.143816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:46.143933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:47.145002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:48.145083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:49.145229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:50.145309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:51.145428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:52.145532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:53.145747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:54.145855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:12:54.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6976" for this suite. @ 04/23/23 18:12:54.617
• [20.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/23/23 18:12:54.627
  Apr 23 18:12:54.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 18:12:54.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:12:54.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:12:54.652
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/23/23 18:12:54.656
  E0423 18:12:55.146169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:56.146464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:57.146932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:12:58.147040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:12:58.688
  Apr 23 18:12:58.693: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-4ab02ee0-547a-429f-bcae-64ac350fdf26 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 18:12:58.714
  Apr 23 18:12:58.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4624" for this suite. @ 04/23/23 18:12:58.747
• [4.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/23/23 18:12:58.777
  Apr 23 18:12:58.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename secrets @ 04/23/23 18:12:58.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:12:58.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:12:58.809
  STEP: Creating secret with name secret-test-map-c11ee95c-09ad-48fc-9677-289f16ef78fe @ 04/23/23 18:12:58.813
  STEP: Creating a pod to test consume secrets @ 04/23/23 18:12:58.821
  E0423 18:12:59.148113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:00.148216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:01.148318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:02.148932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:13:02.849
  Apr 23 18:13:02.853: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-secrets-defec91a-3799-4a1f-97fc-515596612a71 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/23/23 18:13:02.861
  Apr 23 18:13:02.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-69" for this suite. @ 04/23/23 18:13:02.887
• [4.119 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/23/23 18:13:02.896
  Apr 23 18:13:02.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename replication-controller @ 04/23/23 18:13:02.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:13:02.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:13:02.919
  STEP: Creating replication controller my-hostname-basic-8c93b444-5f96-4741-b151-a46885a6401c @ 04/23/23 18:13:02.923
  Apr 23 18:13:02.937: INFO: Pod name my-hostname-basic-8c93b444-5f96-4741-b151-a46885a6401c: Found 0 pods out of 1
  E0423 18:13:03.149606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:04.150147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:05.150255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:06.150449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:07.150850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:13:07.942: INFO: Pod name my-hostname-basic-8c93b444-5f96-4741-b151-a46885a6401c: Found 1 pods out of 1
  Apr 23 18:13:07.943: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8c93b444-5f96-4741-b151-a46885a6401c" are running
  Apr 23 18:13:07.947: INFO: Pod "my-hostname-basic-8c93b444-5f96-4741-b151-a46885a6401c-9n984" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 18:13:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 18:13:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 18:13:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-23 18:13:02 +0000 UTC Reason: Message:}])
  Apr 23 18:13:07.947: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/23/23 18:13:07.947
  Apr 23 18:13:07.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5585" for this suite. @ 04/23/23 18:13:07.973
• [5.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/23/23 18:13:07.988
  Apr 23 18:13:07.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 18:13:07.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:13:08.011
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:13:08.018
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/23/23 18:13:08.027
  E0423 18:13:08.151783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:09.152206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:10.152261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:11.152333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:13:12.071
  Apr 23 18:13:12.075: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-ecc608fe-04e0-4c99-a68f-60535c992986 container test-container: <nil>
  STEP: delete the pod @ 04/23/23 18:13:12.083
  Apr 23 18:13:12.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5781" for this suite. @ 04/23/23 18:13:12.114
• [4.135 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/23/23 18:13:12.123
  Apr 23 18:13:12.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename pods @ 04/23/23 18:13:12.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:13:12.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:13:12.152
  E0423 18:13:12.152318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:13.152846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:14.152969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:15.153047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:16.153385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:17.153466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:18.153590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:13:18.242
  Apr 23 18:13:18.245: INFO: Trying to get logs from node ip-172-31-70-241 pod client-envvars-296e15a7-98be-431b-a376-2d323b628669 container env3cont: <nil>
  STEP: delete the pod @ 04/23/23 18:13:18.255
  Apr 23 18:13:18.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4001" for this suite. @ 04/23/23 18:13:18.279
• [6.165 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 04/23/23 18:13:18.289
  Apr 23 18:13:18.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename daemonsets @ 04/23/23 18:13:18.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:13:18.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:13:18.313
  Apr 23 18:13:18.348: INFO: Create a RollingUpdate DaemonSet
  Apr 23 18:13:18.356: INFO: Check that daemon pods launch on every node of the cluster
  Apr 23 18:13:18.361: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:18.361: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:18.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 18:13:18.365: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  E0423 18:13:19.154636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:13:19.372: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:19.372: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:19.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 18:13:19.377: INFO: Node ip-172-31-37-26 is running 0 daemon pod, expected 1
  E0423 18:13:20.154746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:13:20.371: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:20.371: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:20.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 23 18:13:20.375: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Apr 23 18:13:20.375: INFO: Update the DaemonSet to trigger a rollout
  Apr 23 18:13:20.388: INFO: Updating DaemonSet daemon-set
  E0423 18:13:21.154984      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:22.155935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:13:22.408: INFO: Roll back the DaemonSet before rollout is complete
  Apr 23 18:13:22.420: INFO: Updating DaemonSet daemon-set
  Apr 23 18:13:22.420: INFO: Make sure DaemonSet rollback is complete
  Apr 23 18:13:22.427: INFO: Wrong image for pod: daemon-set-lm2fw. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 23 18:13:22.427: INFO: Pod daemon-set-lm2fw is not available
  Apr 23 18:13:22.432: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:22.432: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0423 18:13:23.156041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:13:23.442: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:23.442: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0423 18:13:24.156198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:13:24.437: INFO: Pod daemon-set-nhlc2 is not available
  Apr 23 18:13:24.441: INFO: DaemonSet pods can't tolerate node ip-172-31-16-187 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 23 18:13:24.442: INFO: DaemonSet pods can't tolerate node ip-172-31-92-52 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 04/23/23 18:13:24.45
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9623, will wait for the garbage collector to delete the pods @ 04/23/23 18:13:24.45
  Apr 23 18:13:24.515: INFO: Deleting DaemonSet.extensions daemon-set took: 10.276596ms
  Apr 23 18:13:24.616: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.357838ms
  E0423 18:13:25.156360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:26.156818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:13:26.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 23 18:13:26.824: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 23 18:13:26.829: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42418"},"items":null}

  Apr 23 18:13:26.833: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42418"},"items":null}

  Apr 23 18:13:26.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9623" for this suite. @ 04/23/23 18:13:26.857
• [8.578 seconds]
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/23/23 18:13:26.867
  Apr 23 18:13:26.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename security-context-test @ 04/23/23 18:13:26.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:13:26.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:13:26.897
  E0423 18:13:27.157346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:28.157465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:29.158257      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:30.158468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:13:30.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7011" for this suite. @ 04/23/23 18:13:30.942
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/23/23 18:13:30.956
  Apr 23 18:13:30.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename cronjob @ 04/23/23 18:13:30.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:13:30.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:13:30.983
  STEP: Creating a cronjob @ 04/23/23 18:13:30.99
  STEP: creating @ 04/23/23 18:13:30.99
  STEP: getting @ 04/23/23 18:13:30.997
  STEP: listing @ 04/23/23 18:13:31.006
  STEP: watching @ 04/23/23 18:13:31.01
  Apr 23 18:13:31.010: INFO: starting watch
  STEP: cluster-wide listing @ 04/23/23 18:13:31.012
  STEP: cluster-wide watching @ 04/23/23 18:13:31.017
  Apr 23 18:13:31.017: INFO: starting watch
  STEP: patching @ 04/23/23 18:13:31.019
  STEP: updating @ 04/23/23 18:13:31.028
  Apr 23 18:13:31.041: INFO: waiting for watch events with expected annotations
  Apr 23 18:13:31.041: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/23/23 18:13:31.041
  STEP: updating /status @ 04/23/23 18:13:31.048
  STEP: get /status @ 04/23/23 18:13:31.063
  STEP: deleting @ 04/23/23 18:13:31.067
  STEP: deleting a collection @ 04/23/23 18:13:31.087
  Apr 23 18:13:31.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-442" for this suite. @ 04/23/23 18:13:31.11
• [0.164 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/23/23 18:13:31.121
  Apr 23 18:13:31.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-preemption @ 04/23/23 18:13:31.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:13:31.148
  E0423 18:13:31.158719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:13:31.164
  Apr 23 18:13:31.193: INFO: Waiting up to 1m0s for all nodes to be ready
  E0423 18:13:32.158996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:33.159331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:34.159409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:35.160107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:36.160175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:37.160989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:38.161379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:39.161713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:40.161845      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:41.162077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:42.162530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:43.162662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:44.163018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:45.163466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:46.163528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:47.163918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:48.164517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:49.164635      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:50.164922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:51.165007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:52.165976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:53.166064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:54.166272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:55.166507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:56.167039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:57.167146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:58.168079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:13:59.168296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:00.168520      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:01.170930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:02.171501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:03.171625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:04.171787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:05.172095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:06.172996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:07.173123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:08.173230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:09.173289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:10.174247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:11.174358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:12.175915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:13.176004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:14.176128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:15.176299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:16.177251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:17.178022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:18.178171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:19.178846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:20.179100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:21.180087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:22.181046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:23.181170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:24.181239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:25.182238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:26.183028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:27.183130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:28.183678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:29.184502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:30.184612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:31.184748      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:14:31.217: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/23/23 18:14:31.221
  Apr 23 18:14:31.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/23/23 18:14:31.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:14:31.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:14:31.245
  Apr 23 18:14:31.269: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Apr 23 18:14:31.275: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 23 18:14:31.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 23 18:14:31.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-7629" for this suite. @ 04/23/23 18:14:31.38
  STEP: Destroying namespace "sched-preemption-4557" for this suite. @ 04/23/23 18:14:31.388
• [60.280 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/23/23 18:14:31.401
  Apr 23 18:14:31.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename field-validation @ 04/23/23 18:14:31.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:14:31.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:14:31.427
  STEP: apply creating a deployment @ 04/23/23 18:14:31.431
  Apr 23 18:14:31.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4446" for this suite. @ 04/23/23 18:14:31.459
• [0.066 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/23/23 18:14:31.469
  Apr 23 18:14:31.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 18:14:31.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:14:31.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:14:31.491
  STEP: Counting existing ResourceQuota @ 04/23/23 18:14:31.494
  E0423 18:14:32.185626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:33.185725      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:34.186607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:35.186950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:36.187655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 18:14:36.501
  STEP: Ensuring resource quota status is calculated @ 04/23/23 18:14:36.512
  E0423 18:14:37.188497      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:38.188822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 04/23/23 18:14:38.517
  STEP: Ensuring resource quota status captures replication controller creation @ 04/23/23 18:14:38.534
  E0423 18:14:39.188891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:40.188987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 04/23/23 18:14:40.539
  STEP: Ensuring resource quota status released usage @ 04/23/23 18:14:40.548
  E0423 18:14:41.190049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:42.190108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:14:42.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9853" for this suite. @ 04/23/23 18:14:42.56
• [11.099 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/23/23 18:14:42.568
  Apr 23 18:14:42.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 18:14:42.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:14:42.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:14:42.604
  Apr 23 18:14:42.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 18:14:43.190814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:44.191545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/23/23 18:14:44.317
  Apr 23 18:14:44.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8620 --namespace=crd-publish-openapi-8620 create -f -'
  Apr 23 18:14:44.982: INFO: stderr: ""
  Apr 23 18:14:44.982: INFO: stdout: "e2e-test-crd-publish-openapi-5887-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 23 18:14:44.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8620 --namespace=crd-publish-openapi-8620 delete e2e-test-crd-publish-openapi-5887-crds test-cr'
  Apr 23 18:14:45.076: INFO: stderr: ""
  Apr 23 18:14:45.076: INFO: stdout: "e2e-test-crd-publish-openapi-5887-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 23 18:14:45.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8620 --namespace=crd-publish-openapi-8620 apply -f -'
  E0423 18:14:45.192260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:14:45.600: INFO: stderr: ""
  Apr 23 18:14:45.600: INFO: stdout: "e2e-test-crd-publish-openapi-5887-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 23 18:14:45.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8620 --namespace=crd-publish-openapi-8620 delete e2e-test-crd-publish-openapi-5887-crds test-cr'
  Apr 23 18:14:45.680: INFO: stderr: ""
  Apr 23 18:14:45.680: INFO: stdout: "e2e-test-crd-publish-openapi-5887-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/23/23 18:14:45.68
  Apr 23 18:14:45.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=crd-publish-openapi-8620 explain e2e-test-crd-publish-openapi-5887-crds'
  Apr 23 18:14:45.892: INFO: stderr: ""
  Apr 23 18:14:45.892: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-5887-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0423 18:14:46.192990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:47.194017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:14:47.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8620" for this suite. @ 04/23/23 18:14:47.51
• [4.950 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/23/23 18:14:47.519
  Apr 23 18:14:47.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 18:14:47.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:14:47.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:14:47.542
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/23/23 18:14:47.546
  E0423 18:14:48.195016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:49.195108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:50.195213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:51.195330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:14:51.569
  Apr 23 18:14:51.573: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-6addad54-b608-4a28-a85e-e33f158868cd container test-container: <nil>
  STEP: delete the pod @ 04/23/23 18:14:51.581
  Apr 23 18:14:51.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1795" for this suite. @ 04/23/23 18:14:51.607
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 04/23/23 18:14:51.617
  Apr 23 18:14:51.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 18:14:51.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:14:51.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:14:51.635
  STEP: Creating service test in namespace statefulset-7778 @ 04/23/23 18:14:51.645
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/23/23 18:14:51.651
  STEP: Creating stateful set ss in namespace statefulset-7778 @ 04/23/23 18:14:51.658
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7778 @ 04/23/23 18:14:51.665
  Apr 23 18:14:51.670: INFO: Found 0 stateful pods, waiting for 1
  E0423 18:14:52.195391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:53.195665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:54.196100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:55.196386      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:56.196537      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:57.197677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:58.197798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:14:59.197904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:00.198136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:01.198379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:01.676: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/23/23 18:15:01.676
  Apr 23 18:15:01.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-7778 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 18:15:01.881: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 18:15:01.881: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 18:15:01.881: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 18:15:01.886: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0423 18:15:02.198496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:03.198785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:04.198967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:05.199031      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:06.199130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:07.199796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:08.200331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:09.201220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:10.201330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:11.201533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:11.890: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 18:15:11.890: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 18:15:11.912: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999606s
  E0423 18:15:12.201989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:12.917: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994066607s
  E0423 18:15:13.202463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:13.922: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989411627s
  E0423 18:15:14.203030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:14.934: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983798777s
  E0423 18:15:15.204092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:15.939: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971731373s
  E0423 18:15:16.204718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:16.945: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.966654797s
  E0423 18:15:17.204824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:17.951: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96061392s
  E0423 18:15:18.205193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:18.958: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.954594809s
  E0423 18:15:19.205368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:19.964: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.947558652s
  E0423 18:15:20.205849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:20.969: INFO: Verifying statefulset ss doesn't scale past 1 for another 941.993215ms
  E0423 18:15:21.206639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7778 @ 04/23/23 18:15:21.969
  Apr 23 18:15:21.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-7778 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 18:15:22.180: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 18:15:22.180: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 18:15:22.180: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 18:15:22.186: INFO: Found 1 stateful pods, waiting for 3
  E0423 18:15:22.207308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:23.207964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:24.208137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:25.208248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:26.208368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:27.208470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:28.208587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:29.208697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:30.208811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:31.209026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:32.192: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 18:15:32.192: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 23 18:15:32.192: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/23/23 18:15:32.192
  STEP: Scale down will halt with unhealthy stateful pod @ 04/23/23 18:15:32.193
  Apr 23 18:15:32.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-7778 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0423 18:15:32.209463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:32.371: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 18:15:32.371: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 18:15:32.371: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 18:15:32.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-7778 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 18:15:32.556: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 18:15:32.556: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 18:15:32.556: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 18:15:32.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-7778 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 23 18:15:32.713: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 23 18:15:32.713: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 23 18:15:32.713: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 23 18:15:32.713: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 18:15:32.717: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0423 18:15:33.210156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:34.210936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:35.211090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:36.211208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:37.212104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:38.212230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:39.212467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:40.212559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:41.212672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:42.213030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:42.726: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 18:15:42.726: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 18:15:42.726: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 23 18:15:42.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999675s
  E0423 18:15:43.213131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:43.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994355883s
  E0423 18:15:44.213502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:44.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988014516s
  E0423 18:15:45.213885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:45.764: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982119949s
  E0423 18:15:46.213997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:46.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972518661s
  E0423 18:15:47.214958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:47.777: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967580454s
  E0423 18:15:48.215048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:48.782: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960501036s
  E0423 18:15:49.215850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:49.788: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95513491s
  E0423 18:15:50.216245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:50.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949265352s
  E0423 18:15:51.217288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:51.798: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.333214ms
  E0423 18:15:52.217820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7778 @ 04/23/23 18:15:52.798
  Apr 23 18:15:52.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-7778 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 18:15:52.986: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 18:15:52.986: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 18:15:52.986: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 18:15:52.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-7778 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 23 18:15:53.146: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 18:15:53.146: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 18:15:53.146: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 18:15:53.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=statefulset-7778 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0423 18:15:53.219020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:15:53.332: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 23 18:15:53.332: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 23 18:15:53.332: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 23 18:15:53.333: INFO: Scaling statefulset ss to 0
  E0423 18:15:54.219738      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:55.220620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:56.220651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:57.220717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:58.220822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:15:59.221154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:00.221344      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:01.221539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:02.221657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:03.221750      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/23/23 18:16:03.358
  Apr 23 18:16:03.358: INFO: Deleting all statefulset in ns statefulset-7778
  Apr 23 18:16:03.363: INFO: Scaling statefulset ss to 0
  Apr 23 18:16:03.385: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 18:16:03.394: INFO: Deleting statefulset ss
  Apr 23 18:16:03.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7778" for this suite. @ 04/23/23 18:16:03.423
• [71.820 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/23/23 18:16:03.44
  Apr 23 18:16:03.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename watch @ 04/23/23 18:16:03.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:16:03.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:16:03.465
  STEP: getting a starting resourceVersion @ 04/23/23 18:16:03.468
  STEP: starting a background goroutine to produce watch events @ 04/23/23 18:16:03.474
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/23/23 18:16:03.474
  E0423 18:16:04.222720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:05.222728      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:06.225573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:16:06.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5005" for this suite. @ 04/23/23 18:16:06.298
• [2.911 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 04/23/23 18:16:06.351
  Apr 23 18:16:06.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename statefulset @ 04/23/23 18:16:06.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:16:06.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:16:06.381
  STEP: Creating service test in namespace statefulset-5067 @ 04/23/23 18:16:06.384
  STEP: Creating statefulset ss in namespace statefulset-5067 @ 04/23/23 18:16:06.391
  Apr 23 18:16:06.420: INFO: Found 0 stateful pods, waiting for 1
  E0423 18:16:07.226603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:08.226789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:09.227065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:10.227144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:11.228093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:12.228397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:13.228499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:14.229276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:15.230065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:16.230569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:16:16.426: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/23/23 18:16:16.434
  STEP: updating a scale subresource @ 04/23/23 18:16:16.439
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/23/23 18:16:16.446
  STEP: Patch a scale subresource @ 04/23/23 18:16:16.455
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/23/23 18:16:16.479
  Apr 23 18:16:16.487: INFO: Deleting all statefulset in ns statefulset-5067
  Apr 23 18:16:16.491: INFO: Scaling statefulset ss to 0
  E0423 18:16:17.230703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:18.231085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:19.232231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:20.232463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:21.232726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:22.232922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:23.233925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:24.234160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:25.234367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:26.234790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:16:26.519: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 23 18:16:26.530: INFO: Deleting statefulset ss
  Apr 23 18:16:26.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5067" for this suite. @ 04/23/23 18:16:26.567
• [20.294 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/23/23 18:16:26.646
  Apr 23 18:16:26.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename resourcequota @ 04/23/23 18:16:26.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:16:26.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:16:26.676
  E0423 18:16:27.235645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:28.235787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:29.235807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:30.236700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:31.236807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:32.237622      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:33.238448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:34.238580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:35.239532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:36.239669      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:37.240610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:38.241600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:39.242461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:40.243104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:41.243134      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:42.244024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:43.245064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/23/23 18:16:43.789
  E0423 18:16:44.245166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:45.245168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:46.246144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:47.246601      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:48.247210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/23/23 18:16:48.794
  STEP: Ensuring resource quota status is calculated @ 04/23/23 18:16:48.802
  E0423 18:16:49.248070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:50.248175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 04/23/23 18:16:50.815
  STEP: Ensuring resource quota status captures configMap creation @ 04/23/23 18:16:50.83
  E0423 18:16:51.248236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:52.249129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 04/23/23 18:16:52.836
  STEP: Ensuring resource quota status released usage @ 04/23/23 18:16:52.846
  E0423 18:16:53.249819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:54.249961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:16:54.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-802" for this suite. @ 04/23/23 18:16:54.856
• [28.219 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/23/23 18:16:54.866
  Apr 23 18:16:54.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/23/23 18:16:54.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:16:54.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:16:54.892
  STEP: mirroring a new custom Endpoint @ 04/23/23 18:16:54.907
  Apr 23 18:16:54.922: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0423 18:16:55.250539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:56.250656      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 04/23/23 18:16:56.93
  Apr 23 18:16:56.944: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0423 18:16:57.250768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:16:58.250868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 04/23/23 18:16:58.949
  Apr 23 18:16:58.964: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0423 18:16:59.251951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:00.252070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:17:00.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-5690" for this suite. @ 04/23/23 18:17:00.974
• [6.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/23/23 18:17:00.984
  Apr 23 18:17:00.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename subpath @ 04/23/23 18:17:00.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:17:01.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:17:01.016
  STEP: Setting up data @ 04/23/23 18:17:01.019
  STEP: Creating pod pod-subpath-test-configmap-dcf4 @ 04/23/23 18:17:01.032
  STEP: Creating a pod to test atomic-volume-subpath @ 04/23/23 18:17:01.032
  E0423 18:17:01.252172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:02.253079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:03.254159      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:04.254322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:05.254485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:06.254565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:07.254919      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:08.255070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:09.258636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:10.258736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:11.259615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:12.260103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:13.260155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:14.260437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:15.261016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:16.261221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:17.262005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:18.262233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:19.262591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:20.262704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:21.263195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:22.263899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:23.264237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:24.264584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:17:25.133
  Apr 23 18:17:25.137: INFO: Trying to get logs from node ip-172-31-70-241 pod pod-subpath-test-configmap-dcf4 container test-container-subpath-configmap-dcf4: <nil>
  STEP: delete the pod @ 04/23/23 18:17:25.159
  STEP: Deleting pod pod-subpath-test-configmap-dcf4 @ 04/23/23 18:17:25.185
  Apr 23 18:17:25.185: INFO: Deleting pod "pod-subpath-test-configmap-dcf4" in namespace "subpath-6420"
  Apr 23 18:17:25.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6420" for this suite. @ 04/23/23 18:17:25.195
• [24.224 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/23/23 18:17:25.209
  Apr 23 18:17:25.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename emptydir @ 04/23/23 18:17:25.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:17:25.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:17:25.237
  STEP: Creating Pod @ 04/23/23 18:17:25.241
  E0423 18:17:25.264690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:26.264849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:27.265013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 04/23/23 18:17:27.265
  Apr 23 18:17:27.265: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4245 PodName:pod-sharedvolume-6f3320b3-adfa-4e5f-88ba-a733585f030b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 23 18:17:27.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  Apr 23 18:17:27.265: INFO: ExecWithOptions: Clientset creation
  Apr 23 18:17:27.266: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-4245/pods/pod-sharedvolume-6f3320b3-adfa-4e5f-88ba-a733585f030b/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Apr 23 18:17:27.374: INFO: Exec stderr: ""
  Apr 23 18:17:27.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4245" for this suite. @ 04/23/23 18:17:27.381
• [2.187 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/23/23 18:17:27.396
  Apr 23 18:17:27.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/23/23 18:17:27.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:17:27.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:17:27.423
  STEP: set up a multi version CRD @ 04/23/23 18:17:27.429
  Apr 23 18:17:27.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 18:17:28.265911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:29.266343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:30.267271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 04/23/23 18:17:31.081
  STEP: check the new version name is served @ 04/23/23 18:17:31.097
  E0423 18:17:31.268122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:32.268560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 04/23/23 18:17:32.728
  E0423 18:17:33.269022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/23/23 18:17:33.471
  E0423 18:17:34.269803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:35.270915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:36.271207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:17:36.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6362" for this suite. @ 04/23/23 18:17:36.792
• [9.404 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/23/23 18:17:36.801
  Apr 23 18:17:36.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename webhook @ 04/23/23 18:17:36.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:17:36.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:17:36.825
  STEP: Setting up server cert @ 04/23/23 18:17:36.856
  E0423 18:17:37.271545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/23/23 18:17:37.278
  STEP: Deploying the webhook pod @ 04/23/23 18:17:37.29
  STEP: Wait for the deployment to be ready @ 04/23/23 18:17:37.314
  Apr 23 18:17:37.336: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0423 18:17:38.272171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:39.272383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/23/23 18:17:39.35
  STEP: Verifying the service has paired with the endpoint @ 04/23/23 18:17:39.368
  E0423 18:17:40.273078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:17:40.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/23/23 18:17:40.373
  STEP: create a pod @ 04/23/23 18:17:40.392
  E0423 18:17:41.273096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:42.273208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/23/23 18:17:42.413
  Apr 23 18:17:42.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3446369140 --namespace=webhook-8712 attach --namespace=webhook-8712 to-be-attached-pod -i -c=container1'
  Apr 23 18:17:42.563: INFO: rc: 1
  Apr 23 18:17:42.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8712" for this suite. @ 04/23/23 18:17:42.66
  STEP: Destroying namespace "webhook-markers-831" for this suite. @ 04/23/23 18:17:42.668
• [5.876 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/23/23 18:17:42.678
  Apr 23 18:17:42.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/23/23 18:17:42.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:17:42.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:17:42.7
  STEP: creating @ 04/23/23 18:17:42.704
  STEP: getting @ 04/23/23 18:17:42.725
  STEP: listing @ 04/23/23 18:17:42.732
  STEP: deleting @ 04/23/23 18:17:42.736
  Apr 23 18:17:42.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-4107" for this suite. @ 04/23/23 18:17:42.763
• [0.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/23/23 18:17:42.773
  Apr 23 18:17:42.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename downward-api @ 04/23/23 18:17:42.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:17:42.796
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:17:42.799
  STEP: Creating a pod to test downward api env vars @ 04/23/23 18:17:42.813
  E0423 18:17:43.274167      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:44.274271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:45.275336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:46.275436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/23/23 18:17:46.842
  Apr 23 18:17:46.846: INFO: Trying to get logs from node ip-172-31-70-241 pod downward-api-e34145de-0b9c-4305-a305-5373847ced7c container dapi-container: <nil>
  STEP: delete the pod @ 04/23/23 18:17:46.858
  Apr 23 18:17:46.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9514" for this suite. @ 04/23/23 18:17:46.885
• [4.121 seconds]
------------------------------
SSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/23/23 18:17:46.895
  Apr 23 18:17:46.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename prestop @ 04/23/23 18:17:46.896
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:17:46.911
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:17:46.913
  STEP: Creating server pod server in namespace prestop-2758 @ 04/23/23 18:17:46.918
  STEP: Waiting for pods to come up. @ 04/23/23 18:17:46.936
  E0423 18:17:47.275976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:48.276216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-2758 @ 04/23/23 18:17:48.953
  E0423 18:17:49.276328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:50.276498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 04/23/23 18:17:50.978
  E0423 18:17:51.277240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:52.277664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:53.277768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:54.277859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:55.278157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:17:55.995: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 23 18:17:55.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/23/23 18:17:56
  STEP: Destroying namespace "prestop-2758" for this suite. @ 04/23/23 18:17:56.017
• [9.132 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/23/23 18:17:56.028
  Apr 23 18:17:56.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/23/23 18:17:56.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/23/23 18:17:56.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/23/23 18:17:56.053
  Apr 23 18:17:56.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3446369140
  E0423 18:17:56.278612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:57.278871      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0423 18:17:58.279307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 23 18:17:59.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-996" for this suite. @ 04/23/23 18:17:59.194
• [3.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 23 18:17:59.205: INFO: Running AfterSuite actions on node 1
  Apr 23 18:17:59.205: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.054 seconds]
------------------------------

Ran 378 of 7207 Specs in 6031.882 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
  E0423 18:17:59.279575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
PASS

Ginkgo ran 1 suite in 1h40m32.337623503s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

