  I0424 03:39:56.870801      14 e2e.go:117] Starting e2e run "48ff365e-287c-4da3-a1bd-7e72510f2298" on Ginkgo node 1
  Apr 24 03:39:56.941: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682307596 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 24 03:39:57.282: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:39:57.292: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 24 03:39:57.383: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 24 03:39:57.407: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
  Apr 24 03:39:57.408: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
  Apr 24 03:39:57.408: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Apr 24 03:39:57.409: INFO: e2e test version: v1.27.1
  Apr 24 03:39:57.411: INFO: kube-apiserver version: v1.27.1
  Apr 24 03:39:57.411: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:39:57.426: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/24/23 03:39:57.905
  Apr 24 03:39:57.905: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename ingressclass @ 04/24/23 03:39:57.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:39:57.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:39:57.943
  STEP: getting /apis @ 04/24/23 03:39:57.949
  STEP: getting /apis/networking.k8s.io @ 04/24/23 03:39:57.958
  STEP: getting /apis/networking.k8s.iov1 @ 04/24/23 03:39:57.96
  STEP: creating @ 04/24/23 03:39:57.962
  STEP: getting @ 04/24/23 03:39:57.988
  STEP: listing @ 04/24/23 03:39:57.992
  STEP: watching @ 04/24/23 03:39:57.999
  Apr 24 03:39:57.999: INFO: starting watch
  STEP: patching @ 04/24/23 03:39:58.001
  STEP: updating @ 04/24/23 03:39:58.01
  Apr 24 03:39:58.023: INFO: waiting for watch events with expected annotations
  Apr 24 03:39:58.023: INFO: saw patched and updated annotations
  STEP: deleting @ 04/24/23 03:39:58.024
  STEP: deleting a collection @ 04/24/23 03:39:58.042
  Apr 24 03:39:58.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-4219" for this suite. @ 04/24/23 03:39:58.075
• [0.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/24/23 03:39:58.094
  Apr 24 03:39:58.095: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 03:39:58.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:39:58.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:39:58.125
  Apr 24 03:39:58.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5913" for this suite. @ 04/24/23 03:39:58.199
• [0.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/24/23 03:39:58.215
  Apr 24 03:39:58.215: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename subjectreview @ 04/24/23 03:39:58.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:39:58.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:39:58.247
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-6591" @ 04/24/23 03:39:58.252
  Apr 24 03:39:58.265: INFO: saUsername: "system:serviceaccount:subjectreview-6591:e2e"
  Apr 24 03:39:58.265: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-6591"}
  Apr 24 03:39:58.265: INFO: saUID: "5f200a64-87fb-421c-a353-3440e6e0337f"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-6591:e2e" @ 04/24/23 03:39:58.265
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-6591:e2e" @ 04/24/23 03:39:58.266
  Apr 24 03:39:58.270: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-6591:e2e" api 'list' configmaps in "subjectreview-6591" namespace @ 04/24/23 03:39:58.271
  Apr 24 03:39:58.274: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-6591:e2e" @ 04/24/23 03:39:58.274
  Apr 24 03:39:58.277: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Apr 24 03:39:58.277: INFO: LocalSubjectAccessReview has been verified
  Apr 24 03:39:58.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-6591" for this suite. @ 04/24/23 03:39:58.285
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/24/23 03:39:58.304
  Apr 24 03:39:58.304: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replicaset @ 04/24/23 03:39:58.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:39:58.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:39:58.332
  Apr 24 03:39:58.335: INFO: Creating ReplicaSet my-hostname-basic-1da3eeee-e6b9-47a6-a53e-7525afa24717
  Apr 24 03:39:58.348: INFO: Pod name my-hostname-basic-1da3eeee-e6b9-47a6-a53e-7525afa24717: Found 0 pods out of 1
  Apr 24 03:40:03.354: INFO: Pod name my-hostname-basic-1da3eeee-e6b9-47a6-a53e-7525afa24717: Found 1 pods out of 1
  Apr 24 03:40:03.354: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1da3eeee-e6b9-47a6-a53e-7525afa24717" is running
  Apr 24 03:40:09.399: INFO: Pod "my-hostname-basic-1da3eeee-e6b9-47a6-a53e-7525afa24717-p7qf9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-24 03:39:58 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-24 03:39:58 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-1da3eeee-e6b9-47a6-a53e-7525afa24717]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-24 03:39:58 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-1da3eeee-e6b9-47a6-a53e-7525afa24717]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-24 03:39:58 +0000 UTC Reason: Message:}])
  Apr 24 03:40:09.399: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/24/23 03:40:09.399
  Apr 24 03:40:09.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-158" for this suite. @ 04/24/23 03:40:09.431
• [11.155 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/24/23 03:40:09.46
  Apr 24 03:40:09.460: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename field-validation @ 04/24/23 03:40:09.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:40:09.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:40:09.497
  STEP: apply creating a deployment @ 04/24/23 03:40:09.507
  Apr 24 03:40:09.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4691" for this suite. @ 04/24/23 03:40:09.541
• [0.091 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/24/23 03:40:09.552
  Apr 24 03:40:09.552: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-webhook @ 04/24/23 03:40:09.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:40:09.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:40:09.582
  STEP: Setting up server cert @ 04/24/23 03:40:09.586
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/24/23 03:40:10.411
  STEP: Deploying the custom resource conversion webhook pod @ 04/24/23 03:40:10.422
  STEP: Wait for the deployment to be ready @ 04/24/23 03:40:10.438
  Apr 24 03:40:10.458: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/24/23 03:40:12.484
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 03:40:12.506
  Apr 24 03:40:13.507: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 24 03:40:13.530: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Creating a v1 custom resource @ 04/24/23 03:40:16.36
  STEP: v2 custom resource should be converted @ 04/24/23 03:40:16.378
  Apr 24 03:40:16.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-4936" for this suite. @ 04/24/23 03:40:17.056
• [7.531 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/24/23 03:40:17.084
  Apr 24 03:40:17.084: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 03:40:17.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:40:17.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:40:17.132
  STEP: creating a Pod with a static label @ 04/24/23 03:40:17.147
  STEP: watching for Pod to be ready @ 04/24/23 03:40:17.159
  Apr 24 03:40:17.165: INFO: observed Pod pod-test in namespace pods-8193 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 24 03:40:17.167: INFO: observed Pod pod-test in namespace pods-8193 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:17 +0000 UTC  }]
  Apr 24 03:40:17.199: INFO: observed Pod pod-test in namespace pods-8193 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:17 +0000 UTC  }]
  Apr 24 03:40:19.094: INFO: Found Pod pod-test in namespace pods-8193 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 03:40:17 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/24/23 03:40:19.103
  STEP: getting the Pod and ensuring that it's patched @ 04/24/23 03:40:19.13
  STEP: replacing the Pod's status Ready condition to False @ 04/24/23 03:40:19.14
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/24/23 03:40:19.157
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/24/23 03:40:19.157
  STEP: watching for the Pod to be deleted @ 04/24/23 03:40:19.176
  Apr 24 03:40:19.178: INFO: observed event type MODIFIED
  Apr 24 03:40:21.101: INFO: observed event type MODIFIED
  Apr 24 03:40:21.307: INFO: observed event type MODIFIED
  Apr 24 03:40:22.128: INFO: observed event type MODIFIED
  Apr 24 03:40:22.172: INFO: observed event type MODIFIED
  Apr 24 03:40:22.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8193" for this suite. @ 04/24/23 03:40:22.2
• [5.135 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/24/23 03:40:22.22
  Apr 24 03:40:22.221: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 03:40:22.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:40:22.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:40:22.259
  STEP: creating pod @ 04/24/23 03:40:22.265
  Apr 24 03:40:24.314: INFO: Pod pod-hostip-e8823ea9-e5ca-47b1-9492-999c8d389374 has hostIP: 192.168.121.18
  Apr 24 03:40:24.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3662" for this suite. @ 04/24/23 03:40:24.323
• [2.122 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/24/23 03:40:24.344
  Apr 24 03:40:24.344: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename cronjob @ 04/24/23 03:40:24.359
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:40:24.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:40:24.444
  STEP: Creating a cronjob @ 04/24/23 03:40:24.448
  STEP: Ensuring more than one job is running at a time @ 04/24/23 03:40:24.459
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/24/23 03:42:00.467
  STEP: Removing cronjob @ 04/24/23 03:42:00.482
  Apr 24 03:42:00.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4280" for this suite. @ 04/24/23 03:42:00.513
• [96.183 seconds]
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/24/23 03:42:00.528
  Apr 24 03:42:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 03:42:00.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:42:00.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:42:00.604
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-2632 @ 04/24/23 03:42:00.611
  STEP: changing the ExternalName service to type=ClusterIP @ 04/24/23 03:42:00.619
  STEP: creating replication controller externalname-service in namespace services-2632 @ 04/24/23 03:42:00.644
  I0424 03:42:00.699463      14 runners.go:194] Created replication controller with name: externalname-service, namespace: services-2632, replica count: 2
  I0424 03:42:03.750926      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0424 03:42:06.753319      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0424 03:42:09.754309      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0424 03:42:12.755209      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 03:42:12.755: INFO: Creating new exec pod
  Apr 24 03:42:15.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-2632 exec execpodmpvpb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 24 03:42:16.265: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 24 03:42:16.265: INFO: stdout: ""
  Apr 24 03:42:17.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-2632 exec execpodmpvpb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 24 03:42:17.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 24 03:42:17.566: INFO: stdout: "externalname-service-vqn8k"
  Apr 24 03:42:17.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-2632 exec execpodmpvpb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.12.42 80'
  Apr 24 03:42:17.810: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.12.42 80\nConnection to 10.233.12.42 80 port [tcp/http] succeeded!\n"
  Apr 24 03:42:17.810: INFO: stdout: "externalname-service-jmt57"
  Apr 24 03:42:17.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 03:42:17.818: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-2632" for this suite. @ 04/24/23 03:42:17.866
• [17.354 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/24/23 03:42:17.884
  Apr 24 03:42:17.885: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/24/23 03:42:17.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:42:17.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:42:17.922
  STEP: Setting up the test @ 04/24/23 03:42:17.931
  STEP: Creating hostNetwork=false pod @ 04/24/23 03:42:17.931
  STEP: Creating hostNetwork=true pod @ 04/24/23 03:42:21.978
  STEP: Running the test @ 04/24/23 03:42:32.051
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/24/23 03:42:32.052
  Apr 24 03:42:32.052: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.053: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.055: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.055: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 24 03:42:32.195: INFO: Exec stderr: ""
  Apr 24 03:42:32.196: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.196: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.198: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.198: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 24 03:42:32.300: INFO: Exec stderr: ""
  Apr 24 03:42:32.301: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.301: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.303: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.303: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 24 03:42:32.453: INFO: Exec stderr: ""
  Apr 24 03:42:32.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.453: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.454: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.454: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 24 03:42:32.559: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/24/23 03:42:32.559
  Apr 24 03:42:32.559: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.559: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.562: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.563: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 24 03:42:32.661: INFO: Exec stderr: ""
  Apr 24 03:42:32.661: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.661: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.664: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.665: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 24 03:42:32.769: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/24/23 03:42:32.77
  Apr 24 03:42:32.770: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.772: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.772: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 24 03:42:32.887: INFO: Exec stderr: ""
  Apr 24 03:42:32.887: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.887: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.891: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.892: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 24 03:42:32.987: INFO: Exec stderr: ""
  Apr 24 03:42:32.988: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:32.988: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:32.990: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:32.990: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 24 03:42:33.091: INFO: Exec stderr: ""
  Apr 24 03:42:33.092: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4789 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:42:33.092: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:33.093: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:42:33.093: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 24 03:42:33.176: INFO: Exec stderr: ""
  Apr 24 03:42:33.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-4789" for this suite. @ 04/24/23 03:42:33.191
• [15.320 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/24/23 03:42:33.206
  Apr 24 03:42:33.206: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/24/23 03:42:33.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:42:33.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:42:33.244
  Apr 24 03:42:33.251: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:42:34.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4801" for this suite. @ 04/24/23 03:42:34.32
• [1.125 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 04/24/23 03:42:34.335
  Apr 24 03:42:34.335: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename statefulset @ 04/24/23 03:42:34.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:42:34.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:42:34.369
  STEP: Creating service test in namespace statefulset-1249 @ 04/24/23 03:42:34.373
  STEP: Creating statefulset ss in namespace statefulset-1249 @ 04/24/23 03:42:34.385
  Apr 24 03:42:34.420: INFO: Found 0 stateful pods, waiting for 1
  Apr 24 03:42:44.441: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/24/23 03:42:44.521
  STEP: updating a scale subresource @ 04/24/23 03:42:44.526
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/24/23 03:42:44.541
  STEP: Patch a scale subresource @ 04/24/23 03:42:44.546
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/24/23 03:42:44.566
  Apr 24 03:42:44.574: INFO: Deleting all statefulset in ns statefulset-1249
  Apr 24 03:42:44.579: INFO: Scaling statefulset ss to 0
  Apr 24 03:42:54.644: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 03:42:54.651: INFO: Deleting statefulset ss
  Apr 24 03:42:54.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1249" for this suite. @ 04/24/23 03:42:54.711
• [20.390 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/24/23 03:42:54.733
  Apr 24 03:42:54.733: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 03:42:54.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:42:54.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:42:54.767
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9141 @ 04/24/23 03:42:54.772
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/24/23 03:42:54.789
  STEP: creating service externalsvc in namespace services-9141 @ 04/24/23 03:42:54.79
  STEP: creating replication controller externalsvc in namespace services-9141 @ 04/24/23 03:42:54.821
  I0424 03:42:54.835229      14 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9141, replica count: 2
  I0424 03:42:57.886740      14 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/24/23 03:42:57.898
  Apr 24 03:42:57.925: INFO: Creating new exec pod
  Apr 24 03:42:59.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-9141 exec execpod9w95z -- /bin/sh -x -c nslookup clusterip-service.services-9141.svc.cluster.local'
  Apr 24 03:43:00.283: INFO: stderr: "+ nslookup clusterip-service.services-9141.svc.cluster.local\n"
  Apr 24 03:43:00.283: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-9141.svc.cluster.local\tcanonical name = externalsvc.services-9141.svc.cluster.local.\nName:\texternalsvc.services-9141.svc.cluster.local\nAddress: 10.233.7.78\n\n"
  Apr 24 03:43:00.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9141, will wait for the garbage collector to delete the pods @ 04/24/23 03:43:00.29
  Apr 24 03:43:00.361: INFO: Deleting ReplicationController externalsvc took: 14.282781ms
  Apr 24 03:43:00.462: INFO: Terminating ReplicationController externalsvc pods took: 100.651756ms
  Apr 24 03:43:02.802: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-9141" for this suite. @ 04/24/23 03:43:02.826
• [8.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/24/23 03:43:02.848
  Apr 24 03:43:02.848: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 03:43:02.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:43:02.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:43:02.873
  STEP: Creating configMap with name configmap-test-upd-6c6ccafb-15f1-4bbc-90e3-6686336d79aa @ 04/24/23 03:43:02.888
  STEP: Creating the pod @ 04/24/23 03:43:02.899
  STEP: Waiting for pod with text data @ 04/24/23 03:43:04.932
  STEP: Waiting for pod with binary data @ 04/24/23 03:43:04.968
  Apr 24 03:43:04.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-508" for this suite. @ 04/24/23 03:43:04.988
• [2.150 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/24/23 03:43:05
  Apr 24 03:43:05.000: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 03:43:05.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:43:05.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:43:05.031
  STEP: Creating configMap with name projected-configmap-test-volume-4fc03203-b285-46a3-86cc-30b4c8eb8fa3 @ 04/24/23 03:43:05.035
  STEP: Creating a pod to test consume configMaps @ 04/24/23 03:43:05.04
  STEP: Saw pod success @ 04/24/23 03:43:09.081
  Apr 24 03:43:09.087: INFO: Trying to get logs from node aeveeng9ieph-1 pod pod-projected-configmaps-86050a92-264d-4a25-b5bf-b9bfa4a75350 container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 03:43:09.127
  Apr 24 03:43:09.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3058" for this suite. @ 04/24/23 03:43:09.163
• [4.174 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/24/23 03:43:09.174
  Apr 24 03:43:09.174: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 03:43:09.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:43:09.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:43:09.201
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/24/23 03:43:09.205
  STEP: Saw pod success @ 04/24/23 03:43:13.246
  Apr 24 03:43:13.253: INFO: Trying to get logs from node aeveeng9ieph-1 pod pod-b59ffc3c-b1b0-46b1-94f6-c90293538f2c container test-container: <nil>
  STEP: delete the pod @ 04/24/23 03:43:13.267
  Apr 24 03:43:13.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-348" for this suite. @ 04/24/23 03:43:13.31
• [4.152 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/24/23 03:43:13.33
  Apr 24 03:43:13.330: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 03:43:13.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:43:13.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:43:13.361
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 03:43:13.365
  STEP: Saw pod success @ 04/24/23 03:43:17.416
  Apr 24 03:43:17.425: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-5c728734-48b8-46c5-a946-895cedab3798 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 03:43:17.439
  Apr 24 03:43:17.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7853" for this suite. @ 04/24/23 03:43:17.476
• [4.165 seconds]
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/24/23 03:43:17.496
  Apr 24 03:43:17.496: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-preemption @ 04/24/23 03:43:17.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:43:17.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:43:17.533
  Apr 24 03:43:17.566: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 24 03:44:17.626: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/24/23 03:44:17.633
  Apr 24 03:44:17.674: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 24 03:44:17.687: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 24 03:44:17.753: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 24 03:44:17.766: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 24 03:44:17.818: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 24 03:44:17.830: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/24/23 03:44:17.831
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/24/23 03:44:21.899
  Apr 24 03:44:26.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9868" for this suite. @ 04/24/23 03:44:26.113
• [68.630 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/24/23 03:44:26.127
  Apr 24 03:44:26.127: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename gc @ 04/24/23 03:44:26.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:26.155
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:26.162
  Apr 24 03:44:26.237: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"549deab4-a998-40a5-9d81-005555f7aeb5", Controller:(*bool)(0xc00566603e), BlockOwnerDeletion:(*bool)(0xc00566603f)}}
  Apr 24 03:44:26.259: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0bc77457-e34d-436a-ba89-9893acd61f18", Controller:(*bool)(0xc0056662de), BlockOwnerDeletion:(*bool)(0xc0056662df)}}
  Apr 24 03:44:26.271: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ba96aa25-66ef-4b3c-b7fa-cb15c71d64cc", Controller:(*bool)(0xc005666576), BlockOwnerDeletion:(*bool)(0xc005666577)}}
  Apr 24 03:44:31.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4333" for this suite. @ 04/24/23 03:44:31.297
• [5.178 seconds]
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/24/23 03:44:31.306
  Apr 24 03:44:31.306: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename security-context-test @ 04/24/23 03:44:31.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:31.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:31.334
  Apr 24 03:44:35.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4394" for this suite. @ 04/24/23 03:44:35.388
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/24/23 03:44:35.415
  Apr 24 03:44:35.415: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename conformance-tests @ 04/24/23 03:44:35.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:35.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:35.453
  STEP: Getting node addresses @ 04/24/23 03:44:35.466
  Apr 24 03:44:35.467: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 24 03:44:35.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-1491" for this suite. @ 04/24/23 03:44:35.483
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/24/23 03:44:35.503
  Apr 24 03:44:35.503: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 03:44:35.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:35.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:35.53
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 03:44:35.537
  STEP: Saw pod success @ 04/24/23 03:44:39.586
  Apr 24 03:44:39.595: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-4aea9fa0-1ed9-4ade-87f1-8388e72928d8 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 03:44:39.622
  Apr 24 03:44:39.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2114" for this suite. @ 04/24/23 03:44:39.665
• [4.173 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/24/23 03:44:39.677
  Apr 24 03:44:39.678: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 03:44:39.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:39.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:39.757
  STEP: creating the pod @ 04/24/23 03:44:39.778
  STEP: submitting the pod to kubernetes @ 04/24/23 03:44:39.779
  W0424 03:44:39.808966      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 04/24/23 03:44:41.833
  STEP: updating the pod @ 04/24/23 03:44:41.838
  Apr 24 03:44:42.359: INFO: Successfully updated pod "pod-update-activedeadlineseconds-dcd52fbe-fba8-4c53-8f24-20375bc95098"
  Apr 24 03:44:46.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2501" for this suite. @ 04/24/23 03:44:46.387
• [6.720 seconds]
------------------------------
S
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/24/23 03:44:46.399
  Apr 24 03:44:46.399: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 03:44:46.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:46.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:46.436
  STEP: creating a ConfigMap @ 04/24/23 03:44:46.442
  STEP: fetching the ConfigMap @ 04/24/23 03:44:46.454
  STEP: patching the ConfigMap @ 04/24/23 03:44:46.46
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/24/23 03:44:46.474
  STEP: deleting the ConfigMap by collection with a label selector @ 04/24/23 03:44:46.483
  STEP: listing all ConfigMaps in test namespace @ 04/24/23 03:44:46.494
  Apr 24 03:44:46.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5203" for this suite. @ 04/24/23 03:44:46.507
• [0.120 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/24/23 03:44:46.521
  Apr 24 03:44:46.521: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 03:44:46.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:46.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:46.556
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 03:44:46.562
  STEP: Saw pod success @ 04/24/23 03:44:50.605
  Apr 24 03:44:50.610: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-fab5224e-b789-4b12-8488-3a92799bc2c8 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 03:44:50.625
  Apr 24 03:44:50.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3293" for this suite. @ 04/24/23 03:44:50.685
• [4.174 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/24/23 03:44:50.698
  Apr 24 03:44:50.698: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename namespaces @ 04/24/23 03:44:50.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:50.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:50.735
  STEP: Updating Namespace "namespaces-8571" @ 04/24/23 03:44:50.739
  Apr 24 03:44:50.753: INFO: Namespace "namespaces-8571" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"48ff365e-287c-4da3-a1bd-7e72510f2298", "kubernetes.io/metadata.name":"namespaces-8571", "namespaces-8571":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 24 03:44:50.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8571" for this suite. @ 04/24/23 03:44:50.759
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/24/23 03:44:50.779
  Apr 24 03:44:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 03:44:50.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:50.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:50.807
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 03:44:50.812
  STEP: Saw pod success @ 04/24/23 03:44:54.862
  Apr 24 03:44:54.872: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-9abfd489-c8f3-48e5-95fb-3da7121f0d48 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 03:44:54.886
  Apr 24 03:44:54.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1792" for this suite. @ 04/24/23 03:44:54.919
• [4.151 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/24/23 03:44:54.932
  Apr 24 03:44:54.932: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 03:44:54.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:54.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:54.957
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/24/23 03:44:54.961
  STEP: Saw pod success @ 04/24/23 03:44:58.997
  Apr 24 03:44:59.003: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-32fcc90c-8945-4855-859d-bcc52e37d318 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 03:44:59.014
  Apr 24 03:44:59.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-564" for this suite. @ 04/24/23 03:44:59.048
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/24/23 03:44:59.064
  Apr 24 03:44:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 03:44:59.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:44:59.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:44:59.096
  STEP: Creating configMap with name configmap-test-volume-5348ff93-dc44-4e77-9e5f-1ed61d21245b @ 04/24/23 03:44:59.101
  STEP: Creating a pod to test consume configMaps @ 04/24/23 03:44:59.109
  STEP: Saw pod success @ 04/24/23 03:45:03.153
  Apr 24 03:45:03.158: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-26641b35-71d6-4034-bf0a-f3d9ee8cf669 container configmap-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 03:45:03.17
  Apr 24 03:45:03.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5080" for this suite. @ 04/24/23 03:45:03.211
• [4.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/24/23 03:45:03.225
  Apr 24 03:45:03.225: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename init-container @ 04/24/23 03:45:03.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:45:03.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:45:03.301
  STEP: creating the pod @ 04/24/23 03:45:03.308
  Apr 24 03:45:03.308: INFO: PodSpec: initContainers in spec.initContainers
  Apr 24 03:45:08.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8278" for this suite. @ 04/24/23 03:45:08.052
• [4.839 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/24/23 03:45:08.077
  Apr 24 03:45:08.077: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubelet-test @ 04/24/23 03:45:08.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:45:08.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:45:08.114
  Apr 24 03:45:10.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5537" for this suite. @ 04/24/23 03:45:10.179
• [2.113 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/24/23 03:45:10.194
  Apr 24 03:45:10.194: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 03:45:10.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:45:10.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:45:10.22
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/24/23 03:45:10.225
  STEP: Saw pod success @ 04/24/23 03:45:14.265
  Apr 24 03:45:14.272: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-9e9f45d5-835b-414d-aa8b-d860815e36a2 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 03:45:14.293
  Apr 24 03:45:14.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9525" for this suite. @ 04/24/23 03:45:14.323
• [4.139 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/24/23 03:45:14.334
  Apr 24 03:45:14.334: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replication-controller @ 04/24/23 03:45:14.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:45:14.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:45:14.363
  STEP: Given a ReplicationController is created @ 04/24/23 03:45:14.369
  STEP: When the matched label of one of its pods change @ 04/24/23 03:45:14.378
  Apr 24 03:45:14.385: INFO: Pod name pod-release: Found 0 pods out of 1
  Apr 24 03:45:19.398: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/24/23 03:45:19.424
  Apr 24 03:45:19.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1160" for this suite. @ 04/24/23 03:45:19.471
• [5.158 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/24/23 03:45:19.493
  Apr 24 03:45:19.493: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename var-expansion @ 04/24/23 03:45:19.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:45:19.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:45:19.581
  STEP: creating the pod with failed condition @ 04/24/23 03:45:19.586
  STEP: updating the pod @ 04/24/23 03:47:19.609
  Apr 24 03:47:20.135: INFO: Successfully updated pod "var-expansion-6e0ae32e-9277-4b65-9c25-35c675dac2b4"
  STEP: waiting for pod running @ 04/24/23 03:47:20.136
  STEP: deleting the pod gracefully @ 04/24/23 03:47:22.151
  Apr 24 03:47:22.152: INFO: Deleting pod "var-expansion-6e0ae32e-9277-4b65-9c25-35c675dac2b4" in namespace "var-expansion-2454"
  Apr 24 03:47:22.165: INFO: Wait up to 5m0s for pod "var-expansion-6e0ae32e-9277-4b65-9c25-35c675dac2b4" to be fully deleted
  Apr 24 03:47:54.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2454" for this suite. @ 04/24/23 03:47:54.356
• [154.877 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/24/23 03:47:54.382
  Apr 24 03:47:54.382: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 03:47:54.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:47:54.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:47:54.415
  STEP: creating a secret @ 04/24/23 03:47:54.42
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/24/23 03:47:54.428
  STEP: patching the secret @ 04/24/23 03:47:54.436
  STEP: deleting the secret using a LabelSelector @ 04/24/23 03:47:54.458
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/24/23 03:47:54.472
  Apr 24 03:47:54.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1583" for this suite. @ 04/24/23 03:47:54.487
• [0.116 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/24/23 03:47:54.503
  Apr 24 03:47:54.503: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 03:47:54.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:47:54.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:47:54.537
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/24/23 03:47:54.542
  STEP: Saw pod success @ 04/24/23 03:47:58.593
  Apr 24 03:47:58.604: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-8e90b09b-e729-426c-9255-ad50366133ee container test-container: <nil>
  STEP: delete the pod @ 04/24/23 03:47:58.64
  Apr 24 03:47:58.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4048" for this suite. @ 04/24/23 03:47:58.673
• [4.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/24/23 03:47:58.692
  Apr 24 03:47:58.692: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename tables @ 04/24/23 03:47:58.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:47:58.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:47:58.724
  Apr 24 03:47:58.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-369" for this suite. @ 04/24/23 03:47:58.739
• [0.057 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/24/23 03:47:58.751
  Apr 24 03:47:58.751: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 03:47:58.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:47:58.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:47:58.787
  STEP: Creating configMap with name configmap-test-volume-0eb16c57-7f2e-4d5a-a3d3-7d9bf914bd8d @ 04/24/23 03:47:58.795
  STEP: Creating a pod to test consume configMaps @ 04/24/23 03:47:58.805
  STEP: Saw pod success @ 04/24/23 03:48:02.841
  Apr 24 03:48:02.848: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-d1bdaf09-b7a5-4117-9729-aedd1526c18b container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 03:48:02.866
  Apr 24 03:48:02.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-733" for this suite. @ 04/24/23 03:48:02.909
• [4.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/24/23 03:48:02.931
  Apr 24 03:48:02.931: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename dns @ 04/24/23 03:48:02.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:48:02.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:48:02.961
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/24/23 03:48:02.966
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/24/23 03:48:02.966
  STEP: creating a pod to probe DNS @ 04/24/23 03:48:02.966
  STEP: submitting the pod to kubernetes @ 04/24/23 03:48:02.966
  STEP: retrieving the pod @ 04/24/23 03:48:25.102
  STEP: looking for the results for each expected name from probers @ 04/24/23 03:48:25.114
  Apr 24 03:48:25.152: INFO: DNS probes using dns-2196/dns-test-59293fb9-d41d-43c5-99fa-9c5aa12e00a1 succeeded

  Apr 24 03:48:25.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 03:48:25.163
  STEP: Destroying namespace "dns-2196" for this suite. @ 04/24/23 03:48:25.186
• [22.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/24/23 03:48:25.206
  Apr 24 03:48:25.207: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename svc-latency @ 04/24/23 03:48:25.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:48:25.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:48:25.242
  Apr 24 03:48:25.246: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-4348 @ 04/24/23 03:48:25.247
  I0424 03:48:25.256224      14 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4348, replica count: 1
  I0424 03:48:26.307560      14 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0424 03:48:27.308448      14 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 03:48:27.437: INFO: Created: latency-svc-bz2fj
  Apr 24 03:48:27.442: INFO: Got endpoints: latency-svc-bz2fj [32.70646ms]
  Apr 24 03:48:27.485: INFO: Created: latency-svc-5djjr
  Apr 24 03:48:27.496: INFO: Got endpoints: latency-svc-5djjr [53.797501ms]
  Apr 24 03:48:27.498: INFO: Created: latency-svc-vmjns
  Apr 24 03:48:27.509: INFO: Got endpoints: latency-svc-vmjns [65.83375ms]
  Apr 24 03:48:27.510: INFO: Created: latency-svc-c5rsk
  Apr 24 03:48:27.523: INFO: Got endpoints: latency-svc-c5rsk [78.955736ms]
  Apr 24 03:48:27.537: INFO: Created: latency-svc-wdmbg
  Apr 24 03:48:27.554: INFO: Created: latency-svc-k8rr2
  Apr 24 03:48:27.580: INFO: Got endpoints: latency-svc-wdmbg [136.437726ms]
  Apr 24 03:48:27.584: INFO: Got endpoints: latency-svc-k8rr2 [139.809984ms]
  Apr 24 03:48:27.588: INFO: Created: latency-svc-4nq9b
  Apr 24 03:48:27.608: INFO: Created: latency-svc-b4s2j
  Apr 24 03:48:27.617: INFO: Got endpoints: latency-svc-4nq9b [171.967028ms]
  Apr 24 03:48:27.628: INFO: Got endpoints: latency-svc-b4s2j [184.166803ms]
  Apr 24 03:48:27.631: INFO: Created: latency-svc-mjmj6
  Apr 24 03:48:27.648: INFO: Created: latency-svc-hvwh4
  Apr 24 03:48:27.649: INFO: Got endpoints: latency-svc-mjmj6 [205.089583ms]
  Apr 24 03:48:27.671: INFO: Got endpoints: latency-svc-hvwh4 [225.857821ms]
  Apr 24 03:48:27.672: INFO: Created: latency-svc-pfpvt
  Apr 24 03:48:27.673: INFO: Got endpoints: latency-svc-pfpvt [228.579839ms]
  Apr 24 03:48:27.679: INFO: Created: latency-svc-pjw25
  Apr 24 03:48:27.712: INFO: Got endpoints: latency-svc-pjw25 [267.169365ms]
  Apr 24 03:48:27.732: INFO: Created: latency-svc-5g57s
  Apr 24 03:48:27.751: INFO: Created: latency-svc-zvph7
  Apr 24 03:48:27.751: INFO: Got endpoints: latency-svc-5g57s [307.975419ms]
  Apr 24 03:48:27.758: INFO: Created: latency-svc-wc8sm
  Apr 24 03:48:27.762: INFO: Got endpoints: latency-svc-zvph7 [318.648194ms]
  Apr 24 03:48:27.776: INFO: Created: latency-svc-k84qn
  Apr 24 03:48:27.784: INFO: Got endpoints: latency-svc-wc8sm [339.800159ms]
  Apr 24 03:48:27.786: INFO: Got endpoints: latency-svc-k84qn [342.112783ms]
  Apr 24 03:48:27.798: INFO: Created: latency-svc-92qnf
  Apr 24 03:48:27.805: INFO: Got endpoints: latency-svc-92qnf [309.367106ms]
  Apr 24 03:48:27.813: INFO: Created: latency-svc-8bxf6
  Apr 24 03:48:27.824: INFO: Created: latency-svc-bjpvc
  Apr 24 03:48:27.832: INFO: Got endpoints: latency-svc-bjpvc [308.530054ms]
  Apr 24 03:48:27.833: INFO: Created: latency-svc-zb4vs
  Apr 24 03:48:27.835: INFO: Got endpoints: latency-svc-8bxf6 [325.806603ms]
  Apr 24 03:48:27.854: INFO: Created: latency-svc-qn47n
  Apr 24 03:48:27.856: INFO: Got endpoints: latency-svc-zb4vs [276.246091ms]
  Apr 24 03:48:27.863: INFO: Created: latency-svc-c9lmk
  Apr 24 03:48:27.870: INFO: Got endpoints: latency-svc-qn47n [286.256726ms]
  Apr 24 03:48:27.879: INFO: Got endpoints: latency-svc-c9lmk [262.195758ms]
  Apr 24 03:48:27.888: INFO: Created: latency-svc-mpkw7
  Apr 24 03:48:27.895: INFO: Got endpoints: latency-svc-mpkw7 [266.377491ms]
  Apr 24 03:48:27.902: INFO: Created: latency-svc-rlvdz
  Apr 24 03:48:27.904: INFO: Created: latency-svc-kkdf6
  Apr 24 03:48:27.909: INFO: Got endpoints: latency-svc-rlvdz [258.877199ms]
  Apr 24 03:48:27.921: INFO: Created: latency-svc-mhbtl
  Apr 24 03:48:27.927: INFO: Got endpoints: latency-svc-kkdf6 [254.069571ms]
  Apr 24 03:48:27.939: INFO: Got endpoints: latency-svc-mhbtl [268.008545ms]
  Apr 24 03:48:27.947: INFO: Created: latency-svc-9l7vf
  Apr 24 03:48:27.966: INFO: Got endpoints: latency-svc-9l7vf [253.740614ms]
  Apr 24 03:48:27.967: INFO: Created: latency-svc-6g5hw
  Apr 24 03:48:27.974: INFO: Got endpoints: latency-svc-6g5hw [222.596404ms]
  Apr 24 03:48:27.979: INFO: Created: latency-svc-hv644
  Apr 24 03:48:27.997: INFO: Got endpoints: latency-svc-hv644 [232.611851ms]
  Apr 24 03:48:28.007: INFO: Created: latency-svc-z7g88
  Apr 24 03:48:28.031: INFO: Created: latency-svc-bkl7f
  Apr 24 03:48:28.032: INFO: Got endpoints: latency-svc-z7g88 [247.676137ms]
  Apr 24 03:48:28.058: INFO: Created: latency-svc-5hbl9
  Apr 24 03:48:28.074: INFO: Got endpoints: latency-svc-bkl7f [287.489032ms]
  Apr 24 03:48:28.076: INFO: Created: latency-svc-tbxww
  Apr 24 03:48:28.085: INFO: Got endpoints: latency-svc-5hbl9 [279.800721ms]
  Apr 24 03:48:28.086: INFO: Got endpoints: latency-svc-tbxww [254.775741ms]
  Apr 24 03:48:28.102: INFO: Created: latency-svc-mjs5r
  Apr 24 03:48:28.128: INFO: Got endpoints: latency-svc-mjs5r [292.715591ms]
  Apr 24 03:48:28.137: INFO: Created: latency-svc-r967n
  Apr 24 03:48:28.149: INFO: Got endpoints: latency-svc-r967n [292.512052ms]
  Apr 24 03:48:28.154: INFO: Created: latency-svc-tb7x5
  Apr 24 03:48:28.163: INFO: Created: latency-svc-kp4d2
  Apr 24 03:48:28.172: INFO: Got endpoints: latency-svc-tb7x5 [301.589495ms]
  Apr 24 03:48:28.177: INFO: Created: latency-svc-8thmt
  Apr 24 03:48:28.189: INFO: Got endpoints: latency-svc-kp4d2 [310.084636ms]
  Apr 24 03:48:28.201: INFO: Got endpoints: latency-svc-8thmt [305.626213ms]
  Apr 24 03:48:28.213: INFO: Created: latency-svc-kf6jn
  Apr 24 03:48:28.218: INFO: Created: latency-svc-kpdnp
  Apr 24 03:48:28.224: INFO: Got endpoints: latency-svc-kf6jn [296.636181ms]
  Apr 24 03:48:28.231: INFO: Got endpoints: latency-svc-kpdnp [322.227122ms]
  Apr 24 03:48:28.239: INFO: Created: latency-svc-jkhvh
  Apr 24 03:48:28.256: INFO: Got endpoints: latency-svc-jkhvh [316.58578ms]
  Apr 24 03:48:28.258: INFO: Created: latency-svc-jlz5g
  Apr 24 03:48:28.265: INFO: Created: latency-svc-skpzv
  Apr 24 03:48:28.274: INFO: Got endpoints: latency-svc-jlz5g [307.588991ms]
  Apr 24 03:48:28.279: INFO: Created: latency-svc-9j96z
  Apr 24 03:48:28.283: INFO: Got endpoints: latency-svc-skpzv [308.05235ms]
  Apr 24 03:48:28.291: INFO: Created: latency-svc-q44rw
  Apr 24 03:48:28.303: INFO: Got endpoints: latency-svc-9j96z [305.273582ms]
  Apr 24 03:48:28.308: INFO: Created: latency-svc-2pq8f
  Apr 24 03:48:28.313: INFO: Got endpoints: latency-svc-q44rw [280.732392ms]
  Apr 24 03:48:28.328: INFO: Got endpoints: latency-svc-2pq8f [254.656546ms]
  Apr 24 03:48:28.331: INFO: Created: latency-svc-dxsxb
  Apr 24 03:48:28.349: INFO: Got endpoints: latency-svc-dxsxb [263.49882ms]
  Apr 24 03:48:28.351: INFO: Created: latency-svc-mnksh
  Apr 24 03:48:28.361: INFO: Got endpoints: latency-svc-mnksh [274.842591ms]
  Apr 24 03:48:28.365: INFO: Created: latency-svc-fwsl5
  Apr 24 03:48:28.440: INFO: Got endpoints: latency-svc-fwsl5 [311.976437ms]
  Apr 24 03:48:28.451: INFO: Created: latency-svc-l4jjg
  Apr 24 03:48:28.483: INFO: Created: latency-svc-vbsc8
  Apr 24 03:48:28.491: INFO: Got endpoints: latency-svc-l4jjg [341.71221ms]
  Apr 24 03:48:28.501: INFO: Got endpoints: latency-svc-vbsc8 [328.81415ms]
  Apr 24 03:48:28.517: INFO: Created: latency-svc-swzcz
  Apr 24 03:48:28.524: INFO: Created: latency-svc-wh74b
  Apr 24 03:48:28.535: INFO: Created: latency-svc-qn4dc
  Apr 24 03:48:28.544: INFO: Created: latency-svc-w7bdc
  Apr 24 03:48:28.551: INFO: Got endpoints: latency-svc-swzcz [361.716245ms]
  Apr 24 03:48:28.559: INFO: Created: latency-svc-mlwx4
  Apr 24 03:48:28.572: INFO: Created: latency-svc-d57rb
  Apr 24 03:48:28.578: INFO: Created: latency-svc-2x6dm
  Apr 24 03:48:28.591: INFO: Got endpoints: latency-svc-wh74b [390.224532ms]
  Apr 24 03:48:28.641: INFO: Got endpoints: latency-svc-qn4dc [417.140414ms]
  Apr 24 03:48:28.653: INFO: Created: latency-svc-vtf5l
  Apr 24 03:48:28.664: INFO: Created: latency-svc-q4pbh
  Apr 24 03:48:28.664: INFO: Created: latency-svc-xn9hv
  Apr 24 03:48:28.671: INFO: Created: latency-svc-pbvqn
  Apr 24 03:48:28.674: INFO: Created: latency-svc-bhdxc
  Apr 24 03:48:28.677: INFO: Created: latency-svc-vgf46
  Apr 24 03:48:28.679: INFO: Created: latency-svc-cr8nm
  Apr 24 03:48:28.683: INFO: Created: latency-svc-bt8l9
  Apr 24 03:48:28.683: INFO: Created: latency-svc-phszs
  Apr 24 03:48:28.684: INFO: Created: latency-svc-h4v8x
  Apr 24 03:48:28.688: INFO: Created: latency-svc-ppds6
  Apr 24 03:48:28.694: INFO: Got endpoints: latency-svc-w7bdc [462.053309ms]
  Apr 24 03:48:28.710: INFO: Created: latency-svc-x7rmp
  Apr 24 03:48:28.744: INFO: Got endpoints: latency-svc-mlwx4 [487.784634ms]
  Apr 24 03:48:28.759: INFO: Created: latency-svc-sx4hq
  Apr 24 03:48:28.796: INFO: Got endpoints: latency-svc-d57rb [521.740532ms]
  Apr 24 03:48:28.821: INFO: Created: latency-svc-57cvv
  Apr 24 03:48:28.846: INFO: Got endpoints: latency-svc-2x6dm [562.093801ms]
  Apr 24 03:48:28.865: INFO: Created: latency-svc-djtn4
  Apr 24 03:48:28.895: INFO: Got endpoints: latency-svc-vtf5l [591.220475ms]
  Apr 24 03:48:28.912: INFO: Created: latency-svc-pbzlv
  Apr 24 03:48:28.943: INFO: Got endpoints: latency-svc-xn9hv [629.688414ms]
  Apr 24 03:48:28.965: INFO: Created: latency-svc-wndps
  Apr 24 03:48:28.996: INFO: Got endpoints: latency-svc-q4pbh [667.345294ms]
  Apr 24 03:48:29.015: INFO: Created: latency-svc-pp78d
  Apr 24 03:48:29.045: INFO: Got endpoints: latency-svc-pbvqn [695.277544ms]
  Apr 24 03:48:29.066: INFO: Created: latency-svc-dvcnc
  Apr 24 03:48:29.094: INFO: Got endpoints: latency-svc-vgf46 [732.478559ms]
  Apr 24 03:48:29.115: INFO: Created: latency-svc-ztqrm
  Apr 24 03:48:29.142: INFO: Got endpoints: latency-svc-bhdxc [650.769434ms]
  Apr 24 03:48:29.158: INFO: Created: latency-svc-ngmmv
  Apr 24 03:48:29.201: INFO: Got endpoints: latency-svc-cr8nm [700.545642ms]
  Apr 24 03:48:29.219: INFO: Created: latency-svc-24v72
  Apr 24 03:48:29.242: INFO: Got endpoints: latency-svc-ppds6 [600.583417ms]
  Apr 24 03:48:29.260: INFO: Created: latency-svc-nn647
  Apr 24 03:48:29.300: INFO: Got endpoints: latency-svc-h4v8x [708.644606ms]
  Apr 24 03:48:29.315: INFO: Created: latency-svc-jkgkf
  Apr 24 03:48:29.342: INFO: Got endpoints: latency-svc-bt8l9 [902.11832ms]
  Apr 24 03:48:29.360: INFO: Created: latency-svc-jq2jf
  Apr 24 03:48:29.394: INFO: Got endpoints: latency-svc-phszs [841.699242ms]
  Apr 24 03:48:29.415: INFO: Created: latency-svc-9xq4m
  Apr 24 03:48:29.445: INFO: Got endpoints: latency-svc-x7rmp [750.351362ms]
  Apr 24 03:48:29.474: INFO: Created: latency-svc-ztvnl
  Apr 24 03:48:29.499: INFO: Got endpoints: latency-svc-sx4hq [755.152507ms]
  Apr 24 03:48:29.532: INFO: Created: latency-svc-mhvtp
  Apr 24 03:48:29.547: INFO: Got endpoints: latency-svc-57cvv [750.674065ms]
  Apr 24 03:48:29.567: INFO: Created: latency-svc-z728g
  Apr 24 03:48:29.596: INFO: Got endpoints: latency-svc-djtn4 [750.49457ms]
  Apr 24 03:48:29.611: INFO: Created: latency-svc-lmjn9
  Apr 24 03:48:29.643: INFO: Got endpoints: latency-svc-pbzlv [747.957065ms]
  Apr 24 03:48:29.664: INFO: Created: latency-svc-ccs9w
  Apr 24 03:48:29.696: INFO: Got endpoints: latency-svc-wndps [752.849355ms]
  Apr 24 03:48:29.722: INFO: Created: latency-svc-nkjzn
  Apr 24 03:48:29.748: INFO: Got endpoints: latency-svc-pp78d [750.619474ms]
  Apr 24 03:48:29.763: INFO: Created: latency-svc-ltxwc
  Apr 24 03:48:29.793: INFO: Got endpoints: latency-svc-dvcnc [748.249811ms]
  Apr 24 03:48:29.816: INFO: Created: latency-svc-dhzml
  Apr 24 03:48:29.842: INFO: Got endpoints: latency-svc-ztqrm [747.767007ms]
  Apr 24 03:48:29.867: INFO: Created: latency-svc-pdh62
  Apr 24 03:48:29.900: INFO: Got endpoints: latency-svc-ngmmv [757.809912ms]
  Apr 24 03:48:29.916: INFO: Created: latency-svc-jqt4l
  Apr 24 03:48:29.944: INFO: Got endpoints: latency-svc-24v72 [742.201217ms]
  Apr 24 03:48:29.972: INFO: Created: latency-svc-fhc6l
  Apr 24 03:48:30.009: INFO: Got endpoints: latency-svc-nn647 [766.526699ms]
  Apr 24 03:48:30.033: INFO: Created: latency-svc-fw5md
  Apr 24 03:48:30.040: INFO: Got endpoints: latency-svc-jkgkf [739.640885ms]
  Apr 24 03:48:30.057: INFO: Created: latency-svc-flqmx
  Apr 24 03:48:30.094: INFO: Got endpoints: latency-svc-jq2jf [751.487095ms]
  Apr 24 03:48:30.113: INFO: Created: latency-svc-sdq76
  Apr 24 03:48:30.143: INFO: Got endpoints: latency-svc-9xq4m [748.901921ms]
  Apr 24 03:48:30.161: INFO: Created: latency-svc-q85j4
  Apr 24 03:48:30.195: INFO: Got endpoints: latency-svc-ztvnl [750.199039ms]
  Apr 24 03:48:30.210: INFO: Created: latency-svc-rgmg4
  Apr 24 03:48:30.244: INFO: Got endpoints: latency-svc-mhvtp [744.494628ms]
  Apr 24 03:48:30.260: INFO: Created: latency-svc-jc7nb
  Apr 24 03:48:30.289: INFO: Got endpoints: latency-svc-z728g [742.054807ms]
  Apr 24 03:48:30.312: INFO: Created: latency-svc-vk9fn
  Apr 24 03:48:30.341: INFO: Got endpoints: latency-svc-lmjn9 [744.399432ms]
  Apr 24 03:48:30.356: INFO: Created: latency-svc-dknps
  Apr 24 03:48:30.394: INFO: Got endpoints: latency-svc-ccs9w [751.056476ms]
  Apr 24 03:48:30.412: INFO: Created: latency-svc-rdlmn
  Apr 24 03:48:30.439: INFO: Got endpoints: latency-svc-nkjzn [742.973224ms]
  Apr 24 03:48:30.459: INFO: Created: latency-svc-nghgf
  Apr 24 03:48:30.492: INFO: Got endpoints: latency-svc-ltxwc [744.750304ms]
  Apr 24 03:48:30.508: INFO: Created: latency-svc-7b9nk
  Apr 24 03:48:30.540: INFO: Got endpoints: latency-svc-dhzml [746.942748ms]
  Apr 24 03:48:30.554: INFO: Created: latency-svc-lz4ct
  Apr 24 03:48:30.596: INFO: Got endpoints: latency-svc-pdh62 [753.274514ms]
  Apr 24 03:48:30.611: INFO: Created: latency-svc-5p7mk
  Apr 24 03:48:30.647: INFO: Got endpoints: latency-svc-jqt4l [746.512084ms]
  Apr 24 03:48:30.667: INFO: Created: latency-svc-6q75t
  Apr 24 03:48:30.697: INFO: Got endpoints: latency-svc-fhc6l [753.193769ms]
  Apr 24 03:48:30.726: INFO: Created: latency-svc-k2x47
  Apr 24 03:48:30.740: INFO: Got endpoints: latency-svc-fw5md [730.966661ms]
  Apr 24 03:48:30.757: INFO: Created: latency-svc-jz8w6
  Apr 24 03:48:30.792: INFO: Got endpoints: latency-svc-flqmx [751.400988ms]
  Apr 24 03:48:30.815: INFO: Created: latency-svc-qkq5c
  Apr 24 03:48:30.844: INFO: Got endpoints: latency-svc-sdq76 [749.843585ms]
  Apr 24 03:48:30.862: INFO: Created: latency-svc-t7lj7
  Apr 24 03:48:30.893: INFO: Got endpoints: latency-svc-q85j4 [749.757553ms]
  Apr 24 03:48:30.909: INFO: Created: latency-svc-2xjbk
  Apr 24 03:48:30.946: INFO: Got endpoints: latency-svc-rgmg4 [750.84586ms]
  Apr 24 03:48:30.964: INFO: Created: latency-svc-5cbfw
  Apr 24 03:48:30.993: INFO: Got endpoints: latency-svc-jc7nb [749.433972ms]
  Apr 24 03:48:31.011: INFO: Created: latency-svc-cdkxs
  Apr 24 03:48:31.045: INFO: Got endpoints: latency-svc-vk9fn [755.194891ms]
  Apr 24 03:48:31.063: INFO: Created: latency-svc-mdbb5
  Apr 24 03:48:31.093: INFO: Got endpoints: latency-svc-dknps [752.391057ms]
  Apr 24 03:48:31.115: INFO: Created: latency-svc-jdtjl
  Apr 24 03:48:31.142: INFO: Got endpoints: latency-svc-rdlmn [747.816224ms]
  Apr 24 03:48:31.164: INFO: Created: latency-svc-cwrjs
  Apr 24 03:48:31.191: INFO: Got endpoints: latency-svc-nghgf [751.519707ms]
  Apr 24 03:48:31.216: INFO: Created: latency-svc-qj5m8
  Apr 24 03:48:31.255: INFO: Got endpoints: latency-svc-7b9nk [762.793335ms]
  Apr 24 03:48:31.289: INFO: Created: latency-svc-9gw5z
  Apr 24 03:48:31.291: INFO: Got endpoints: latency-svc-lz4ct [750.6098ms]
  Apr 24 03:48:31.312: INFO: Created: latency-svc-nhpdl
  Apr 24 03:48:31.345: INFO: Got endpoints: latency-svc-5p7mk [749.545346ms]
  Apr 24 03:48:31.360: INFO: Created: latency-svc-kvbwr
  Apr 24 03:48:31.393: INFO: Got endpoints: latency-svc-6q75t [746.029256ms]
  Apr 24 03:48:31.412: INFO: Created: latency-svc-284t7
  Apr 24 03:48:31.440: INFO: Got endpoints: latency-svc-k2x47 [742.223381ms]
  Apr 24 03:48:31.466: INFO: Created: latency-svc-zv87d
  Apr 24 03:48:31.506: INFO: Got endpoints: latency-svc-jz8w6 [765.608858ms]
  Apr 24 03:48:31.525: INFO: Created: latency-svc-vbv97
  Apr 24 03:48:31.552: INFO: Got endpoints: latency-svc-qkq5c [760.489007ms]
  Apr 24 03:48:31.570: INFO: Created: latency-svc-g5nhl
  Apr 24 03:48:31.597: INFO: Got endpoints: latency-svc-t7lj7 [753.106723ms]
  Apr 24 03:48:31.609: INFO: Created: latency-svc-99w4f
  Apr 24 03:48:31.644: INFO: Got endpoints: latency-svc-2xjbk [750.335117ms]
  Apr 24 03:48:31.662: INFO: Created: latency-svc-x6z8g
  Apr 24 03:48:31.695: INFO: Got endpoints: latency-svc-5cbfw [749.160689ms]
  Apr 24 03:48:31.716: INFO: Created: latency-svc-t8r7p
  Apr 24 03:48:31.746: INFO: Got endpoints: latency-svc-cdkxs [751.480291ms]
  Apr 24 03:48:31.761: INFO: Created: latency-svc-w2kcv
  Apr 24 03:48:31.795: INFO: Got endpoints: latency-svc-mdbb5 [750.034942ms]
  Apr 24 03:48:31.809: INFO: Created: latency-svc-w5kxx
  Apr 24 03:48:31.847: INFO: Got endpoints: latency-svc-jdtjl [752.374587ms]
  Apr 24 03:48:31.863: INFO: Created: latency-svc-bctps
  Apr 24 03:48:31.890: INFO: Got endpoints: latency-svc-cwrjs [747.076108ms]
  Apr 24 03:48:31.904: INFO: Created: latency-svc-s67hq
  Apr 24 03:48:31.946: INFO: Got endpoints: latency-svc-qj5m8 [754.569195ms]
  Apr 24 03:48:31.962: INFO: Created: latency-svc-4f6xl
  Apr 24 03:48:31.995: INFO: Got endpoints: latency-svc-9gw5z [736.085533ms]
  Apr 24 03:48:32.045: INFO: Got endpoints: latency-svc-nhpdl [754.182641ms]
  Apr 24 03:48:32.047: INFO: Created: latency-svc-6jnfk
  Apr 24 03:48:32.065: INFO: Created: latency-svc-bmfxb
  Apr 24 03:48:32.092: INFO: Got endpoints: latency-svc-kvbwr [746.710287ms]
  Apr 24 03:48:32.111: INFO: Created: latency-svc-c64xd
  Apr 24 03:48:32.146: INFO: Got endpoints: latency-svc-284t7 [752.165159ms]
  Apr 24 03:48:32.164: INFO: Created: latency-svc-59925
  Apr 24 03:48:32.198: INFO: Got endpoints: latency-svc-zv87d [758.551583ms]
  Apr 24 03:48:32.216: INFO: Created: latency-svc-2t92v
  Apr 24 03:48:32.246: INFO: Got endpoints: latency-svc-vbv97 [739.493557ms]
  Apr 24 03:48:32.265: INFO: Created: latency-svc-kkjb7
  Apr 24 03:48:32.300: INFO: Got endpoints: latency-svc-g5nhl [747.445896ms]
  Apr 24 03:48:32.318: INFO: Created: latency-svc-8x29x
  Apr 24 03:48:32.363: INFO: Got endpoints: latency-svc-99w4f [765.564219ms]
  Apr 24 03:48:32.464: INFO: Got endpoints: latency-svc-x6z8g [820.02356ms]
  Apr 24 03:48:32.468: INFO: Got endpoints: latency-svc-t8r7p [772.155172ms]
  Apr 24 03:48:32.468: INFO: Created: latency-svc-zl9zq
  Apr 24 03:48:32.483: INFO: Created: latency-svc-lfdm6
  Apr 24 03:48:32.499: INFO: Got endpoints: latency-svc-w2kcv [752.547125ms]
  Apr 24 03:48:32.515: INFO: Created: latency-svc-6r8vt
  Apr 24 03:48:32.525: INFO: Created: latency-svc-spsnn
  Apr 24 03:48:32.543: INFO: Got endpoints: latency-svc-w5kxx [748.045975ms]
  Apr 24 03:48:32.561: INFO: Created: latency-svc-6vl6z
  Apr 24 03:48:32.598: INFO: Got endpoints: latency-svc-bctps [751.180138ms]
  Apr 24 03:48:32.616: INFO: Created: latency-svc-6ss82
  Apr 24 03:48:32.642: INFO: Got endpoints: latency-svc-s67hq [751.477843ms]
  Apr 24 03:48:32.657: INFO: Created: latency-svc-jl42r
  Apr 24 03:48:32.691: INFO: Got endpoints: latency-svc-4f6xl [745.324127ms]
  Apr 24 03:48:32.711: INFO: Created: latency-svc-xkssj
  Apr 24 03:48:32.742: INFO: Got endpoints: latency-svc-6jnfk [746.649324ms]
  Apr 24 03:48:32.777: INFO: Created: latency-svc-tkjcc
  Apr 24 03:48:32.792: INFO: Got endpoints: latency-svc-bmfxb [746.527451ms]
  Apr 24 03:48:32.827: INFO: Created: latency-svc-kwmmk
  Apr 24 03:48:32.847: INFO: Got endpoints: latency-svc-c64xd [754.57321ms]
  Apr 24 03:48:32.867: INFO: Created: latency-svc-tntxd
  Apr 24 03:48:32.900: INFO: Got endpoints: latency-svc-59925 [753.777353ms]
  Apr 24 03:48:32.915: INFO: Created: latency-svc-b9ps2
  Apr 24 03:48:32.942: INFO: Got endpoints: latency-svc-2t92v [743.270803ms]
  Apr 24 03:48:32.958: INFO: Created: latency-svc-t2t82
  Apr 24 03:48:32.994: INFO: Got endpoints: latency-svc-kkjb7 [747.61163ms]
  Apr 24 03:48:33.010: INFO: Created: latency-svc-jhk4l
  Apr 24 03:48:33.044: INFO: Got endpoints: latency-svc-8x29x [743.819021ms]
  Apr 24 03:48:33.074: INFO: Created: latency-svc-q49w4
  Apr 24 03:48:33.093: INFO: Got endpoints: latency-svc-zl9zq [730.552602ms]
  Apr 24 03:48:33.108: INFO: Created: latency-svc-wgssq
  Apr 24 03:48:33.145: INFO: Got endpoints: latency-svc-lfdm6 [680.782041ms]
  Apr 24 03:48:33.163: INFO: Created: latency-svc-9lb8c
  Apr 24 03:48:33.198: INFO: Got endpoints: latency-svc-6r8vt [729.580513ms]
  Apr 24 03:48:33.215: INFO: Created: latency-svc-k2866
  Apr 24 03:48:33.242: INFO: Got endpoints: latency-svc-spsnn [743.467196ms]
  Apr 24 03:48:33.260: INFO: Created: latency-svc-nxgbw
  Apr 24 03:48:33.298: INFO: Got endpoints: latency-svc-6vl6z [754.483047ms]
  Apr 24 03:48:33.314: INFO: Created: latency-svc-fphtm
  Apr 24 03:48:33.340: INFO: Got endpoints: latency-svc-6ss82 [741.34839ms]
  Apr 24 03:48:33.355: INFO: Created: latency-svc-l6vbp
  Apr 24 03:48:33.394: INFO: Got endpoints: latency-svc-jl42r [752.633436ms]
  Apr 24 03:48:33.411: INFO: Created: latency-svc-qwrhc
  Apr 24 03:48:33.442: INFO: Got endpoints: latency-svc-xkssj [750.829695ms]
  Apr 24 03:48:33.462: INFO: Created: latency-svc-pp7vn
  Apr 24 03:48:33.496: INFO: Got endpoints: latency-svc-tkjcc [754.354593ms]
  Apr 24 03:48:33.518: INFO: Created: latency-svc-4xzsg
  Apr 24 03:48:33.541: INFO: Got endpoints: latency-svc-kwmmk [747.840963ms]
  Apr 24 03:48:33.559: INFO: Created: latency-svc-slfzj
  Apr 24 03:48:33.596: INFO: Got endpoints: latency-svc-tntxd [748.443629ms]
  Apr 24 03:48:33.611: INFO: Created: latency-svc-xlhmh
  Apr 24 03:48:33.644: INFO: Got endpoints: latency-svc-b9ps2 [744.278586ms]
  Apr 24 03:48:33.670: INFO: Created: latency-svc-s89kd
  Apr 24 03:48:33.698: INFO: Got endpoints: latency-svc-t2t82 [756.413054ms]
  Apr 24 03:48:33.715: INFO: Created: latency-svc-svdnj
  Apr 24 03:48:33.742: INFO: Got endpoints: latency-svc-jhk4l [747.149615ms]
  Apr 24 03:48:33.759: INFO: Created: latency-svc-z28fn
  Apr 24 03:48:33.797: INFO: Got endpoints: latency-svc-q49w4 [752.675382ms]
  Apr 24 03:48:33.815: INFO: Created: latency-svc-mns7p
  Apr 24 03:48:33.843: INFO: Got endpoints: latency-svc-wgssq [749.294963ms]
  Apr 24 03:48:33.861: INFO: Created: latency-svc-tql8j
  Apr 24 03:48:33.892: INFO: Got endpoints: latency-svc-9lb8c [746.704535ms]
  Apr 24 03:48:33.914: INFO: Created: latency-svc-b4swx
  Apr 24 03:48:33.941: INFO: Got endpoints: latency-svc-k2866 [743.487612ms]
  Apr 24 03:48:33.962: INFO: Created: latency-svc-fmtb8
  Apr 24 03:48:33.993: INFO: Got endpoints: latency-svc-nxgbw [750.940118ms]
  Apr 24 03:48:34.014: INFO: Created: latency-svc-ncdrb
  Apr 24 03:48:34.046: INFO: Got endpoints: latency-svc-fphtm [748.21597ms]
  Apr 24 03:48:34.063: INFO: Created: latency-svc-72x7m
  Apr 24 03:48:34.095: INFO: Got endpoints: latency-svc-l6vbp [755.026432ms]
  Apr 24 03:48:34.111: INFO: Created: latency-svc-f9jkt
  Apr 24 03:48:34.146: INFO: Got endpoints: latency-svc-qwrhc [750.373378ms]
  Apr 24 03:48:34.167: INFO: Created: latency-svc-j9j6p
  Apr 24 03:48:34.194: INFO: Got endpoints: latency-svc-pp7vn [752.153261ms]
  Apr 24 03:48:34.213: INFO: Created: latency-svc-ftkqk
  Apr 24 03:48:34.241: INFO: Got endpoints: latency-svc-4xzsg [744.508166ms]
  Apr 24 03:48:34.258: INFO: Created: latency-svc-pnfl2
  Apr 24 03:48:34.293: INFO: Got endpoints: latency-svc-slfzj [752.130268ms]
  Apr 24 03:48:34.311: INFO: Created: latency-svc-26qn7
  Apr 24 03:48:34.347: INFO: Got endpoints: latency-svc-xlhmh [750.945946ms]
  Apr 24 03:48:34.366: INFO: Created: latency-svc-74d5m
  Apr 24 03:48:34.393: INFO: Got endpoints: latency-svc-s89kd [748.623668ms]
  Apr 24 03:48:34.409: INFO: Created: latency-svc-gknlv
  Apr 24 03:48:34.442: INFO: Got endpoints: latency-svc-svdnj [743.15516ms]
  Apr 24 03:48:34.467: INFO: Created: latency-svc-8vdbr
  Apr 24 03:48:34.492: INFO: Got endpoints: latency-svc-z28fn [749.85807ms]
  Apr 24 03:48:34.512: INFO: Created: latency-svc-rml4b
  Apr 24 03:48:34.539: INFO: Got endpoints: latency-svc-mns7p [742.055506ms]
  Apr 24 03:48:34.556: INFO: Created: latency-svc-5pjmz
  Apr 24 03:48:34.590: INFO: Got endpoints: latency-svc-tql8j [746.337587ms]
  Apr 24 03:48:34.604: INFO: Created: latency-svc-f27jl
  Apr 24 03:48:34.643: INFO: Got endpoints: latency-svc-b4swx [750.622102ms]
  Apr 24 03:48:34.660: INFO: Created: latency-svc-rv7fv
  Apr 24 03:48:34.697: INFO: Got endpoints: latency-svc-fmtb8 [755.01416ms]
  Apr 24 03:48:34.716: INFO: Created: latency-svc-tstct
  Apr 24 03:48:34.747: INFO: Got endpoints: latency-svc-ncdrb [752.801734ms]
  Apr 24 03:48:34.764: INFO: Created: latency-svc-8bzcl
  Apr 24 03:48:34.790: INFO: Got endpoints: latency-svc-72x7m [743.283865ms]
  Apr 24 03:48:34.807: INFO: Created: latency-svc-hlk5n
  Apr 24 03:48:34.841: INFO: Got endpoints: latency-svc-f9jkt [746.097015ms]
  Apr 24 03:48:34.856: INFO: Created: latency-svc-22zwb
  Apr 24 03:48:34.894: INFO: Got endpoints: latency-svc-j9j6p [746.647703ms]
  Apr 24 03:48:34.914: INFO: Created: latency-svc-7482n
  Apr 24 03:48:34.947: INFO: Got endpoints: latency-svc-ftkqk [752.692438ms]
  Apr 24 03:48:34.964: INFO: Created: latency-svc-zdx8p
  Apr 24 03:48:34.996: INFO: Got endpoints: latency-svc-pnfl2 [755.238083ms]
  Apr 24 03:48:35.014: INFO: Created: latency-svc-l9h8k
  Apr 24 03:48:35.046: INFO: Got endpoints: latency-svc-26qn7 [752.616486ms]
  Apr 24 03:48:35.066: INFO: Created: latency-svc-xp7p7
  Apr 24 03:48:35.091: INFO: Got endpoints: latency-svc-74d5m [742.667033ms]
  Apr 24 03:48:35.109: INFO: Created: latency-svc-rr688
  Apr 24 03:48:35.147: INFO: Got endpoints: latency-svc-gknlv [753.597657ms]
  Apr 24 03:48:35.166: INFO: Created: latency-svc-dbwgp
  Apr 24 03:48:35.196: INFO: Got endpoints: latency-svc-8vdbr [753.811363ms]
  Apr 24 03:48:35.214: INFO: Created: latency-svc-rwgwj
  Apr 24 03:48:35.242: INFO: Got endpoints: latency-svc-rml4b [750.605938ms]
  Apr 24 03:48:35.260: INFO: Created: latency-svc-cbq9b
  Apr 24 03:48:35.290: INFO: Got endpoints: latency-svc-5pjmz [750.839717ms]
  Apr 24 03:48:35.343: INFO: Got endpoints: latency-svc-f27jl [753.2926ms]
  Apr 24 03:48:35.392: INFO: Got endpoints: latency-svc-rv7fv [748.413877ms]
  Apr 24 03:48:35.448: INFO: Got endpoints: latency-svc-tstct [750.929798ms]
  Apr 24 03:48:35.490: INFO: Got endpoints: latency-svc-8bzcl [743.383316ms]
  Apr 24 03:48:35.541: INFO: Got endpoints: latency-svc-hlk5n [750.548331ms]
  Apr 24 03:48:35.602: INFO: Got endpoints: latency-svc-22zwb [760.676527ms]
  Apr 24 03:48:35.642: INFO: Got endpoints: latency-svc-7482n [748.546333ms]
  Apr 24 03:48:35.698: INFO: Got endpoints: latency-svc-zdx8p [750.466206ms]
  Apr 24 03:48:35.752: INFO: Got endpoints: latency-svc-l9h8k [755.654158ms]
  Apr 24 03:48:35.794: INFO: Got endpoints: latency-svc-xp7p7 [747.838658ms]
  Apr 24 03:48:35.845: INFO: Got endpoints: latency-svc-rr688 [754.52935ms]
  Apr 24 03:48:35.893: INFO: Got endpoints: latency-svc-dbwgp [746.571758ms]
  Apr 24 03:48:35.944: INFO: Got endpoints: latency-svc-rwgwj [748.084726ms]
  Apr 24 03:48:35.993: INFO: Got endpoints: latency-svc-cbq9b [750.196168ms]
  Apr 24 03:48:35.994: INFO: Latencies: [53.797501ms 65.83375ms 78.955736ms 136.437726ms 139.809984ms 171.967028ms 184.166803ms 205.089583ms 222.596404ms 225.857821ms 228.579839ms 232.611851ms 247.676137ms 253.740614ms 254.069571ms 254.656546ms 254.775741ms 258.877199ms 262.195758ms 263.49882ms 266.377491ms 267.169365ms 268.008545ms 274.842591ms 276.246091ms 279.800721ms 280.732392ms 286.256726ms 287.489032ms 292.512052ms 292.715591ms 296.636181ms 301.589495ms 305.273582ms 305.626213ms 307.588991ms 307.975419ms 308.05235ms 308.530054ms 309.367106ms 310.084636ms 311.976437ms 316.58578ms 318.648194ms 322.227122ms 325.806603ms 328.81415ms 339.800159ms 341.71221ms 342.112783ms 361.716245ms 390.224532ms 417.140414ms 462.053309ms 487.784634ms 521.740532ms 562.093801ms 591.220475ms 600.583417ms 629.688414ms 650.769434ms 667.345294ms 680.782041ms 695.277544ms 700.545642ms 708.644606ms 729.580513ms 730.552602ms 730.966661ms 732.478559ms 736.085533ms 739.493557ms 739.640885ms 741.34839ms 742.054807ms 742.055506ms 742.201217ms 742.223381ms 742.667033ms 742.973224ms 743.15516ms 743.270803ms 743.283865ms 743.383316ms 743.467196ms 743.487612ms 743.819021ms 744.278586ms 744.399432ms 744.494628ms 744.508166ms 744.750304ms 745.324127ms 746.029256ms 746.097015ms 746.337587ms 746.512084ms 746.527451ms 746.571758ms 746.647703ms 746.649324ms 746.704535ms 746.710287ms 746.942748ms 747.076108ms 747.149615ms 747.445896ms 747.61163ms 747.767007ms 747.816224ms 747.838658ms 747.840963ms 747.957065ms 748.045975ms 748.084726ms 748.21597ms 748.249811ms 748.413877ms 748.443629ms 748.546333ms 748.623668ms 748.901921ms 749.160689ms 749.294963ms 749.433972ms 749.545346ms 749.757553ms 749.843585ms 749.85807ms 750.034942ms 750.196168ms 750.199039ms 750.335117ms 750.351362ms 750.373378ms 750.466206ms 750.49457ms 750.548331ms 750.605938ms 750.6098ms 750.619474ms 750.622102ms 750.674065ms 750.829695ms 750.839717ms 750.84586ms 750.929798ms 750.940118ms 750.945946ms 751.056476ms 751.180138ms 751.400988ms 751.477843ms 751.480291ms 751.487095ms 751.519707ms 752.130268ms 752.153261ms 752.165159ms 752.374587ms 752.391057ms 752.547125ms 752.616486ms 752.633436ms 752.675382ms 752.692438ms 752.801734ms 752.849355ms 753.106723ms 753.193769ms 753.274514ms 753.2926ms 753.597657ms 753.777353ms 753.811363ms 754.182641ms 754.354593ms 754.483047ms 754.52935ms 754.569195ms 754.57321ms 755.01416ms 755.026432ms 755.152507ms 755.194891ms 755.238083ms 755.654158ms 756.413054ms 757.809912ms 758.551583ms 760.489007ms 760.676527ms 762.793335ms 765.564219ms 765.608858ms 766.526699ms 772.155172ms 820.02356ms 841.699242ms 902.11832ms]
  Apr 24 03:48:35.994: INFO: 50 %ile: 746.649324ms
  Apr 24 03:48:35.995: INFO: 90 %ile: 754.57321ms
  Apr 24 03:48:35.995: INFO: 99 %ile: 841.699242ms
  Apr 24 03:48:35.996: INFO: Total sample count: 200
  Apr 24 03:48:35.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-4348" for this suite. @ 04/24/23 03:48:36.006
• [10.820 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/24/23 03:48:36.036
  Apr 24 03:48:36.036: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 03:48:36.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:48:36.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:48:36.068
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/24/23 03:48:36.074
  Apr 24 03:48:36.076: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:48:37.978: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:48:46.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8966" for this suite. @ 04/24/23 03:48:46.865
• [10.878 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/24/23 03:48:46.924
  Apr 24 03:48:46.924: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 03:48:46.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:48:46.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:48:46.973
  STEP: creating a collection of services @ 04/24/23 03:48:46.985
  Apr 24 03:48:46.985: INFO: Creating e2e-svc-a-fsfkd
  Apr 24 03:48:47.004: INFO: Creating e2e-svc-b-st27d
  Apr 24 03:48:47.031: INFO: Creating e2e-svc-c-9vdk4
  STEP: deleting service collection @ 04/24/23 03:48:47.06
  Apr 24 03:48:47.129: INFO: Collection of services has been deleted
  Apr 24 03:48:47.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8872" for this suite. @ 04/24/23 03:48:47.143
• [0.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/24/23 03:48:47.163
  Apr 24 03:48:47.164: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 03:48:47.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:48:47.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:48:47.202
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 03:48:47.207
  STEP: Saw pod success @ 04/24/23 03:48:51.269
  Apr 24 03:48:51.273: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-72551397-31c6-45d2-8724-1a314ad45502 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 03:48:51.301
  Apr 24 03:48:51.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5590" for this suite. @ 04/24/23 03:48:51.343
• [4.190 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/24/23 03:48:51.355
  Apr 24 03:48:51.355: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/24/23 03:48:51.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:48:51.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:48:51.38
  STEP: create the container to handle the HTTPGet hook request. @ 04/24/23 03:48:51.392
  STEP: create the pod with lifecycle hook @ 04/24/23 03:48:53.425
  STEP: check poststart hook @ 04/24/23 03:48:55.456
  STEP: delete the pod with lifecycle hook @ 04/24/23 03:48:55.517
  Apr 24 03:48:57.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4397" for this suite. @ 04/24/23 03:48:57.567
• [6.225 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/24/23 03:48:57.581
  Apr 24 03:48:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 03:48:57.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:48:57.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:48:57.611
  Apr 24 03:48:57.615: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: creating the pod @ 04/24/23 03:48:57.617
  STEP: submitting the pod to kubernetes @ 04/24/23 03:48:57.617
  Apr 24 03:48:59.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6443" for this suite. @ 04/24/23 03:48:59.684
• [2.116 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/24/23 03:48:59.699
  Apr 24 03:48:59.699: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename gc @ 04/24/23 03:48:59.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:48:59.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:48:59.735
  STEP: create the rc1 @ 04/24/23 03:48:59.744
  STEP: create the rc2 @ 04/24/23 03:48:59.752
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/24/23 03:49:05.798
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/24/23 03:49:09.366
  STEP: wait for the rc to be deleted @ 04/24/23 03:49:09.378
  Apr 24 03:49:14.475: INFO: 90 pods remaining
  Apr 24 03:49:14.475: INFO: 71 pods has nil DeletionTimestamp
  Apr 24 03:49:14.476: INFO: 
  Apr 24 03:49:19.490: INFO: 81 pods remaining
  Apr 24 03:49:19.491: INFO: 50 pods has nil DeletionTimestamp
  Apr 24 03:49:19.491: INFO: 
  STEP: Gathering metrics @ 04/24/23 03:49:24.501
  Apr 24 03:49:24.673: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 24 03:49:24.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-22lvx" in namespace "gc-6986"
  Apr 24 03:49:24.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-2652w" in namespace "gc-6986"
  Apr 24 03:49:24.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-27zbz" in namespace "gc-6986"
  Apr 24 03:49:24.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-2htz8" in namespace "gc-6986"
  Apr 24 03:49:24.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rx2t" in namespace "gc-6986"
  Apr 24 03:49:24.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z2vv" in namespace "gc-6986"
  Apr 24 03:49:24.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zsq5" in namespace "gc-6986"
  Apr 24 03:49:24.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-47nhn" in namespace "gc-6986"
  Apr 24 03:49:24.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-47txx" in namespace "gc-6986"
  Apr 24 03:49:25.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-4k6hh" in namespace "gc-6986"
  Apr 24 03:49:25.153: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xgtn" in namespace "gc-6986"
  Apr 24 03:49:25.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-58x57" in namespace "gc-6986"
  Apr 24 03:49:25.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-595gr" in namespace "gc-6986"
  Apr 24 03:49:25.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-597dp" in namespace "gc-6986"
  Apr 24 03:49:25.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gqq5" in namespace "gc-6986"
  Apr 24 03:49:25.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-5h7ls" in namespace "gc-6986"
  Apr 24 03:49:25.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xskd" in namespace "gc-6986"
  Apr 24 03:49:25.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-6l94j" in namespace "gc-6986"
  Apr 24 03:49:25.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mpfg" in namespace "gc-6986"
  Apr 24 03:49:25.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tp5h" in namespace "gc-6986"
  Apr 24 03:49:25.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-75rzc" in namespace "gc-6986"
  Apr 24 03:49:25.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-7phlh" in namespace "gc-6986"
  Apr 24 03:49:25.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tcnr" in namespace "gc-6986"
  Apr 24 03:49:26.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-7x5j2" in namespace "gc-6986"
  Apr 24 03:49:26.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-827wl" in namespace "gc-6986"
  Apr 24 03:49:26.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-86wkr" in namespace "gc-6986"
  Apr 24 03:49:26.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fx46" in namespace "gc-6986"
  Apr 24 03:49:26.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cgbm" in namespace "gc-6986"
  Apr 24 03:49:26.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-9m79b" in namespace "gc-6986"
  Apr 24 03:49:26.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vrnd" in namespace "gc-6986"
  Apr 24 03:49:26.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4j5z" in namespace "gc-6986"
  Apr 24 03:49:26.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6z7l" in namespace "gc-6986"
  Apr 24 03:49:26.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqg5r" in namespace "gc-6986"
  Apr 24 03:49:26.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-btwtt" in namespace "gc-6986"
  Apr 24 03:49:26.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-btxjl" in namespace "gc-6986"
  Apr 24 03:49:26.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv7bg" in namespace "gc-6986"
  Apr 24 03:49:26.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwddb" in namespace "gc-6986"
  Apr 24 03:49:26.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-c97tv" in namespace "gc-6986"
  Apr 24 03:49:26.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-cghdx" in namespace "gc-6986"
  Apr 24 03:49:27.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-d27rb" in namespace "gc-6986"
  Apr 24 03:49:27.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-dx52t" in namespace "gc-6986"
  Apr 24 03:49:27.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7zjk" in namespace "gc-6986"
  Apr 24 03:49:27.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj8gc" in namespace "gc-6986"
  Apr 24 03:49:27.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnq4r" in namespace "gc-6986"
  Apr 24 03:49:27.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnxxt" in namespace "gc-6986"
  Apr 24 03:49:27.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftfg2" in namespace "gc-6986"
  Apr 24 03:49:27.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzbvh" in namespace "gc-6986"
  Apr 24 03:49:27.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbbs8" in namespace "gc-6986"
  Apr 24 03:49:27.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-gk6tr" in namespace "gc-6986"
  Apr 24 03:49:27.627: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq5kd" in namespace "gc-6986"
  Apr 24 03:49:27.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6986" for this suite. @ 04/24/23 03:49:27.724
• [28.048 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 04/24/23 03:49:27.755
  Apr 24 03:49:27.756: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename statefulset @ 04/24/23 03:49:27.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:49:27.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:49:27.823
  STEP: Creating service test in namespace statefulset-7861 @ 04/24/23 03:49:27.833
  STEP: Creating statefulset ss in namespace statefulset-7861 @ 04/24/23 03:49:27.876
  Apr 24 03:49:27.904: INFO: Found 0 stateful pods, waiting for 1
  Apr 24 03:49:37.911: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/24/23 03:49:37.92
  STEP: Getting /status @ 04/24/23 03:49:37.934
  Apr 24 03:49:37.942: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/24/23 03:49:37.942
  Apr 24 03:49:37.954: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/24/23 03:49:37.954
  Apr 24 03:49:37.960: INFO: Observed &StatefulSet event: ADDED
  Apr 24 03:49:37.960: INFO: Found Statefulset ss in namespace statefulset-7861 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 24 03:49:37.961: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/24/23 03:49:37.961
  Apr 24 03:49:37.961: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 24 03:49:37.977: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/24/23 03:49:37.977
  Apr 24 03:49:37.980: INFO: Observed &StatefulSet event: ADDED
  Apr 24 03:49:37.981: INFO: Observed Statefulset ss in namespace statefulset-7861 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 24 03:49:37.981: INFO: Observed &StatefulSet event: MODIFIED
  Apr 24 03:49:37.982: INFO: Deleting all statefulset in ns statefulset-7861
  Apr 24 03:49:37.987: INFO: Scaling statefulset ss to 0
  Apr 24 03:49:48.024: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 03:49:48.027: INFO: Deleting statefulset ss
  Apr 24 03:49:48.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7861" for this suite. @ 04/24/23 03:49:48.067
• [20.326 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/24/23 03:49:48.085
  Apr 24 03:49:48.085: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 03:49:48.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:49:48.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:49:48.114
  STEP: creating service in namespace services-5789 @ 04/24/23 03:49:48.12
  STEP: creating service affinity-nodeport in namespace services-5789 @ 04/24/23 03:49:48.12
  STEP: creating replication controller affinity-nodeport in namespace services-5789 @ 04/24/23 03:49:48.147
  I0424 03:49:48.156905      14 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-5789, replica count: 3
  I0424 03:49:51.209234      14 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 03:49:51.240: INFO: Creating new exec pod
  Apr 24 03:49:54.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5789 exec execpod-affinitywbfj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Apr 24 03:49:54.539: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 24 03:49:54.539: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 03:49:54.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5789 exec execpod-affinitywbfj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.127 80'
  Apr 24 03:49:54.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.21.127 80\nConnection to 10.233.21.127 80 port [tcp/http] succeeded!\n"
  Apr 24 03:49:54.924: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 03:49:54.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5789 exec execpod-affinitywbfj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 31711'
  Apr 24 03:49:55.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 31711\nConnection to 192.168.121.43 31711 port [tcp/*] succeeded!\n"
  Apr 24 03:49:55.178: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 03:49:55.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5789 exec execpod-affinitywbfj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.18 31711'
  Apr 24 03:49:55.407: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.18 31711\nConnection to 192.168.121.18 31711 port [tcp/*] succeeded!\n"
  Apr 24 03:49:55.407: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 03:49:55.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5789 exec execpod-affinitywbfj4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.252:31711/ ; done'
  Apr 24 03:49:55.792: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:31711/\n"
  Apr 24 03:49:55.792: INFO: stdout: "\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8\naffinity-nodeport-x9dp8"
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Received response from host: affinity-nodeport-x9dp8
  Apr 24 03:49:55.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 03:49:55.804: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-5789, will wait for the garbage collector to delete the pods @ 04/24/23 03:49:55.832
  Apr 24 03:49:55.901: INFO: Deleting ReplicationController affinity-nodeport took: 11.675696ms
  Apr 24 03:49:56.002: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.933709ms
  STEP: Destroying namespace "services-5789" for this suite. @ 04/24/23 03:49:58.444
• [10.371 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/24/23 03:49:58.464
  Apr 24 03:49:58.464: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 03:49:58.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:49:58.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:49:58.503
  STEP: validating cluster-info @ 04/24/23 03:49:58.507
  Apr 24 03:49:58.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7911 cluster-info'
  Apr 24 03:49:58.641: INFO: stderr: ""
  Apr 24 03:49:58.641: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 24 03:49:58.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7911" for this suite. @ 04/24/23 03:49:58.649
• [0.200 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/24/23 03:49:58.666
  Apr 24 03:49:58.666: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 03:49:58.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:49:58.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:49:58.764
  Apr 24 03:49:58.773: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: creating the pod @ 04/24/23 03:49:58.778
  STEP: submitting the pod to kubernetes @ 04/24/23 03:49:58.779
  Apr 24 03:50:00.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8758" for this suite. @ 04/24/23 03:50:00.939
• [2.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/24/23 03:50:00.95
  Apr 24 03:50:00.950: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 03:50:00.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:50:00.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:50:00.983
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 03:50:00.987
  STEP: Saw pod success @ 04/24/23 03:50:05.021
  Apr 24 03:50:05.028: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-38aa0c40-96f3-4db7-9251-8a5d2bb16c27 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 03:50:05.042
  Apr 24 03:50:05.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5065" for this suite. @ 04/24/23 03:50:05.073
• [4.133 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/24/23 03:50:05.083
  Apr 24 03:50:05.083: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 03:50:05.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:50:05.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:50:05.118
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 03:50:05.122
  STEP: Saw pod success @ 04/24/23 03:50:09.163
  Apr 24 03:50:09.171: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-5fbad2d4-6ad9-47f8-94b9-41264eaae1ea container client-container: <nil>
  STEP: delete the pod @ 04/24/23 03:50:09.189
  Apr 24 03:50:09.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9493" for this suite. @ 04/24/23 03:50:09.239
• [4.166 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/24/23 03:50:09.255
  Apr 24 03:50:09.255: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 03:50:09.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:50:09.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:50:09.296
  STEP: Setting up server cert @ 04/24/23 03:50:09.337
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 03:50:09.887
  STEP: Deploying the webhook pod @ 04/24/23 03:50:09.902
  STEP: Wait for the deployment to be ready @ 04/24/23 03:50:09.923
  Apr 24 03:50:09.935: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Apr 24 03:50:11.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 3, 50, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 3, 50, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 3, 50, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 3, 50, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/24/23 03:50:13.977
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 03:50:13.996
  Apr 24 03:50:14.997: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/24/23 03:50:15.004
  STEP: create a pod @ 04/24/23 03:50:15.037
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/24/23 03:50:17.075
  Apr 24 03:50:17.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=webhook-154 attach --namespace=webhook-154 to-be-attached-pod -i -c=container1'
  Apr 24 03:50:17.240: INFO: rc: 1
  Apr 24 03:50:17.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-154" for this suite. @ 04/24/23 03:50:17.332
  STEP: Destroying namespace "webhook-markers-5349" for this suite. @ 04/24/23 03:50:17.345
• [8.109 seconds]
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/24/23 03:50:17.365
  Apr 24 03:50:17.365: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 03:50:17.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:50:17.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:50:17.438
  STEP: Creating configMap with name configmap-test-upd-4875df9c-a883-4110-bac1-396fd3bc01a2 @ 04/24/23 03:50:17.451
  STEP: Creating the pod @ 04/24/23 03:50:17.462
  STEP: Updating configmap configmap-test-upd-4875df9c-a883-4110-bac1-396fd3bc01a2 @ 04/24/23 03:50:19.513
  STEP: waiting to observe update in volume @ 04/24/23 03:50:19.524
  Apr 24 03:51:44.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6247" for this suite. @ 04/24/23 03:51:44.443
• [87.088 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/24/23 03:51:44.458
  Apr 24 03:51:44.458: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replication-controller @ 04/24/23 03:51:44.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:51:44.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:51:44.509
  STEP: Creating replication controller my-hostname-basic-536a678e-0c7f-40cd-addc-23d4bb6f4d6a @ 04/24/23 03:51:44.514
  Apr 24 03:51:44.529: INFO: Pod name my-hostname-basic-536a678e-0c7f-40cd-addc-23d4bb6f4d6a: Found 0 pods out of 1
  Apr 24 03:51:49.538: INFO: Pod name my-hostname-basic-536a678e-0c7f-40cd-addc-23d4bb6f4d6a: Found 1 pods out of 1
  Apr 24 03:51:49.538: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-536a678e-0c7f-40cd-addc-23d4bb6f4d6a" are running
  Apr 24 03:51:49.544: INFO: Pod "my-hostname-basic-536a678e-0c7f-40cd-addc-23d4bb6f4d6a-tbvzt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-24 03:51:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-24 03:51:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-24 03:51:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-24 03:51:44 +0000 UTC Reason: Message:}])
  Apr 24 03:51:49.544: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/24/23 03:51:49.545
  Apr 24 03:51:49.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4132" for this suite. @ 04/24/23 03:51:49.577
• [5.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/24/23 03:51:49.591
  Apr 24 03:51:49.591: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename var-expansion @ 04/24/23 03:51:49.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:51:49.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:51:49.634
  STEP: Creating a pod to test env composition @ 04/24/23 03:51:49.638
  STEP: Saw pod success @ 04/24/23 03:51:55.702
  Apr 24 03:51:55.709: INFO: Trying to get logs from node aeveeng9ieph-1 pod var-expansion-a8dfaa0a-f810-4383-a610-92b39c6bdbea container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 03:51:55.748
  Apr 24 03:51:55.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8379" for this suite. @ 04/24/23 03:51:55.79
• [6.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/24/23 03:51:55.812
  Apr 24 03:51:55.812: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename gc @ 04/24/23 03:51:55.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:51:55.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:51:55.851
  STEP: create the rc @ 04/24/23 03:51:55.856
  W0424 03:51:55.868410      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/24/23 03:52:00.879
  STEP: wait for all pods to be garbage collected @ 04/24/23 03:52:00.89
  STEP: Gathering metrics @ 04/24/23 03:52:05.907
  Apr 24 03:52:06.121: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 24 03:52:06.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-602" for this suite. @ 04/24/23 03:52:06.143
• [10.351 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/24/23 03:52:06.163
  Apr 24 03:52:06.163: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 03:52:06.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:52:06.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:52:06.207
  STEP: Creating pod test-grpc-febe5c66-bb05-44bc-b2b5-127ac6bc9eb6 in namespace container-probe-8323 @ 04/24/23 03:52:06.214
  Apr 24 03:52:08.248: INFO: Started pod test-grpc-febe5c66-bb05-44bc-b2b5-127ac6bc9eb6 in namespace container-probe-8323
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/24/23 03:52:08.248
  Apr 24 03:52:08.254: INFO: Initial restart count of pod test-grpc-febe5c66-bb05-44bc-b2b5-127ac6bc9eb6 is 0
  Apr 24 03:56:09.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 03:56:09.632
  STEP: Destroying namespace "container-probe-8323" for this suite. @ 04/24/23 03:56:09.662
• [243.512 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/24/23 03:56:09.678
  Apr 24 03:56:09.678: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 03:56:09.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:56:09.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:56:09.764
  STEP: creating service in namespace services-421 @ 04/24/23 03:56:09.77
  STEP: creating service affinity-clusterip in namespace services-421 @ 04/24/23 03:56:09.77
  STEP: creating replication controller affinity-clusterip in namespace services-421 @ 04/24/23 03:56:09.789
  I0424 03:56:09.803531      14 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-421, replica count: 3
  I0424 03:56:12.855316      14 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 03:56:12.869: INFO: Creating new exec pod
  Apr 24 03:56:15.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-421 exec execpod-affinityzlw6j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Apr 24 03:56:16.211: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 24 03:56:16.211: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 03:56:16.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-421 exec execpod-affinityzlw6j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.12.146 80'
  Apr 24 03:56:16.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.12.146 80\nConnection to 10.233.12.146 80 port [tcp/http] succeeded!\n"
  Apr 24 03:56:16.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 03:56:16.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-421 exec execpod-affinityzlw6j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.12.146:80/ ; done'
  Apr 24 03:56:17.072: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.12.146:80/\n"
  Apr 24 03:56:17.072: INFO: stdout: "\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d\naffinity-clusterip-gbj5d"
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Received response from host: affinity-clusterip-gbj5d
  Apr 24 03:56:17.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 03:56:17.082: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-421, will wait for the garbage collector to delete the pods @ 04/24/23 03:56:17.104
  Apr 24 03:56:17.198: INFO: Deleting ReplicationController affinity-clusterip took: 31.84908ms
  Apr 24 03:56:17.299: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.473084ms
  STEP: Destroying namespace "services-421" for this suite. @ 04/24/23 03:56:19.128
• [9.463 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/24/23 03:56:19.141
  Apr 24 03:56:19.141: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/24/23 03:56:19.145
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:56:19.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:56:19.18
  STEP: create the container to handle the HTTPGet hook request. @ 04/24/23 03:56:19.195
  STEP: create the pod with lifecycle hook @ 04/24/23 03:56:21.236
  STEP: check poststart hook @ 04/24/23 03:56:23.268
  STEP: delete the pod with lifecycle hook @ 04/24/23 03:56:23.3
  Apr 24 03:56:27.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3691" for this suite. @ 04/24/23 03:56:27.348
• [8.217 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/24/23 03:56:27.362
  Apr 24 03:56:27.362: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pod-network-test @ 04/24/23 03:56:27.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:56:27.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:56:27.396
  STEP: Performing setup for networking test in namespace pod-network-test-2430 @ 04/24/23 03:56:27.402
  STEP: creating a selector @ 04/24/23 03:56:27.402
  STEP: Creating the service pods in kubernetes @ 04/24/23 03:56:27.402
  Apr 24 03:56:27.402: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/24/23 03:56:49.64
  Apr 24 03:56:51.713: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 24 03:56:51.713: INFO: Going to poll 10.233.64.122 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 24 03:56:51.718: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.122:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:56:51.718: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:56:51.719: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:56:51.719: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.122%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 24 03:56:51.861: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 24 03:56:51.861: INFO: Going to poll 10.233.65.182 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 24 03:56:51.867: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.182:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:56:51.867: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:56:51.869: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:56:51.869: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.182%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 24 03:56:51.971: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 24 03:56:51.971: INFO: Going to poll 10.233.66.17 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 24 03:56:51.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.17:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 03:56:51.977: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 03:56:51.980: INFO: ExecWithOptions: Clientset creation
  Apr 24 03:56:51.980: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.17%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 24 03:56:52.095: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 24 03:56:52.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2430" for this suite. @ 04/24/23 03:56:52.104
• [24.755 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/24/23 03:56:52.119
  Apr 24 03:56:52.119: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 03:56:52.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:56:52.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:56:52.163
  STEP: Creating configMap with name configmap-test-volume-map-ea25b783-a64a-4de0-8e01-d17d0a96d396 @ 04/24/23 03:56:52.168
  STEP: Creating a pod to test consume configMaps @ 04/24/23 03:56:52.175
  STEP: Saw pod success @ 04/24/23 03:56:56.232
  Apr 24 03:56:56.238: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-257de288-4879-4462-a0fd-aef77e76547b container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 03:56:56.274
  Apr 24 03:56:56.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9845" for this suite. @ 04/24/23 03:56:56.311
• [4.211 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/24/23 03:56:56.333
  Apr 24 03:56:56.333: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 03:56:56.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:56:56.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:56:56.432
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 03:56:56.437
  STEP: Saw pod success @ 04/24/23 03:57:00.475
  Apr 24 03:57:00.483: INFO: Trying to get logs from node aeveeng9ieph-1 pod downwardapi-volume-c870f946-325e-44f3-aaa2-e564e8a559e7 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 03:57:00.503
  Apr 24 03:57:00.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9999" for this suite. @ 04/24/23 03:57:00.542
• [4.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/24/23 03:57:00.579
  Apr 24 03:57:00.579: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 03:57:00.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:57:00.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:57:00.615
  Apr 24 03:57:00.620: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/24/23 03:57:02.585
  Apr 24 03:57:02.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-5882 --namespace=crd-publish-openapi-5882 create -f -'
  Apr 24 03:57:04.089: INFO: stderr: ""
  Apr 24 03:57:04.089: INFO: stdout: "e2e-test-crd-publish-openapi-1661-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 24 03:57:04.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-5882 --namespace=crd-publish-openapi-5882 delete e2e-test-crd-publish-openapi-1661-crds test-cr'
  Apr 24 03:57:04.263: INFO: stderr: ""
  Apr 24 03:57:04.263: INFO: stdout: "e2e-test-crd-publish-openapi-1661-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 24 03:57:04.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-5882 --namespace=crd-publish-openapi-5882 apply -f -'
  Apr 24 03:57:05.356: INFO: stderr: ""
  Apr 24 03:57:05.356: INFO: stdout: "e2e-test-crd-publish-openapi-1661-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 24 03:57:05.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-5882 --namespace=crd-publish-openapi-5882 delete e2e-test-crd-publish-openapi-1661-crds test-cr'
  Apr 24 03:57:05.509: INFO: stderr: ""
  Apr 24 03:57:05.509: INFO: stdout: "e2e-test-crd-publish-openapi-1661-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/24/23 03:57:05.509
  Apr 24 03:57:05.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-5882 explain e2e-test-crd-publish-openapi-1661-crds'
  Apr 24 03:57:05.955: INFO: stderr: ""
  Apr 24 03:57:05.955: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-1661-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Apr 24 03:57:08.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5882" for this suite. @ 04/24/23 03:57:08.301
• [7.732 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/24/23 03:57:08.317
  Apr 24 03:57:08.318: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename ingress @ 04/24/23 03:57:08.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:57:08.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:57:08.35
  STEP: getting /apis @ 04/24/23 03:57:08.354
  STEP: getting /apis/networking.k8s.io @ 04/24/23 03:57:08.363
  STEP: getting /apis/networking.k8s.iov1 @ 04/24/23 03:57:08.364
  STEP: creating @ 04/24/23 03:57:08.366
  STEP: getting @ 04/24/23 03:57:08.412
  STEP: listing @ 04/24/23 03:57:08.426
  STEP: watching @ 04/24/23 03:57:08.432
  Apr 24 03:57:08.433: INFO: starting watch
  STEP: cluster-wide listing @ 04/24/23 03:57:08.435
  STEP: cluster-wide watching @ 04/24/23 03:57:08.442
  Apr 24 03:57:08.442: INFO: starting watch
  STEP: patching @ 04/24/23 03:57:08.445
  STEP: updating @ 04/24/23 03:57:08.455
  Apr 24 03:57:08.475: INFO: waiting for watch events with expected annotations
  Apr 24 03:57:08.476: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/24/23 03:57:08.477
  STEP: updating /status @ 04/24/23 03:57:08.488
  STEP: get /status @ 04/24/23 03:57:08.502
  STEP: deleting @ 04/24/23 03:57:08.51
  STEP: deleting a collection @ 04/24/23 03:57:08.532
  Apr 24 03:57:08.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9946" for this suite. @ 04/24/23 03:57:08.568
• [0.260 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/24/23 03:57:08.579
  Apr 24 03:57:08.579: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 03:57:08.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:57:08.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:57:08.615
  STEP: Setting up server cert @ 04/24/23 03:57:08.653
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 03:57:10.297
  STEP: Deploying the webhook pod @ 04/24/23 03:57:10.314
  STEP: Wait for the deployment to be ready @ 04/24/23 03:57:10.335
  Apr 24 03:57:10.348: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Apr 24 03:57:12.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 3, 57, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 3, 57, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 3, 57, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 3, 57, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/24/23 03:57:14.44
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 03:57:14.46
  Apr 24 03:57:15.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 24 03:57:15.472: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8329-crds.webhook.example.com via the AdmissionRegistration API @ 04/24/23 03:57:15.999
  Apr 24 03:57:16.042: INFO: Waiting for webhook configuration to be ready...
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/24/23 03:57:16.169
  Apr 24 03:57:18.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5035" for this suite. @ 04/24/23 03:57:18.931
  STEP: Destroying namespace "webhook-markers-8588" for this suite. @ 04/24/23 03:57:18.942
• [10.375 seconds]
------------------------------
SSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/24/23 03:57:18.956
  Apr 24 03:57:18.956: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename containers @ 04/24/23 03:57:18.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:57:18.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:57:19.001
  STEP: Creating a pod to test override arguments @ 04/24/23 03:57:19.006
  STEP: Saw pod success @ 04/24/23 03:57:23.064
  Apr 24 03:57:23.070: INFO: Trying to get logs from node aeveeng9ieph-3 pod client-containers-1dcf60fe-dd8a-40b8-b905-b80d969df484 container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 03:57:23.082
  Apr 24 03:57:23.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7687" for this suite. @ 04/24/23 03:57:23.111
• [4.164 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/24/23 03:57:23.123
  Apr 24 03:57:23.124: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 03:57:23.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 03:57:23.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 03:57:23.151
  STEP: Creating pod test-webserver-01b8d1f0-c65d-4552-94eb-2b30f788be0f in namespace container-probe-6486 @ 04/24/23 03:57:23.156
  Apr 24 03:57:25.181: INFO: Started pod test-webserver-01b8d1f0-c65d-4552-94eb-2b30f788be0f in namespace container-probe-6486
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/24/23 03:57:25.182
  Apr 24 03:57:25.190: INFO: Initial restart count of pod test-webserver-01b8d1f0-c65d-4552-94eb-2b30f788be0f is 0
  Apr 24 04:01:26.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 04:01:26.522
  STEP: Destroying namespace "container-probe-6486" for this suite. @ 04/24/23 04:01:26.579
• [243.468 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/24/23 04:01:26.593
  Apr 24 04:01:26.593: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 04:01:26.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:01:26.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:01:26.635
  STEP: Creating a pod to test downward api env vars @ 04/24/23 04:01:26.639
  STEP: Saw pod success @ 04/24/23 04:01:30.677
  Apr 24 04:01:30.682: INFO: Trying to get logs from node aeveeng9ieph-3 pod downward-api-4194da9f-5b70-401f-8816-8a171b5943e2 container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 04:01:30.828
  Apr 24 04:01:30.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4410" for this suite. @ 04/24/23 04:01:30.855
• [4.277 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/24/23 04:01:30.872
  Apr 24 04:01:30.872: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:01:30.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:01:30.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:01:30.898
  STEP: Creating projection with secret that has name projected-secret-test-map-da02550e-fb66-416d-96a5-02ab9a74fec6 @ 04/24/23 04:01:30.901
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:01:30.908
  STEP: Saw pod success @ 04/24/23 04:01:34.942
  Apr 24 04:01:34.947: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-secrets-55c3e052-f433-4ee4-8cb7-e76d1e9c3177 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:01:34.959
  Apr 24 04:01:34.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6340" for this suite. @ 04/24/23 04:01:34.996
• [4.134 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/24/23 04:01:35.008
  Apr 24 04:01:35.008: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 04:01:35.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:01:35.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:01:35.044
  STEP: Saw pod success @ 04/24/23 04:01:41.127
  Apr 24 04:01:41.133: INFO: Trying to get logs from node aeveeng9ieph-3 pod client-envvars-6b41c792-99df-42a5-88d4-14867fc79f7b container env3cont: <nil>
  STEP: delete the pod @ 04/24/23 04:01:41.15
  Apr 24 04:01:41.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9453" for this suite. @ 04/24/23 04:01:41.182
• [6.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/24/23 04:01:41.199
  Apr 24 04:01:41.199: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/24/23 04:01:41.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:01:41.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:01:41.233
  STEP: create the container to handle the HTTPGet hook request. @ 04/24/23 04:01:41.244
  STEP: create the pod with lifecycle hook @ 04/24/23 04:01:43.278
  STEP: delete the pod with lifecycle hook @ 04/24/23 04:01:45.308
  STEP: check prestop hook @ 04/24/23 04:01:49.349
  Apr 24 04:01:49.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1641" for this suite. @ 04/24/23 04:01:49.379
• [8.194 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/24/23 04:01:49.395
  Apr 24 04:01:49.395: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename events @ 04/24/23 04:01:49.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:01:49.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:01:49.429
  STEP: creating a test event @ 04/24/23 04:01:49.434
  STEP: listing events in all namespaces @ 04/24/23 04:01:49.455
  STEP: listing events in test namespace @ 04/24/23 04:01:49.481
  STEP: listing events with field selection filtering on source @ 04/24/23 04:01:49.488
  STEP: listing events with field selection filtering on reportingController @ 04/24/23 04:01:49.494
  STEP: getting the test event @ 04/24/23 04:01:49.5
  STEP: patching the test event @ 04/24/23 04:01:49.506
  STEP: getting the test event @ 04/24/23 04:01:49.524
  STEP: updating the test event @ 04/24/23 04:01:49.528
  STEP: getting the test event @ 04/24/23 04:01:49.539
  STEP: deleting the test event @ 04/24/23 04:01:49.542
  STEP: listing events in all namespaces @ 04/24/23 04:01:49.556
  STEP: listing events in test namespace @ 04/24/23 04:01:49.578
  Apr 24 04:01:49.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4471" for this suite. @ 04/24/23 04:01:49.596
• [0.214 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 04/24/23 04:01:49.613
  Apr 24 04:01:49.613: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename daemonsets @ 04/24/23 04:01:49.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:01:49.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:01:49.65
  STEP: Creating simple DaemonSet "daemon-set" @ 04/24/23 04:01:49.692
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/24/23 04:01:49.707
  Apr 24 04:01:49.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:01:49.723: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 04:01:50.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:01:50.740: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 04:01:51.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:01:51.738: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  Apr 24 04:01:52.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:01:52.736: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  Apr 24 04:01:53.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:01:53.743: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  Apr 24 04:01:54.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:01:54.739: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  Apr 24 04:01:55.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:01:55.737: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  Apr 24 04:01:56.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:01:56.741: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  Apr 24 04:01:57.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:01:57.742: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  Apr 24 04:01:58.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 04:01:58.738: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 04/24/23 04:01:58.742
  Apr 24 04:01:58.748: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/24/23 04:01:58.748
  Apr 24 04:01:58.762: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/24/23 04:01:58.763
  Apr 24 04:01:58.769: INFO: Observed &DaemonSet event: ADDED
  Apr 24 04:01:58.769: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.769: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.771: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.772: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.773: INFO: Found daemon set daemon-set in namespace daemonsets-1553 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 24 04:01:58.773: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/24/23 04:01:58.774
  STEP: watching for the daemon set status to be patched @ 04/24/23 04:01:58.786
  Apr 24 04:01:58.788: INFO: Observed &DaemonSet event: ADDED
  Apr 24 04:01:58.789: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.789: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.789: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.789: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.790: INFO: Observed daemon set daemon-set in namespace daemonsets-1553 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 24 04:01:58.790: INFO: Observed &DaemonSet event: MODIFIED
  Apr 24 04:01:58.790: INFO: Found daemon set daemon-set in namespace daemonsets-1553 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 24 04:01:58.791: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/24/23 04:01:58.797
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1553, will wait for the garbage collector to delete the pods @ 04/24/23 04:01:58.797
  Apr 24 04:01:58.866: INFO: Deleting DaemonSet.extensions daemon-set took: 12.979948ms
  Apr 24 04:01:58.967: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.970392ms
  Apr 24 04:02:01.676: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:02:01.677: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 24 04:02:01.682: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16186"},"items":null}

  Apr 24 04:02:01.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16186"},"items":null}

  Apr 24 04:02:01.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1553" for this suite. @ 04/24/23 04:02:01.741
• [12.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/24/23 04:02:01.754
  Apr 24 04:02:01.754: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename runtimeclass @ 04/24/23 04:02:01.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:02:01.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:02:01.784
  STEP: getting /apis @ 04/24/23 04:02:01.79
  STEP: getting /apis/node.k8s.io @ 04/24/23 04:02:01.798
  STEP: getting /apis/node.k8s.io/v1 @ 04/24/23 04:02:01.8
  STEP: creating @ 04/24/23 04:02:01.806
  STEP: watching @ 04/24/23 04:02:01.834
  Apr 24 04:02:01.835: INFO: starting watch
  STEP: getting @ 04/24/23 04:02:01.845
  STEP: listing @ 04/24/23 04:02:01.85
  STEP: patching @ 04/24/23 04:02:01.854
  STEP: updating @ 04/24/23 04:02:01.869
  Apr 24 04:02:01.878: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/24/23 04:02:01.878
  STEP: deleting a collection @ 04/24/23 04:02:01.895
  Apr 24 04:02:01.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2726" for this suite. @ 04/24/23 04:02:01.925
• [0.180 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 04/24/23 04:02:01.941
  Apr 24 04:02:01.941: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename daemonsets @ 04/24/23 04:02:01.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:02:01.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:02:01.973
  STEP: Creating simple DaemonSet "daemon-set" @ 04/24/23 04:02:02.022
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/24/23 04:02:02.029
  Apr 24 04:02:02.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:02:02.043: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 04:02:03.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:02:03.057: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 04:02:04.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 04:02:04.057: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/24/23 04:02:04.063
  Apr 24 04:02:04.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:02:04.097: INFO: Node aeveeng9ieph-3 is running 0 daemon pod, expected 1
  Apr 24 04:02:05.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:02:05.120: INFO: Node aeveeng9ieph-3 is running 0 daemon pod, expected 1
  Apr 24 04:02:06.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:02:06.112: INFO: Node aeveeng9ieph-3 is running 0 daemon pod, expected 1
  Apr 24 04:02:07.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 04:02:07.111: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/24/23 04:02:07.116
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3644, will wait for the garbage collector to delete the pods @ 04/24/23 04:02:07.116
  Apr 24 04:02:07.197: INFO: Deleting DaemonSet.extensions daemon-set took: 27.489665ms
  Apr 24 04:02:07.298: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.693756ms
  Apr 24 04:02:08.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:02:08.905: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 24 04:02:08.910: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16326"},"items":null}

  Apr 24 04:02:08.915: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16326"},"items":null}

  Apr 24 04:02:08.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3644" for this suite. @ 04/24/23 04:02:08.942
• [7.014 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/24/23 04:02:08.958
  Apr 24 04:02:08.958: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:02:08.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:02:08.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:02:08.986
  STEP: creating all guestbook components @ 04/24/23 04:02:08.99
  Apr 24 04:02:08.990: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 24 04:02:08.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 create -f -'
  Apr 24 04:02:09.864: INFO: stderr: ""
  Apr 24 04:02:09.864: INFO: stdout: "service/agnhost-replica created\n"
  Apr 24 04:02:09.864: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 24 04:02:09.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 create -f -'
  Apr 24 04:02:10.403: INFO: stderr: ""
  Apr 24 04:02:10.403: INFO: stdout: "service/agnhost-primary created\n"
  Apr 24 04:02:10.405: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 24 04:02:10.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 create -f -'
  Apr 24 04:02:11.186: INFO: stderr: ""
  Apr 24 04:02:11.186: INFO: stdout: "service/frontend created\n"
  Apr 24 04:02:11.186: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 24 04:02:11.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 create -f -'
  Apr 24 04:02:12.731: INFO: stderr: ""
  Apr 24 04:02:12.731: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 24 04:02:12.732: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 24 04:02:12.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 create -f -'
  Apr 24 04:02:13.426: INFO: stderr: ""
  Apr 24 04:02:13.426: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 24 04:02:13.426: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 24 04:02:13.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 create -f -'
  Apr 24 04:02:14.414: INFO: stderr: ""
  Apr 24 04:02:14.414: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/24/23 04:02:14.414
  Apr 24 04:02:14.414: INFO: Waiting for all frontend pods to be Running.
  Apr 24 04:02:19.467: INFO: Waiting for frontend to serve content.
  Apr 24 04:02:19.494: INFO: Trying to add a new entry to the guestbook.
  Apr 24 04:02:19.512: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/24/23 04:02:19.534
  Apr 24 04:02:19.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 delete --grace-period=0 --force -f -'
  Apr 24 04:02:19.696: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:02:19.697: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/24/23 04:02:19.697
  Apr 24 04:02:19.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 delete --grace-period=0 --force -f -'
  Apr 24 04:02:19.854: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:02:19.854: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/24/23 04:02:19.854
  Apr 24 04:02:19.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 delete --grace-period=0 --force -f -'
  Apr 24 04:02:20.025: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:02:20.025: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/24/23 04:02:20.026
  Apr 24 04:02:20.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 delete --grace-period=0 --force -f -'
  Apr 24 04:02:20.141: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:02:20.141: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/24/23 04:02:20.142
  Apr 24 04:02:20.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 delete --grace-period=0 --force -f -'
  Apr 24 04:02:20.466: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:02:20.466: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/24/23 04:02:20.467
  Apr 24 04:02:20.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7733 delete --grace-period=0 --force -f -'
  Apr 24 04:02:20.655: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:02:20.655: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 24 04:02:20.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7733" for this suite. @ 04/24/23 04:02:20.683
• [11.751 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/24/23 04:02:20.711
  Apr 24 04:02:20.711: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replication-controller @ 04/24/23 04:02:20.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:02:20.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:02:20.8
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/24/23 04:02:20.811
  STEP: When a replication controller with a matching selector is created @ 04/24/23 04:02:22.85
  STEP: Then the orphan pod is adopted @ 04/24/23 04:02:22.869
  Apr 24 04:02:23.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6763" for this suite. @ 04/24/23 04:02:23.896
• [3.198 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/24/23 04:02:23.909
  Apr 24 04:02:23.909: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-preemption @ 04/24/23 04:02:23.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:02:23.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:02:23.944
  Apr 24 04:02:23.974: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 24 04:03:24.028: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/24/23 04:03:24.033
  Apr 24 04:03:24.060: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 24 04:03:24.069: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 24 04:03:24.151: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 24 04:03:24.186: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 24 04:03:24.248: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 24 04:03:24.271: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/24/23 04:03:24.272
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/24/23 04:03:26.315
  Apr 24 04:03:32.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1065" for this suite. @ 04/24/23 04:03:32.549
• [68.654 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/24/23 04:03:32.576
  Apr 24 04:03:32.576: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:03:32.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:03:32.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:03:32.62
  STEP: Creating configMap with name cm-test-opt-del-766f09fc-b5ac-40e9-a389-b63e9cab7e77 @ 04/24/23 04:03:32.632
  STEP: Creating configMap with name cm-test-opt-upd-63de3e74-6e0e-4244-bde5-551ff6db4bbd @ 04/24/23 04:03:32.639
  STEP: Creating the pod @ 04/24/23 04:03:32.646
  STEP: Deleting configmap cm-test-opt-del-766f09fc-b5ac-40e9-a389-b63e9cab7e77 @ 04/24/23 04:03:36.849
  STEP: Updating configmap cm-test-opt-upd-63de3e74-6e0e-4244-bde5-551ff6db4bbd @ 04/24/23 04:03:36.858
  STEP: Creating configMap with name cm-test-opt-create-84ae14dc-b239-44da-a36d-5f73667f0dbb @ 04/24/23 04:03:36.867
  STEP: waiting to observe update in volume @ 04/24/23 04:03:36.873
  Apr 24 04:05:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8888" for this suite. @ 04/24/23 04:05:07.882
• [95.317 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/24/23 04:05:07.909
  Apr 24 04:05:07.909: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename var-expansion @ 04/24/23 04:05:07.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:05:07.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:05:07.953
  STEP: Creating a pod to test substitution in volume subpath @ 04/24/23 04:05:07.957
  STEP: Saw pod success @ 04/24/23 04:05:11.992
  Apr 24 04:05:11.996: INFO: Trying to get logs from node aeveeng9ieph-1 pod var-expansion-4861a3ee-2d9e-4842-b667-afabde5f70c2 container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 04:05:12.057
  Apr 24 04:05:12.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4290" for this suite. @ 04/24/23 04:05:12.099
• [4.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/24/23 04:05:12.126
  Apr 24 04:05:12.126: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename security-context-test @ 04/24/23 04:05:12.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:05:12.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:05:12.166
  Apr 24 04:05:18.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2142" for this suite. @ 04/24/23 04:05:18.27
• [6.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/24/23 04:05:18.286
  Apr 24 04:05:18.286: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:05:18.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:05:18.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:05:18.322
  STEP: creating the pod @ 04/24/23 04:05:18.326
  Apr 24 04:05:18.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3003 create -f -'
  Apr 24 04:05:19.186: INFO: stderr: ""
  Apr 24 04:05:19.186: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/24/23 04:05:21.203
  Apr 24 04:05:21.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3003 label pods pause testing-label=testing-label-value'
  Apr 24 04:05:21.388: INFO: stderr: ""
  Apr 24 04:05:21.388: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/24/23 04:05:21.389
  Apr 24 04:05:21.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3003 get pod pause -L testing-label'
  Apr 24 04:05:21.512: INFO: stderr: ""
  Apr 24 04:05:21.512: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/24/23 04:05:21.512
  Apr 24 04:05:21.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3003 label pods pause testing-label-'
  Apr 24 04:05:21.666: INFO: stderr: ""
  Apr 24 04:05:21.666: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/24/23 04:05:21.666
  Apr 24 04:05:21.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3003 get pod pause -L testing-label'
  Apr 24 04:05:21.825: INFO: stderr: ""
  Apr 24 04:05:21.825: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 04/24/23 04:05:21.825
  Apr 24 04:05:21.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3003 delete --grace-period=0 --force -f -'
  Apr 24 04:05:22.131: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:05:22.131: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 24 04:05:22.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3003 get rc,svc -l name=pause --no-headers'
  Apr 24 04:05:22.291: INFO: stderr: "No resources found in kubectl-3003 namespace.\n"
  Apr 24 04:05:22.291: INFO: stdout: ""
  Apr 24 04:05:22.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3003 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 24 04:05:22.420: INFO: stderr: ""
  Apr 24 04:05:22.421: INFO: stdout: ""
  Apr 24 04:05:22.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3003" for this suite. @ 04/24/23 04:05:22.43
• [4.155 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/24/23 04:05:22.444
  Apr 24 04:05:22.444: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/24/23 04:05:22.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:05:22.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:05:22.48
  Apr 24 04:05:22.485: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:05:23.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6596" for this suite. @ 04/24/23 04:05:23.07
• [0.638 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/24/23 04:05:23.083
  Apr 24 04:05:23.083: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 04:05:23.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:05:23.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:05:23.122
  STEP: creating service endpoint-test2 in namespace services-6623 @ 04/24/23 04:05:23.127
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6623 to expose endpoints map[] @ 04/24/23 04:05:23.141
  Apr 24 04:05:23.147: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Apr 24 04:05:24.169: INFO: successfully validated that service endpoint-test2 in namespace services-6623 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6623 @ 04/24/23 04:05:24.17
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6623 to expose endpoints map[pod1:[80]] @ 04/24/23 04:05:26.209
  Apr 24 04:05:26.228: INFO: successfully validated that service endpoint-test2 in namespace services-6623 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/24/23 04:05:26.228
  Apr 24 04:05:26.228: INFO: Creating new exec pod
  Apr 24 04:05:29.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6623 exec execpodk8rcw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 24 04:05:29.555: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 24 04:05:29.555: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:05:29.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6623 exec execpodk8rcw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.1.179 80'
  Apr 24 04:05:29.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.1.179 80\nConnection to 10.233.1.179 80 port [tcp/http] succeeded!\n"
  Apr 24 04:05:29.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-6623 @ 04/24/23 04:05:29.811
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6623 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/24/23 04:05:31.845
  Apr 24 04:05:31.867: INFO: successfully validated that service endpoint-test2 in namespace services-6623 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/24/23 04:05:31.867
  Apr 24 04:05:32.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6623 exec execpodk8rcw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 24 04:05:33.246: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 24 04:05:33.246: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:05:33.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6623 exec execpodk8rcw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.1.179 80'
  Apr 24 04:05:33.489: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.1.179 80\nConnection to 10.233.1.179 80 port [tcp/http] succeeded!\n"
  Apr 24 04:05:33.489: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6623 @ 04/24/23 04:05:33.489
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6623 to expose endpoints map[pod2:[80]] @ 04/24/23 04:05:33.517
  Apr 24 04:05:34.570: INFO: successfully validated that service endpoint-test2 in namespace services-6623 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/24/23 04:05:34.572
  Apr 24 04:05:35.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6623 exec execpodk8rcw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 24 04:05:35.882: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 24 04:05:35.882: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:05:35.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6623 exec execpodk8rcw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.1.179 80'
  Apr 24 04:05:36.123: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.1.179 80\nConnection to 10.233.1.179 80 port [tcp/http] succeeded!\n"
  Apr 24 04:05:36.123: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-6623 @ 04/24/23 04:05:36.123
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6623 to expose endpoints map[] @ 04/24/23 04:05:36.141
  Apr 24 04:05:38.160: INFO: successfully validated that service endpoint-test2 in namespace services-6623 exposes endpoints map[]
  Apr 24 04:05:38.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6623" for this suite. @ 04/24/23 04:05:38.214
• [15.142 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/24/23 04:05:38.225
  Apr 24 04:05:38.227: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 04:05:38.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:05:38.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:05:38.259
  STEP: Creating pod busybox-025297e4-a01a-4336-ab4a-c047fde13cc2 in namespace container-probe-7535 @ 04/24/23 04:05:38.264
  Apr 24 04:05:40.292: INFO: Started pod busybox-025297e4-a01a-4336-ab4a-c047fde13cc2 in namespace container-probe-7535
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/24/23 04:05:40.292
  Apr 24 04:05:40.299: INFO: Initial restart count of pod busybox-025297e4-a01a-4336-ab4a-c047fde13cc2 is 0
  Apr 24 04:06:30.666: INFO: Restart count of pod container-probe-7535/busybox-025297e4-a01a-4336-ab4a-c047fde13cc2 is now 1 (50.366858573s elapsed)
  Apr 24 04:06:30.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 04:06:30.678
  STEP: Destroying namespace "container-probe-7535" for this suite. @ 04/24/23 04:06:30.693
• [52.476 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/24/23 04:06:30.703
  Apr 24 04:06:30.703: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:06:30.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:06:30.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:06:30.739
  STEP: Setting up server cert @ 04/24/23 04:06:30.78
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:06:31.378
  STEP: Deploying the webhook pod @ 04/24/23 04:06:31.397
  STEP: Wait for the deployment to be ready @ 04/24/23 04:06:31.415
  Apr 24 04:06:31.424: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/24/23 04:06:33.453
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:06:33.472
  Apr 24 04:06:34.473: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 24 04:06:34.485: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3556-crds.webhook.example.com via the AdmissionRegistration API @ 04/24/23 04:06:35.011
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/24/23 04:06:35.047
  Apr 24 04:06:37.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4325" for this suite. @ 04/24/23 04:06:37.911
  STEP: Destroying namespace "webhook-markers-4012" for this suite. @ 04/24/23 04:06:37.926
• [7.234 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 04/24/23 04:06:37.939
  Apr 24 04:06:37.939: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename daemonsets @ 04/24/23 04:06:37.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:06:37.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:06:37.969
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/24/23 04:06:38.019
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/24/23 04:06:38.03
  Apr 24 04:06:38.047: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:06:38.047: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 04:06:39.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:06:39.063: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 04:06:40.062: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:06:40.062: INFO: Node aeveeng9ieph-3 is running 0 daemon pod, expected 1
  Apr 24 04:06:41.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 04:06:41.064: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/24/23 04:06:41.07
  Apr 24 04:06:41.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:06:41.118: INFO: Node aeveeng9ieph-3 is running 0 daemon pod, expected 1
  Apr 24 04:06:42.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:06:42.141: INFO: Node aeveeng9ieph-3 is running 0 daemon pod, expected 1
  Apr 24 04:06:43.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 04:06:43.134: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/24/23 04:06:43.134
  STEP: Deleting DaemonSet "daemon-set" @ 04/24/23 04:06:43.144
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8350, will wait for the garbage collector to delete the pods @ 04/24/23 04:06:43.144
  Apr 24 04:06:43.216: INFO: Deleting DaemonSet.extensions daemon-set took: 11.551925ms
  Apr 24 04:06:43.317: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.408212ms
  Apr 24 04:06:44.325: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:06:44.326: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 24 04:06:44.330: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17817"},"items":null}

  Apr 24 04:06:44.335: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17817"},"items":null}

  Apr 24 04:06:44.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8350" for this suite. @ 04/24/23 04:06:44.46
• [6.550 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/24/23 04:06:44.491
  Apr 24 04:06:44.491: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename deployment @ 04/24/23 04:06:44.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:06:44.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:06:44.62
  STEP: creating a Deployment @ 04/24/23 04:06:44.63
  STEP: waiting for Deployment to be created @ 04/24/23 04:06:44.637
  STEP: waiting for all Replicas to be Ready @ 04/24/23 04:06:44.64
  Apr 24 04:06:44.642: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 24 04:06:44.643: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 24 04:06:44.667: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 24 04:06:44.667: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 24 04:06:44.695: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 24 04:06:44.696: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 24 04:06:44.777: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 24 04:06:44.778: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 24 04:06:46.230: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 24 04:06:46.231: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 24 04:06:46.243: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/24/23 04:06:46.243
  W0424 04:06:46.261716      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 24 04:06:46.265: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/24/23 04:06:46.265
  Apr 24 04:06:46.267: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0
  Apr 24 04:06:46.267: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0
  Apr 24 04:06:46.267: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0
  Apr 24 04:06:46.267: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0
  Apr 24 04:06:46.268: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0
  Apr 24 04:06:46.268: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0
  Apr 24 04:06:46.268: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0
  Apr 24 04:06:46.268: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 0
  Apr 24 04:06:46.268: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  Apr 24 04:06:46.268: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  Apr 24 04:06:46.268: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:46.268: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:46.269: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:46.269: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:46.283: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:46.288: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:46.315: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:46.315: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:46.361: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  Apr 24 04:06:46.362: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  Apr 24 04:06:48.291: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:48.292: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:48.433: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  STEP: listing Deployments @ 04/24/23 04:06:48.433
  Apr 24 04:06:48.442: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/24/23 04:06:48.442
  Apr 24 04:06:48.463: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/24/23 04:06:48.463
  Apr 24 04:06:48.475: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 24 04:06:48.484: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 24 04:06:48.576: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 24 04:06:48.603: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 24 04:06:49.922: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 24 04:06:50.273: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 24 04:06:50.313: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 24 04:06:50.377: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 24 04:06:52.264: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/24/23 04:06:52.355
  STEP: fetching the DeploymentStatus @ 04/24/23 04:06:52.437
  Apr 24 04:06:52.451: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  Apr 24 04:06:52.452: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  Apr 24 04:06:52.452: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  Apr 24 04:06:52.452: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 1
  Apr 24 04:06:52.452: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:52.453: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 3
  Apr 24 04:06:52.453: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:52.453: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 2
  Apr 24 04:06:52.453: INFO: observed Deployment test-deployment in namespace deployment-4597 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/24/23 04:06:52.454
  Apr 24 04:06:52.491: INFO: observed event type MODIFIED
  Apr 24 04:06:52.492: INFO: observed event type MODIFIED
  Apr 24 04:06:52.492: INFO: observed event type MODIFIED
  Apr 24 04:06:52.492: INFO: observed event type MODIFIED
  Apr 24 04:06:52.492: INFO: observed event type MODIFIED
  Apr 24 04:06:52.493: INFO: observed event type MODIFIED
  Apr 24 04:06:52.493: INFO: observed event type MODIFIED
  Apr 24 04:06:52.493: INFO: observed event type MODIFIED
  Apr 24 04:06:52.494: INFO: observed event type MODIFIED
  Apr 24 04:06:52.494: INFO: observed event type MODIFIED
  Apr 24 04:06:52.503: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 24 04:06:52.512: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-4597  c710965b-cadb-4527-9808-924744008570 17900 3 2023-04-24 04:06:44 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cca00a6d-abae-4a4f-b222-788c92ef7c12 0xc002ba75c7 0xc002ba75c8}] [] [{kube-controller-manager Update apps/v1 2023-04-24 04:06:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca00a6d-abae-4a4f-b222-788c92ef7c12\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:06:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ba7650 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 24 04:06:52.518: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-4597  15e527b9-0375-4504-9982-6a223e249474 18009 2 2023-04-24 04:06:48 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cca00a6d-abae-4a4f-b222-788c92ef7c12 0xc002ba76b7 0xc002ba76b8}] [] [{kube-controller-manager Update apps/v1 2023-04-24 04:06:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca00a6d-abae-4a4f-b222-788c92ef7c12\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:06:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ba7740 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Apr 24 04:06:52.524: INFO: pod: "test-deployment-6fc78d85c6-46x46":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-46x46 test-deployment-6fc78d85c6- deployment-4597  7d0f63d1-5d3a-44bd-a407-46588312b843 18008 0 2023-04-24 04:06:50 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 15e527b9-0375-4504-9982-6a223e249474 0xc00293a6e7 0xc00293a6e8}] [] [{kube-controller-manager Update v1 2023-04-24 04:06:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15e527b9-0375-4504-9982-6a223e249474\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 04:06:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5klwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5klwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:06:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:06:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:06:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:10.233.64.47,StartTime:2023-04-24 04:06:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 04:06:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2e3b2f4ffc44d006aa2359fb9dda9c73293ed10e0173342975503dc7fbe88df4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.47,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 24 04:06:52.525: INFO: pod: "test-deployment-6fc78d85c6-mgrxn":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-mgrxn test-deployment-6fc78d85c6- deployment-4597  b3c8a47d-63a3-4e8d-9f91-bfd7df36a8f5 17973 0 2023-04-24 04:06:48 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 15e527b9-0375-4504-9982-6a223e249474 0xc00293a8d7 0xc00293a8d8}] [] [{kube-controller-manager Update v1 2023-04-24 04:06:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15e527b9-0375-4504-9982-6a223e249474\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 04:06:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4g4sg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4g4sg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:06:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:06:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:06:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:10.233.66.96,StartTime:2023-04-24 04:06:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 04:06:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://31ba0448553e82a26295105c7ddbe0ce16832ba639b7b347455ad0d42a3ae8c0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.96,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 24 04:06:52.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4597" for this suite. @ 04/24/23 04:06:52.535
• [8.074 seconds]
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/24/23 04:06:52.566
  Apr 24 04:06:52.566: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename security-context @ 04/24/23 04:06:52.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:06:52.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:06:52.61
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/24/23 04:06:52.614
  STEP: Saw pod success @ 04/24/23 04:06:56.646
  Apr 24 04:06:56.651: INFO: Trying to get logs from node aeveeng9ieph-3 pod security-context-68e9eb22-99bd-4e14-bc65-6bb961be05a4 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:06:56.753
  Apr 24 04:06:56.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-568" for this suite. @ 04/24/23 04:06:56.784
• [4.228 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/24/23 04:06:56.796
  Apr 24 04:06:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename security-context-test @ 04/24/23 04:06:56.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:06:56.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:06:56.836
  Apr 24 04:07:00.891: INFO: Got logs for pod "busybox-privileged-false-feb3d143-8448-47b2-abf4-854dcfc010ec": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 24 04:07:00.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6603" for this suite. @ 04/24/23 04:07:00.899
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/24/23 04:07:00.916
  Apr 24 04:07:00.916: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename namespaces @ 04/24/23 04:07:00.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:00.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:00.947
  STEP: Read namespace status @ 04/24/23 04:07:00.951
  Apr 24 04:07:00.957: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/24/23 04:07:00.957
  Apr 24 04:07:00.965: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/24/23 04:07:00.965
  Apr 24 04:07:00.982: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 24 04:07:00.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1044" for this suite. @ 04/24/23 04:07:00.987
• [0.083 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 04/24/23 04:07:01.003
  Apr 24 04:07:01.003: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename field-validation @ 04/24/23 04:07:01.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:01.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:01.033
  Apr 24 04:07:01.037: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  W0424 04:07:03.836016      14 warnings.go:70] unknown field "alpha"
  W0424 04:07:03.836126      14 warnings.go:70] unknown field "beta"
  W0424 04:07:03.836194      14 warnings.go:70] unknown field "delta"
  W0424 04:07:03.836237      14 warnings.go:70] unknown field "epsilon"
  W0424 04:07:03.836359      14 warnings.go:70] unknown field "gamma"
  Apr 24 04:07:03.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1834" for this suite. @ 04/24/23 04:07:03.899
• [2.908 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/24/23 04:07:03.921
  Apr 24 04:07:03.921: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 04:07:03.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:03.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:03.961
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/24/23 04:07:03.965
  Apr 24 04:07:03.965: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/24/23 04:07:11.777
  Apr 24 04:07:11.778: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:07:13.455: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:07:21.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7457" for this suite. @ 04/24/23 04:07:21.963
• [18.051 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/24/23 04:07:21.973
  Apr 24 04:07:21.973: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:07:21.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:22.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:22.016
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 04:07:22.021
  STEP: Saw pod success @ 04/24/23 04:07:26.066
  Apr 24 04:07:26.073: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-8a47aebe-4172-4460-9a00-3dcd1caa8cb4 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 04:07:26.102
  Apr 24 04:07:26.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6733" for this suite. @ 04/24/23 04:07:26.136
• [4.171 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/24/23 04:07:26.147
  Apr 24 04:07:26.147: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename podtemplate @ 04/24/23 04:07:26.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:26.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:26.178
  Apr 24 04:07:26.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4054" for this suite. @ 04/24/23 04:07:26.236
• [0.099 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/24/23 04:07:26.247
  Apr 24 04:07:26.247: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:07:26.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:26.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:26.277
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/24/23 04:07:26.281
  STEP: Saw pod success @ 04/24/23 04:07:30.323
  Apr 24 04:07:30.332: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-e616d4b5-2149-4dd1-9bb0-fff41b126618 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:07:30.347
  Apr 24 04:07:30.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3559" for this suite. @ 04/24/23 04:07:30.383
• [4.146 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/24/23 04:07:30.394
  Apr 24 04:07:30.394: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:07:30.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:30.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:30.426
  Apr 24 04:07:30.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-5860 version'
  Apr 24 04:07:30.580: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 24 04:07:30.581: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 24 04:07:30.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5860" for this suite. @ 04/24/23 04:07:30.59
• [0.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/24/23 04:07:30.604
  Apr 24 04:07:30.604: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/24/23 04:07:30.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:30.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:30.63
  STEP: Creating 50 configmaps @ 04/24/23 04:07:30.634
  STEP: Creating RC which spawns configmap-volume pods @ 04/24/23 04:07:30.979
  Apr 24 04:07:31.038: INFO: Pod name wrapped-volume-race-5d9fa62d-6eb8-4891-bbcf-c596d6f4c554: Found 0 pods out of 5
  Apr 24 04:07:36.058: INFO: Pod name wrapped-volume-race-5d9fa62d-6eb8-4891-bbcf-c596d6f4c554: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/24/23 04:07:36.058
  STEP: Creating RC which spawns configmap-volume pods @ 04/24/23 04:07:36.119
  Apr 24 04:07:36.155: INFO: Pod name wrapped-volume-race-62f2e9ec-cd8b-415f-9f92-f662ab162e5b: Found 0 pods out of 5
  Apr 24 04:07:41.175: INFO: Pod name wrapped-volume-race-62f2e9ec-cd8b-415f-9f92-f662ab162e5b: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/24/23 04:07:41.176
  STEP: Creating RC which spawns configmap-volume pods @ 04/24/23 04:07:41.22
  Apr 24 04:07:41.244: INFO: Pod name wrapped-volume-race-cd0b5b7a-1dfb-436e-88c2-87eebbb43a58: Found 0 pods out of 5
  Apr 24 04:07:46.287: INFO: Pod name wrapped-volume-race-cd0b5b7a-1dfb-436e-88c2-87eebbb43a58: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/24/23 04:07:46.289
  Apr 24 04:07:46.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-cd0b5b7a-1dfb-436e-88c2-87eebbb43a58 in namespace emptydir-wrapper-467, will wait for the garbage collector to delete the pods @ 04/24/23 04:07:46.39
  Apr 24 04:07:46.466: INFO: Deleting ReplicationController wrapped-volume-race-cd0b5b7a-1dfb-436e-88c2-87eebbb43a58 took: 17.249535ms
  Apr 24 04:07:46.567: INFO: Terminating ReplicationController wrapped-volume-race-cd0b5b7a-1dfb-436e-88c2-87eebbb43a58 pods took: 101.035973ms
  STEP: deleting ReplicationController wrapped-volume-race-62f2e9ec-cd8b-415f-9f92-f662ab162e5b in namespace emptydir-wrapper-467, will wait for the garbage collector to delete the pods @ 04/24/23 04:07:48.569
  Apr 24 04:07:48.650: INFO: Deleting ReplicationController wrapped-volume-race-62f2e9ec-cd8b-415f-9f92-f662ab162e5b took: 18.653661ms
  Apr 24 04:07:48.851: INFO: Terminating ReplicationController wrapped-volume-race-62f2e9ec-cd8b-415f-9f92-f662ab162e5b pods took: 200.964259ms
  STEP: deleting ReplicationController wrapped-volume-race-5d9fa62d-6eb8-4891-bbcf-c596d6f4c554 in namespace emptydir-wrapper-467, will wait for the garbage collector to delete the pods @ 04/24/23 04:07:50.852
  Apr 24 04:07:50.925: INFO: Deleting ReplicationController wrapped-volume-race-5d9fa62d-6eb8-4891-bbcf-c596d6f4c554 took: 16.561183ms
  Apr 24 04:07:51.026: INFO: Terminating ReplicationController wrapped-volume-race-5d9fa62d-6eb8-4891-bbcf-c596d6f4c554 pods took: 100.696327ms
  STEP: Cleaning up the configMaps @ 04/24/23 04:07:52.928
  STEP: Destroying namespace "emptydir-wrapper-467" for this suite. @ 04/24/23 04:07:53.369
• [22.776 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/24/23 04:07:53.385
  Apr 24 04:07:53.385: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename job @ 04/24/23 04:07:53.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:07:53.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:07:53.417
  STEP: Creating a job @ 04/24/23 04:07:53.423
  STEP: Ensuring active pods == parallelism @ 04/24/23 04:07:53.433
  STEP: Orphaning one of the Job's Pods @ 04/24/23 04:07:55.455
  Apr 24 04:07:55.999: INFO: Successfully updated pod "adopt-release-b8m5s"
  STEP: Checking that the Job readopts the Pod @ 04/24/23 04:07:55.999
  STEP: Removing the labels from the Job's Pod @ 04/24/23 04:07:58.045
  Apr 24 04:07:58.568: INFO: Successfully updated pod "adopt-release-b8m5s"
  STEP: Checking that the Job releases the Pod @ 04/24/23 04:07:58.568
  Apr 24 04:08:00.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6783" for this suite. @ 04/24/23 04:08:00.607
• [7.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/24/23 04:08:00.632
  Apr 24 04:08:00.632: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:08:00.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:08:00.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:08:00.661
  STEP: Setting up server cert @ 04/24/23 04:08:00.7
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:08:01.528
  STEP: Deploying the webhook pod @ 04/24/23 04:08:01.537
  STEP: Wait for the deployment to be ready @ 04/24/23 04:08:01.555
  Apr 24 04:08:01.566: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Apr 24 04:08:03.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 8, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 8, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 8, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 8, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/24/23 04:08:05.593
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:08:05.618
  Apr 24 04:08:06.619: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/24/23 04:08:06.627
  STEP: create a pod that should be denied by the webhook @ 04/24/23 04:08:06.663
  STEP: create a pod that causes the webhook to hang @ 04/24/23 04:08:06.686
  STEP: create a configmap that should be denied by the webhook @ 04/24/23 04:08:16.699
  STEP: create a configmap that should be admitted by the webhook @ 04/24/23 04:08:16.744
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/24/23 04:08:16.766
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/24/23 04:08:16.779
  STEP: create a namespace that bypass the webhook @ 04/24/23 04:08:16.789
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/24/23 04:08:16.814
  Apr 24 04:08:16.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6404" for this suite. @ 04/24/23 04:08:16.943
  STEP: Destroying namespace "webhook-markers-3926" for this suite. @ 04/24/23 04:08:16.977
  STEP: Destroying namespace "exempted-namespace-4849" for this suite. @ 04/24/23 04:08:16.99
• [16.372 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/24/23 04:08:17.006
  Apr 24 04:08:17.006: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:08:17.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:08:17.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:08:17.05
  STEP: Setting up server cert @ 04/24/23 04:08:17.128
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:08:17.907
  STEP: Deploying the webhook pod @ 04/24/23 04:08:17.926
  STEP: Wait for the deployment to be ready @ 04/24/23 04:08:17.95
  Apr 24 04:08:17.969: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Apr 24 04:08:20.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 8, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 8, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 8, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 8, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/24/23 04:08:22.043
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:08:22.078
  Apr 24 04:08:23.078: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/24/23 04:08:23.085
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/24/23 04:08:23.087
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/24/23 04:08:23.087
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/24/23 04:08:23.087
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/24/23 04:08:23.089
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/24/23 04:08:23.09
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/24/23 04:08:23.092
  Apr 24 04:08:23.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8008" for this suite. @ 04/24/23 04:08:23.183
  STEP: Destroying namespace "webhook-markers-1540" for this suite. @ 04/24/23 04:08:23.203
• [6.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/24/23 04:08:23.222
  Apr 24 04:08:23.222: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename proxy @ 04/24/23 04:08:23.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:08:23.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:08:23.249
  Apr 24 04:08:23.255: INFO: Creating pod...
  Apr 24 04:08:25.283: INFO: Creating service...
  Apr 24 04:08:25.303: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/pods/agnhost/proxy/some/path/with/DELETE
  Apr 24 04:08:25.329: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 24 04:08:25.330: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/pods/agnhost/proxy/some/path/with/GET
  Apr 24 04:08:25.338: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 24 04:08:25.339: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/pods/agnhost/proxy/some/path/with/HEAD
  Apr 24 04:08:25.344: INFO: http.Client request:HEAD | StatusCode:200
  Apr 24 04:08:25.344: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 24 04:08:25.349: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 24 04:08:25.349: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/pods/agnhost/proxy/some/path/with/PATCH
  Apr 24 04:08:25.353: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 24 04:08:25.353: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/pods/agnhost/proxy/some/path/with/POST
  Apr 24 04:08:25.358: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 24 04:08:25.358: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/pods/agnhost/proxy/some/path/with/PUT
  Apr 24 04:08:25.363: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 24 04:08:25.363: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/services/test-service/proxy/some/path/with/DELETE
  Apr 24 04:08:25.377: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 24 04:08:25.378: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/services/test-service/proxy/some/path/with/GET
  Apr 24 04:08:25.387: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 24 04:08:25.387: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/services/test-service/proxy/some/path/with/HEAD
  Apr 24 04:08:25.393: INFO: http.Client request:HEAD | StatusCode:200
  Apr 24 04:08:25.394: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/services/test-service/proxy/some/path/with/OPTIONS
  Apr 24 04:08:25.402: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 24 04:08:25.402: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/services/test-service/proxy/some/path/with/PATCH
  Apr 24 04:08:25.408: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 24 04:08:25.408: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/services/test-service/proxy/some/path/with/POST
  Apr 24 04:08:25.415: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 24 04:08:25.415: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-52/services/test-service/proxy/some/path/with/PUT
  Apr 24 04:08:25.424: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 24 04:08:25.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-52" for this suite. @ 04/24/23 04:08:25.432
• [2.234 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/24/23 04:08:25.458
  Apr 24 04:08:25.458: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename var-expansion @ 04/24/23 04:08:25.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:08:25.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:08:25.493
  STEP: creating the pod @ 04/24/23 04:08:25.504
  STEP: waiting for pod running @ 04/24/23 04:08:25.526
  STEP: creating a file in subpath @ 04/24/23 04:08:27.542
  Apr 24 04:08:27.548: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8487 PodName:var-expansion-819c8d00-00a5-44ed-9865-7e226a7413dc ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:08:27.549: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:08:27.551: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:08:27.552: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8487/pods/var-expansion-819c8d00-00a5-44ed-9865-7e226a7413dc/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/24/23 04:08:27.7
  Apr 24 04:08:27.704: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8487 PodName:var-expansion-819c8d00-00a5-44ed-9865-7e226a7413dc ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:08:27.704: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:08:27.706: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:08:27.707: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8487/pods/var-expansion-819c8d00-00a5-44ed-9865-7e226a7413dc/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/24/23 04:08:27.83
  Apr 24 04:08:28.377: INFO: Successfully updated pod "var-expansion-819c8d00-00a5-44ed-9865-7e226a7413dc"
  STEP: waiting for annotated pod running @ 04/24/23 04:08:28.377
  STEP: deleting the pod gracefully @ 04/24/23 04:08:28.383
  Apr 24 04:08:28.383: INFO: Deleting pod "var-expansion-819c8d00-00a5-44ed-9865-7e226a7413dc" in namespace "var-expansion-8487"
  Apr 24 04:08:28.431: INFO: Wait up to 5m0s for pod "var-expansion-819c8d00-00a5-44ed-9865-7e226a7413dc" to be fully deleted
  Apr 24 04:09:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8487" for this suite. @ 04/24/23 04:09:00.657
• [35.211 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/24/23 04:09:00.678
  Apr 24 04:09:00.678: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename disruption @ 04/24/23 04:09:00.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:09:00.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:09:00.712
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/24/23 04:09:00.719
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:09:00.734
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/24/23 04:09:02.766
  STEP: Waiting for all pods to be running @ 04/24/23 04:09:02.767
  Apr 24 04:09:02.776: INFO: pods: 0 < 3
  STEP: locating a running pod @ 04/24/23 04:09:04.789
  STEP: Updating the pdb to allow a pod to be evicted @ 04/24/23 04:09:04.809
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:09:04.824
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/24/23 04:09:06.842
  STEP: Waiting for all pods to be running @ 04/24/23 04:09:06.842
  STEP: Waiting for the pdb to observed all healthy pods @ 04/24/23 04:09:06.851
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/24/23 04:09:06.9
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:09:06.968
  STEP: Waiting for all pods to be running @ 04/24/23 04:09:08.982
  STEP: locating a running pod @ 04/24/23 04:09:08.991
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/24/23 04:09:09.011
  STEP: Waiting for the pdb to be deleted @ 04/24/23 04:09:09.028
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/24/23 04:09:09.031
  STEP: Waiting for all pods to be running @ 04/24/23 04:09:09.031
  Apr 24 04:09:09.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4558" for this suite. @ 04/24/23 04:09:09.099
• [8.478 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/24/23 04:09:09.158
  Apr 24 04:09:09.158: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 04:09:09.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:09:09.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:09:09.21
  STEP: creating a Service @ 04/24/23 04:09:09.237
  STEP: watching for the Service to be added @ 04/24/23 04:09:09.256
  Apr 24 04:09:09.261: INFO: Found Service test-service-ndvn5 in namespace services-2196 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 24 04:09:09.261: INFO: Service test-service-ndvn5 created
  STEP: Getting /status @ 04/24/23 04:09:09.261
  Apr 24 04:09:09.276: INFO: Service test-service-ndvn5 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/24/23 04:09:09.277
  STEP: watching for the Service to be patched @ 04/24/23 04:09:09.291
  Apr 24 04:09:09.294: INFO: observed Service test-service-ndvn5 in namespace services-2196 with annotations: map[] & LoadBalancer: {[]}
  Apr 24 04:09:09.294: INFO: Found Service test-service-ndvn5 in namespace services-2196 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 24 04:09:09.295: INFO: Service test-service-ndvn5 has service status patched
  STEP: updating the ServiceStatus @ 04/24/23 04:09:09.295
  Apr 24 04:09:09.307: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/24/23 04:09:09.307
  Apr 24 04:09:09.309: INFO: Observed Service test-service-ndvn5 in namespace services-2196 with annotations: map[] & Conditions: {[]}
  Apr 24 04:09:09.309: INFO: Observed event: &Service{ObjectMeta:{test-service-ndvn5  services-2196  3b8b8f5e-b270-4e2c-82c3-37d21d6fd26b 19467 0 2023-04-24 04:09:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-24 04:09:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-24 04:09:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.61.52,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.61.52],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 24 04:09:09.310: INFO: Found Service test-service-ndvn5 in namespace services-2196 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 24 04:09:09.310: INFO: Service test-service-ndvn5 has service status updated
  STEP: patching the service @ 04/24/23 04:09:09.31
  STEP: watching for the Service to be patched @ 04/24/23 04:09:09.346
  Apr 24 04:09:09.351: INFO: observed Service test-service-ndvn5 in namespace services-2196 with labels: map[test-service-static:true]
  Apr 24 04:09:09.351: INFO: observed Service test-service-ndvn5 in namespace services-2196 with labels: map[test-service-static:true]
  Apr 24 04:09:09.351: INFO: observed Service test-service-ndvn5 in namespace services-2196 with labels: map[test-service-static:true]
  Apr 24 04:09:09.351: INFO: Found Service test-service-ndvn5 in namespace services-2196 with labels: map[test-service:patched test-service-static:true]
  Apr 24 04:09:09.351: INFO: Service test-service-ndvn5 patched
  STEP: deleting the service @ 04/24/23 04:09:09.351
  STEP: watching for the Service to be deleted @ 04/24/23 04:09:09.37
  Apr 24 04:09:09.373: INFO: Observed event: ADDED
  Apr 24 04:09:09.373: INFO: Observed event: MODIFIED
  Apr 24 04:09:09.374: INFO: Observed event: MODIFIED
  Apr 24 04:09:09.374: INFO: Observed event: MODIFIED
  Apr 24 04:09:09.375: INFO: Found Service test-service-ndvn5 in namespace services-2196 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 24 04:09:09.375: INFO: Service test-service-ndvn5 deleted
  Apr 24 04:09:09.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2196" for this suite. @ 04/24/23 04:09:09.392
• [0.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/24/23 04:09:09.405
  Apr 24 04:09:09.405: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:09:09.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:09:09.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:09:09.444
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/24/23 04:09:09.45
  STEP: Saw pod success @ 04/24/23 04:09:13.498
  Apr 24 04:09:13.502: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-7f93a243-49a2-446d-8997-9ef0902b5928 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:09:13.531
  Apr 24 04:09:13.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1113" for this suite. @ 04/24/23 04:09:13.564
• [4.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/24/23 04:09:13.582
  Apr 24 04:09:13.582: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename dns @ 04/24/23 04:09:13.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:09:13.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:09:13.611
  STEP: Creating a test headless service @ 04/24/23 04:09:13.615
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local;sleep 1; done
   @ 04/24/23 04:09:13.632
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local;sleep 1; done
   @ 04/24/23 04:09:13.632
  STEP: creating a pod to probe DNS @ 04/24/23 04:09:13.632
  STEP: submitting the pod to kubernetes @ 04/24/23 04:09:13.632
  STEP: retrieving the pod @ 04/24/23 04:09:17.674
  STEP: looking for the results for each expected name from probers @ 04/24/23 04:09:17.68
  Apr 24 04:09:17.691: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:17.698: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:17.708: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:17.715: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:17.726: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:17.733: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:17.738: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:17.745: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:17.745: INFO: Lookups using dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local]

  Apr 24 04:09:22.755: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:22.765: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:22.772: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:22.780: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:22.789: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:22.798: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:22.805: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:22.813: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:22.813: INFO: Lookups using dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local]

  Apr 24 04:09:27.755: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:27.762: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:27.768: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:27.775: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:27.783: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:27.791: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:27.796: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:27.801: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:27.801: INFO: Lookups using dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local]

  Apr 24 04:09:32.754: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:32.762: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:32.769: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:32.775: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:32.780: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:32.785: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:32.790: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:32.794: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:32.794: INFO: Lookups using dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local]

  Apr 24 04:09:37.761: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:37.769: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:37.775: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:37.781: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:37.787: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:37.793: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:37.799: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:37.804: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:37.804: INFO: Lookups using dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local]

  Apr 24 04:09:42.759: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:42.764: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:42.769: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:42.774: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:42.784: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:42.789: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:42.798: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:42.805: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local from pod dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e: the server could not find the requested resource (get pods dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e)
  Apr 24 04:09:42.805: INFO: Lookups using dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6771.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6771.svc.cluster.local jessie_udp@dns-test-service-2.dns-6771.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6771.svc.cluster.local]

  Apr 24 04:09:47.804: INFO: DNS probes using dns-6771/dns-test-495a7e60-dc09-40e3-ab88-b3157eee719e succeeded

  Apr 24 04:09:47.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 04:09:47.816
  STEP: deleting the test headless service @ 04/24/23 04:09:47.855
  STEP: Destroying namespace "dns-6771" for this suite. @ 04/24/23 04:09:47.901
• [34.336 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/24/23 04:09:47.919
  Apr 24 04:09:47.919: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:09:47.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:09:47.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:09:47.963
  STEP: Creating configMap with name projected-configmap-test-volume-f6f22230-e353-44ca-83e0-34699144142a @ 04/24/23 04:09:47.968
  STEP: Creating a pod to test consume configMaps @ 04/24/23 04:09:47.976
  STEP: Saw pod success @ 04/24/23 04:09:52.076
  Apr 24 04:09:52.083: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-configmaps-28476544-47bb-4015-b63e-12e3bb005c5c container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 04:09:52.1
  Apr 24 04:09:52.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1700" for this suite. @ 04/24/23 04:09:52.137
• [4.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/24/23 04:09:52.172
  Apr 24 04:09:52.172: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:09:52.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:09:52.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:09:52.211
  STEP: Creating a ResourceQuota with terminating scope @ 04/24/23 04:09:52.216
  STEP: Ensuring ResourceQuota status is calculated @ 04/24/23 04:09:52.229
  STEP: Creating a ResourceQuota with not terminating scope @ 04/24/23 04:09:54.237
  STEP: Ensuring ResourceQuota status is calculated @ 04/24/23 04:09:54.247
  STEP: Creating a long running pod @ 04/24/23 04:09:56.255
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/24/23 04:09:56.283
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/24/23 04:09:58.292
  STEP: Deleting the pod @ 04/24/23 04:10:00.303
  STEP: Ensuring resource quota status released the pod usage @ 04/24/23 04:10:00.334
  STEP: Creating a terminating pod @ 04/24/23 04:10:02.345
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/24/23 04:10:02.366
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/24/23 04:10:04.378
  STEP: Deleting the pod @ 04/24/23 04:10:06.388
  STEP: Ensuring resource quota status released the pod usage @ 04/24/23 04:10:06.414
  Apr 24 04:10:08.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1631" for this suite. @ 04/24/23 04:10:08.445
• [16.287 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/24/23 04:10:08.468
  Apr 24 04:10:08.468: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename lease-test @ 04/24/23 04:10:08.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:10:08.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:10:08.507
  Apr 24 04:10:08.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-7489" for this suite. @ 04/24/23 04:10:08.616
• [0.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/24/23 04:10:08.631
  Apr 24 04:10:08.632: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 04:10:08.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:10:08.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:10:08.676
  STEP: creating service in namespace services-6551 @ 04/24/23 04:10:08.683
  STEP: creating service affinity-clusterip-transition in namespace services-6551 @ 04/24/23 04:10:08.683
  STEP: creating replication controller affinity-clusterip-transition in namespace services-6551 @ 04/24/23 04:10:08.701
  I0424 04:10:08.710570      14 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-6551, replica count: 3
  I0424 04:10:11.764023      14 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 04:10:11.779: INFO: Creating new exec pod
  Apr 24 04:10:14.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6551 exec execpod-affinityhs4rt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Apr 24 04:10:15.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 24 04:10:15.370: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:10:15.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6551 exec execpod-affinityhs4rt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.213 80'
  Apr 24 04:10:15.632: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.213 80\nConnection to 10.233.16.213 80 port [tcp/http] succeeded!\n"
  Apr 24 04:10:15.632: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:10:15.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6551 exec execpod-affinityhs4rt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.16.213:80/ ; done'
  Apr 24 04:10:16.092: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n"
  Apr 24 04:10:16.092: INFO: stdout: "\naffinity-clusterip-transition-lkfcn\naffinity-clusterip-transition-ltrd2\naffinity-clusterip-transition-ltrd2\naffinity-clusterip-transition-lkfcn\naffinity-clusterip-transition-lkfcn\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-lkfcn\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-lkfcn\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-ltrd2\naffinity-clusterip-transition-ltrd2\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-lkfcn\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-ltrd2"
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-lkfcn
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-ltrd2
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-ltrd2
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-lkfcn
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-lkfcn
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-lkfcn
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-lkfcn
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-ltrd2
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-ltrd2
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-lkfcn
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.092: INFO: Received response from host: affinity-clusterip-transition-ltrd2
  Apr 24 04:10:16.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6551 exec execpod-affinityhs4rt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.16.213:80/ ; done'
  Apr 24 04:10:16.505: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.213:80/\n"
  Apr 24 04:10:16.505: INFO: stdout: "\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm\naffinity-clusterip-transition-8cdfm"
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Received response from host: affinity-clusterip-transition-8cdfm
  Apr 24 04:10:16.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 04:10:16.516: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6551, will wait for the garbage collector to delete the pods @ 04/24/23 04:10:16.548
  Apr 24 04:10:16.624: INFO: Deleting ReplicationController affinity-clusterip-transition took: 15.199471ms
  Apr 24 04:10:16.726: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.450025ms
  STEP: Destroying namespace "services-6551" for this suite. @ 04/24/23 04:10:18.558
• [9.936 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/24/23 04:10:18.573
  Apr 24 04:10:18.573: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 04:10:18.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:10:18.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:10:18.602
  Apr 24 04:10:18.607: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/24/23 04:10:20.684
  Apr 24 04:10:20.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 create -f -'
  Apr 24 04:10:22.180: INFO: stderr: ""
  Apr 24 04:10:22.180: INFO: stdout: "e2e-test-crd-publish-openapi-7413-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 24 04:10:22.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 delete e2e-test-crd-publish-openapi-7413-crds test-foo'
  Apr 24 04:10:22.376: INFO: stderr: ""
  Apr 24 04:10:22.376: INFO: stdout: "e2e-test-crd-publish-openapi-7413-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 24 04:10:22.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 apply -f -'
  Apr 24 04:10:22.999: INFO: stderr: ""
  Apr 24 04:10:22.999: INFO: stdout: "e2e-test-crd-publish-openapi-7413-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 24 04:10:22.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 delete e2e-test-crd-publish-openapi-7413-crds test-foo'
  Apr 24 04:10:23.219: INFO: stderr: ""
  Apr 24 04:10:23.219: INFO: stdout: "e2e-test-crd-publish-openapi-7413-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/24/23 04:10:23.219
  Apr 24 04:10:23.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 create -f -'
  Apr 24 04:10:24.555: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/24/23 04:10:24.556
  Apr 24 04:10:24.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 create -f -'
  Apr 24 04:10:25.209: INFO: rc: 1
  Apr 24 04:10:25.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 apply -f -'
  Apr 24 04:10:25.627: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/24/23 04:10:25.628
  Apr 24 04:10:25.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 create -f -'
  Apr 24 04:10:26.053: INFO: rc: 1
  Apr 24 04:10:26.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 --namespace=crd-publish-openapi-4116 apply -f -'
  Apr 24 04:10:26.480: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/24/23 04:10:26.48
  Apr 24 04:10:26.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 explain e2e-test-crd-publish-openapi-7413-crds'
  Apr 24 04:10:27.151: INFO: stderr: ""
  Apr 24 04:10:27.151: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7413-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/24/23 04:10:27.151
  Apr 24 04:10:27.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 explain e2e-test-crd-publish-openapi-7413-crds.metadata'
  Apr 24 04:10:27.620: INFO: stderr: ""
  Apr 24 04:10:27.620: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7413-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 24 04:10:27.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 explain e2e-test-crd-publish-openapi-7413-crds.spec'
  Apr 24 04:10:28.036: INFO: stderr: ""
  Apr 24 04:10:28.036: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7413-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 24 04:10:28.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 explain e2e-test-crd-publish-openapi-7413-crds.spec.bars'
  Apr 24 04:10:28.452: INFO: stderr: ""
  Apr 24 04:10:28.452: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7413-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/24/23 04:10:28.452
  Apr 24 04:10:28.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4116 explain e2e-test-crd-publish-openapi-7413-crds.spec.bars2'
  Apr 24 04:10:29.045: INFO: rc: 1
  Apr 24 04:10:31.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4116" for this suite. @ 04/24/23 04:10:31.476
• [12.915 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/24/23 04:10:31.496
  Apr 24 04:10:31.497: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename runtimeclass @ 04/24/23 04:10:31.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:10:31.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:10:31.527
  Apr 24 04:10:31.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9728" for this suite. @ 04/24/23 04:10:31.547
• [0.059 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/24/23 04:10:31.559
  Apr 24 04:10:31.559: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:10:31.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:10:31.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:10:31.587
  STEP: creating Agnhost RC @ 04/24/23 04:10:31.591
  Apr 24 04:10:31.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-2431 create -f -'
  Apr 24 04:10:33.016: INFO: stderr: ""
  Apr 24 04:10:33.016: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/24/23 04:10:33.016
  Apr 24 04:10:34.028: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 04:10:34.028: INFO: Found 0 / 1
  Apr 24 04:10:35.023: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 04:10:35.023: INFO: Found 1 / 1
  Apr 24 04:10:35.023: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/24/23 04:10:35.023
  Apr 24 04:10:35.029: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 04:10:35.029: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 24 04:10:35.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-2431 patch pod agnhost-primary-bflrl -p {"metadata":{"annotations":{"x":"y"}}}'
  Apr 24 04:10:35.253: INFO: stderr: ""
  Apr 24 04:10:35.253: INFO: stdout: "pod/agnhost-primary-bflrl patched\n"
  STEP: checking annotations @ 04/24/23 04:10:35.253
  Apr 24 04:10:35.261: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 04:10:35.261: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 24 04:10:35.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2431" for this suite. @ 04/24/23 04:10:35.269
• [3.721 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/24/23 04:10:35.287
  Apr 24 04:10:35.287: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-runtime @ 04/24/23 04:10:35.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:10:35.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:10:35.321
  STEP: create the container @ 04/24/23 04:10:35.326
  W0424 04:10:35.336742      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/24/23 04:10:35.337
  STEP: get the container status @ 04/24/23 04:10:38.373
  STEP: the container should be terminated @ 04/24/23 04:10:38.377
  STEP: the termination message should be set @ 04/24/23 04:10:38.378
  Apr 24 04:10:38.378: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/24/23 04:10:38.378
  Apr 24 04:10:38.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5204" for this suite. @ 04/24/23 04:10:38.409
• [3.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/24/23 04:10:38.421
  Apr 24 04:10:38.421: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubelet-test @ 04/24/23 04:10:38.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:10:38.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:10:38.459
  Apr 24 04:10:42.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2595" for this suite. @ 04/24/23 04:10:42.499
• [4.085 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/24/23 04:10:42.518
  Apr 24 04:10:42.518: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:10:42.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:10:42.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:10:42.558
  STEP: creating a replication controller @ 04/24/23 04:10:42.562
  Apr 24 04:10:42.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 create -f -'
  Apr 24 04:10:43.251: INFO: stderr: ""
  Apr 24 04:10:43.251: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/24/23 04:10:43.251
  Apr 24 04:10:43.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 24 04:10:43.465: INFO: stderr: ""
  Apr 24 04:10:43.465: INFO: stdout: "update-demo-nautilus-fpgd2 update-demo-nautilus-sbvzf "
  Apr 24 04:10:43.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-fpgd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:10:43.644: INFO: stderr: ""
  Apr 24 04:10:43.644: INFO: stdout: ""
  Apr 24 04:10:43.644: INFO: update-demo-nautilus-fpgd2 is created but not running
  Apr 24 04:10:48.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 24 04:10:48.965: INFO: stderr: ""
  Apr 24 04:10:48.965: INFO: stdout: "update-demo-nautilus-fpgd2 update-demo-nautilus-sbvzf "
  Apr 24 04:10:48.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-fpgd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:10:49.165: INFO: stderr: ""
  Apr 24 04:10:49.165: INFO: stdout: ""
  Apr 24 04:10:49.165: INFO: update-demo-nautilus-fpgd2 is created but not running
  Apr 24 04:10:54.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 24 04:10:54.314: INFO: stderr: ""
  Apr 24 04:10:54.319: INFO: stdout: "update-demo-nautilus-fpgd2 update-demo-nautilus-sbvzf "
  Apr 24 04:10:54.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-fpgd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:10:54.453: INFO: stderr: ""
  Apr 24 04:10:54.453: INFO: stdout: ""
  Apr 24 04:10:54.453: INFO: update-demo-nautilus-fpgd2 is created but not running
  Apr 24 04:10:59.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 24 04:10:59.590: INFO: stderr: ""
  Apr 24 04:10:59.590: INFO: stdout: "update-demo-nautilus-fpgd2 update-demo-nautilus-sbvzf "
  Apr 24 04:10:59.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-fpgd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:10:59.717: INFO: stderr: ""
  Apr 24 04:10:59.717: INFO: stdout: "true"
  Apr 24 04:10:59.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-fpgd2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 24 04:10:59.838: INFO: stderr: ""
  Apr 24 04:10:59.838: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 24 04:10:59.838: INFO: validating pod update-demo-nautilus-fpgd2
  Apr 24 04:10:59.861: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 24 04:10:59.862: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 24 04:10:59.862: INFO: update-demo-nautilus-fpgd2 is verified up and running
  Apr 24 04:10:59.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-sbvzf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:10:59.979: INFO: stderr: ""
  Apr 24 04:10:59.979: INFO: stdout: "true"
  Apr 24 04:10:59.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-sbvzf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 24 04:11:00.105: INFO: stderr: ""
  Apr 24 04:11:00.105: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 24 04:11:00.105: INFO: validating pod update-demo-nautilus-sbvzf
  Apr 24 04:11:00.121: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 24 04:11:00.121: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 24 04:11:00.121: INFO: update-demo-nautilus-sbvzf is verified up and running
  STEP: scaling down the replication controller @ 04/24/23 04:11:00.121
  Apr 24 04:11:00.138: INFO: scanned /root for discovery docs: <nil>
  Apr 24 04:11:00.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Apr 24 04:11:01.333: INFO: stderr: ""
  Apr 24 04:11:01.333: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/24/23 04:11:01.334
  Apr 24 04:11:01.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 24 04:11:01.468: INFO: stderr: ""
  Apr 24 04:11:01.468: INFO: stdout: "update-demo-nautilus-sbvzf "
  Apr 24 04:11:01.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-sbvzf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:11:01.586: INFO: stderr: ""
  Apr 24 04:11:01.586: INFO: stdout: "true"
  Apr 24 04:11:01.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-sbvzf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 24 04:11:01.711: INFO: stderr: ""
  Apr 24 04:11:01.711: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 24 04:11:01.711: INFO: validating pod update-demo-nautilus-sbvzf
  Apr 24 04:11:01.717: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 24 04:11:01.717: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 24 04:11:01.717: INFO: update-demo-nautilus-sbvzf is verified up and running
  STEP: scaling up the replication controller @ 04/24/23 04:11:01.718
  Apr 24 04:11:01.733: INFO: scanned /root for discovery docs: <nil>
  Apr 24 04:11:01.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Apr 24 04:11:02.917: INFO: stderr: ""
  Apr 24 04:11:02.917: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/24/23 04:11:02.917
  Apr 24 04:11:02.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 24 04:11:03.149: INFO: stderr: ""
  Apr 24 04:11:03.149: INFO: stdout: "update-demo-nautilus-nzh9f update-demo-nautilus-sbvzf "
  Apr 24 04:11:03.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-nzh9f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:11:03.274: INFO: stderr: ""
  Apr 24 04:11:03.274: INFO: stdout: ""
  Apr 24 04:11:03.274: INFO: update-demo-nautilus-nzh9f is created but not running
  Apr 24 04:11:08.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 24 04:11:08.409: INFO: stderr: ""
  Apr 24 04:11:08.409: INFO: stdout: "update-demo-nautilus-nzh9f update-demo-nautilus-sbvzf "
  Apr 24 04:11:08.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-nzh9f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:11:08.531: INFO: stderr: ""
  Apr 24 04:11:08.531: INFO: stdout: "true"
  Apr 24 04:11:08.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-nzh9f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 24 04:11:08.653: INFO: stderr: ""
  Apr 24 04:11:08.654: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 24 04:11:08.654: INFO: validating pod update-demo-nautilus-nzh9f
  Apr 24 04:11:08.670: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 24 04:11:08.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 24 04:11:08.670: INFO: update-demo-nautilus-nzh9f is verified up and running
  Apr 24 04:11:08.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-sbvzf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:11:08.859: INFO: stderr: ""
  Apr 24 04:11:08.859: INFO: stdout: "true"
  Apr 24 04:11:08.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods update-demo-nautilus-sbvzf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 24 04:11:09.094: INFO: stderr: ""
  Apr 24 04:11:09.094: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 24 04:11:09.094: INFO: validating pod update-demo-nautilus-sbvzf
  Apr 24 04:11:09.104: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 24 04:11:09.104: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 24 04:11:09.104: INFO: update-demo-nautilus-sbvzf is verified up and running
  STEP: using delete to clean up resources @ 04/24/23 04:11:09.104
  Apr 24 04:11:09.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 delete --grace-period=0 --force -f -'
  Apr 24 04:11:09.289: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:11:09.289: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 24 04:11:09.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get rc,svc -l name=update-demo --no-headers'
  Apr 24 04:11:09.499: INFO: stderr: "No resources found in kubectl-8331 namespace.\n"
  Apr 24 04:11:09.499: INFO: stdout: ""
  Apr 24 04:11:09.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8331 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 24 04:11:09.673: INFO: stderr: ""
  Apr 24 04:11:09.673: INFO: stdout: ""
  Apr 24 04:11:09.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8331" for this suite. @ 04/24/23 04:11:09.685
• [27.179 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/24/23 04:11:09.699
  Apr 24 04:11:09.699: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:11:09.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:11:09.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:11:09.742
  STEP: Creating a ResourceQuota with best effort scope @ 04/24/23 04:11:09.749
  STEP: Ensuring ResourceQuota status is calculated @ 04/24/23 04:11:09.76
  STEP: Creating a ResourceQuota with not best effort scope @ 04/24/23 04:11:11.772
  STEP: Ensuring ResourceQuota status is calculated @ 04/24/23 04:11:11.779
  STEP: Creating a best-effort pod @ 04/24/23 04:11:13.791
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/24/23 04:11:13.818
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/24/23 04:11:15.828
  STEP: Deleting the pod @ 04/24/23 04:11:17.838
  STEP: Ensuring resource quota status released the pod usage @ 04/24/23 04:11:17.859
  STEP: Creating a not best-effort pod @ 04/24/23 04:11:19.872
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/24/23 04:11:19.887
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/24/23 04:11:21.894
  STEP: Deleting the pod @ 04/24/23 04:11:23.903
  STEP: Ensuring resource quota status released the pod usage @ 04/24/23 04:11:23.925
  Apr 24 04:11:25.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3469" for this suite. @ 04/24/23 04:11:25.939
• [16.251 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/24/23 04:11:25.954
  Apr 24 04:11:25.954: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename dns @ 04/24/23 04:11:25.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:11:25.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:11:25.989
  STEP: Creating a test headless service @ 04/24/23 04:11:25.994
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4525.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4525.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4525.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4525.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4525.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4525.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4525.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4525.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4525.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 81.1.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.1.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.1.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.1.81_tcp@PTR;sleep 1; done
   @ 04/24/23 04:11:26.043
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4525.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4525.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4525.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4525.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4525.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4525.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4525.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4525.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4525.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4525.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 81.1.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.1.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.1.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.1.81_tcp@PTR;sleep 1; done
   @ 04/24/23 04:11:26.043
  STEP: creating a pod to probe DNS @ 04/24/23 04:11:26.043
  STEP: submitting the pod to kubernetes @ 04/24/23 04:11:26.044
  STEP: retrieving the pod @ 04/24/23 04:11:30.092
  STEP: looking for the results for each expected name from probers @ 04/24/23 04:11:30.098
  Apr 24 04:11:30.110: INFO: Unable to read wheezy_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:30.120: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:30.130: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:30.138: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:30.177: INFO: Unable to read jessie_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:30.184: INFO: Unable to read jessie_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:30.190: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:30.199: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:30.237: INFO: Lookups using dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928 failed for: [wheezy_udp@dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_udp@dns-test-service.dns-4525.svc.cluster.local jessie_tcp@dns-test-service.dns-4525.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local]

  Apr 24 04:11:35.254: INFO: Unable to read wheezy_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:35.263: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:35.270: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:35.275: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:35.314: INFO: Unable to read jessie_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:35.320: INFO: Unable to read jessie_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:35.326: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:35.334: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:35.357: INFO: Lookups using dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928 failed for: [wheezy_udp@dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_udp@dns-test-service.dns-4525.svc.cluster.local jessie_tcp@dns-test-service.dns-4525.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local]

  Apr 24 04:11:40.254: INFO: Unable to read wheezy_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:40.261: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:40.268: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:40.276: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:40.313: INFO: Unable to read jessie_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:40.321: INFO: Unable to read jessie_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:40.329: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:40.336: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:40.385: INFO: Lookups using dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928 failed for: [wheezy_udp@dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_udp@dns-test-service.dns-4525.svc.cluster.local jessie_tcp@dns-test-service.dns-4525.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local]

  Apr 24 04:11:45.253: INFO: Unable to read wheezy_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:45.263: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:45.272: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:45.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:45.309: INFO: Unable to read jessie_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:45.316: INFO: Unable to read jessie_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:45.322: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:45.328: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:45.354: INFO: Lookups using dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928 failed for: [wheezy_udp@dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_udp@dns-test-service.dns-4525.svc.cluster.local jessie_tcp@dns-test-service.dns-4525.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local]

  Apr 24 04:11:50.253: INFO: Unable to read wheezy_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:50.261: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:50.268: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:50.273: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:50.303: INFO: Unable to read jessie_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:50.308: INFO: Unable to read jessie_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:50.315: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:50.321: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:50.343: INFO: Lookups using dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928 failed for: [wheezy_udp@dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_udp@dns-test-service.dns-4525.svc.cluster.local jessie_tcp@dns-test-service.dns-4525.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local]

  Apr 24 04:11:55.248: INFO: Unable to read wheezy_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:55.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:55.264: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:55.269: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:55.301: INFO: Unable to read jessie_udp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:55.305: INFO: Unable to read jessie_tcp@dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:55.311: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:55.317: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local from pod dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928: the server could not find the requested resource (get pods dns-test-f503446d-6461-4495-8591-b8b0b0e19928)
  Apr 24 04:11:55.345: INFO: Lookups using dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928 failed for: [wheezy_udp@dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@dns-test-service.dns-4525.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_udp@dns-test-service.dns-4525.svc.cluster.local jessie_tcp@dns-test-service.dns-4525.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4525.svc.cluster.local]

  Apr 24 04:12:00.349: INFO: DNS probes using dns-4525/dns-test-f503446d-6461-4495-8591-b8b0b0e19928 succeeded

  Apr 24 04:12:00.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 04:12:00.367
  STEP: deleting the test service @ 04/24/23 04:12:00.464
  STEP: deleting the test headless service @ 04/24/23 04:12:00.497
  STEP: Destroying namespace "dns-4525" for this suite. @ 04/24/23 04:12:00.522
• [34.583 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/24/23 04:12:00.542
  Apr 24 04:12:00.542: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubelet-test @ 04/24/23 04:12:00.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:12:00.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:12:00.579
  STEP: Waiting for pod completion @ 04/24/23 04:12:00.599
  Apr 24 04:12:04.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-685" for this suite. @ 04/24/23 04:12:04.67
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/24/23 04:12:04.688
  Apr 24 04:12:04.688: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/24/23 04:12:04.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:12:04.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:12:04.735
  Apr 24 04:12:04.744: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 24 04:13:04.867: INFO: Waiting for terminating namespaces to be deleted...
  Apr 24 04:13:04.884: INFO: Starting informer...
  STEP: Starting pods... @ 04/24/23 04:13:04.884
  Apr 24 04:13:05.120: INFO: Pod1 is running on aeveeng9ieph-3. Tainting Node
  Apr 24 04:13:07.362: INFO: Pod2 is running on aeveeng9ieph-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/24/23 04:13:07.362
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/24/23 04:13:07.382
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/24/23 04:13:07.387
  Apr 24 04:13:13.235: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Apr 24 04:13:33.247: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 24 04:13:33.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/24/23 04:13:33.275
  STEP: Destroying namespace "taint-multiple-pods-6748" for this suite. @ 04/24/23 04:13:33.283
• [88.608 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/24/23 04:13:33.305
  Apr 24 04:13:33.305: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename var-expansion @ 04/24/23 04:13:33.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:13:33.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:13:33.346
  Apr 24 04:13:35.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 04:13:35.393: INFO: Deleting pod "var-expansion-18ae55c9-553b-4682-8616-7bb653f0a450" in namespace "var-expansion-7038"
  Apr 24 04:13:35.413: INFO: Wait up to 5m0s for pod "var-expansion-18ae55c9-553b-4682-8616-7bb653f0a450" to be fully deleted
  STEP: Destroying namespace "var-expansion-7038" for this suite. @ 04/24/23 04:13:37.437
• [4.146 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/24/23 04:13:37.452
  Apr 24 04:13:37.452: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 04:13:37.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:13:37.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:13:37.484
  STEP: creating service nodeport-test with type=NodePort in namespace services-6015 @ 04/24/23 04:13:37.489
  STEP: creating replication controller nodeport-test in namespace services-6015 @ 04/24/23 04:13:37.513
  I0424 04:13:37.527594      14 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-6015, replica count: 2
  I0424 04:13:40.584823      14 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 04:13:40.585: INFO: Creating new exec pod
  Apr 24 04:13:43.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6015 exec execpodk6q48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 24 04:13:43.940: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 24 04:13:43.940: INFO: stdout: "nodeport-test-l7j77"
  Apr 24 04:13:43.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6015 exec execpodk6q48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.19.215 80'
  Apr 24 04:13:44.188: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.19.215 80\nConnection to 10.233.19.215 80 port [tcp/http] succeeded!\n"
  Apr 24 04:13:44.188: INFO: stdout: "nodeport-test-l7j77"
  Apr 24 04:13:44.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6015 exec execpodk6q48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.18 31629'
  Apr 24 04:13:44.420: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.18 31629\nConnection to 192.168.121.18 31629 port [tcp/*] succeeded!\n"
  Apr 24 04:13:44.420: INFO: stdout: ""
  Apr 24 04:13:45.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6015 exec execpodk6q48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.18 31629'
  Apr 24 04:13:45.676: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.18 31629\nConnection to 192.168.121.18 31629 port [tcp/*] succeeded!\n"
  Apr 24 04:13:45.676: INFO: stdout: "nodeport-test-hz2n4"
  Apr 24 04:13:45.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6015 exec execpodk6q48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 31629'
  Apr 24 04:13:45.946: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 31629\nConnection to 192.168.121.43 31629 port [tcp/*] succeeded!\n"
  Apr 24 04:13:45.946: INFO: stdout: ""
  Apr 24 04:13:46.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6015 exec execpodk6q48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 31629'
  Apr 24 04:13:47.251: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 31629\nConnection to 192.168.121.43 31629 port [tcp/*] succeeded!\n"
  Apr 24 04:13:47.251: INFO: stdout: ""
  Apr 24 04:13:47.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-6015 exec execpodk6q48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 31629'
  Apr 24 04:13:48.183: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 31629\nConnection to 192.168.121.43 31629 port [tcp/*] succeeded!\n"
  Apr 24 04:13:48.183: INFO: stdout: "nodeport-test-hz2n4"
  Apr 24 04:13:48.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6015" for this suite. @ 04/24/23 04:13:48.192
• [10.751 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/24/23 04:13:48.204
  Apr 24 04:13:48.204: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename subpath @ 04/24/23 04:13:48.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:13:48.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:13:48.242
  STEP: Setting up data @ 04/24/23 04:13:48.246
  STEP: Creating pod pod-subpath-test-projected-4dvh @ 04/24/23 04:13:48.261
  STEP: Creating a pod to test atomic-volume-subpath @ 04/24/23 04:13:48.261
  STEP: Saw pod success @ 04/24/23 04:14:12.452
  Apr 24 04:14:12.459: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-subpath-test-projected-4dvh container test-container-subpath-projected-4dvh: <nil>
  STEP: delete the pod @ 04/24/23 04:14:12.505
  STEP: Deleting pod pod-subpath-test-projected-4dvh @ 04/24/23 04:14:12.537
  Apr 24 04:14:12.537: INFO: Deleting pod "pod-subpath-test-projected-4dvh" in namespace "subpath-4515"
  Apr 24 04:14:12.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4515" for this suite. @ 04/24/23 04:14:12.549
• [24.358 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/24/23 04:14:12.564
  Apr 24 04:14:12.564: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/24/23 04:14:12.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:14:12.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:14:12.606
  Apr 24 04:14:12.647: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:14:19.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9803" for this suite. @ 04/24/23 04:14:19.142
• [6.591 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/24/23 04:14:19.157
  Apr 24 04:14:19.157: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 04:14:19.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:14:19.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:14:19.198
  STEP: Creating pod liveness-8c1eb422-bc9f-4bbf-9e6a-2ea799ddcbb2 in namespace container-probe-6900 @ 04/24/23 04:14:19.203
  Apr 24 04:14:21.236: INFO: Started pod liveness-8c1eb422-bc9f-4bbf-9e6a-2ea799ddcbb2 in namespace container-probe-6900
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/24/23 04:14:21.236
  Apr 24 04:14:21.243: INFO: Initial restart count of pod liveness-8c1eb422-bc9f-4bbf-9e6a-2ea799ddcbb2 is 0
  Apr 24 04:14:41.364: INFO: Restart count of pod container-probe-6900/liveness-8c1eb422-bc9f-4bbf-9e6a-2ea799ddcbb2 is now 1 (20.121188107s elapsed)
  Apr 24 04:14:41.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 04:14:41.374
  STEP: Destroying namespace "container-probe-6900" for this suite. @ 04/24/23 04:14:41.395
• [22.248 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/24/23 04:14:41.406
  Apr 24 04:14:41.406: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:14:41.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:14:41.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:14:41.439
  STEP: Creating configMap with name configmap-projected-all-test-volume-b7b4b66d-0484-4773-9b97-8751d8f95671 @ 04/24/23 04:14:41.444
  STEP: Creating secret with name secret-projected-all-test-volume-a2ae86af-d19b-4f79-965f-7776c012368b @ 04/24/23 04:14:41.451
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/24/23 04:14:41.457
  STEP: Saw pod success @ 04/24/23 04:14:45.493
  Apr 24 04:14:45.498: INFO: Trying to get logs from node aeveeng9ieph-3 pod projected-volume-2acd10c7-28be-470d-a206-c697634f0ac2 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:14:45.514
  Apr 24 04:14:45.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6950" for this suite. @ 04/24/23 04:14:45.551
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/24/23 04:14:45.567
  Apr 24 04:14:45.567: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:14:45.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:14:45.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:14:45.604
  STEP: starting the proxy server @ 04/24/23 04:14:45.608
  Apr 24 04:14:45.608: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3543 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/24/23 04:14:45.721
  Apr 24 04:14:45.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3543" for this suite. @ 04/24/23 04:14:45.753
• [0.196 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/24/23 04:14:45.766
  Apr 24 04:14:45.766: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-pred @ 04/24/23 04:14:45.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:14:45.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:14:45.809
  Apr 24 04:14:45.813: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 24 04:14:45.829: INFO: Waiting for terminating namespaces to be deleted...
  Apr 24 04:14:45.835: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-1 before test
  Apr 24 04:14:45.850: INFO: cilium-4hwfb from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.850: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:14:45.850: INFO: cilium-node-init-57zjr from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.850: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:14:45.850: INFO: coredns-5d78c9869d-dv4j2 from kube-system started at 2023-04-24 04:13:07 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.850: INFO: 	Container coredns ready: true, restart count 0
  Apr 24 04:14:45.850: INFO: kube-addon-manager-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.850: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 24 04:14:45.850: INFO: kube-apiserver-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.850: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 24 04:14:45.850: INFO: kube-controller-manager-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.850: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 24 04:14:45.850: INFO: kube-proxy-stmcm from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.850: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:14:45.851: INFO: kube-scheduler-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.851: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 24 04:14:45.851: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-9gjts from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:14:45.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:14:45.851: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 24 04:14:45.851: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-2 before test
  Apr 24 04:14:45.866: INFO: cilium-jjltm from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.866: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:14:45.866: INFO: cilium-node-init-8k2gd from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.866: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:14:45.866: INFO: coredns-5d78c9869d-7w9jr from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.867: INFO: 	Container coredns ready: true, restart count 0
  Apr 24 04:14:45.867: INFO: kube-addon-manager-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.867: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 24 04:14:45.867: INFO: kube-apiserver-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.868: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 24 04:14:45.868: INFO: kube-controller-manager-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.868: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 24 04:14:45.868: INFO: kube-proxy-w69d6 from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.868: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:14:45.869: INFO: kube-scheduler-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.869: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 24 04:14:45.869: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-fbwcp from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:14:45.869: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:14:45.869: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 24 04:14:45.870: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-3 before test
  Apr 24 04:14:45.890: INFO: cilium-c6lwz from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.890: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:14:45.890: INFO: cilium-node-init-gglwd from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.890: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:14:45.890: INFO: cilium-operator-85fcfcb8b4-tsmz8 from kube-system started at 2023-04-24 03:37:49 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.891: INFO: 	Container cilium-operator ready: true, restart count 0
  Apr 24 04:14:45.891: INFO: kube-proxy-q5ck5 from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.891: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:14:45.891: INFO: sonobuoy from sonobuoy started at 2023-04-24 03:39:06 +0000 UTC (1 container statuses recorded)
  Apr 24 04:14:45.892: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 24 04:14:45.892: INFO: sonobuoy-e2e-job-32c32a5c56a94f9e from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:14:45.892: INFO: 	Container e2e ready: true, restart count 0
  Apr 24 04:14:45.892: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:14:45.893: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-c2fm7 from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:14:45.893: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:14:45.893: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/24/23 04:14:45.894
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.1758c3bf0222ee9f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 04/24/23 04:14:45.975
  Apr 24 04:14:46.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1361" for this suite. @ 04/24/23 04:14:46.974
• [1.226 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 04/24/23 04:14:46.991
  Apr 24 04:14:46.992: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename statefulset @ 04/24/23 04:14:46.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:14:47.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:14:47.056
  STEP: Creating service test in namespace statefulset-8003 @ 04/24/23 04:14:47.065
  Apr 24 04:14:47.102: INFO: Found 0 stateful pods, waiting for 1
  Apr 24 04:14:57.109: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/24/23 04:14:57.12
  W0424 04:14:57.131156      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 24 04:14:57.146: INFO: Found 1 stateful pods, waiting for 2
  Apr 24 04:15:07.159: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 04:15:07.161: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/24/23 04:15:07.177
  STEP: Delete all of the StatefulSets @ 04/24/23 04:15:07.18
  STEP: Verify that StatefulSets have been deleted @ 04/24/23 04:15:07.194
  Apr 24 04:15:07.200: INFO: Deleting all statefulset in ns statefulset-8003
  Apr 24 04:15:07.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8003" for this suite. @ 04/24/23 04:15:07.241
• [20.306 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/24/23 04:15:07.299
  Apr 24 04:15:07.299: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename runtimeclass @ 04/24/23 04:15:07.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:15:07.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:15:07.345
  Apr 24 04:15:07.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9086" for this suite. @ 04/24/23 04:15:07.42
• [0.134 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/24/23 04:15:07.434
  Apr 24 04:15:07.434: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename init-container @ 04/24/23 04:15:07.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:15:07.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:15:07.478
  STEP: creating the pod @ 04/24/23 04:15:07.485
  Apr 24 04:15:07.486: INFO: PodSpec: initContainers in spec.initContainers
  Apr 24 04:15:10.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7675" for this suite. @ 04/24/23 04:15:10.961
• [3.537 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/24/23 04:15:10.971
  Apr 24 04:15:10.971: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/24/23 04:15:10.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:15:11.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:15:11.007
  STEP: creating @ 04/24/23 04:15:11.011
  STEP: getting @ 04/24/23 04:15:11.035
  STEP: listing @ 04/24/23 04:15:11.047
  STEP: deleting @ 04/24/23 04:15:11.05
  Apr 24 04:15:11.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-2779" for this suite. @ 04/24/23 04:15:11.088
• [0.126 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/24/23 04:15:11.1
  Apr 24 04:15:11.100: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:15:11.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:15:11.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:15:11.126
  STEP: Creating Pod @ 04/24/23 04:15:11.129
  STEP: Reading file content from the nginx-container @ 04/24/23 04:15:15.177
  Apr 24 04:15:15.177: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5925 PodName:pod-sharedvolume-29c3358d-7bfc-4d44-af7b-b03e4cfe6172 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:15:15.177: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:15:15.178: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:15:15.178: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-5925/pods/pod-sharedvolume-29c3358d-7bfc-4d44-af7b-b03e4cfe6172/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Apr 24 04:15:15.295: INFO: Exec stderr: ""
  Apr 24 04:15:15.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5925" for this suite. @ 04/24/23 04:15:15.305
• [4.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 04/24/23 04:15:15.321
  Apr 24 04:15:15.321: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename aggregator @ 04/24/23 04:15:15.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:15:15.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:15:15.357
  Apr 24 04:15:15.362: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Registering the sample API server. @ 04/24/23 04:15:15.364
  Apr 24 04:15:15.903: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 24 04:15:15.962: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Apr 24 04:15:18.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:20.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:22.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:24.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:26.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:28.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:30.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:32.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:34.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:36.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:38.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:40.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 15, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 24 04:15:42.196: INFO: Waited 142.619467ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/24/23 04:15:42.277
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/24/23 04:15:42.283
  STEP: List APIServices @ 04/24/23 04:15:42.295
  Apr 24 04:15:42.304: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/24/23 04:15:42.308
  Apr 24 04:15:42.338: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/24/23 04:15:42.339
  Apr 24 04:15:42.359: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.April, 24, 4, 15, 42, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/24/23 04:15:42.36
  Apr 24 04:15:42.368: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-04-24 04:15:42 +0000 UTC Passed all checks passed}
  Apr 24 04:15:42.368: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 24 04:15:42.369: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/24/23 04:15:42.369
  Apr 24 04:15:42.389: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-151138949" @ 04/24/23 04:15:42.39
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/24/23 04:15:42.424
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/24/23 04:15:42.437
  STEP: Patch APIService Status @ 04/24/23 04:15:42.446
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/24/23 04:15:42.461
  Apr 24 04:15:42.466: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-04-24 04:15:42 +0000 UTC Passed all checks passed}
  Apr 24 04:15:42.466: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 24 04:15:42.467: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 24 04:15:42.467: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/24/23 04:15:42.467
  STEP: Confirm that the generated APIService has been deleted @ 04/24/23 04:15:42.479
  Apr 24 04:15:42.479: INFO: Requesting list of APIServices to confirm quantity
  Apr 24 04:15:42.487: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 24 04:15:42.487: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 24 04:15:42.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-7166" for this suite. @ 04/24/23 04:15:42.676
• [27.370 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/24/23 04:15:42.695
  Apr 24 04:15:42.695: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename svcaccounts @ 04/24/23 04:15:42.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:15:42.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:15:42.753
  STEP: Creating a pod to test service account token:  @ 04/24/23 04:15:42.757
  STEP: Saw pod success @ 04/24/23 04:15:46.791
  Apr 24 04:15:46.798: INFO: Trying to get logs from node aeveeng9ieph-3 pod test-pod-6d8b737f-0fde-4619-9895-66b274e3648f container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 04:15:46.881
  Apr 24 04:15:46.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9977" for this suite. @ 04/24/23 04:15:46.918
• [4.232 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/24/23 04:15:46.935
  Apr 24 04:15:46.935: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 04:15:46.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:15:46.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:15:46.965
  STEP: Creating pod test-grpc-56cb2824-7b86-495b-bef1-b9397ff1a8d5 in namespace container-probe-840 @ 04/24/23 04:15:46.972
  Apr 24 04:15:49.002: INFO: Started pod test-grpc-56cb2824-7b86-495b-bef1-b9397ff1a8d5 in namespace container-probe-840
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/24/23 04:15:49.002
  Apr 24 04:15:49.008: INFO: Initial restart count of pod test-grpc-56cb2824-7b86-495b-bef1-b9397ff1a8d5 is 0
  Apr 24 04:16:53.316: INFO: Restart count of pod container-probe-840/test-grpc-56cb2824-7b86-495b-bef1-b9397ff1a8d5 is now 1 (1m4.307630756s elapsed)
  Apr 24 04:16:53.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 04:16:53.327
  STEP: Destroying namespace "container-probe-840" for this suite. @ 04/24/23 04:16:53.348
• [66.434 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/24/23 04:16:53.372
  Apr 24 04:16:53.372: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename certificates @ 04/24/23 04:16:53.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:16:53.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:16:53.406
  STEP: getting /apis @ 04/24/23 04:16:54.199
  STEP: getting /apis/certificates.k8s.io @ 04/24/23 04:16:54.206
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/24/23 04:16:54.211
  STEP: creating @ 04/24/23 04:16:54.214
  STEP: getting @ 04/24/23 04:16:54.244
  STEP: listing @ 04/24/23 04:16:54.249
  STEP: watching @ 04/24/23 04:16:54.257
  Apr 24 04:16:54.257: INFO: starting watch
  STEP: patching @ 04/24/23 04:16:54.259
  STEP: updating @ 04/24/23 04:16:54.268
  Apr 24 04:16:54.277: INFO: waiting for watch events with expected annotations
  Apr 24 04:16:54.277: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/24/23 04:16:54.278
  STEP: patching /approval @ 04/24/23 04:16:54.285
  STEP: updating /approval @ 04/24/23 04:16:54.302
  STEP: getting /status @ 04/24/23 04:16:54.318
  STEP: patching /status @ 04/24/23 04:16:54.323
  STEP: updating /status @ 04/24/23 04:16:54.338
  STEP: deleting @ 04/24/23 04:16:54.349
  STEP: deleting a collection @ 04/24/23 04:16:54.374
  Apr 24 04:16:54.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-324" for this suite. @ 04/24/23 04:16:54.409
• [1.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/24/23 04:16:54.425
  Apr 24 04:16:54.425: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename events @ 04/24/23 04:16:54.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:16:54.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:16:54.462
  STEP: creating a test event @ 04/24/23 04:16:54.466
  STEP: listing all events in all namespaces @ 04/24/23 04:16:54.478
  STEP: patching the test event @ 04/24/23 04:16:54.494
  STEP: fetching the test event @ 04/24/23 04:16:54.506
  STEP: updating the test event @ 04/24/23 04:16:54.511
  STEP: getting the test event @ 04/24/23 04:16:54.529
  STEP: deleting the test event @ 04/24/23 04:16:54.536
  STEP: listing all events in all namespaces @ 04/24/23 04:16:54.55
  Apr 24 04:16:54.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2229" for this suite. @ 04/24/23 04:16:54.576
• [0.165 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/24/23 04:16:54.59
  Apr 24 04:16:54.590: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename subpath @ 04/24/23 04:16:54.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:16:54.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:16:54.62
  STEP: Setting up data @ 04/24/23 04:16:54.624
  STEP: Creating pod pod-subpath-test-downwardapi-kfdl @ 04/24/23 04:16:54.642
  STEP: Creating a pod to test atomic-volume-subpath @ 04/24/23 04:16:54.642
  STEP: Saw pod success @ 04/24/23 04:17:18.799
  Apr 24 04:17:18.804: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-subpath-test-downwardapi-kfdl container test-container-subpath-downwardapi-kfdl: <nil>
  STEP: delete the pod @ 04/24/23 04:17:18.859
  STEP: Deleting pod pod-subpath-test-downwardapi-kfdl @ 04/24/23 04:17:18.884
  Apr 24 04:17:18.884: INFO: Deleting pod "pod-subpath-test-downwardapi-kfdl" in namespace "subpath-8723"
  Apr 24 04:17:18.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8723" for this suite. @ 04/24/23 04:17:18.901
• [24.321 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/24/23 04:17:18.918
  Apr 24 04:17:18.918: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 04:17:18.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:18.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:18.96
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 04:17:18.964
  STEP: Saw pod success @ 04/24/23 04:17:23.001
  Apr 24 04:17:23.007: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-311459a5-2380-4011-a3e8-e2ad3c65f8bb container client-container: <nil>
  STEP: delete the pod @ 04/24/23 04:17:23.019
  Apr 24 04:17:23.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2069" for this suite. @ 04/24/23 04:17:23.063
• [4.158 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/24/23 04:17:23.078
  Apr 24 04:17:23.079: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:17:23.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:23.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:23.115
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 04:17:23.12
  STEP: Saw pod success @ 04/24/23 04:17:27.151
  Apr 24 04:17:27.156: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-d2c63a1a-b700-417c-87ca-527cdfb918b1 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 04:17:27.166
  Apr 24 04:17:27.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5520" for this suite. @ 04/24/23 04:17:27.193
• [4.125 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/24/23 04:17:27.204
  Apr 24 04:17:27.204: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename events @ 04/24/23 04:17:27.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:27.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:27.234
  STEP: Create set of events @ 04/24/23 04:17:27.239
  Apr 24 04:17:27.245: INFO: created test-event-1
  Apr 24 04:17:27.261: INFO: created test-event-2
  Apr 24 04:17:27.268: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/24/23 04:17:27.268
  STEP: delete collection of events @ 04/24/23 04:17:27.278
  Apr 24 04:17:27.278: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/24/23 04:17:27.312
  Apr 24 04:17:27.312: INFO: requesting list of events to confirm quantity
  Apr 24 04:17:27.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4129" for this suite. @ 04/24/23 04:17:27.322
• [0.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/24/23 04:17:27.335
  Apr 24 04:17:27.335: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:17:27.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:27.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:27.369
  STEP: Creating a ResourceQuota @ 04/24/23 04:17:27.373
  STEP: Getting a ResourceQuota @ 04/24/23 04:17:27.382
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/24/23 04:17:27.388
  STEP: Patching the ResourceQuota @ 04/24/23 04:17:27.394
  STEP: Deleting a Collection of ResourceQuotas @ 04/24/23 04:17:27.404
  STEP: Verifying the deleted ResourceQuota @ 04/24/23 04:17:27.424
  Apr 24 04:17:27.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-241" for this suite. @ 04/24/23 04:17:27.437
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/24/23 04:17:27.45
  Apr 24 04:17:27.450: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:17:27.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:27.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:27.48
  STEP: Creating projection with secret that has name projected-secret-test-2294fa8c-23fb-4da4-a7e6-ac69343c53d2 @ 04/24/23 04:17:27.485
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:17:27.492
  STEP: Saw pod success @ 04/24/23 04:17:31.522
  Apr 24 04:17:31.527: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-secrets-c95f4ff7-e153-4232-86e5-d3385bec7337 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:17:31.539
  Apr 24 04:17:31.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-417" for this suite. @ 04/24/23 04:17:31.569
• [4.134 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/24/23 04:17:31.587
  Apr 24 04:17:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename podtemplate @ 04/24/23 04:17:31.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:31.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:31.626
  STEP: Create a pod template @ 04/24/23 04:17:31.631
  STEP: Replace a pod template @ 04/24/23 04:17:31.64
  Apr 24 04:17:31.654: INFO: Found updated podtemplate annotation: "true"

  Apr 24 04:17:31.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1611" for this suite. @ 04/24/23 04:17:31.66
• [0.081 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/24/23 04:17:31.67
  Apr 24 04:17:31.670: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename limitrange @ 04/24/23 04:17:31.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:31.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:31.705
  STEP: Creating a LimitRange @ 04/24/23 04:17:31.709
  STEP: Setting up watch @ 04/24/23 04:17:31.709
  STEP: Submitting a LimitRange @ 04/24/23 04:17:31.814
  STEP: Verifying LimitRange creation was observed @ 04/24/23 04:17:31.826
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/24/23 04:17:31.829
  Apr 24 04:17:31.833: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 24 04:17:31.833: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/24/23 04:17:31.834
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/24/23 04:17:31.841
  Apr 24 04:17:31.848: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 24 04:17:31.848: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/24/23 04:17:31.849
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/24/23 04:17:31.86
  Apr 24 04:17:31.865: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 24 04:17:31.865: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/24/23 04:17:31.865
  STEP: Failing to create a Pod with more than max resources @ 04/24/23 04:17:31.869
  STEP: Updating a LimitRange @ 04/24/23 04:17:31.873
  STEP: Verifying LimitRange updating is effective @ 04/24/23 04:17:31.886
  STEP: Creating a Pod with less than former min resources @ 04/24/23 04:17:33.892
  STEP: Failing to create a Pod with more than max resources @ 04/24/23 04:17:33.904
  STEP: Deleting a LimitRange @ 04/24/23 04:17:33.907
  STEP: Verifying the LimitRange was deleted @ 04/24/23 04:17:33.928
  Apr 24 04:17:38.932: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/24/23 04:17:38.932
  Apr 24 04:17:38.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-94" for this suite. @ 04/24/23 04:17:38.955
• [7.294 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/24/23 04:17:38.965
  Apr 24 04:17:38.965: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 04:17:38.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:39.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:39.012
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 04:17:39.016
  STEP: Saw pod success @ 04/24/23 04:17:43.054
  Apr 24 04:17:43.059: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-f6a8332a-b0c9-4f6c-8e50-b3d6884fdd4b container client-container: <nil>
  STEP: delete the pod @ 04/24/23 04:17:43.075
  Apr 24 04:17:43.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1146" for this suite. @ 04/24/23 04:17:43.106
• [4.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/24/23 04:17:43.121
  Apr 24 04:17:43.121: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:17:43.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:43.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:43.153
  STEP: Creating secret with name secret-test-856fb664-999f-47cb-a03b-3992f7986f3c @ 04/24/23 04:17:43.157
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:17:43.163
  STEP: Saw pod success @ 04/24/23 04:17:47.197
  Apr 24 04:17:47.204: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-secrets-80591dd1-de7d-4685-b0fa-b5ea2ea7ec63 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:17:47.233
  Apr 24 04:17:47.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5152" for this suite. @ 04/24/23 04:17:47.271
• [4.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/24/23 04:17:47.284
  Apr 24 04:17:47.284: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:17:47.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:47.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:47.315
  STEP: creating secret secrets-6137/secret-test-1c89b342-f988-4f8c-a888-ade04c07dc78 @ 04/24/23 04:17:47.319
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:17:47.327
  STEP: Saw pod success @ 04/24/23 04:17:51.372
  Apr 24 04:17:51.377: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-b22d1146-4eee-4fee-bfbe-99e80bfa0f9b container env-test: <nil>
  STEP: delete the pod @ 04/24/23 04:17:51.406
  Apr 24 04:17:51.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6137" for this suite. @ 04/24/23 04:17:51.446
• [4.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/24/23 04:17:51.465
  Apr 24 04:17:51.465: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename field-validation @ 04/24/23 04:17:51.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:51.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:51.496
  STEP: apply creating a deployment @ 04/24/23 04:17:51.5
  Apr 24 04:17:51.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6099" for this suite. @ 04/24/23 04:17:51.543
• [0.087 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/24/23 04:17:51.553
  Apr 24 04:17:51.553: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename job @ 04/24/23 04:17:51.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:51.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:51.585
  STEP: Creating a suspended job @ 04/24/23 04:17:51.594
  STEP: Patching the Job @ 04/24/23 04:17:51.603
  STEP: Watching for Job to be patched @ 04/24/23 04:17:51.639
  Apr 24 04:17:51.641: INFO: Event ADDED observed for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 24 04:17:51.641: INFO: Event MODIFIED observed for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 24 04:17:51.642: INFO: Event MODIFIED found for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/24/23 04:17:51.642
  STEP: Watching for Job to be updated @ 04/24/23 04:17:51.661
  Apr 24 04:17:51.663: INFO: Event MODIFIED found for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 24 04:17:51.663: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/24/23 04:17:51.663
  Apr 24 04:17:51.668: INFO: Job: e2e-j44nm as labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm]
  STEP: Waiting for job to complete @ 04/24/23 04:17:51.668
  STEP: Delete a job collection with a labelselector @ 04/24/23 04:17:59.678
  STEP: Watching for Job to be deleted @ 04/24/23 04:17:59.697
  Apr 24 04:17:59.700: INFO: Event MODIFIED observed for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 24 04:17:59.701: INFO: Event MODIFIED observed for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 24 04:17:59.701: INFO: Event MODIFIED observed for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 24 04:17:59.702: INFO: Event MODIFIED observed for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 24 04:17:59.702: INFO: Event MODIFIED observed for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 24 04:17:59.703: INFO: Event DELETED found for Job e2e-j44nm in namespace job-2041 with labels: map[e2e-j44nm:patched e2e-job-label:e2e-j44nm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/24/23 04:17:59.704
  Apr 24 04:17:59.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2041" for this suite. @ 04/24/23 04:17:59.732
• [8.199 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/24/23 04:17:59.755
  Apr 24 04:17:59.755: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 04:17:59.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:17:59.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:17:59.815
  Apr 24 04:18:59.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6951" for this suite. @ 04/24/23 04:18:59.858
• [60.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/24/23 04:18:59.871
  Apr 24 04:18:59.871: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 04:18:59.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:18:59.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:18:59.912
  STEP: creating the pod @ 04/24/23 04:18:59.916
  STEP: submitting the pod to kubernetes @ 04/24/23 04:18:59.916
  STEP: verifying QOS class is set on the pod @ 04/24/23 04:18:59.928
  Apr 24 04:18:59.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7996" for this suite. @ 04/24/23 04:18:59.938
• [0.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/24/23 04:18:59.965
  Apr 24 04:18:59.965: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 04:18:59.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:18:59.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:00.002
  Apr 24 04:19:00.007: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/24/23 04:19:02.06
  Apr 24 04:19:02.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4688 --namespace=crd-publish-openapi-4688 create -f -'
  Apr 24 04:19:03.621: INFO: stderr: ""
  Apr 24 04:19:03.621: INFO: stdout: "e2e-test-crd-publish-openapi-8100-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 24 04:19:03.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4688 --namespace=crd-publish-openapi-4688 delete e2e-test-crd-publish-openapi-8100-crds test-cr'
  Apr 24 04:19:03.760: INFO: stderr: ""
  Apr 24 04:19:03.760: INFO: stdout: "e2e-test-crd-publish-openapi-8100-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 24 04:19:03.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4688 --namespace=crd-publish-openapi-4688 apply -f -'
  Apr 24 04:19:04.299: INFO: stderr: ""
  Apr 24 04:19:04.299: INFO: stdout: "e2e-test-crd-publish-openapi-8100-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 24 04:19:04.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4688 --namespace=crd-publish-openapi-4688 delete e2e-test-crd-publish-openapi-8100-crds test-cr'
  Apr 24 04:19:04.443: INFO: stderr: ""
  Apr 24 04:19:04.443: INFO: stdout: "e2e-test-crd-publish-openapi-8100-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/24/23 04:19:04.443
  Apr 24 04:19:04.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-4688 explain e2e-test-crd-publish-openapi-8100-crds'
  Apr 24 04:19:05.023: INFO: stderr: ""
  Apr 24 04:19:05.023: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-8100-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Apr 24 04:19:07.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4688" for this suite. @ 04/24/23 04:19:07.532
• [7.575 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/24/23 04:19:07.544
  Apr 24 04:19:07.544: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:19:07.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:19:07.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:07.572
  STEP: Starting the proxy @ 04/24/23 04:19:07.579
  Apr 24 04:19:07.581: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-8209 proxy --unix-socket=/tmp/kubectl-proxy-unix3535800054/test'
  STEP: retrieving proxy /api/ output @ 04/24/23 04:19:07.683
  Apr 24 04:19:07.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8209" for this suite. @ 04/24/23 04:19:07.695
• [0.162 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/24/23 04:19:07.707
  Apr 24 04:19:07.707: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename job @ 04/24/23 04:19:07.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:19:07.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:07.735
  STEP: Creating a job @ 04/24/23 04:19:07.74
  STEP: Ensuring job reaches completions @ 04/24/23 04:19:07.754
  Apr 24 04:19:19.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5271" for this suite. @ 04/24/23 04:19:19.776
• [12.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/24/23 04:19:19.792
  Apr 24 04:19:19.793: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/24/23 04:19:19.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:19:19.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:19.829
  STEP: creating a target pod @ 04/24/23 04:19:19.834
  STEP: adding an ephemeral container @ 04/24/23 04:19:21.881
  STEP: checking pod container endpoints @ 04/24/23 04:19:23.926
  Apr 24 04:19:23.926: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4682 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:19:23.926: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:19:23.928: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:19:23.928: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-4682/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Apr 24 04:19:24.065: INFO: Exec stderr: ""
  Apr 24 04:19:24.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-4682" for this suite. @ 04/24/23 04:19:24.098
• [4.322 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/24/23 04:19:24.118
  Apr 24 04:19:24.118: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename disruption @ 04/24/23 04:19:24.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:19:24.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:24.158
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:19:24.172
  STEP: Updating PodDisruptionBudget status @ 04/24/23 04:19:26.188
  STEP: Waiting for all pods to be running @ 04/24/23 04:19:26.207
  Apr 24 04:19:26.216: INFO: running pods: 0 < 1
  Apr 24 04:19:28.232: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 04/24/23 04:19:30.23
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:19:30.252
  STEP: Patching PodDisruptionBudget status @ 04/24/23 04:19:30.265
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:19:30.281
  Apr 24 04:19:30.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9965" for this suite. @ 04/24/23 04:19:30.296
• [6.192 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/24/23 04:19:30.31
  Apr 24 04:19:30.311: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:19:30.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:19:30.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:30.341
  STEP: Setting up server cert @ 04/24/23 04:19:30.391
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:19:31.45
  STEP: Deploying the webhook pod @ 04/24/23 04:19:31.471
  STEP: Wait for the deployment to be ready @ 04/24/23 04:19:31.49
  Apr 24 04:19:31.501: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/24/23 04:19:33.521
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:19:33.535
  Apr 24 04:19:34.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/24/23 04:19:34.644
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/24/23 04:19:34.743
  STEP: Deleting the collection of validation webhooks @ 04/24/23 04:19:34.828
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/24/23 04:19:34.965
  Apr 24 04:19:34.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7287" for this suite. @ 04/24/23 04:19:35.074
  STEP: Destroying namespace "webhook-markers-7287" for this suite. @ 04/24/23 04:19:35.084
• [4.786 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/24/23 04:19:35.101
  Apr 24 04:19:35.101: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:19:35.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:19:35.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:35.15
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/24/23 04:19:35.159
  STEP: Saw pod success @ 04/24/23 04:19:39.203
  Apr 24 04:19:39.207: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-03ec5953-ad17-4b33-801a-2ce5f1d4bfcb container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:19:39.229
  Apr 24 04:19:39.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2842" for this suite. @ 04/24/23 04:19:39.268
• [4.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/24/23 04:19:39.287
  Apr 24 04:19:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:19:39.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:19:39.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:39.328
  STEP: Creating secret with name secret-test-map-ec72f273-d2da-4671-827b-8c9ec0783db2 @ 04/24/23 04:19:39.334
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:19:39.341
  STEP: Saw pod success @ 04/24/23 04:19:43.383
  Apr 24 04:19:43.397: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-secrets-01c2dc25-8d62-47cc-ab0d-f9ee3980dc62 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:19:43.409
  Apr 24 04:19:43.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6868" for this suite. @ 04/24/23 04:19:43.44
• [4.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 04/24/23 04:19:43.457
  Apr 24 04:19:43.457: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename statefulset @ 04/24/23 04:19:43.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:19:43.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:19:43.499
  STEP: Creating service test in namespace statefulset-5518 @ 04/24/23 04:19:43.503
  STEP: Creating stateful set ss in namespace statefulset-5518 @ 04/24/23 04:19:43.511
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5518 @ 04/24/23 04:19:43.525
  Apr 24 04:19:43.532: INFO: Found 0 stateful pods, waiting for 1
  Apr 24 04:19:53.542: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/24/23 04:19:53.543
  Apr 24 04:19:53.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-5518 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 24 04:19:53.822: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 04:19:53.822: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 04:19:53.822: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 24 04:19:53.830: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 24 04:20:03.837: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 24 04:20:03.838: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 04:20:03.860: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 24 04:20:03.860: INFO: ss-0  aeveeng9ieph-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:19:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:19:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:19:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:19:43 +0000 UTC  }]
  Apr 24 04:20:03.860: INFO: 
  Apr 24 04:20:03.860: INFO: StatefulSet ss has not reached scale 3, at 1
  Apr 24 04:20:04.869: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994908565s
  Apr 24 04:20:05.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985838959s
  Apr 24 04:20:06.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973966183s
  Apr 24 04:20:07.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.96678146s
  Apr 24 04:20:08.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957233701s
  Apr 24 04:20:09.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947372382s
  Apr 24 04:20:10.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.935798006s
  Apr 24 04:20:11.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927806354s
  Apr 24 04:20:12.949: INFO: Verifying statefulset ss doesn't scale past 3 for another 918.035663ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5518 @ 04/24/23 04:20:13.949
  Apr 24 04:20:13.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-5518 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 04:20:14.166: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 24 04:20:14.166: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 24 04:20:14.166: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 24 04:20:14.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-5518 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 04:20:14.407: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 24 04:20:14.407: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 24 04:20:14.407: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 24 04:20:14.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-5518 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 04:20:14.684: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 24 04:20:14.684: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 24 04:20:14.684: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 24 04:20:14.693: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Apr 24 04:20:24.702: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 04:20:24.702: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 04:20:24.702: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/24/23 04:20:24.702
  Apr 24 04:20:24.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-5518 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 24 04:20:25.021: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 04:20:25.021: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 04:20:25.021: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 24 04:20:25.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-5518 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 24 04:20:25.295: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 04:20:25.295: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 04:20:25.295: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 24 04:20:25.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-5518 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 24 04:20:25.694: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 04:20:25.694: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 04:20:25.694: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 24 04:20:25.694: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 04:20:25.706: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Apr 24 04:20:35.744: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 24 04:20:35.744: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 24 04:20:35.744: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 24 04:20:35.768: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 24 04:20:35.769: INFO: ss-0  aeveeng9ieph-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:19:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:19:43 +0000 UTC  }]
  Apr 24 04:20:35.769: INFO: ss-1  aeveeng9ieph-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:03 +0000 UTC  }]
  Apr 24 04:20:35.770: INFO: ss-2  aeveeng9ieph-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:03 +0000 UTC  }]
  Apr 24 04:20:35.770: INFO: 
  Apr 24 04:20:35.770: INFO: StatefulSet ss has not reached scale 0, at 3
  Apr 24 04:20:36.785: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Apr 24 04:20:36.785: INFO: ss-1  aeveeng9ieph-2  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:03 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:26 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:26 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-24 04:20:03 +0000 UTC  }]
  Apr 24 04:20:36.785: INFO: 
  Apr 24 04:20:36.785: INFO: StatefulSet ss has not reached scale 0, at 1
  Apr 24 04:20:37.792: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.975632116s
  Apr 24 04:20:38.800: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.968411966s
  Apr 24 04:20:39.810: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.960451303s
  Apr 24 04:20:40.821: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.951064358s
  Apr 24 04:20:41.831: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.939985863s
  Apr 24 04:20:42.842: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.928706617s
  Apr 24 04:20:43.851: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.919081181s
  Apr 24 04:20:44.861: INFO: Verifying statefulset ss doesn't scale past 0 for another 909.57656ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5518 @ 04/24/23 04:20:45.863
  Apr 24 04:20:45.871: INFO: Scaling statefulset ss to 0
  Apr 24 04:20:45.888: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 04:20:45.895: INFO: Deleting all statefulset in ns statefulset-5518
  Apr 24 04:20:45.901: INFO: Scaling statefulset ss to 0
  Apr 24 04:20:45.919: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 04:20:45.925: INFO: Deleting statefulset ss
  Apr 24 04:20:45.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5518" for this suite. @ 04/24/23 04:20:45.95
• [62.505 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/24/23 04:20:45.972
  Apr 24 04:20:45.972: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename gc @ 04/24/23 04:20:45.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:20:46.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:20:46.009
  STEP: create the rc @ 04/24/23 04:20:46.022
  W0424 04:20:46.034162      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/24/23 04:20:52.172
  STEP: wait for the rc to be deleted @ 04/24/23 04:20:52.374
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/24/23 04:20:57.381
  STEP: Gathering metrics @ 04/24/23 04:21:27.423
  Apr 24 04:21:27.586: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 24 04:21:27.592: INFO: Deleting pod "simpletest.rc-27zrq" in namespace "gc-1639"
  Apr 24 04:21:27.611: INFO: Deleting pod "simpletest.rc-2phl6" in namespace "gc-1639"
  Apr 24 04:21:27.627: INFO: Deleting pod "simpletest.rc-2rgrw" in namespace "gc-1639"
  Apr 24 04:21:27.701: INFO: Deleting pod "simpletest.rc-4dst4" in namespace "gc-1639"
  Apr 24 04:21:27.865: INFO: Deleting pod "simpletest.rc-4f65r" in namespace "gc-1639"
  Apr 24 04:21:27.917: INFO: Deleting pod "simpletest.rc-4kvgm" in namespace "gc-1639"
  Apr 24 04:21:27.999: INFO: Deleting pod "simpletest.rc-4p5qm" in namespace "gc-1639"
  Apr 24 04:21:28.100: INFO: Deleting pod "simpletest.rc-4q28k" in namespace "gc-1639"
  Apr 24 04:21:28.209: INFO: Deleting pod "simpletest.rc-54g5g" in namespace "gc-1639"
  Apr 24 04:21:28.288: INFO: Deleting pod "simpletest.rc-5d6zx" in namespace "gc-1639"
  Apr 24 04:21:28.365: INFO: Deleting pod "simpletest.rc-5vwvm" in namespace "gc-1639"
  Apr 24 04:21:28.425: INFO: Deleting pod "simpletest.rc-68btr" in namespace "gc-1639"
  Apr 24 04:21:28.593: INFO: Deleting pod "simpletest.rc-6dvmv" in namespace "gc-1639"
  Apr 24 04:21:28.625: INFO: Deleting pod "simpletest.rc-6kpbt" in namespace "gc-1639"
  Apr 24 04:21:28.678: INFO: Deleting pod "simpletest.rc-6mvrz" in namespace "gc-1639"
  Apr 24 04:21:28.787: INFO: Deleting pod "simpletest.rc-6xmpw" in namespace "gc-1639"
  Apr 24 04:21:28.831: INFO: Deleting pod "simpletest.rc-775vz" in namespace "gc-1639"
  Apr 24 04:21:28.872: INFO: Deleting pod "simpletest.rc-7k4m8" in namespace "gc-1639"
  Apr 24 04:21:28.966: INFO: Deleting pod "simpletest.rc-7khk6" in namespace "gc-1639"
  Apr 24 04:21:29.025: INFO: Deleting pod "simpletest.rc-7vnch" in namespace "gc-1639"
  Apr 24 04:21:29.092: INFO: Deleting pod "simpletest.rc-8g24m" in namespace "gc-1639"
  Apr 24 04:21:29.142: INFO: Deleting pod "simpletest.rc-8h67c" in namespace "gc-1639"
  Apr 24 04:21:29.217: INFO: Deleting pod "simpletest.rc-8lzhq" in namespace "gc-1639"
  Apr 24 04:21:29.296: INFO: Deleting pod "simpletest.rc-8mjxn" in namespace "gc-1639"
  Apr 24 04:21:29.343: INFO: Deleting pod "simpletest.rc-8zvfm" in namespace "gc-1639"
  Apr 24 04:21:29.402: INFO: Deleting pod "simpletest.rc-994l9" in namespace "gc-1639"
  Apr 24 04:21:29.446: INFO: Deleting pod "simpletest.rc-9jqlm" in namespace "gc-1639"
  Apr 24 04:21:29.485: INFO: Deleting pod "simpletest.rc-9x48c" in namespace "gc-1639"
  Apr 24 04:21:29.578: INFO: Deleting pod "simpletest.rc-9xbm7" in namespace "gc-1639"
  Apr 24 04:21:29.763: INFO: Deleting pod "simpletest.rc-bdpc9" in namespace "gc-1639"
  Apr 24 04:21:29.851: INFO: Deleting pod "simpletest.rc-bt4lc" in namespace "gc-1639"
  Apr 24 04:21:29.891: INFO: Deleting pod "simpletest.rc-c2ptf" in namespace "gc-1639"
  Apr 24 04:21:29.998: INFO: Deleting pod "simpletest.rc-cjdh9" in namespace "gc-1639"
  Apr 24 04:21:30.103: INFO: Deleting pod "simpletest.rc-ck56g" in namespace "gc-1639"
  Apr 24 04:21:30.172: INFO: Deleting pod "simpletest.rc-cs9vt" in namespace "gc-1639"
  Apr 24 04:21:30.304: INFO: Deleting pod "simpletest.rc-csvwr" in namespace "gc-1639"
  Apr 24 04:21:30.394: INFO: Deleting pod "simpletest.rc-dc62d" in namespace "gc-1639"
  Apr 24 04:21:30.458: INFO: Deleting pod "simpletest.rc-dcmcd" in namespace "gc-1639"
  Apr 24 04:21:30.493: INFO: Deleting pod "simpletest.rc-drh49" in namespace "gc-1639"
  Apr 24 04:21:30.568: INFO: Deleting pod "simpletest.rc-dsg2l" in namespace "gc-1639"
  Apr 24 04:21:30.594: INFO: Deleting pod "simpletest.rc-f6bns" in namespace "gc-1639"
  Apr 24 04:21:30.715: INFO: Deleting pod "simpletest.rc-gdcpb" in namespace "gc-1639"
  Apr 24 04:21:30.785: INFO: Deleting pod "simpletest.rc-gpjv9" in namespace "gc-1639"
  Apr 24 04:21:30.847: INFO: Deleting pod "simpletest.rc-gvtg2" in namespace "gc-1639"
  Apr 24 04:21:30.904: INFO: Deleting pod "simpletest.rc-hg6k2" in namespace "gc-1639"
  Apr 24 04:21:30.960: INFO: Deleting pod "simpletest.rc-hrp5q" in namespace "gc-1639"
  Apr 24 04:21:31.023: INFO: Deleting pod "simpletest.rc-hxrmv" in namespace "gc-1639"
  Apr 24 04:21:31.128: INFO: Deleting pod "simpletest.rc-j4b2r" in namespace "gc-1639"
  Apr 24 04:21:31.200: INFO: Deleting pod "simpletest.rc-j5jpk" in namespace "gc-1639"
  Apr 24 04:21:31.246: INFO: Deleting pod "simpletest.rc-k2msm" in namespace "gc-1639"
  Apr 24 04:21:31.287: INFO: Deleting pod "simpletest.rc-k5snw" in namespace "gc-1639"
  Apr 24 04:21:31.354: INFO: Deleting pod "simpletest.rc-k865t" in namespace "gc-1639"
  Apr 24 04:21:31.381: INFO: Deleting pod "simpletest.rc-khfsz" in namespace "gc-1639"
  Apr 24 04:21:31.446: INFO: Deleting pod "simpletest.rc-khwjt" in namespace "gc-1639"
  Apr 24 04:21:31.583: INFO: Deleting pod "simpletest.rc-kskt7" in namespace "gc-1639"
  Apr 24 04:21:31.645: INFO: Deleting pod "simpletest.rc-kxps8" in namespace "gc-1639"
  Apr 24 04:21:31.712: INFO: Deleting pod "simpletest.rc-l2md7" in namespace "gc-1639"
  Apr 24 04:21:31.757: INFO: Deleting pod "simpletest.rc-lfgp8" in namespace "gc-1639"
  Apr 24 04:21:31.793: INFO: Deleting pod "simpletest.rc-lhsk5" in namespace "gc-1639"
  Apr 24 04:21:31.841: INFO: Deleting pod "simpletest.rc-lsvp5" in namespace "gc-1639"
  Apr 24 04:21:31.947: INFO: Deleting pod "simpletest.rc-lwg88" in namespace "gc-1639"
  Apr 24 04:21:32.033: INFO: Deleting pod "simpletest.rc-mbkhp" in namespace "gc-1639"
  Apr 24 04:21:32.164: INFO: Deleting pod "simpletest.rc-mfldr" in namespace "gc-1639"
  Apr 24 04:21:32.626: INFO: Deleting pod "simpletest.rc-mgp7n" in namespace "gc-1639"
  Apr 24 04:21:32.728: INFO: Deleting pod "simpletest.rc-mkqkt" in namespace "gc-1639"
  Apr 24 04:21:32.861: INFO: Deleting pod "simpletest.rc-mqv95" in namespace "gc-1639"
  Apr 24 04:21:32.961: INFO: Deleting pod "simpletest.rc-mxm2p" in namespace "gc-1639"
  Apr 24 04:21:33.023: INFO: Deleting pod "simpletest.rc-nhpbh" in namespace "gc-1639"
  Apr 24 04:21:33.101: INFO: Deleting pod "simpletest.rc-nv58w" in namespace "gc-1639"
  Apr 24 04:21:33.145: INFO: Deleting pod "simpletest.rc-p9kmk" in namespace "gc-1639"
  Apr 24 04:21:33.180: INFO: Deleting pod "simpletest.rc-pcspq" in namespace "gc-1639"
  Apr 24 04:21:33.218: INFO: Deleting pod "simpletest.rc-ptm45" in namespace "gc-1639"
  Apr 24 04:21:33.288: INFO: Deleting pod "simpletest.rc-q656s" in namespace "gc-1639"
  Apr 24 04:21:33.339: INFO: Deleting pod "simpletest.rc-q6pnl" in namespace "gc-1639"
  Apr 24 04:21:33.388: INFO: Deleting pod "simpletest.rc-q75w6" in namespace "gc-1639"
  Apr 24 04:21:33.483: INFO: Deleting pod "simpletest.rc-qkzbz" in namespace "gc-1639"
  Apr 24 04:21:33.549: INFO: Deleting pod "simpletest.rc-r64mh" in namespace "gc-1639"
  Apr 24 04:21:33.675: INFO: Deleting pod "simpletest.rc-rkfnt" in namespace "gc-1639"
  Apr 24 04:21:33.719: INFO: Deleting pod "simpletest.rc-rm5mc" in namespace "gc-1639"
  Apr 24 04:21:33.781: INFO: Deleting pod "simpletest.rc-rqv4t" in namespace "gc-1639"
  Apr 24 04:21:33.846: INFO: Deleting pod "simpletest.rc-rs7cc" in namespace "gc-1639"
  Apr 24 04:21:33.898: INFO: Deleting pod "simpletest.rc-rs8sb" in namespace "gc-1639"
  Apr 24 04:21:34.028: INFO: Deleting pod "simpletest.rc-rv5gf" in namespace "gc-1639"
  Apr 24 04:21:34.158: INFO: Deleting pod "simpletest.rc-s9pth" in namespace "gc-1639"
  Apr 24 04:21:34.209: INFO: Deleting pod "simpletest.rc-snsmg" in namespace "gc-1639"
  Apr 24 04:21:34.300: INFO: Deleting pod "simpletest.rc-std6g" in namespace "gc-1639"
  Apr 24 04:21:34.379: INFO: Deleting pod "simpletest.rc-t48zw" in namespace "gc-1639"
  Apr 24 04:21:34.464: INFO: Deleting pod "simpletest.rc-vn5j9" in namespace "gc-1639"
  Apr 24 04:21:34.546: INFO: Deleting pod "simpletest.rc-vvqdf" in namespace "gc-1639"
  Apr 24 04:21:34.687: INFO: Deleting pod "simpletest.rc-wcpgt" in namespace "gc-1639"
  Apr 24 04:21:34.817: INFO: Deleting pod "simpletest.rc-wf9q5" in namespace "gc-1639"
  Apr 24 04:21:34.867: INFO: Deleting pod "simpletest.rc-wj95z" in namespace "gc-1639"
  Apr 24 04:21:34.934: INFO: Deleting pod "simpletest.rc-wq2p4" in namespace "gc-1639"
  Apr 24 04:21:35.003: INFO: Deleting pod "simpletest.rc-wz6vk" in namespace "gc-1639"
  Apr 24 04:21:35.075: INFO: Deleting pod "simpletest.rc-x6pxv" in namespace "gc-1639"
  Apr 24 04:21:35.158: INFO: Deleting pod "simpletest.rc-xbhfr" in namespace "gc-1639"
  Apr 24 04:21:35.218: INFO: Deleting pod "simpletest.rc-xll89" in namespace "gc-1639"
  Apr 24 04:21:35.250: INFO: Deleting pod "simpletest.rc-z6jvw" in namespace "gc-1639"
  Apr 24 04:21:35.326: INFO: Deleting pod "simpletest.rc-z9hkl" in namespace "gc-1639"
  Apr 24 04:21:35.359: INFO: Deleting pod "simpletest.rc-zpkcn" in namespace "gc-1639"
  Apr 24 04:21:35.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1639" for this suite. @ 04/24/23 04:21:35.417
• [49.484 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/24/23 04:21:35.461
  Apr 24 04:21:35.462: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/24/23 04:21:35.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:21:35.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:21:35.562
  Apr 24 04:21:39.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/24/23 04:21:39.792
  STEP: Cleaning up the configmap @ 04/24/23 04:21:39.801
  STEP: Cleaning up the pod @ 04/24/23 04:21:39.819
  STEP: Destroying namespace "emptydir-wrapper-5439" for this suite. @ 04/24/23 04:21:39.86
• [4.443 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/24/23 04:21:39.911
  Apr 24 04:21:39.911: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replicaset @ 04/24/23 04:21:39.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:21:40.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:21:40.019
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/24/23 04:21:40.024
  Apr 24 04:21:40.047: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 24 04:21:45.067: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/24/23 04:21:45.068
  STEP: getting scale subresource @ 04/24/23 04:21:45.068
  STEP: updating a scale subresource @ 04/24/23 04:21:45.074
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/24/23 04:21:45.086
  STEP: Patch a scale subresource @ 04/24/23 04:21:45.091
  Apr 24 04:21:45.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3649" for this suite. @ 04/24/23 04:21:45.145
• [5.256 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/24/23 04:21:45.168
  Apr 24 04:21:45.168: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-preemption @ 04/24/23 04:21:45.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:21:45.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:21:45.214
  Apr 24 04:21:45.259: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 24 04:22:45.315: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/24/23 04:22:45.326
  Apr 24 04:22:45.327: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/24/23 04:22:45.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:22:45.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:22:45.373
  STEP: Finding an available node @ 04/24/23 04:22:45.378
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/24/23 04:22:45.379
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/24/23 04:22:47.426
  Apr 24 04:22:47.450: INFO: found a healthy node: aeveeng9ieph-3
  Apr 24 04:22:53.604: INFO: pods created so far: [1 1 1]
  Apr 24 04:22:53.604: INFO: length of pods created so far: 3
  Apr 24 04:22:55.624: INFO: pods created so far: [2 2 1]
  Apr 24 04:23:02.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 04:23:02.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-26" for this suite. @ 04/24/23 04:23:02.791
  STEP: Destroying namespace "sched-preemption-5134" for this suite. @ 04/24/23 04:23:02.806
• [77.649 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/24/23 04:23:02.843
  Apr 24 04:23:02.843: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename discovery @ 04/24/23 04:23:02.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:23:02.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:23:02.887
  STEP: Setting up server cert @ 04/24/23 04:23:02.892
  Apr 24 04:23:04.120: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 24 04:23:04.121: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 24 04:23:04.121: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 24 04:23:04.121: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 24 04:23:04.121: INFO: Checking APIGroup: apps
  Apr 24 04:23:04.123: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 24 04:23:04.123: INFO: Versions found [{apps/v1 v1}]
  Apr 24 04:23:04.123: INFO: apps/v1 matches apps/v1
  Apr 24 04:23:04.123: INFO: Checking APIGroup: events.k8s.io
  Apr 24 04:23:04.124: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 24 04:23:04.125: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 24 04:23:04.125: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 24 04:23:04.126: INFO: Checking APIGroup: authentication.k8s.io
  Apr 24 04:23:04.128: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 24 04:23:04.128: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 24 04:23:04.128: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 24 04:23:04.128: INFO: Checking APIGroup: authorization.k8s.io
  Apr 24 04:23:04.130: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 24 04:23:04.130: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 24 04:23:04.131: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 24 04:23:04.131: INFO: Checking APIGroup: autoscaling
  Apr 24 04:23:04.133: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 24 04:23:04.133: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 24 04:23:04.133: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 24 04:23:04.134: INFO: Checking APIGroup: batch
  Apr 24 04:23:04.136: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 24 04:23:04.136: INFO: Versions found [{batch/v1 v1}]
  Apr 24 04:23:04.136: INFO: batch/v1 matches batch/v1
  Apr 24 04:23:04.136: INFO: Checking APIGroup: certificates.k8s.io
  Apr 24 04:23:04.137: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 24 04:23:04.138: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 24 04:23:04.138: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 24 04:23:04.139: INFO: Checking APIGroup: networking.k8s.io
  Apr 24 04:23:04.140: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 24 04:23:04.140: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 24 04:23:04.140: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 24 04:23:04.140: INFO: Checking APIGroup: policy
  Apr 24 04:23:04.142: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 24 04:23:04.142: INFO: Versions found [{policy/v1 v1}]
  Apr 24 04:23:04.142: INFO: policy/v1 matches policy/v1
  Apr 24 04:23:04.142: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 24 04:23:04.144: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 24 04:23:04.144: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 24 04:23:04.144: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 24 04:23:04.145: INFO: Checking APIGroup: storage.k8s.io
  Apr 24 04:23:04.147: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 24 04:23:04.147: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 24 04:23:04.148: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 24 04:23:04.148: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 24 04:23:04.150: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 24 04:23:04.150: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 24 04:23:04.151: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 24 04:23:04.151: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 24 04:23:04.153: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 24 04:23:04.154: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 24 04:23:04.154: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 24 04:23:04.155: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 24 04:23:04.157: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 24 04:23:04.157: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 24 04:23:04.157: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 24 04:23:04.158: INFO: Checking APIGroup: coordination.k8s.io
  Apr 24 04:23:04.160: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 24 04:23:04.160: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 24 04:23:04.160: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 24 04:23:04.161: INFO: Checking APIGroup: node.k8s.io
  Apr 24 04:23:04.163: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 24 04:23:04.163: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 24 04:23:04.164: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 24 04:23:04.164: INFO: Checking APIGroup: discovery.k8s.io
  Apr 24 04:23:04.166: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 24 04:23:04.166: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 24 04:23:04.166: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 24 04:23:04.167: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 24 04:23:04.169: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 24 04:23:04.169: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 24 04:23:04.170: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 24 04:23:04.170: INFO: Checking APIGroup: cilium.io
  Apr 24 04:23:04.172: INFO: PreferredVersion.GroupVersion: cilium.io/v2
  Apr 24 04:23:04.173: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
  Apr 24 04:23:04.173: INFO: cilium.io/v2 matches cilium.io/v2
  Apr 24 04:23:04.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-5312" for this suite. @ 04/24/23 04:23:04.18
• [1.350 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 04/24/23 04:23:04.201
  Apr 24 04:23:04.202: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename daemonsets @ 04/24/23 04:23:04.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:23:04.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:23:04.238
  Apr 24 04:23:04.301: INFO: Create a RollingUpdate DaemonSet
  Apr 24 04:23:04.313: INFO: Check that daemon pods launch on every node of the cluster
  Apr 24 04:23:04.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:23:04.328: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 04:23:05.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:23:05.344: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 04:23:06.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:23:06.350: INFO: Node aeveeng9ieph-3 is running 0 daemon pod, expected 1
  Apr 24 04:23:07.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 04:23:07.350: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Apr 24 04:23:07.350: INFO: Update the DaemonSet to trigger a rollout
  Apr 24 04:23:07.371: INFO: Updating DaemonSet daemon-set
  Apr 24 04:23:08.464: INFO: Roll back the DaemonSet before rollout is complete
  Apr 24 04:23:08.476: INFO: Updating DaemonSet daemon-set
  Apr 24 04:23:08.476: INFO: Make sure DaemonSet rollback is complete
  Apr 24 04:23:08.498: INFO: Wrong image for pod: daemon-set-zdj9p. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 24 04:23:08.498: INFO: Pod daemon-set-zdj9p is not available
  Apr 24 04:23:15.530: INFO: Pod daemon-set-c52wj is not available
  STEP: Deleting DaemonSet "daemon-set" @ 04/24/23 04:23:15.546
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1734, will wait for the garbage collector to delete the pods @ 04/24/23 04:23:15.547
  Apr 24 04:23:15.621: INFO: Deleting DaemonSet.extensions daemon-set took: 19.813946ms
  Apr 24 04:23:15.722: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.210669ms
  Apr 24 04:23:18.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:23:18.433: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 24 04:23:18.438: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26261"},"items":null}

  Apr 24 04:23:18.444: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26261"},"items":null}

  Apr 24 04:23:18.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1734" for this suite. @ 04/24/23 04:23:18.504
• [14.316 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/24/23 04:23:18.519
  Apr 24 04:23:18.519: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:23:18.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:23:18.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:23:18.551
  STEP: Counting existing ResourceQuota @ 04/24/23 04:23:18.556
  STEP: Creating a ResourceQuota @ 04/24/23 04:23:23.563
  STEP: Ensuring resource quota status is calculated @ 04/24/23 04:23:23.571
  STEP: Creating a ReplicationController @ 04/24/23 04:23:25.584
  STEP: Ensuring resource quota status captures replication controller creation @ 04/24/23 04:23:25.609
  STEP: Deleting a ReplicationController @ 04/24/23 04:23:27.617
  STEP: Ensuring resource quota status released usage @ 04/24/23 04:23:27.627
  Apr 24 04:23:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1575" for this suite. @ 04/24/23 04:23:29.644
• [11.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/24/23 04:23:29.66
  Apr 24 04:23:29.660: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 04:23:29.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:23:29.698
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:23:29.703
  STEP: Creating configMap that has name configmap-test-emptyKey-e3612bcf-96ee-4b6c-b832-89e38fa9e9e2 @ 04/24/23 04:23:29.707
  Apr 24 04:23:29.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1314" for this suite. @ 04/24/23 04:23:29.718
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/24/23 04:23:29.735
  Apr 24 04:23:29.735: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 04:23:29.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:23:29.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:23:29.786
  STEP: Creating configMap with name configmap-test-volume-map-8613d145-48b8-4c2a-ae52-696a87f7a44c @ 04/24/23 04:23:29.791
  STEP: Creating a pod to test consume configMaps @ 04/24/23 04:23:29.799
  STEP: Saw pod success @ 04/24/23 04:23:33.867
  Apr 24 04:23:33.875: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-4fca22ea-a116-41ed-8f49-ded8fb89e7f1 container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 04:23:33.92
  Apr 24 04:23:33.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7917" for this suite. @ 04/24/23 04:23:33.955
• [4.237 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/24/23 04:23:33.977
  Apr 24 04:23:33.977: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:23:33.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:23:34.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:23:34.026
  STEP: Creating secret with name s-test-opt-del-d03c2eea-7b22-4172-8e79-b5293697a1bd @ 04/24/23 04:23:34.043
  STEP: Creating secret with name s-test-opt-upd-4f7d66c8-5b91-492a-980e-7a8c1e8d2142 @ 04/24/23 04:23:34.049
  STEP: Creating the pod @ 04/24/23 04:23:34.054
  STEP: Deleting secret s-test-opt-del-d03c2eea-7b22-4172-8e79-b5293697a1bd @ 04/24/23 04:23:38.143
  STEP: Updating secret s-test-opt-upd-4f7d66c8-5b91-492a-980e-7a8c1e8d2142 @ 04/24/23 04:23:38.155
  STEP: Creating secret with name s-test-opt-create-c5b6c8a3-8a1c-4e87-9540-95d6a050b2d6 @ 04/24/23 04:23:38.163
  STEP: waiting to observe update in volume @ 04/24/23 04:23:38.173
  Apr 24 04:24:57.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1279" for this suite. @ 04/24/23 04:24:57.379
• [83.414 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/24/23 04:24:57.402
  Apr 24 04:24:57.402: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename cronjob @ 04/24/23 04:24:57.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:24:57.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:24:57.441
  STEP: Creating a ReplaceConcurrent cronjob @ 04/24/23 04:24:57.447
  STEP: Ensuring a job is scheduled @ 04/24/23 04:24:57.476
  STEP: Ensuring exactly one is scheduled @ 04/24/23 04:25:01.483
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/24/23 04:25:01.489
  STEP: Ensuring the job is replaced with a new one @ 04/24/23 04:25:01.494
  STEP: Removing cronjob @ 04/24/23 04:26:01.501
  Apr 24 04:26:01.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4735" for this suite. @ 04/24/23 04:26:01.52
• [64.138 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/24/23 04:26:01.545
  Apr 24 04:26:01.545: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pod-network-test @ 04/24/23 04:26:01.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:01.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:01.589
  STEP: Performing setup for networking test in namespace pod-network-test-2935 @ 04/24/23 04:26:01.593
  STEP: creating a selector @ 04/24/23 04:26:01.593
  STEP: Creating the service pods in kubernetes @ 04/24/23 04:26:01.593
  Apr 24 04:26:01.594: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/24/23 04:26:23.806
  Apr 24 04:26:25.861: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 24 04:26:25.861: INFO: Breadth first check of 10.233.64.179 on host 192.168.121.252...
  Apr 24 04:26:25.868: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.20:9080/dial?request=hostname&protocol=udp&host=10.233.64.179&port=8081&tries=1'] Namespace:pod-network-test-2935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:26:25.868: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:26:25.871: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:26:25.871: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 24 04:26:26.010: INFO: Waiting for responses: map[]
  Apr 24 04:26:26.010: INFO: reached 10.233.64.179 after 0/1 tries
  Apr 24 04:26:26.010: INFO: Breadth first check of 10.233.65.29 on host 192.168.121.43...
  Apr 24 04:26:26.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.20:9080/dial?request=hostname&protocol=udp&host=10.233.65.29&port=8081&tries=1'] Namespace:pod-network-test-2935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:26:26.021: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:26:26.022: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:26:26.023: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.29%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 24 04:26:26.141: INFO: Waiting for responses: map[]
  Apr 24 04:26:26.142: INFO: reached 10.233.65.29 after 0/1 tries
  Apr 24 04:26:26.142: INFO: Breadth first check of 10.233.66.98 on host 192.168.121.18...
  Apr 24 04:26:26.149: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.20:9080/dial?request=hostname&protocol=udp&host=10.233.66.98&port=8081&tries=1'] Namespace:pod-network-test-2935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:26:26.149: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:26:26.151: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:26:26.151: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.98%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 24 04:26:26.238: INFO: Waiting for responses: map[]
  Apr 24 04:26:26.238: INFO: reached 10.233.66.98 after 0/1 tries
  Apr 24 04:26:26.238: INFO: Going to retry 0 out of 3 pods....
  Apr 24 04:26:26.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2935" for this suite. @ 04/24/23 04:26:26.25
• [24.727 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/24/23 04:26:26.273
  Apr 24 04:26:26.273: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:26:26.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:26.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:26.322
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/24/23 04:26:26.327
  STEP: Saw pod success @ 04/24/23 04:26:30.363
  Apr 24 04:26:30.370: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-b8b8bc48-87ee-40fa-b87b-3741b313636d container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:26:30.403
  Apr 24 04:26:30.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3369" for this suite. @ 04/24/23 04:26:30.439
• [4.180 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/24/23 04:26:30.456
  Apr 24 04:26:30.456: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename var-expansion @ 04/24/23 04:26:30.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:30.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:30.495
  STEP: Creating a pod to test substitution in container's args @ 04/24/23 04:26:30.499
  STEP: Saw pod success @ 04/24/23 04:26:34.543
  Apr 24 04:26:34.553: INFO: Trying to get logs from node aeveeng9ieph-3 pod var-expansion-ea17d4ad-6b80-4d2e-9a68-d6bc4b02c698 container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 04:26:34.564
  Apr 24 04:26:34.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9293" for this suite. @ 04/24/23 04:26:34.594
• [4.151 seconds]
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/24/23 04:26:34.608
  Apr 24 04:26:34.608: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename security-context-test @ 04/24/23 04:26:34.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:34.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:34.639
  Apr 24 04:26:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7219" for this suite. @ 04/24/23 04:26:38.697
• [4.096 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 04/24/23 04:26:38.706
  Apr 24 04:26:38.706: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename statefulset @ 04/24/23 04:26:38.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:38.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:38.733
  STEP: Creating service test in namespace statefulset-6002 @ 04/24/23 04:26:38.737
  STEP: Looking for a node to schedule stateful set and pod @ 04/24/23 04:26:38.75
  STEP: Creating pod with conflicting port in namespace statefulset-6002 @ 04/24/23 04:26:38.764
  STEP: Waiting until pod test-pod will start running in namespace statefulset-6002 @ 04/24/23 04:26:38.777
  STEP: Creating statefulset with conflicting port in namespace statefulset-6002 @ 04/24/23 04:26:40.8
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6002 @ 04/24/23 04:26:40.813
  Apr 24 04:26:40.840: INFO: Observed stateful pod in namespace: statefulset-6002, name: ss-0, uid: 525e4de1-79a0-4fdc-8928-2cf634190cc0, status phase: Pending. Waiting for statefulset controller to delete.
  Apr 24 04:26:40.964: INFO: Observed stateful pod in namespace: statefulset-6002, name: ss-0, uid: 525e4de1-79a0-4fdc-8928-2cf634190cc0, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 24 04:26:40.980: INFO: Observed stateful pod in namespace: statefulset-6002, name: ss-0, uid: 525e4de1-79a0-4fdc-8928-2cf634190cc0, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 24 04:26:40.988: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6002
  STEP: Removing pod with conflicting port in namespace statefulset-6002 @ 04/24/23 04:26:40.988
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6002 and will be in running state @ 04/24/23 04:26:41.006
  Apr 24 04:26:43.022: INFO: Deleting all statefulset in ns statefulset-6002
  Apr 24 04:26:43.029: INFO: Scaling statefulset ss to 0
  Apr 24 04:26:53.059: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 04:26:53.065: INFO: Deleting statefulset ss
  Apr 24 04:26:53.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6002" for this suite. @ 04/24/23 04:26:53.099
• [14.404 seconds]
------------------------------
S
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/24/23 04:26:53.11
  Apr 24 04:26:53.110: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:26:53.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:53.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:53.148
  STEP: Creating projection with secret that has name secret-emptykey-test-b96d568e-97a4-46bf-88f3-5a31c1cd41a1 @ 04/24/23 04:26:53.152
  Apr 24 04:26:53.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7616" for this suite. @ 04/24/23 04:26:53.161
• [0.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/24/23 04:26:53.178
  Apr 24 04:26:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replication-controller @ 04/24/23 04:26:53.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:53.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:53.211
  STEP: creating a ReplicationController @ 04/24/23 04:26:53.22
  STEP: waiting for RC to be added @ 04/24/23 04:26:53.231
  STEP: waiting for available Replicas @ 04/24/23 04:26:53.232
  STEP: patching ReplicationController @ 04/24/23 04:26:54.328
  STEP: waiting for RC to be modified @ 04/24/23 04:26:54.345
  STEP: patching ReplicationController status @ 04/24/23 04:26:54.346
  STEP: waiting for RC to be modified @ 04/24/23 04:26:54.354
  STEP: waiting for available Replicas @ 04/24/23 04:26:54.354
  STEP: fetching ReplicationController status @ 04/24/23 04:26:54.369
  STEP: patching ReplicationController scale @ 04/24/23 04:26:54.378
  STEP: waiting for RC to be modified @ 04/24/23 04:26:54.391
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/24/23 04:26:54.402
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/24/23 04:26:55.382
  STEP: updating ReplicationController status @ 04/24/23 04:26:55.387
  STEP: waiting for RC to be modified @ 04/24/23 04:26:55.398
  STEP: listing all ReplicationControllers @ 04/24/23 04:26:55.398
  STEP: checking that ReplicationController has expected values @ 04/24/23 04:26:55.403
  STEP: deleting ReplicationControllers by collection @ 04/24/23 04:26:55.403
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/24/23 04:26:55.416
  Apr 24 04:26:55.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0424 04:26:55.498406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-6912" for this suite. @ 04/24/23 04:26:55.505
• [2.340 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/24/23 04:26:55.519
  Apr 24 04:26:55.520: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename disruption @ 04/24/23 04:26:55.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:55.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:55.566
  STEP: Creating a kubernetes client @ 04/24/23 04:26:55.57
  Apr 24 04:26:55.571: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename disruption-2 @ 04/24/23 04:26:55.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:26:55.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:26:55.603
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:26:55.616
  E0424 04:26:56.499396      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:26:57.500040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:26:57.635
  E0424 04:26:58.500509      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:26:59.500623      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:26:59.658
  E0424 04:27:00.501689      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:01.502354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 04/24/23 04:27:01.67
  STEP: listing a collection of PDBs in namespace disruption-4009 @ 04/24/23 04:27:01.676
  STEP: deleting a collection of PDBs @ 04/24/23 04:27:01.682
  STEP: Waiting for the PDB collection to be deleted @ 04/24/23 04:27:01.711
  Apr 24 04:27:01.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 04:27:01.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-6507" for this suite. @ 04/24/23 04:27:01.739
  STEP: Destroying namespace "disruption-4009" for this suite. @ 04/24/23 04:27:01.748
• [6.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/24/23 04:27:01.77
  Apr 24 04:27:01.770: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-runtime @ 04/24/23 04:27:01.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:01.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:01.801
  STEP: create the container @ 04/24/23 04:27:01.805
  W0424 04:27:01.821416      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/24/23 04:27:01.821
  E0424 04:27:02.502432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:03.502777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:04.503297      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/24/23 04:27:04.861
  STEP: the container should be terminated @ 04/24/23 04:27:04.869
  STEP: the termination message should be set @ 04/24/23 04:27:04.869
  Apr 24 04:27:04.869: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/24/23 04:27:04.869
  Apr 24 04:27:04.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7453" for this suite. @ 04/24/23 04:27:04.901
• [3.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/24/23 04:27:04.916
  Apr 24 04:27:04.917: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:27:04.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:04.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:04.947
  STEP: Setting up server cert @ 04/24/23 04:27:04.975
  E0424 04:27:05.503352      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:27:06.033
  STEP: Deploying the webhook pod @ 04/24/23 04:27:06.051
  STEP: Wait for the deployment to be ready @ 04/24/23 04:27:06.069
  Apr 24 04:27:06.085: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0424 04:27:06.504610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:07.505098      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:27:08.104
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:27:08.134
  E0424 04:27:08.505825      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:09.135: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/24/23 04:27:09.141
  STEP: create a namespace for the webhook @ 04/24/23 04:27:09.17
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/24/23 04:27:09.208
  Apr 24 04:27:09.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6073" for this suite. @ 04/24/23 04:27:09.352
  STEP: Destroying namespace "webhook-markers-129" for this suite. @ 04/24/23 04:27:09.376
  STEP: Destroying namespace "fail-closed-namespace-1088" for this suite. @ 04/24/23 04:27:09.387
• [4.485 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/24/23 04:27:09.404
  Apr 24 04:27:09.404: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replicaset @ 04/24/23 04:27:09.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:09.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:09.445
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/24/23 04:27:09.449
  E0424 04:27:09.506673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:10.506864      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 04/24/23 04:27:11.48
  STEP: Then the orphan pod is adopted @ 04/24/23 04:27:11.488
  E0424 04:27:11.507531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 04/24/23 04:27:12.504
  E0424 04:27:12.508588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:12.510: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/24/23 04:27:12.531
  Apr 24 04:27:12.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6881" for this suite. @ 04/24/23 04:27:12.56
• [3.168 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/24/23 04:27:12.574
  Apr 24 04:27:12.574: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 04:27:12.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:12.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:12.619
  STEP: creating service in namespace services-8275 @ 04/24/23 04:27:12.625
  STEP: creating service affinity-nodeport-transition in namespace services-8275 @ 04/24/23 04:27:12.625
  STEP: creating replication controller affinity-nodeport-transition in namespace services-8275 @ 04/24/23 04:27:12.652
  I0424 04:27:12.673161      14 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-8275, replica count: 3
  E0424 04:27:13.509489      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:14.510294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:15.510484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0424 04:27:15.724432      14 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 04:27:15.746: INFO: Creating new exec pod
  E0424 04:27:16.514375      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:17.513796      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:18.514285      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:18.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-8275 exec execpod-affinityplwt7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 24 04:27:19.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 24 04:27:19.289: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:27:19.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-8275 exec execpod-affinityplwt7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.34.72 80'
  E0424 04:27:19.519269      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:19.617: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.34.72 80\nConnection to 10.233.34.72 80 port [tcp/http] succeeded!\n"
  Apr 24 04:27:19.617: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:27:19.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-8275 exec execpod-affinityplwt7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.252 30015'
  Apr 24 04:27:19.897: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.252 30015\nConnection to 192.168.121.252 30015 port [tcp/*] succeeded!\n"
  Apr 24 04:27:19.897: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:27:19.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-8275 exec execpod-affinityplwt7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.18 30015'
  Apr 24 04:27:20.152: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.18 30015\nConnection to 192.168.121.18 30015 port [tcp/*] succeeded!\n"
  Apr 24 04:27:20.152: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:27:20.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-8275 exec execpod-affinityplwt7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.252:30015/ ; done'
  E0424 04:27:20.519721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:20.608: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n"
  Apr 24 04:27:20.608: INFO: stdout: "\naffinity-nodeport-transition-dttct\naffinity-nodeport-transition-dttct\naffinity-nodeport-transition-dttct\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-9gtlz\naffinity-nodeport-transition-dttct\naffinity-nodeport-transition-9gtlz\naffinity-nodeport-transition-dttct\naffinity-nodeport-transition-9gtlz\naffinity-nodeport-transition-9gtlz\naffinity-nodeport-transition-9gtlz\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-dttct\naffinity-nodeport-transition-9gtlz\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-dttct"
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-dttct
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-dttct
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-dttct
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-9gtlz
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-dttct
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-9gtlz
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-dttct
  Apr 24 04:27:20.608: INFO: Received response from host: affinity-nodeport-transition-9gtlz
  Apr 24 04:27:20.609: INFO: Received response from host: affinity-nodeport-transition-9gtlz
  Apr 24 04:27:20.609: INFO: Received response from host: affinity-nodeport-transition-9gtlz
  Apr 24 04:27:20.609: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:20.609: INFO: Received response from host: affinity-nodeport-transition-dttct
  Apr 24 04:27:20.609: INFO: Received response from host: affinity-nodeport-transition-9gtlz
  Apr 24 04:27:20.609: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:20.609: INFO: Received response from host: affinity-nodeport-transition-dttct
  Apr 24 04:27:20.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-8275 exec execpod-affinityplwt7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.252:30015/ ; done'
  Apr 24 04:27:21.435: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.252:30015/\n"
  Apr 24 04:27:21.435: INFO: stdout: "\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282\naffinity-nodeport-transition-7t282"
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.436: INFO: Received response from host: affinity-nodeport-transition-7t282
  Apr 24 04:27:21.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 04:27:21.446: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8275, will wait for the garbage collector to delete the pods @ 04/24/23 04:27:21.47
  E0424 04:27:21.519638      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:21.538: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.313284ms
  Apr 24 04:27:21.639: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.344988ms
  E0424 04:27:22.519650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:23.519919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8275" for this suite. @ 04/24/23 04:27:24.188
• [11.626 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/24/23 04:27:24.204
  Apr 24 04:27:24.204: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename deployment @ 04/24/23 04:27:24.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:24.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:24.241
  Apr 24 04:27:24.247: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 24 04:27:24.275: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0424 04:27:24.521144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:25.521869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:26.522346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:27.523365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:28.523644      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:29.281: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/24/23 04:27:29.281
  Apr 24 04:27:29.281: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 24 04:27:29.288: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 24 04:27:29.297: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0424 04:27:29.524388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:30.525359      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:31.311: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 24 04:27:31.316: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 24 04:27:31.348: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4080  1ab04285-9c10-4ca4-84b4-1d94fc14ae27 27722 1 2023-04-24 04:27:29 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-24 04:27:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:27:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00433d2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-24 04:27:29 +0000 UTC,LastTransitionTime:2023-04-24 04:27:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-04-24 04:27:30 +0000 UTC,LastTransitionTime:2023-04-24 04:27:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 24 04:27:31.369: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-4080  e482aecf-725d-47b1-9ee5-e05bb5d6e816 27712 1 2023-04-24 04:27:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1ab04285-9c10-4ca4-84b4-1d94fc14ae27 0xc00659e747 0xc00659e748}] [] [{kube-controller-manager Update apps/v1 2023-04-24 04:27:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ab04285-9c10-4ca4-84b4-1d94fc14ae27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:27:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00659e7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 04:27:31.369: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 24 04:27:31.369: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4080  aac85c2f-bfee-44c8-a862-cb48e8795e31 27721 2 2023-04-24 04:27:24 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1ab04285-9c10-4ca4-84b4-1d94fc14ae27 0xc00659e617 0xc00659e618}] [] [{e2e.test Update apps/v1 2023-04-24 04:27:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:27:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ab04285-9c10-4ca4-84b4-1d94fc14ae27\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:27:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00659e6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 04:27:31.383: INFO: Pod "test-rolling-update-deployment-656d657cd8-c4w2r" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-c4w2r test-rolling-update-deployment-656d657cd8- deployment-4080  6bc3cdc9-7db9-43e6-915c-4bfbb13558ba 27711 0 2023-04-24 04:27:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 e482aecf-725d-47b1-9ee5-e05bb5d6e816 0xc00433df77 0xc00433df78}] [] [{kube-controller-manager Update v1 2023-04-24 04:27:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e482aecf-725d-47b1-9ee5-e05bb5d6e816\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 04:27:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8wsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8wsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:27:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:27:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:10.233.66.254,StartTime:2023-04-24 04:27:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 04:27:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://92128d4bc26550db331aa47ef6733abc427a7c8853662381a4a6138e95f450bc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.254,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 04:27:31.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4080" for this suite. @ 04/24/23 04:27:31.392
• [7.199 seconds]
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/24/23 04:27:31.404
  Apr 24 04:27:31.404: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sysctl @ 04/24/23 04:27:31.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:31.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:31.441
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/24/23 04:27:31.447
  Apr 24 04:27:31.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-8596" for this suite. @ 04/24/23 04:27:31.46
• [0.067 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/24/23 04:27:31.472
  Apr 24 04:27:31.472: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename proxy @ 04/24/23 04:27:31.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:31.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:31.503
  STEP: starting an echo server on multiple ports @ 04/24/23 04:27:31.523
  STEP: creating replication controller proxy-service-fwxhr in namespace proxy-2909 @ 04/24/23 04:27:31.523
  E0424 04:27:31.524839      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0424 04:27:31.535638      14 runners.go:194] Created replication controller with name: proxy-service-fwxhr, namespace: proxy-2909, replica count: 1
  E0424 04:27:32.525579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0424 04:27:32.587286      14 runners.go:194] proxy-service-fwxhr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0424 04:27:33.525479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0424 04:27:33.587996      14 runners.go:194] proxy-service-fwxhr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0424 04:27:34.526276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0424 04:27:34.588967      14 runners.go:194] proxy-service-fwxhr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 04:27:34.596: INFO: setup took 3.08849293s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/24/23 04:27:34.596
  Apr 24 04:27:34.611: INFO: (0) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 14.820064ms)
  Apr 24 04:27:34.612: INFO: (0) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 15.769811ms)
  Apr 24 04:27:34.623: INFO: (0) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 25.482868ms)
  Apr 24 04:27:34.625: INFO: (0) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 27.467411ms)
  Apr 24 04:27:34.625: INFO: (0) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 27.832336ms)
  Apr 24 04:27:34.625: INFO: (0) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 26.978589ms)
  Apr 24 04:27:34.626: INFO: (0) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 28.619076ms)
  Apr 24 04:27:34.626: INFO: (0) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 29.2964ms)
  Apr 24 04:27:34.627: INFO: (0) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 31.050695ms)
  Apr 24 04:27:34.627: INFO: (0) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 29.533477ms)
  Apr 24 04:27:34.635: INFO: (0) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 37.948017ms)
  Apr 24 04:27:34.635: INFO: (0) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 37.3793ms)
  Apr 24 04:27:34.635: INFO: (0) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 38.459645ms)
  Apr 24 04:27:34.635: INFO: (0) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 37.968464ms)
  Apr 24 04:27:34.635: INFO: (0) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 38.32242ms)
  Apr 24 04:27:34.640: INFO: (0) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 42.951754ms)
  Apr 24 04:27:34.656: INFO: (1) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 15.191351ms)
  Apr 24 04:27:34.656: INFO: (1) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 14.593899ms)
  Apr 24 04:27:34.667: INFO: (1) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 25.650214ms)
  Apr 24 04:27:34.667: INFO: (1) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 26.350316ms)
  Apr 24 04:27:34.667: INFO: (1) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 25.715128ms)
  Apr 24 04:27:34.667: INFO: (1) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 24.320414ms)
  Apr 24 04:27:34.668: INFO: (1) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 24.476721ms)
  Apr 24 04:27:34.668: INFO: (1) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 24.667718ms)
  Apr 24 04:27:34.668: INFO: (1) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 26.367563ms)
  Apr 24 04:27:34.673: INFO: (1) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 30.198384ms)
  Apr 24 04:27:34.673: INFO: (1) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 31.356439ms)
  Apr 24 04:27:34.673: INFO: (1) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 30.782834ms)
  Apr 24 04:27:34.673: INFO: (1) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 31.030597ms)
  Apr 24 04:27:34.674: INFO: (1) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 30.511672ms)
  Apr 24 04:27:34.674: INFO: (1) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 30.851244ms)
  Apr 24 04:27:34.679: INFO: (1) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 36.940616ms)
  Apr 24 04:27:34.691: INFO: (2) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 10.428694ms)
  Apr 24 04:27:34.692: INFO: (2) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 12.978622ms)
  Apr 24 04:27:34.693: INFO: (2) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 11.56366ms)
  Apr 24 04:27:34.698: INFO: (2) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 15.721644ms)
  Apr 24 04:27:34.700: INFO: (2) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 17.722399ms)
  Apr 24 04:27:34.701: INFO: (2) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 18.436227ms)
  Apr 24 04:27:34.701: INFO: (2) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 21.557424ms)
  Apr 24 04:27:34.701: INFO: (2) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 20.207421ms)
  Apr 24 04:27:34.702: INFO: (2) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 20.100752ms)
  Apr 24 04:27:34.703: INFO: (2) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 22.831899ms)
  Apr 24 04:27:34.704: INFO: (2) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 21.099723ms)
  Apr 24 04:27:34.705: INFO: (2) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 22.21089ms)
  Apr 24 04:27:34.706: INFO: (2) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 23.33238ms)
  Apr 24 04:27:34.711: INFO: (2) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 29.239548ms)
  Apr 24 04:27:34.711: INFO: (2) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 29.057863ms)
  Apr 24 04:27:34.712: INFO: (2) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 29.316453ms)
  Apr 24 04:27:34.721: INFO: (3) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 8.026095ms)
  Apr 24 04:27:34.727: INFO: (3) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 13.553956ms)
  Apr 24 04:27:34.731: INFO: (3) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 17.335097ms)
  Apr 24 04:27:34.731: INFO: (3) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 18.127548ms)
  Apr 24 04:27:34.731: INFO: (3) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 18.276935ms)
  Apr 24 04:27:34.731: INFO: (3) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 18.74379ms)
  Apr 24 04:27:34.731: INFO: (3) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 19.213228ms)
  Apr 24 04:27:34.731: INFO: (3) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 17.914928ms)
  Apr 24 04:27:34.732: INFO: (3) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 17.46337ms)
  Apr 24 04:27:34.739: INFO: (3) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 26.685578ms)
  Apr 24 04:27:34.739: INFO: (3) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 25.409693ms)
  Apr 24 04:27:34.739: INFO: (3) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 25.809259ms)
  Apr 24 04:27:34.739: INFO: (3) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 27.245536ms)
  Apr 24 04:27:34.739: INFO: (3) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 26.64259ms)
  Apr 24 04:27:34.739: INFO: (3) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 26.545291ms)
  Apr 24 04:27:34.743: INFO: (3) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 29.137484ms)
  Apr 24 04:27:34.753: INFO: (4) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 9.503262ms)
  Apr 24 04:27:34.754: INFO: (4) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 11.529945ms)
  Apr 24 04:27:34.755: INFO: (4) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 10.940365ms)
  Apr 24 04:27:34.755: INFO: (4) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 11.34843ms)
  Apr 24 04:27:34.762: INFO: (4) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 19.187111ms)
  Apr 24 04:27:34.763: INFO: (4) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 18.704425ms)
  Apr 24 04:27:34.763: INFO: (4) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 19.037459ms)
  Apr 24 04:27:34.763: INFO: (4) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 18.525532ms)
  Apr 24 04:27:34.763: INFO: (4) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 18.695942ms)
  Apr 24 04:27:34.763: INFO: (4) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 19.367465ms)
  Apr 24 04:27:34.778: INFO: (4) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 33.821967ms)
  Apr 24 04:27:34.779: INFO: (4) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 35.296189ms)
  Apr 24 04:27:34.779: INFO: (4) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 33.600792ms)
  Apr 24 04:27:34.779: INFO: (4) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 34.356561ms)
  Apr 24 04:27:34.779: INFO: (4) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 34.634536ms)
  Apr 24 04:27:34.779: INFO: (4) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 34.306029ms)
  Apr 24 04:27:34.805: INFO: (5) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 14.669862ms)
  Apr 24 04:27:34.805: INFO: (5) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 17.400938ms)
  Apr 24 04:27:34.805: INFO: (5) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 16.326877ms)
  Apr 24 04:27:34.805: INFO: (5) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 17.979038ms)
  Apr 24 04:27:34.807: INFO: (5) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 15.193059ms)
  Apr 24 04:27:34.808: INFO: (5) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 16.463188ms)
  Apr 24 04:27:34.808: INFO: (5) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 19.165178ms)
  Apr 24 04:27:34.809: INFO: (5) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 19.44872ms)
  Apr 24 04:27:34.811: INFO: (5) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 21.844194ms)
  Apr 24 04:27:34.812: INFO: (5) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 19.403566ms)
  Apr 24 04:27:34.814: INFO: (5) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 23.920237ms)
  Apr 24 04:27:34.815: INFO: (5) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 23.886379ms)
  Apr 24 04:27:34.816: INFO: (5) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 25.433347ms)
  Apr 24 04:27:34.816: INFO: (5) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 27.656309ms)
  Apr 24 04:27:34.819: INFO: (5) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 30.540871ms)
  Apr 24 04:27:34.819: INFO: (5) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 26.73833ms)
  Apr 24 04:27:34.832: INFO: (6) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 12.697723ms)
  Apr 24 04:27:34.833: INFO: (6) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 13.208518ms)
  Apr 24 04:27:34.833: INFO: (6) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 13.869224ms)
  Apr 24 04:27:34.833: INFO: (6) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 14.61071ms)
  Apr 24 04:27:34.840: INFO: (6) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 20.242394ms)
  Apr 24 04:27:34.840: INFO: (6) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 20.906684ms)
  Apr 24 04:27:34.842: INFO: (6) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 22.069905ms)
  Apr 24 04:27:34.842: INFO: (6) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 23.305186ms)
  Apr 24 04:27:34.842: INFO: (6) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 22.652411ms)
  Apr 24 04:27:34.843: INFO: (6) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 24.039137ms)
  Apr 24 04:27:34.843: INFO: (6) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 23.93288ms)
  Apr 24 04:27:34.847: INFO: (6) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 27.327005ms)
  Apr 24 04:27:34.848: INFO: (6) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 29.043258ms)
  Apr 24 04:27:34.848: INFO: (6) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 28.687819ms)
  Apr 24 04:27:34.848: INFO: (6) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 28.226967ms)
  Apr 24 04:27:34.848: INFO: (6) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 28.375282ms)
  Apr 24 04:27:34.870: INFO: (7) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 16.372179ms)
  Apr 24 04:27:34.876: INFO: (7) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 21.63625ms)
  Apr 24 04:27:34.876: INFO: (7) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 21.404341ms)
  Apr 24 04:27:34.876: INFO: (7) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 19.407098ms)
  Apr 24 04:27:34.877: INFO: (7) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 21.99915ms)
  Apr 24 04:27:34.878: INFO: (7) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 21.232597ms)
  Apr 24 04:27:34.878: INFO: (7) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 20.952251ms)
  Apr 24 04:27:34.878: INFO: (7) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 22.317281ms)
  Apr 24 04:27:34.887: INFO: (7) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 32.119471ms)
  Apr 24 04:27:34.888: INFO: (7) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 33.821068ms)
  Apr 24 04:27:34.889: INFO: (7) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 33.685219ms)
  Apr 24 04:27:34.895: INFO: (7) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 38.103102ms)
  Apr 24 04:27:34.896: INFO: (7) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 39.817586ms)
  Apr 24 04:27:34.897: INFO: (7) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 40.213225ms)
  Apr 24 04:27:34.898: INFO: (7) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 42.143594ms)
  Apr 24 04:27:34.899: INFO: (7) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 43.563263ms)
  Apr 24 04:27:34.918: INFO: (8) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 14.530013ms)
  Apr 24 04:27:34.918: INFO: (8) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 16.981213ms)
  Apr 24 04:27:34.918: INFO: (8) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 15.28722ms)
  Apr 24 04:27:34.918: INFO: (8) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 16.313252ms)
  Apr 24 04:27:34.918: INFO: (8) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 15.788918ms)
  Apr 24 04:27:34.919: INFO: (8) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 14.106513ms)
  Apr 24 04:27:34.927: INFO: (8) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 23.082364ms)
  Apr 24 04:27:34.927: INFO: (8) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 25.950566ms)
  Apr 24 04:27:34.928: INFO: (8) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 26.33255ms)
  Apr 24 04:27:34.928: INFO: (8) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 23.910295ms)
  Apr 24 04:27:34.929: INFO: (8) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 25.431564ms)
  Apr 24 04:27:34.929: INFO: (8) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 28.275533ms)
  Apr 24 04:27:34.929: INFO: (8) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 24.037569ms)
  Apr 24 04:27:34.929: INFO: (8) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 26.642302ms)
  Apr 24 04:27:34.929: INFO: (8) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 25.116917ms)
  Apr 24 04:27:34.929: INFO: (8) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 24.582044ms)
  Apr 24 04:27:34.957: INFO: (9) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 18.238352ms)
  Apr 24 04:27:34.961: INFO: (9) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 21.074987ms)
  Apr 24 04:27:34.961: INFO: (9) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 20.847126ms)
  Apr 24 04:27:34.961: INFO: (9) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 22.341723ms)
  Apr 24 04:27:34.962: INFO: (9) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 16.051046ms)
  Apr 24 04:27:34.963: INFO: (9) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 16.845808ms)
  Apr 24 04:27:34.966: INFO: (9) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 19.944685ms)
  Apr 24 04:27:34.968: INFO: (9) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 21.528925ms)
  Apr 24 04:27:34.971: INFO: (9) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 24.500341ms)
  Apr 24 04:27:34.972: INFO: (9) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 24.955262ms)
  Apr 24 04:27:34.974: INFO: (9) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 35.177828ms)
  Apr 24 04:27:34.977: INFO: (9) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 29.993495ms)
  Apr 24 04:27:34.977: INFO: (9) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 37.201651ms)
  Apr 24 04:27:34.977: INFO: (9) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 31.0828ms)
  Apr 24 04:27:34.977: INFO: (9) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 30.768314ms)
  Apr 24 04:27:34.984: INFO: (9) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 37.63355ms)
  Apr 24 04:27:35.000: INFO: (10) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 14.476839ms)
  Apr 24 04:27:35.003: INFO: (10) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 18.240551ms)
  Apr 24 04:27:35.006: INFO: (10) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 19.590063ms)
  Apr 24 04:27:35.007: INFO: (10) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 22.201141ms)
  Apr 24 04:27:35.008: INFO: (10) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 22.058777ms)
  Apr 24 04:27:35.009: INFO: (10) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 24.294497ms)
  Apr 24 04:27:35.014: INFO: (10) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 29.625403ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 32.006155ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 33.502974ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 32.330891ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 32.697013ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 33.38837ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 34.401027ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 33.533289ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 34.132063ms)
  Apr 24 04:27:35.019: INFO: (10) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 33.121777ms)
  Apr 24 04:27:35.034: INFO: (11) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 10.31915ms)
  Apr 24 04:27:35.035: INFO: (11) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 11.188101ms)
  Apr 24 04:27:35.037: INFO: (11) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 12.760559ms)
  Apr 24 04:27:35.040: INFO: (11) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 15.086404ms)
  Apr 24 04:27:35.040: INFO: (11) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 16.423227ms)
  Apr 24 04:27:35.042: INFO: (11) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 17.600424ms)
  Apr 24 04:27:35.044: INFO: (11) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 19.18589ms)
  Apr 24 04:27:35.047: INFO: (11) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 21.619747ms)
  Apr 24 04:27:35.047: INFO: (11) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 21.093049ms)
  Apr 24 04:27:35.048: INFO: (11) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 23.544563ms)
  Apr 24 04:27:35.048: INFO: (11) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 22.452281ms)
  Apr 24 04:27:35.050: INFO: (11) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 24.514336ms)
  Apr 24 04:27:35.055: INFO: (11) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 28.858658ms)
  Apr 24 04:27:35.055: INFO: (11) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 29.867769ms)
  Apr 24 04:27:35.055: INFO: (11) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 29.5001ms)
  Apr 24 04:27:35.055: INFO: (11) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 29.655981ms)
  Apr 24 04:27:35.065: INFO: (12) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 7.059139ms)
  Apr 24 04:27:35.066: INFO: (12) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 8.5761ms)
  Apr 24 04:27:35.072: INFO: (12) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 14.464566ms)
  Apr 24 04:27:35.074: INFO: (12) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 14.620232ms)
  Apr 24 04:27:35.074: INFO: (12) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 14.996148ms)
  Apr 24 04:27:35.074: INFO: (12) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 15.654734ms)
  Apr 24 04:27:35.074: INFO: (12) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 15.863515ms)
  Apr 24 04:27:35.075: INFO: (12) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 16.85852ms)
  Apr 24 04:27:35.079: INFO: (12) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 20.539051ms)
  Apr 24 04:27:35.079: INFO: (12) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 21.569003ms)
  Apr 24 04:27:35.079: INFO: (12) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 20.442544ms)
  Apr 24 04:27:35.079: INFO: (12) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 20.765512ms)
  Apr 24 04:27:35.080: INFO: (12) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 21.38342ms)
  Apr 24 04:27:35.080: INFO: (12) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 22.181806ms)
  Apr 24 04:27:35.088: INFO: (12) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 29.849846ms)
  Apr 24 04:27:35.089: INFO: (12) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 29.473053ms)
  Apr 24 04:27:35.101: INFO: (13) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 11.914554ms)
  Apr 24 04:27:35.107: INFO: (13) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 16.876285ms)
  Apr 24 04:27:35.107: INFO: (13) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 17.362883ms)
  Apr 24 04:27:35.108: INFO: (13) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 18.512101ms)
  Apr 24 04:27:35.109: INFO: (13) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 18.090472ms)
  Apr 24 04:27:35.109: INFO: (13) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 18.611147ms)
  Apr 24 04:27:35.109: INFO: (13) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 18.900443ms)
  Apr 24 04:27:35.109: INFO: (13) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 18.454802ms)
  Apr 24 04:27:35.110: INFO: (13) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 19.159353ms)
  Apr 24 04:27:35.110: INFO: (13) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 19.368018ms)
  Apr 24 04:27:35.110: INFO: (13) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 21.159654ms)
  Apr 24 04:27:35.119: INFO: (13) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 28.27647ms)
  Apr 24 04:27:35.119: INFO: (13) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 28.351388ms)
  Apr 24 04:27:35.120: INFO: (13) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 30.53656ms)
  Apr 24 04:27:35.120: INFO: (13) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 29.68457ms)
  Apr 24 04:27:35.120: INFO: (13) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 29.331649ms)
  Apr 24 04:27:35.133: INFO: (14) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 8.027561ms)
  Apr 24 04:27:35.133: INFO: (14) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 9.139969ms)
  Apr 24 04:27:35.137: INFO: (14) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 12.680794ms)
  Apr 24 04:27:35.138: INFO: (14) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 12.908442ms)
  Apr 24 04:27:35.139: INFO: (14) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 14.941458ms)
  Apr 24 04:27:35.141: INFO: (14) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 15.902549ms)
  Apr 24 04:27:35.144: INFO: (14) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 17.321393ms)
  Apr 24 04:27:35.145: INFO: (14) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 19.050658ms)
  Apr 24 04:27:35.145: INFO: (14) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 18.310493ms)
  Apr 24 04:27:35.145: INFO: (14) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 18.912501ms)
  Apr 24 04:27:35.145: INFO: (14) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 19.486079ms)
  Apr 24 04:27:35.149: INFO: (14) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 24.525001ms)
  Apr 24 04:27:35.150: INFO: (14) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 25.178256ms)
  Apr 24 04:27:35.150: INFO: (14) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 24.669597ms)
  Apr 24 04:27:35.151: INFO: (14) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 24.571965ms)
  Apr 24 04:27:35.151: INFO: (14) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 24.441837ms)
  Apr 24 04:27:35.160: INFO: (15) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 7.490991ms)
  Apr 24 04:27:35.166: INFO: (15) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 12.652642ms)
  Apr 24 04:27:35.166: INFO: (15) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 12.443632ms)
  Apr 24 04:27:35.167: INFO: (15) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 13.798128ms)
  Apr 24 04:27:35.167: INFO: (15) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 13.575669ms)
  Apr 24 04:27:35.168: INFO: (15) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 14.953798ms)
  Apr 24 04:27:35.168: INFO: (15) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 14.840031ms)
  Apr 24 04:27:35.168: INFO: (15) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 15.234031ms)
  Apr 24 04:27:35.168: INFO: (15) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 15.218759ms)
  Apr 24 04:27:35.168: INFO: (15) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 15.18385ms)
  Apr 24 04:27:35.173: INFO: (15) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 19.625274ms)
  Apr 24 04:27:35.173: INFO: (15) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 19.298609ms)
  Apr 24 04:27:35.173: INFO: (15) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 19.709824ms)
  Apr 24 04:27:35.174: INFO: (15) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 20.448612ms)
  Apr 24 04:27:35.174: INFO: (15) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 21.129711ms)
  Apr 24 04:27:35.174: INFO: (15) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 20.340516ms)
  Apr 24 04:27:35.186: INFO: (16) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 10.434837ms)
  Apr 24 04:27:35.186: INFO: (16) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 10.668569ms)
  Apr 24 04:27:35.188: INFO: (16) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 13.100424ms)
  Apr 24 04:27:35.190: INFO: (16) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 12.260768ms)
  Apr 24 04:27:35.192: INFO: (16) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 17.688878ms)
  Apr 24 04:27:35.192: INFO: (16) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 13.974352ms)
  Apr 24 04:27:35.193: INFO: (16) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 18.263303ms)
  Apr 24 04:27:35.193: INFO: (16) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 17.676809ms)
  Apr 24 04:27:35.196: INFO: (16) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 18.044777ms)
  Apr 24 04:27:35.196: INFO: (16) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 18.307783ms)
  Apr 24 04:27:35.196: INFO: (16) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 17.840973ms)
  Apr 24 04:27:35.198: INFO: (16) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 21.81622ms)
  Apr 24 04:27:35.198: INFO: (16) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 19.028596ms)
  Apr 24 04:27:35.198: INFO: (16) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 19.435399ms)
  Apr 24 04:27:35.201: INFO: (16) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 22.294805ms)
  Apr 24 04:27:35.201: INFO: (16) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 24.238905ms)
  Apr 24 04:27:35.210: INFO: (17) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 8.027152ms)
  Apr 24 04:27:35.211: INFO: (17) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 9.630351ms)
  Apr 24 04:27:35.212: INFO: (17) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 9.566144ms)
  Apr 24 04:27:35.213: INFO: (17) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 11.20033ms)
  Apr 24 04:27:35.216: INFO: (17) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 14.011668ms)
  Apr 24 04:27:35.216: INFO: (17) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 14.737841ms)
  Apr 24 04:27:35.220: INFO: (17) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 17.77726ms)
  Apr 24 04:27:35.221: INFO: (17) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 18.887565ms)
  Apr 24 04:27:35.222: INFO: (17) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 19.926378ms)
  Apr 24 04:27:35.224: INFO: (17) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 21.502936ms)
  Apr 24 04:27:35.226: INFO: (17) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 24.247723ms)
  Apr 24 04:27:35.227: INFO: (17) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 24.2827ms)
  Apr 24 04:27:35.227: INFO: (17) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 24.062072ms)
  Apr 24 04:27:35.227: INFO: (17) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 24.860743ms)
  Apr 24 04:27:35.228: INFO: (17) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 26.045914ms)
  Apr 24 04:27:35.228: INFO: (17) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 26.854534ms)
  Apr 24 04:27:35.238: INFO: (18) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 8.764039ms)
  Apr 24 04:27:35.240: INFO: (18) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 9.863008ms)
  Apr 24 04:27:35.241: INFO: (18) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 11.470475ms)
  Apr 24 04:27:35.243: INFO: (18) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 13.319866ms)
  Apr 24 04:27:35.243: INFO: (18) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 11.686784ms)
  Apr 24 04:27:35.243: INFO: (18) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 11.81482ms)
  Apr 24 04:27:35.244: INFO: (18) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 13.235442ms)
  Apr 24 04:27:35.244: INFO: (18) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 14.222045ms)
  Apr 24 04:27:35.245: INFO: (18) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 14.13343ms)
  Apr 24 04:27:35.245: INFO: (18) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 14.069522ms)
  Apr 24 04:27:35.245: INFO: (18) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 16.485043ms)
  Apr 24 04:27:35.247: INFO: (18) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 16.885159ms)
  Apr 24 04:27:35.248: INFO: (18) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 17.496872ms)
  Apr 24 04:27:35.248: INFO: (18) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 19.564427ms)
  Apr 24 04:27:35.248: INFO: (18) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 18.571098ms)
  Apr 24 04:27:35.248: INFO: (18) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 18.928774ms)
  Apr 24 04:27:35.259: INFO: (19) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 10.243976ms)
  Apr 24 04:27:35.259: INFO: (19) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf/proxy/rewriteme">test</a> (200; 10.156151ms)
  Apr 24 04:27:35.259: INFO: (19) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">... (200; 10.035947ms)
  Apr 24 04:27:35.267: INFO: (19) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname2/proxy/: bar (200; 17.876072ms)
  Apr 24 04:27:35.267: INFO: (19) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:443/proxy/tlsrewritem... (200; 18.071692ms)
  Apr 24 04:27:35.268: INFO: (19) /api/v1/namespaces/proxy-2909/pods/http:proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 17.356739ms)
  Apr 24 04:27:35.269: INFO: (19) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:160/proxy/: foo (200; 18.597417ms)
  Apr 24 04:27:35.273: INFO: (19) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:162/proxy/: bar (200; 20.478067ms)
  Apr 24 04:27:35.274: INFO: (19) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname2/proxy/: bar (200; 24.38446ms)
  Apr 24 04:27:35.275: INFO: (19) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:460/proxy/: tls baz (200; 24.61454ms)
  Apr 24 04:27:35.275: INFO: (19) /api/v1/namespaces/proxy-2909/pods/https:proxy-service-fwxhr-kr4lf:462/proxy/: tls qux (200; 19.474104ms)
  Apr 24 04:27:35.275: INFO: (19) /api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2909/pods/proxy-service-fwxhr-kr4lf:1080/proxy/rewriteme">test<... (200; 19.545942ms)
  Apr 24 04:27:35.275: INFO: (19) /api/v1/namespaces/proxy-2909/services/http:proxy-service-fwxhr:portname1/proxy/: foo (200; 25.244258ms)
  Apr 24 04:27:35.277: INFO: (19) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname2/proxy/: tls qux (200; 26.261118ms)
  Apr 24 04:27:35.278: INFO: (19) /api/v1/namespaces/proxy-2909/services/https:proxy-service-fwxhr:tlsportname1/proxy/: tls baz (200; 22.131618ms)
  Apr 24 04:27:35.279: INFO: (19) /api/v1/namespaces/proxy-2909/services/proxy-service-fwxhr:portname1/proxy/: foo (200; 28.403054ms)
  Apr 24 04:27:35.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-fwxhr in namespace proxy-2909, will wait for the garbage collector to delete the pods @ 04/24/23 04:27:35.285
  Apr 24 04:27:35.352: INFO: Deleting ReplicationController proxy-service-fwxhr took: 13.223076ms
  Apr 24 04:27:35.453: INFO: Terminating ReplicationController proxy-service-fwxhr pods took: 100.961987ms
  E0424 04:27:35.526728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:36.527713      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-2909" for this suite. @ 04/24/23 04:27:36.754
• [5.300 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/24/23 04:27:36.785
  Apr 24 04:27:36.786: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/24/23 04:27:36.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:36.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:36.83
  Apr 24 04:27:36.834: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 04:27:37.527941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:38.528842      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:39.529818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:27:40.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2985" for this suite. @ 04/24/23 04:27:40.169
• [3.395 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/24/23 04:27:40.189
  Apr 24 04:27:40.189: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pod-network-test @ 04/24/23 04:27:40.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:27:40.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:27:40.243
  STEP: Performing setup for networking test in namespace pod-network-test-3931 @ 04/24/23 04:27:40.248
  STEP: creating a selector @ 04/24/23 04:27:40.248
  STEP: Creating the service pods in kubernetes @ 04/24/23 04:27:40.248
  Apr 24 04:27:40.248: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0424 04:27:40.530479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:41.530772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:42.530879      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:43.531466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:44.532543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:45.533418      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:46.533357      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:47.533903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:48.534094      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:49.534229      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:50.534771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:51.534984      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:52.535640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:53.535634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:54.536192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:55.539275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:56.540410      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:57.540306      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:58.541157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:27:59.541158      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:00.541508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:01.541695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/24/23 04:28:02.503
  E0424 04:28:02.542448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:03.542762      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:28:04.536: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 24 04:28:04.536: INFO: Breadth first check of 10.233.64.60 on host 192.168.121.252...
  E0424 04:28:04.543520      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:28:04.546: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.209:9080/dial?request=hostname&protocol=http&host=10.233.64.60&port=8083&tries=1'] Namespace:pod-network-test-3931 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:28:04.546: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:28:04.549: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:28:04.549: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3931/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.209%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.60%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 24 04:28:04.783: INFO: Waiting for responses: map[]
  Apr 24 04:28:04.783: INFO: reached 10.233.64.60 after 0/1 tries
  Apr 24 04:28:04.783: INFO: Breadth first check of 10.233.65.237 on host 192.168.121.43...
  Apr 24 04:28:04.795: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.209:9080/dial?request=hostname&protocol=http&host=10.233.65.237&port=8083&tries=1'] Namespace:pod-network-test-3931 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:28:04.795: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:28:04.797: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:28:04.798: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3931/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.209%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.237%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 24 04:28:05.221: INFO: Waiting for responses: map[]
  Apr 24 04:28:05.221: INFO: reached 10.233.65.237 after 0/1 tries
  Apr 24 04:28:05.221: INFO: Breadth first check of 10.233.66.119 on host 192.168.121.18...
  Apr 24 04:28:05.227: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.209:9080/dial?request=hostname&protocol=http&host=10.233.66.119&port=8083&tries=1'] Namespace:pod-network-test-3931 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:28:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:28:05.228: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:28:05.228: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3931/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.209%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.119%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 24 04:28:05.354: INFO: Waiting for responses: map[]
  Apr 24 04:28:05.354: INFO: reached 10.233.66.119 after 0/1 tries
  Apr 24 04:28:05.354: INFO: Going to retry 0 out of 3 pods....
  Apr 24 04:28:05.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3931" for this suite. @ 04/24/23 04:28:05.362
• [25.183 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 04/24/23 04:28:05.375
  Apr 24 04:28:05.375: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename field-validation @ 04/24/23 04:28:05.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:28:05.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:28:05.414
  Apr 24 04:28:05.419: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  W0424 04:28:05.420856      14 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00110bb30 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0424 04:28:05.543620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:06.544309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:07.544154      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0424 04:28:08.176216      14 warnings.go:70] unknown field "alpha"
  W0424 04:28:08.176510      14 warnings.go:70] unknown field "beta"
  W0424 04:28:08.176722      14 warnings.go:70] unknown field "delta"
  W0424 04:28:08.176923      14 warnings.go:70] unknown field "epsilon"
  W0424 04:28:08.177124      14 warnings.go:70] unknown field "gamma"
  Apr 24 04:28:08.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7100" for this suite. @ 04/24/23 04:28:08.247
• [2.883 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/24/23 04:28:08.264
  Apr 24 04:28:08.265: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:28:08.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:28:08.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:28:08.311
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 04:28:08.315
  E0424 04:28:08.544639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:09.544497      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:10.544955      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:11.545063      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:28:12.427
  Apr 24 04:28:12.432: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-c63e21e2-890f-47dc-b2df-408c61df5a26 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 04:28:12.469
  Apr 24 04:28:12.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1588" for this suite. @ 04/24/23 04:28:12.501
• [4.247 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/24/23 04:28:12.514
  Apr 24 04:28:12.514: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename deployment @ 04/24/23 04:28:12.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:28:12.542
  E0424 04:28:12.544840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:28:12.547
  Apr 24 04:28:12.551: INFO: Creating simple deployment test-new-deployment
  Apr 24 04:28:12.568: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
  E0424 04:28:13.545591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:14.546374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 04/24/23 04:28:14.598
  STEP: updating a scale subresource @ 04/24/23 04:28:14.605
  STEP: verifying the deployment Spec.Replicas was modified @ 04/24/23 04:28:14.614
  STEP: Patch a scale subresource @ 04/24/23 04:28:14.62
  Apr 24 04:28:14.648: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-1567  f5921878-a287-4f4d-9790-12cdd66b30b4 28071 3 2023-04-24 04:28:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-24 04:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:28:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cebcb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-24 04:28:14 +0000 UTC,LastTransitionTime:2023-04-24 04:28:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-04-24 04:28:14 +0000 UTC,LastTransitionTime:2023-04-24 04:28:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 24 04:28:14.657: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-1567  e64860db-c3de-4436-8761-2c3aa76389c0 28070 2 2023-04-24 04:28:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f5921878-a287-4f4d-9790-12cdd66b30b4 0xc003f5a0d7 0xc003f5a0d8}] [] [{kube-controller-manager Update apps/v1 2023-04-24 04:28:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5921878-a287-4f4d-9790-12cdd66b30b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:28:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f5a168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 04:28:14.667: INFO: Pod "test-new-deployment-67bd4bf6dc-6t7wn" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-6t7wn test-new-deployment-67bd4bf6dc- deployment-1567  35ba14b2-295b-4677-91e5-6ef73a7c1267 28064 0 2023-04-24 04:28:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc e64860db-c3de-4436-8761-2c3aa76389c0 0xc003f5a567 0xc003f5a568}] [] [{kube-controller-manager Update v1 2023-04-24 04:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e64860db-c3de-4436-8761-2c3aa76389c0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 04:28:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jp5n9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jp5n9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:28:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:28:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:28:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:10.233.66.174,StartTime:2023-04-24 04:28:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 04:28:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e113cdf183d7ccbc592cccd03c565c8f3ca77447f7b562aef19516152dcdd400,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.174,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 04:28:14.669: INFO: Pod "test-new-deployment-67bd4bf6dc-d5khp" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-d5khp test-new-deployment-67bd4bf6dc- deployment-1567  db69734c-abe8-49f7-9ec4-9e63eae29e4f 28073 0 2023-04-24 04:28:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc e64860db-c3de-4436-8761-2c3aa76389c0 0xc003f5a767 0xc003f5a768}] [] [{kube-controller-manager Update v1 2023-04-24 04:28:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e64860db-c3de-4436-8761-2c3aa76389c0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vq9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vq9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 04:28:14.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1567" for this suite. @ 04/24/23 04:28:14.701
• [2.209 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/24/23 04:28:14.723
  Apr 24 04:28:14.723: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sysctl @ 04/24/23 04:28:14.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:28:14.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:28:14.753
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/24/23 04:28:14.758
  STEP: Watching for error events or started pod @ 04/24/23 04:28:14.77
  E0424 04:28:15.546751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:16.550117      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 04/24/23 04:28:16.792
  E0424 04:28:17.548227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:18.548757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 04/24/23 04:28:18.823
  STEP: Getting logs from the pod @ 04/24/23 04:28:18.823
  STEP: Checking that the sysctl is actually updated @ 04/24/23 04:28:19.023
  Apr 24 04:28:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-2822" for this suite. @ 04/24/23 04:28:19.031
• [4.321 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/24/23 04:28:19.05
  Apr 24 04:28:19.050: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename taint-single-pod @ 04/24/23 04:28:19.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:28:19.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:28:19.086
  Apr 24 04:28:19.090: INFO: Waiting up to 1m0s for all nodes to be ready
  E0424 04:28:19.555876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:20.552120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:21.552399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:22.553140      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:23.553365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:24.553529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:25.554041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:26.555182      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:27.555458      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:28.556542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:29.557473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:30.557573      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:31.558167      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:32.558580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:33.559221      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:34.559552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:35.559602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:36.560122      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:37.560635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:38.562640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:39.561393      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:40.561799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:41.561944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:42.562255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:43.562792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:44.563883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:45.564965      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:46.566128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:47.566135      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:48.567065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:49.568115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:50.568520      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:51.569481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:52.569360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:53.569993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:54.570380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:55.570608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:56.570787      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:57.571844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:58.572088      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:28:59.573007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:00.573918      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:01.575023      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:02.575350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:03.576562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:04.576802      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:05.577380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:06.578224      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:07.578228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:08.578678      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:09.578806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:10.579121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:11.579202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:12.580450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:13.580942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:14.581252      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:15.581278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:16.581664      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:17.581754      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:18.582434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:29:19.129: INFO: Waiting for terminating namespaces to be deleted...
  Apr 24 04:29:19.137: INFO: Starting informer...
  STEP: Starting pod... @ 04/24/23 04:29:19.138
  Apr 24 04:29:19.366: INFO: Pod is running on aeveeng9ieph-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/24/23 04:29:19.366
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/24/23 04:29:19.393
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/24/23 04:29:19.4
  Apr 24 04:29:19.400: INFO: Pod wasn't evicted. Proceeding
  Apr 24 04:29:19.400: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/24/23 04:29:19.431
  STEP: Waiting some time to make sure that toleration time passed. @ 04/24/23 04:29:19.436
  E0424 04:29:19.583158      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:20.584031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:21.584884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:22.585488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:23.586037      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:24.586682      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:25.587345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:26.588410      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:27.588299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:28.588731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:29.589635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:30.590820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:31.591106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:32.591833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:33.592091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:34.592411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:35.592571      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:36.593096      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:37.593382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:38.593537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:39.593805      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:40.594090      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:41.594177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:42.594503      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:43.596303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:44.596280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:45.596438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:46.596909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:47.597047      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:48.598190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:49.598005      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:50.599226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:51.600851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:52.599776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:53.599865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:54.600315      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:55.600486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:56.600868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:57.601038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:58.601417      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:29:59.602000      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:00.601965      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:01.602313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:02.603447      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:03.603703      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:04.604075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:05.604300      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:06.604769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:07.604864      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:08.605021      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:09.605316      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:10.605655      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:11.605915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:12.606121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:13.606330      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:14.606768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:15.608240      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:16.607830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:17.607957      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:18.608219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:19.608407      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:20.609772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:21.610678      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:22.610878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:23.611657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:24.612271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:25.612958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:26.613461      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:27.614635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:28.615072      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:29.615439      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:30.615917      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:31.616448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:32.617876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:33.617716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:30:34.437: INFO: Pod wasn't evicted. Test successful
  Apr 24 04:30:34.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-3146" for this suite. @ 04/24/23 04:30:34.457
• [135.420 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/24/23 04:30:34.471
  Apr 24 04:30:34.471: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename endpointslice @ 04/24/23 04:30:34.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:30:34.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:30:34.506
  Apr 24 04:30:34.533: INFO: Endpoints addresses: [192.168.121.252 192.168.121.43] , ports: [6443]
  Apr 24 04:30:34.533: INFO: EndpointSlices addresses: [192.168.121.252 192.168.121.43] , ports: [6443]
  Apr 24 04:30:34.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6" for this suite. @ 04/24/23 04:30:34.541
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 04/24/23 04:30:34.556
  Apr 24 04:30:34.556: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename statefulset @ 04/24/23 04:30:34.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:30:34.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:30:34.59
  STEP: Creating service test in namespace statefulset-6014 @ 04/24/23 04:30:34.595
  STEP: Creating a new StatefulSet @ 04/24/23 04:30:34.602
  Apr 24 04:30:34.617: INFO: Found 0 stateful pods, waiting for 3
  E0424 04:30:34.618317      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:35.619567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:36.620720      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:37.620816      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:38.621840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:39.623126      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:40.621877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:41.621889      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:42.622250      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:43.622381      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:44.622621      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:30:44.632: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 04:30:44.632: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 04:30:44.632: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 04:30:44.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-6014 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 24 04:30:44.986: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 04:30:44.986: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 04:30:44.986: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0424 04:30:45.622818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:46.623261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:47.623403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:48.623746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:49.623900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:50.624354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:51.624609      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:52.624642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:53.624877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:54.625218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/24/23 04:30:55.015
  Apr 24 04:30:55.043: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/24/23 04:30:55.043
  E0424 04:30:55.626173      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:56.626526      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:57.627436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:58.628037      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:30:59.628938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:00.630156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:01.631203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:02.631641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:03.632528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:04.632824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 04/24/23 04:31:05.076
  Apr 24 04:31:05.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-6014 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 04:31:05.360: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 24 04:31:05.360: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 24 04:31:05.360: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0424 04:31:05.633048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:06.633314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:07.633576      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:08.634854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:09.634776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:10.635520      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:11.636424      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:12.636531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:13.636265      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:14.636727      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:31:15.401: INFO: Waiting for StatefulSet statefulset-6014/ss2 to complete update
  Apr 24 04:31:15.402: INFO: Waiting for Pod statefulset-6014/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 24 04:31:15.402: INFO: Waiting for Pod statefulset-6014/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0424 04:31:15.637564      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:16.639272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:17.638232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:18.638672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:19.639520      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:20.639660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:21.639917      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:22.640054      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:23.640228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:24.641350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:31:25.417: INFO: Waiting for StatefulSet statefulset-6014/ss2 to complete update
  Apr 24 04:31:25.417: INFO: Waiting for Pod statefulset-6014/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0424 04:31:25.641772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:26.643058      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:27.642511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:28.642705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:29.642891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:30.643242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:31.643349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:32.644226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:33.643580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:34.644263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:31:35.420: INFO: Waiting for StatefulSet statefulset-6014/ss2 to complete update
  E0424 04:31:35.644888      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:36.645111      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:37.646015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:38.645506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:39.645713      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:40.646639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:41.647256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:42.647498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:43.647709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:44.648063      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 04/24/23 04:31:45.415
  Apr 24 04:31:45.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-6014 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0424 04:31:45.648812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:31:45.755: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 04:31:45.755: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 04:31:45.755: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0424 04:31:46.649131      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:47.649556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:48.649884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:49.650301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:50.651288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:51.651636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:52.652479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:53.653076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:54.653683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:55.654291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:31:55.820: INFO: Updating stateful set ss2
  E0424 04:31:56.654714      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:57.655216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:58.655677      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:31:59.656103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:00.656424      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:01.656683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:02.656951      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:03.657163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:04.657751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:05.658570      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 04/24/23 04:32:05.857
  Apr 24 04:32:05.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-6014 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 04:32:06.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 24 04:32:06.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 24 04:32:06.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0424 04:32:06.659736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:07.659772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:08.659872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:09.660230      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:10.660497      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:11.660746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:12.663221      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:13.665212      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:14.665748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:15.666874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:32:16.126: INFO: Deleting all statefulset in ns statefulset-6014
  Apr 24 04:32:16.132: INFO: Scaling statefulset ss2 to 0
  E0424 04:32:16.667872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:17.667404      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:18.667733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:19.668189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:20.668457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:21.669698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:22.669286      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:23.669506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:24.670179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:25.671427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:32:26.162: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 04:32:26.170: INFO: Deleting statefulset ss2
  Apr 24 04:32:26.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6014" for this suite. @ 04/24/23 04:32:26.206
• [111.667 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/24/23 04:32:26.226
  Apr 24 04:32:26.227: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename dns @ 04/24/23 04:32:26.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:32:26.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:32:26.269
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9768.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9768.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/24/23 04:32:26.277
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9768.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9768.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/24/23 04:32:26.277
  STEP: creating a pod to probe /etc/hosts @ 04/24/23 04:32:26.277
  STEP: submitting the pod to kubernetes @ 04/24/23 04:32:26.277
  E0424 04:32:26.671888      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:27.677834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/24/23 04:32:28.315
  STEP: looking for the results for each expected name from probers @ 04/24/23 04:32:28.323
  Apr 24 04:32:28.364: INFO: DNS probes using dns-9768/dns-test-001a4f99-6644-462b-a7a5-04f49b1cf107 succeeded

  Apr 24 04:32:28.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 04:32:28.385
  STEP: Destroying namespace "dns-9768" for this suite. @ 04/24/23 04:32:28.457
• [2.253 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/24/23 04:32:28.483
  Apr 24 04:32:28.483: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename watch @ 04/24/23 04:32:28.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:32:28.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:32:28.518
  STEP: getting a starting resourceVersion @ 04/24/23 04:32:28.521
  STEP: starting a background goroutine to produce watch events @ 04/24/23 04:32:28.527
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/24/23 04:32:28.527
  E0424 04:32:28.676514      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:29.677577      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:30.678599      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:32:31.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-492" for this suite. @ 04/24/23 04:32:31.343
• [2.913 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/24/23 04:32:31.397
  Apr 24 04:32:31.397: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:32:31.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:32:31.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:32:31.428
  STEP: Counting existing ResourceQuota @ 04/24/23 04:32:31.432
  E0424 04:32:31.679157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:32.679390      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:33.679774      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:34.681883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:35.681910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/24/23 04:32:36.44
  STEP: Ensuring resource quota status is calculated @ 04/24/23 04:32:36.451
  E0424 04:32:36.682224      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:37.682657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 04/24/23 04:32:38.465
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/24/23 04:32:38.492
  E0424 04:32:38.682820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:39.684123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/24/23 04:32:40.504
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/24/23 04:32:40.51
  STEP: Ensuring a pod cannot update its resource requirements @ 04/24/23 04:32:40.514
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/24/23 04:32:40.522
  E0424 04:32:40.686173      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:41.686408      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/24/23 04:32:42.532
  STEP: Ensuring resource quota status released the pod usage @ 04/24/23 04:32:42.551
  E0424 04:32:42.687027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:43.687707      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:32:44.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3577" for this suite. @ 04/24/23 04:32:44.567
• [13.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/24/23 04:32:44.585
  Apr 24 04:32:44.585: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 04:32:44.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:32:44.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:32:44.623
  STEP: creating service multi-endpoint-test in namespace services-5765 @ 04/24/23 04:32:44.631
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5765 to expose endpoints map[] @ 04/24/23 04:32:44.651
  Apr 24 04:32:44.658: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0424 04:32:44.687913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:32:45.675: INFO: successfully validated that service multi-endpoint-test in namespace services-5765 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-5765 @ 04/24/23 04:32:45.675
  E0424 04:32:45.688561      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:46.688804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:47.689687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5765 to expose endpoints map[pod1:[100]] @ 04/24/23 04:32:47.72
  Apr 24 04:32:47.737: INFO: successfully validated that service multi-endpoint-test in namespace services-5765 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-5765 @ 04/24/23 04:32:47.737
  E0424 04:32:48.689831      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:49.690099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5765 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/24/23 04:32:49.791
  Apr 24 04:32:49.820: INFO: successfully validated that service multi-endpoint-test in namespace services-5765 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/24/23 04:32:49.82
  Apr 24 04:32:49.820: INFO: Creating new exec pod
  E0424 04:32:50.691003      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:51.691639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:52.691566      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:32:52.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5765 exec execpoddx7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Apr 24 04:32:53.245: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 24 04:32:53.245: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:32:53.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5765 exec execpoddx7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.35 80'
  Apr 24 04:32:53.556: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.35 80\nConnection to 10.233.7.35 80 port [tcp/http] succeeded!\n"
  Apr 24 04:32:53.556: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:32:53.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5765 exec execpoddx7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  E0424 04:32:53.691955      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:32:53.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 24 04:32:53.861: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 24 04:32:53.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-5765 exec execpoddx7dm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.35 81'
  Apr 24 04:32:54.071: INFO: stderr: "+ + nc -vecho -t -w hostName 2\n 10.233.7.35 81\nConnection to 10.233.7.35 81 port [tcp/*] succeeded!\n"
  Apr 24 04:32:54.071: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-5765 @ 04/24/23 04:32:54.071
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5765 to expose endpoints map[pod2:[101]] @ 04/24/23 04:32:54.096
  Apr 24 04:32:54.128: INFO: successfully validated that service multi-endpoint-test in namespace services-5765 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-5765 @ 04/24/23 04:32:54.128
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5765 to expose endpoints map[] @ 04/24/23 04:32:54.151
  E0424 04:32:54.693052      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:32:55.169: INFO: successfully validated that service multi-endpoint-test in namespace services-5765 exposes endpoints map[]
  Apr 24 04:32:55.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5765" for this suite. @ 04/24/23 04:32:55.209
• [10.644 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/24/23 04:32:55.23
  Apr 24 04:32:55.230: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename namespaces @ 04/24/23 04:32:55.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:32:55.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:32:55.268
  STEP: Creating namespace "e2e-ns-nbm2h" @ 04/24/23 04:32:55.272
  Apr 24 04:32:55.297: INFO: Namespace "e2e-ns-nbm2h-7658" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-nbm2h-7658" @ 04/24/23 04:32:55.297
  Apr 24 04:32:55.311: INFO: Namespace "e2e-ns-nbm2h-7658" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-nbm2h-7658" @ 04/24/23 04:32:55.311
  Apr 24 04:32:55.332: INFO: Namespace "e2e-ns-nbm2h-7658" has []v1.FinalizerName{"kubernetes"}
  Apr 24 04:32:55.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5335" for this suite. @ 04/24/23 04:32:55.338
  STEP: Destroying namespace "e2e-ns-nbm2h-7658" for this suite. @ 04/24/23 04:32:55.36
• [0.141 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/24/23 04:32:55.372
  Apr 24 04:32:55.372: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:32:55.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:32:55.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:32:55.403
  STEP: Creating a ResourceQuota @ 04/24/23 04:32:55.407
  STEP: Getting a ResourceQuota @ 04/24/23 04:32:55.414
  STEP: Updating a ResourceQuota @ 04/24/23 04:32:55.417
  STEP: Verifying a ResourceQuota was modified @ 04/24/23 04:32:55.431
  STEP: Deleting a ResourceQuota @ 04/24/23 04:32:55.437
  STEP: Verifying the deleted ResourceQuota @ 04/24/23 04:32:55.447
  Apr 24 04:32:55.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-483" for this suite. @ 04/24/23 04:32:55.458
• [0.103 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/24/23 04:32:55.477
  Apr 24 04:32:55.477: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename cronjob @ 04/24/23 04:32:55.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:32:55.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:32:55.507
  STEP: Creating a ForbidConcurrent cronjob @ 04/24/23 04:32:55.512
  STEP: Ensuring a job is scheduled @ 04/24/23 04:32:55.522
  E0424 04:32:55.693526      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:56.693593      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:57.694135      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:58.694444      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:32:59.694829      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:00.695551      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/24/23 04:33:01.536
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/24/23 04:33:01.54
  STEP: Ensuring no more jobs are scheduled @ 04/24/23 04:33:01.547
  E0424 04:33:01.696370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:02.696907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:03.697777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:04.698313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:05.698824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:06.699263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:07.699845      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:08.700336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:09.700575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:10.702258      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:11.702792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:12.703434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:13.705407      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:14.703990      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:15.704506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:16.704530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:17.705465      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:18.705482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:19.705882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:20.705937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:21.706877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:22.706991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:23.707244      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:24.707388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:25.707886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:26.708132      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:27.709032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:28.709284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:29.709455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:30.709862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:31.711264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:32.710708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:33.711023      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:34.711287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:35.711386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:36.711651      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:37.712916      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:38.712810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:39.713547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:40.714046      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:41.714529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:42.714872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:43.715552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:44.716328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:45.717597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:46.716885      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:47.718492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:48.717381      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:49.717844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:50.718263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:51.719472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:52.719452      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:53.721312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:54.720971      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:55.721384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:56.722456      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:57.722923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:58.722909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:33:59.723846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:00.724041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:01.725192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:02.725414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:03.726256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:04.727017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:05.727298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:06.727973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:07.728778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:08.728760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:09.729735      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:10.730600      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:11.730610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:12.730600      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:13.731392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:14.732384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:15.733518      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:16.733689      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:17.734263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:18.734246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:19.734610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:20.734870      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:21.735292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:22.735304      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:23.736378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:24.737030      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:25.737174      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:26.737776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:27.738588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:28.739212      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:29.739783      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:30.739954      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:31.740222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:32.740368      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:33.740631      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:34.740695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:35.741465      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:36.741454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:37.742200      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:38.742347      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:39.743134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:40.744570      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:41.744255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:42.744255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:43.744676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:44.744881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:45.746204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:46.746138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:47.746649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:48.747246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:49.747813      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:50.748077      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:51.748149      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:52.751335      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:53.752378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:54.752449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:55.753398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:56.753446      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:57.753695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:58.753743      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:34:59.753887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:00.754413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:01.754584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:02.755004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:03.755800      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:04.755494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:05.756102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:06.756298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:07.757364      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:08.759322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:09.759200      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:10.759217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:11.760015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:12.759866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:13.761031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:14.761825      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:15.762086      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:16.762207      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:17.762531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:18.762861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:19.762809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:20.764112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:21.763597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:22.767190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:23.767594      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:24.768236      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:25.769227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:26.769401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:27.769793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:28.770312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:29.771187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:30.771546      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:31.772503      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:32.773527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:33.772902      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:34.773379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:35.774414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:36.775018      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:37.775977      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:38.776241      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:39.776154      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:40.776629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:41.776707      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:42.777192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:43.778653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:44.778757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:45.779501      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:46.779865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:47.780319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:48.780810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:49.780894      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:50.781184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:51.782382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:52.782453      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:53.783031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:54.783184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:55.784339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:56.785569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:57.785606      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:58.785975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:35:59.786616      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:00.787359      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:01.788780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:02.789005      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:03.789709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:04.789976      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:05.791520      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:06.791409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:07.791988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:08.792284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:09.793553      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:10.793376      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:11.794195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:12.794361      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:13.794634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:14.794898      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:15.795273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:16.796550      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:17.797458      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:18.798379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:19.799502      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:20.800398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:21.800569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:22.800773      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:23.801793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:24.802597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:25.802751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:26.802892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:27.803660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:28.804438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:29.805513      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:30.805298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:31.807308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:32.807756      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:33.807692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:34.808277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:35.808039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:36.808190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:37.808339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:38.808840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:39.808749      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:40.813289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:41.813471      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:42.813545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:43.814035      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:44.814447      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:45.815396      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:46.816193      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:47.816427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:48.816708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:49.817915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:50.818104      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:51.819262      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:52.820471      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:53.822207      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:54.822492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:55.823325      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:56.823739      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:57.824894      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:58.825128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:36:59.826265      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:00.826832      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:01.827571      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:02.828427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:03.828486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:04.828626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:05.828880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:06.829507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:07.829755      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:08.830467      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:09.831641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:10.831708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:11.832603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:12.832890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:13.833573      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:14.833893      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:15.834797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:16.834917      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:17.835635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:18.836015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:19.836926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:20.837482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:21.838354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:22.838543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:23.838753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:24.846426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:25.846675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:26.847584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:27.847842      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:28.848401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:29.848628      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:30.849052      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:31.849274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:32.850362      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:33.850560      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:34.850667      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:35.851452      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:36.855241      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:37.856231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:38.856357      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:39.856966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:40.857151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:41.857710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:42.857900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:43.858150      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:44.858765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:45.859259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:46.859794      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:47.860598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:48.861096      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:49.861750      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:50.862410      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:51.862527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:52.863370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:53.863613      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:54.863738      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:55.864351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:56.865751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:57.865933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:58.866112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:37:59.866812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:00.867193      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/24/23 04:38:01.561
  Apr 24 04:38:01.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5436" for this suite. @ 04/24/23 04:38:01.58
• [306.117 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/24/23 04:38:01.595
  Apr 24 04:38:01.595: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 04:38:01.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:01.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:01.654
  STEP: creating the pod @ 04/24/23 04:38:01.659
  STEP: setting up watch @ 04/24/23 04:38:01.66
  STEP: submitting the pod to kubernetes @ 04/24/23 04:38:01.764
  STEP: verifying the pod is in kubernetes @ 04/24/23 04:38:01.788
  STEP: verifying pod creation was observed @ 04/24/23 04:38:01.795
  E0424 04:38:01.867411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:02.867847      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 04/24/23 04:38:03.826
  STEP: verifying pod deletion was observed @ 04/24/23 04:38:03.838
  E0424 04:38:03.868460      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:04.869488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:05.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3238" for this suite. @ 04/24/23 04:38:05.577
• [3.991 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/24/23 04:38:05.6
  Apr 24 04:38:05.600: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 04:38:05.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:05.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:05.639
  STEP: Creating a pod to test downward api env vars @ 04/24/23 04:38:05.643
  E0424 04:38:05.869618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:06.870627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:07.871356      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:08.871481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:38:09.685
  Apr 24 04:38:09.693: INFO: Trying to get logs from node aeveeng9ieph-3 pod downward-api-c11e7b53-92bc-4989-b347-794b81b16839 container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 04:38:09.725
  Apr 24 04:38:09.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5201" for this suite. @ 04/24/23 04:38:09.765
• [4.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/24/23 04:38:09.784
  Apr 24 04:38:09.785: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:38:09.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:09.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:09.814
  E0424 04:38:09.871723      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 04/24/23 04:38:09.873
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:38:10.4
  STEP: Deploying the webhook pod @ 04/24/23 04:38:10.414
  STEP: Wait for the deployment to be ready @ 04/24/23 04:38:10.436
  Apr 24 04:38:10.450: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0424 04:38:10.871989      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:11.873222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:38:12.469
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:38:12.496
  E0424 04:38:12.873656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:13.497: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/24/23 04:38:13.59
  STEP: Creating a configMap that should be mutated @ 04/24/23 04:38:13.617
  STEP: Deleting the collection of validation webhooks @ 04/24/23 04:38:13.668
  STEP: Creating a configMap that should not be mutated @ 04/24/23 04:38:13.81
  E0424 04:38:13.873648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:13.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-322" for this suite. @ 04/24/23 04:38:14.073
  STEP: Destroying namespace "webhook-markers-4900" for this suite. @ 04/24/23 04:38:14.082
• [4.313 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/24/23 04:38:14.1
  Apr 24 04:38:14.100: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 04:38:14.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:14.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:14.157
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 04:38:14.161
  E0424 04:38:14.874837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:15.875294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:16.875534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:17.875729      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:38:18.195
  Apr 24 04:38:18.203: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-7a3e78e5-637d-4cb9-823f-e8933823e125 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 04:38:18.227
  Apr 24 04:38:18.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8239" for this suite. @ 04/24/23 04:38:18.284
• [4.198 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/24/23 04:38:18.302
  Apr 24 04:38:18.302: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:38:18.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:18.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:18.34
  STEP: Creating secret with name secret-test-6b17f363-622c-423f-8ae7-0f96c8129838 @ 04/24/23 04:38:18.346
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:38:18.358
  E0424 04:38:18.877178      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:19.876902      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:20.877283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:21.877804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:38:22.443
  Apr 24 04:38:22.449: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-secrets-cdbaecfb-eca0-4d9a-971e-f66a0333f7ca container secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:38:22.463
  Apr 24 04:38:22.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9714" for this suite. @ 04/24/23 04:38:22.507
• [4.220 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/24/23 04:38:22.525
  Apr 24 04:38:22.525: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:38:22.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:22.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:22.56
  STEP: creating a replication controller @ 04/24/23 04:38:22.566
  Apr 24 04:38:22.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 create -f -'
  E0424 04:38:22.878548      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:23.878659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:24.472: INFO: stderr: ""
  Apr 24 04:38:24.472: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/24/23 04:38:24.472
  Apr 24 04:38:24.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 24 04:38:24.629: INFO: stderr: ""
  Apr 24 04:38:24.629: INFO: stdout: "update-demo-nautilus-f5x2x update-demo-nautilus-tgjlm "
  Apr 24 04:38:24.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get pods update-demo-nautilus-f5x2x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:38:24.841: INFO: stderr: ""
  Apr 24 04:38:24.841: INFO: stdout: ""
  Apr 24 04:38:24.842: INFO: update-demo-nautilus-f5x2x is created but not running
  E0424 04:38:24.879464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:25.879766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:26.879842      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:27.880189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:28.880227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:29.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0424 04:38:29.881882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:29.983: INFO: stderr: ""
  Apr 24 04:38:29.983: INFO: stdout: "update-demo-nautilus-f5x2x update-demo-nautilus-tgjlm "
  Apr 24 04:38:29.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get pods update-demo-nautilus-f5x2x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:38:30.104: INFO: stderr: ""
  Apr 24 04:38:30.104: INFO: stdout: "true"
  Apr 24 04:38:30.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get pods update-demo-nautilus-f5x2x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 24 04:38:30.225: INFO: stderr: ""
  Apr 24 04:38:30.225: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 24 04:38:30.225: INFO: validating pod update-demo-nautilus-f5x2x
  Apr 24 04:38:30.236: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 24 04:38:30.236: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 24 04:38:30.236: INFO: update-demo-nautilus-f5x2x is verified up and running
  Apr 24 04:38:30.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get pods update-demo-nautilus-tgjlm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 24 04:38:30.364: INFO: stderr: ""
  Apr 24 04:38:30.364: INFO: stdout: "true"
  Apr 24 04:38:30.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get pods update-demo-nautilus-tgjlm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 24 04:38:30.522: INFO: stderr: ""
  Apr 24 04:38:30.522: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 24 04:38:30.522: INFO: validating pod update-demo-nautilus-tgjlm
  Apr 24 04:38:30.538: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 24 04:38:30.538: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 24 04:38:30.538: INFO: update-demo-nautilus-tgjlm is verified up and running
  STEP: using delete to clean up resources @ 04/24/23 04:38:30.538
  Apr 24 04:38:30.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 delete --grace-period=0 --force -f -'
  Apr 24 04:38:30.670: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 24 04:38:30.670: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 24 04:38:30.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get rc,svc -l name=update-demo --no-headers'
  E0424 04:38:30.882397      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:30.987: INFO: stderr: "No resources found in kubectl-7114 namespace.\n"
  Apr 24 04:38:30.987: INFO: stdout: ""
  Apr 24 04:38:30.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7114 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 24 04:38:31.339: INFO: stderr: ""
  Apr 24 04:38:31.339: INFO: stdout: ""
  Apr 24 04:38:31.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7114" for this suite. @ 04/24/23 04:38:31.345
• [8.835 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/24/23 04:38:31.361
  Apr 24 04:38:31.362: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:38:31.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:31.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:31.392
  STEP: Setting up server cert @ 04/24/23 04:38:31.423
  E0424 04:38:31.882659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:38:32.318
  STEP: Deploying the webhook pod @ 04/24/23 04:38:32.326
  STEP: Wait for the deployment to be ready @ 04/24/23 04:38:32.347
  Apr 24 04:38:32.358: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0424 04:38:32.883219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:33.883728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:34.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 38, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 38, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 38, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 38, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:38:34.884320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:35.884486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:38:36.4
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:38:36.465
  E0424 04:38:36.884781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:37.466: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/24/23 04:38:37.473
  STEP: create a configmap that should be updated by the webhook @ 04/24/23 04:38:37.505
  Apr 24 04:38:37.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8532" for this suite. @ 04/24/23 04:38:37.61
  STEP: Destroying namespace "webhook-markers-8301" for this suite. @ 04/24/23 04:38:37.623
• [6.274 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/24/23 04:38:37.637
  Apr 24 04:38:37.637: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:38:37.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:37.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:37.668
  STEP: Creating secret with name secret-test-map-5dfc5544-1439-430e-870d-ccd531bfca3f @ 04/24/23 04:38:37.672
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:38:37.679
  E0424 04:38:37.885808      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:38.886789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:39.887238      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:40.887806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:38:41.719
  Apr 24 04:38:41.732: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-secrets-fc0fb2b2-3057-4f61-89a1-90f1c49bf53f container secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:38:41.75
  Apr 24 04:38:41.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3427" for this suite. @ 04/24/23 04:38:41.801
• [4.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/24/23 04:38:41.82
  Apr 24 04:38:41.821: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename svcaccounts @ 04/24/23 04:38:41.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:41.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:41.851
  Apr 24 04:38:41.883: INFO: created pod pod-service-account-defaultsa
  Apr 24 04:38:41.883: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  E0424 04:38:41.888442      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:38:41.900: INFO: created pod pod-service-account-mountsa
  Apr 24 04:38:41.900: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 24 04:38:41.910: INFO: created pod pod-service-account-nomountsa
  Apr 24 04:38:41.910: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 24 04:38:41.924: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 24 04:38:41.924: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Apr 24 04:38:41.933: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 24 04:38:41.933: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Apr 24 04:38:41.943: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 24 04:38:41.944: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 24 04:38:41.951: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 24 04:38:41.951: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 24 04:38:41.981: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 24 04:38:41.981: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Apr 24 04:38:41.999: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 24 04:38:41.999: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 24 04:38:41.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7563" for this suite. @ 04/24/23 04:38:42.02
• [0.222 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/24/23 04:38:42.044
  Apr 24 04:38:42.044: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename svcaccounts @ 04/24/23 04:38:42.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:38:42.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:38:42.152
  Apr 24 04:38:42.171: INFO: created pod
  E0424 04:38:42.889474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:43.890128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:44.890000      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:45.890542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:38:46.187
  E0424 04:38:46.890923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:47.891360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:48.891787      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:49.891806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:50.891961      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:51.892960      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:52.893698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:53.893928      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:54.894093      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:55.894308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:56.895017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:57.895320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:58.895457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:38:59.895712      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:00.896124      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:01.896810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:02.897498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:03.897896      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:04.898180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:05.898733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:06.899082      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:07.899546      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:08.900482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:09.901121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:10.901526      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:11.901777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:12.902133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:13.902515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:14.903128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:15.903567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:16.188: INFO: polling logs
  Apr 24 04:39:16.205: INFO: Pod logs: 
  I0424 04:38:43.806196       1 log.go:198] OK: Got token
  I0424 04:38:43.806385       1 log.go:198] validating with in-cluster discovery
  I0424 04:38:43.807451       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0424 04:38:43.807514       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7327:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682311722, NotBefore:1682311122, IssuedAt:1682311122, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7327", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c45f480c-d566-480e-b138-e0dd2f50658f"}}}
  I0424 04:38:43.852012       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0424 04:38:43.868975       1 log.go:198] OK: Validated signature on JWT
  I0424 04:38:43.869517       1 log.go:198] OK: Got valid claims from token!
  I0424 04:38:43.869604       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7327:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682311722, NotBefore:1682311122, IssuedAt:1682311122, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7327", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c45f480c-d566-480e-b138-e0dd2f50658f"}}}

  Apr 24 04:39:16.206: INFO: completed pod
  Apr 24 04:39:16.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7327" for this suite. @ 04/24/23 04:39:16.237
• [34.203 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 04/24/23 04:39:16.249
  Apr 24 04:39:16.250: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename daemonsets @ 04/24/23 04:39:16.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:39:16.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:39:16.294
  Apr 24 04:39:16.330: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/24/23 04:39:16.356
  Apr 24 04:39:16.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:16.379: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/24/23 04:39:16.379
  Apr 24 04:39:16.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:16.526: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  E0424 04:39:16.904145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:17.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:17.534: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  E0424 04:39:17.904549      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:18.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:18.536: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  E0424 04:39:18.904491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:19.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:19.534: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  E0424 04:39:19.905948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:20.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 24 04:39:20.537: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/24/23 04:39:20.545
  Apr 24 04:39:20.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 24 04:39:20.582: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0424 04:39:20.905009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:21.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:21.595: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/24/23 04:39:21.595
  Apr 24 04:39:21.620: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:21.620: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  E0424 04:39:21.905476      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:22.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:22.634: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  E0424 04:39:22.905975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:23.632: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:23.632: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  E0424 04:39:23.906586      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:24.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 24 04:39:24.629: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/24/23 04:39:24.643
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7267, will wait for the garbage collector to delete the pods @ 04/24/23 04:39:24.643
  Apr 24 04:39:24.715: INFO: Deleting DaemonSet.extensions daemon-set took: 16.201966ms
  Apr 24 04:39:24.816: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.679405ms
  E0424 04:39:24.906552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:25.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:39:25.824: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 24 04:39:25.829: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31052"},"items":null}

  Apr 24 04:39:25.834: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31052"},"items":null}

  Apr 24 04:39:25.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7267" for this suite. @ 04/24/23 04:39:25.892
• [9.655 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  E0424 04:39:25.907437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a kubernetes client @ 04/24/23 04:39:25.907
  Apr 24 04:39:25.907: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename endpointslice @ 04/24/23 04:39:25.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:39:25.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:39:25.937
  E0424 04:39:26.920884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:27.908969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:28.909117      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:29.908710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:30.909668      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 04/24/23 04:39:31.101
  E0424 04:39:31.910058      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:32.910720      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:33.911136      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:34.911547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:35.911644      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 04/24/23 04:39:36.113
  E0424 04:39:36.911926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:37.912406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:38.915830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:39.914191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:40.914736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/24/23 04:39:41.128
  E0424 04:39:41.915580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:42.915872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:43.916303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:44.916594      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:45.917145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 04/24/23 04:39:46.14
  Apr 24 04:39:46.177: INFO: EndpointSlice for Service endpointslice-7274/example-named-port not found
  E0424 04:39:46.918540      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:47.919196      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:48.919123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:49.919527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:50.919737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:51.920591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:52.925985      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:53.923779      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:54.923936      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:55.924112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:56.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7274" for this suite. @ 04/24/23 04:39:56.205
• [30.310 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/24/23 04:39:56.23
  Apr 24 04:39:56.231: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:39:56.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:39:56.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:39:56.267
  STEP: Setting up server cert @ 04/24/23 04:39:56.305
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:39:56.829
  STEP: Deploying the webhook pod @ 04/24/23 04:39:56.848
  STEP: Wait for the deployment to be ready @ 04/24/23 04:39:56.874
  Apr 24 04:39:56.888: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0424 04:39:56.924830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:39:57.925797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:39:58.908
  E0424 04:39:58.926289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:39:58.929
  E0424 04:39:59.926525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:39:59.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 24 04:39:59.934: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/24/23 04:40:00.475
  STEP: Creating a custom resource that should be denied by the webhook @ 04/24/23 04:40:00.507
  E0424 04:40:00.927721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:01.928590      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/24/23 04:40:02.606
  STEP: Updating the custom resource with disallowed data should be denied @ 04/24/23 04:40:02.617
  STEP: Deleting the custom resource should be denied @ 04/24/23 04:40:02.633
  STEP: Remove the offending key and value from the custom resource data @ 04/24/23 04:40:02.646
  STEP: Deleting the updated custom resource should be successful @ 04/24/23 04:40:02.668
  Apr 24 04:40:02.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0424 04:40:02.928789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8674" for this suite. @ 04/24/23 04:40:03.331
  STEP: Destroying namespace "webhook-markers-331" for this suite. @ 04/24/23 04:40:03.343
• [7.125 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/24/23 04:40:03.356
  Apr 24 04:40:03.356: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:40:03.359
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:03.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:03.389
  STEP: Setting up server cert @ 04/24/23 04:40:03.426
  E0424 04:40:03.929105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:40:04.005
  STEP: Deploying the webhook pod @ 04/24/23 04:40:04.013
  STEP: Wait for the deployment to be ready @ 04/24/23 04:40:04.034
  Apr 24 04:40:04.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0424 04:40:04.929266      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:05.930057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:06.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:40:06.930934      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:07.931986      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:40:08.096
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:40:08.119
  E0424 04:40:08.932303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:09.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 24 04:40:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7573-crds.webhook.example.com via the AdmissionRegistration API @ 04/24/23 04:40:09.642
  STEP: Creating a custom resource while v1 is storage version @ 04/24/23 04:40:09.666
  E0424 04:40:09.934120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:10.933749      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/24/23 04:40:11.888
  STEP: Patching the custom resource while v2 is storage version @ 04/24/23 04:40:11.918
  E0424 04:40:11.934075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:12.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0424 04:40:12.935171      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-5173" for this suite. @ 04/24/23 04:40:13.19
  STEP: Destroying namespace "webhook-markers-1746" for this suite. @ 04/24/23 04:40:13.201
• [9.858 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/24/23 04:40:13.216
  Apr 24 04:40:13.216: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename gc @ 04/24/23 04:40:13.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:13.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:13.262
  STEP: create the deployment @ 04/24/23 04:40:13.265
  W0424 04:40:13.274345      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/24/23 04:40:13.274
  STEP: delete the deployment @ 04/24/23 04:40:13.395
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/24/23 04:40:13.403
  E0424 04:40:13.935733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/24/23 04:40:13.96
  Apr 24 04:40:14.326: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 24 04:40:14.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9086" for this suite. @ 04/24/23 04:40:14.347
• [1.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/24/23 04:40:14.378
  Apr 24 04:40:14.379: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/24/23 04:40:14.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:14.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:14.418
  STEP: creating @ 04/24/23 04:40:14.422
  STEP: getting @ 04/24/23 04:40:14.487
  STEP: listing in namespace @ 04/24/23 04:40:14.505
  STEP: patching @ 04/24/23 04:40:14.509
  STEP: deleting @ 04/24/23 04:40:14.529
  Apr 24 04:40:14.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-7408" for this suite. @ 04/24/23 04:40:14.569
• [0.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/24/23 04:40:14.582
  Apr 24 04:40:14.582: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename containers @ 04/24/23 04:40:14.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:14.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:14.613
  STEP: Creating a pod to test override all @ 04/24/23 04:40:14.617
  E0424 04:40:14.936898      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:15.937401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:16.938312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:17.938663      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:40:18.667
  Apr 24 04:40:18.672: INFO: Trying to get logs from node aeveeng9ieph-1 pod client-containers-4b436f3c-4194-455b-9266-667939a88464 container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 04:40:18.701
  Apr 24 04:40:18.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8989" for this suite. @ 04/24/23 04:40:18.737
• [4.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/24/23 04:40:18.754
  Apr 24 04:40:18.754: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename deployment @ 04/24/23 04:40:18.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:18.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:18.789
  Apr 24 04:40:18.805: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0424 04:40:18.939094      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:19.939336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:20.943242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:21.939636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:22.940088      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:23.815: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/24/23 04:40:23.816
  Apr 24 04:40:23.816: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0424 04:40:23.940961      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:24.941205      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:25.827: INFO: Creating deployment "test-rollover-deployment"
  Apr 24 04:40:25.843: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0424 04:40:25.942470      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:26.942619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:27.880: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 24 04:40:27.894: INFO: Ensure that both replica sets have 1 created replica
  Apr 24 04:40:27.906: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 24 04:40:27.923: INFO: Updating deployment test-rollover-deployment
  Apr 24 04:40:27.923: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0424 04:40:27.943281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:28.943598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:29.940: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  E0424 04:40:29.944072      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:29.964: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 24 04:40:29.980: INFO: all replica sets need to contain the pod-template-hash label
  Apr 24 04:40:29.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:40:30.944601      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:31.945689      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:32.032: INFO: all replica sets need to contain the pod-template-hash label
  Apr 24 04:40:32.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:40:32.946328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:33.947071      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:33.999: INFO: all replica sets need to contain the pod-template-hash label
  Apr 24 04:40:33.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:40:34.947660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:35.948413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:35.999: INFO: all replica sets need to contain the pod-template-hash label
  Apr 24 04:40:35.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:40:36.949239      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:37.950007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:38.002: INFO: all replica sets need to contain the pod-template-hash label
  Apr 24 04:40:38.002: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 40, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 40, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:40:38.949957      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:39.950203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:39.996: INFO: 
  Apr 24 04:40:40.005: INFO: Ensure that both old replica sets have no replicas
  Apr 24 04:40:40.036: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1079  18f5a5dc-8b37-42cd-8a45-766c44cad5ff 31662 2 2023-04-24 04:40:25 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-24 04:40:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:40:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040a1a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-24 04:40:25 +0000 UTC,LastTransitionTime:2023-04-24 04:40:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-04-24 04:40:39 +0000 UTC,LastTransitionTime:2023-04-24 04:40:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 24 04:40:40.043: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-1079  fe51a390-49c5-4725-bbdb-1580dffacdff 31651 2 2023-04-24 04:40:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 18f5a5dc-8b37-42cd-8a45-766c44cad5ff 0xc0040a1ef7 0xc0040a1ef8}] [] [{kube-controller-manager Update apps/v1 2023-04-24 04:40:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18f5a5dc-8b37-42cd-8a45-766c44cad5ff\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:40:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040a1fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 04:40:40.044: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 24 04:40:40.045: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1079  06459b55-efcf-4692-8139-0ffedcad9d46 31661 2 2023-04-24 04:40:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 18f5a5dc-8b37-42cd-8a45-766c44cad5ff 0xc0040a1db7 0xc0040a1db8}] [] [{e2e.test Update apps/v1 2023-04-24 04:40:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:40:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18f5a5dc-8b37-42cd-8a45-766c44cad5ff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:40:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0040a1e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 04:40:40.045: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-1079  80898b67-366e-4fcb-97ce-e25ec68be6f9 31617 2 2023-04-24 04:40:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 18f5a5dc-8b37-42cd-8a45-766c44cad5ff 0xc003f5a017 0xc003f5a018}] [] [{kube-controller-manager Update apps/v1 2023-04-24 04:40:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18f5a5dc-8b37-42cd-8a45-766c44cad5ff\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:40:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f5a0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 04:40:40.052: INFO: Pod "test-rollover-deployment-57777854c9-nb98k" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-nb98k test-rollover-deployment-57777854c9- deployment-1079  2c8191ec-8117-4595-8dba-14dcf516362f 31626 0 2023-04-24 04:40:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 fe51a390-49c5-4725-bbdb-1580dffacdff 0xc005ceae97 0xc005ceae98}] [] [{kube-controller-manager Update v1 2023-04-24 04:40:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe51a390-49c5-4725-bbdb-1580dffacdff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 04:40:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72pkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72pkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:40:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:40:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:40:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:40:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:10.233.65.41,StartTime:2023-04-24 04:40:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 04:40:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://b467e2ef90afb57e419794d9e3559fd6178069797a7b3c4d569c306b7c61ed3e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.41,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 04:40:40.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1079" for this suite. @ 04/24/23 04:40:40.059
• [21.316 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/24/23 04:40:40.072
  Apr 24 04:40:40.073: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 04:40:40.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:40.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:40.103
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-2871 @ 04/24/23 04:40:40.108
  STEP: changing the ExternalName service to type=NodePort @ 04/24/23 04:40:40.115
  STEP: creating replication controller externalname-service in namespace services-2871 @ 04/24/23 04:40:40.157
  I0424 04:40:40.213592      14 runners.go:194] Created replication controller with name: externalname-service, namespace: services-2871, replica count: 2
  E0424 04:40:40.955353      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:41.955878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:42.956319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0424 04:40:43.266208      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 24 04:40:43.266: INFO: Creating new exec pod
  E0424 04:40:43.957314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:44.958491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:45.959048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:46.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-2871 exec execpodmbw2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 24 04:40:46.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 24 04:40:46.615: INFO: stdout: "externalname-service-285jj"
  Apr 24 04:40:46.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-2871 exec execpodmbw2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.18.234 80'
  E0424 04:40:46.959444      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:40:47.090: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.18.234 80\nConnection to 10.233.18.234 80 port [tcp/http] succeeded!\n"
  Apr 24 04:40:47.090: INFO: stdout: "externalname-service-285jj"
  Apr 24 04:40:47.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-2871 exec execpodmbw2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 31416'
  Apr 24 04:40:47.417: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 31416\nConnection to 192.168.121.43 31416 port [tcp/*] succeeded!\n"
  Apr 24 04:40:47.417: INFO: stdout: "externalname-service-c8p7b"
  Apr 24 04:40:47.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-2871 exec execpodmbw2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.18 31416'
  Apr 24 04:40:47.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.18 31416\nConnection to 192.168.121.18 31416 port [tcp/*] succeeded!\n"
  Apr 24 04:40:47.728: INFO: stdout: "externalname-service-c8p7b"
  Apr 24 04:40:47.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 04:40:47.736: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-2871" for this suite. @ 04/24/23 04:40:47.788
• [7.731 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/24/23 04:40:47.804
  Apr 24 04:40:47.804: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 04:40:47.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:47.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:47.848
  STEP: Create a pod @ 04/24/23 04:40:47.854
  E0424 04:40:47.960298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:48.960583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 04/24/23 04:40:49.887
  Apr 24 04:40:49.902: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 24 04:40:49.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6666" for this suite. @ 04/24/23 04:40:49.913
• [2.121 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/24/23 04:40:49.93
  Apr 24 04:40:49.930: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename events @ 04/24/23 04:40:49.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:49.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:49.957
  E0424 04:40:49.960380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create set of events @ 04/24/23 04:40:49.963
  STEP: get a list of Events with a label in the current namespace @ 04/24/23 04:40:49.989
  STEP: delete a list of events @ 04/24/23 04:40:49.998
  Apr 24 04:40:49.998: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/24/23 04:40:50.051
  Apr 24 04:40:50.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3266" for this suite. @ 04/24/23 04:40:50.065
• [0.143 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/24/23 04:40:50.074
  Apr 24 04:40:50.074: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:40:50.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:40:50.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:40:50.106
  STEP: Creating resourceQuota "e2e-rq-status-mzxwb" @ 04/24/23 04:40:50.116
  Apr 24 04:40:50.128: INFO: Resource quota "e2e-rq-status-mzxwb" reports spec: hard cpu limit of 500m
  Apr 24 04:40:50.128: INFO: Resource quota "e2e-rq-status-mzxwb" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-mzxwb" /status @ 04/24/23 04:40:50.128
  STEP: Confirm /status for "e2e-rq-status-mzxwb" resourceQuota via watch @ 04/24/23 04:40:50.14
  Apr 24 04:40:50.143: INFO: observed resourceQuota "e2e-rq-status-mzxwb" in namespace "resourcequota-7232" with hard status: v1.ResourceList(nil)
  Apr 24 04:40:50.143: INFO: Found resourceQuota "e2e-rq-status-mzxwb" in namespace "resourcequota-7232" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 24 04:40:50.143: INFO: ResourceQuota "e2e-rq-status-mzxwb" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/24/23 04:40:50.148
  Apr 24 04:40:50.157: INFO: Resource quota "e2e-rq-status-mzxwb" reports spec: hard cpu limit of 1
  Apr 24 04:40:50.157: INFO: Resource quota "e2e-rq-status-mzxwb" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-mzxwb" /status @ 04/24/23 04:40:50.157
  STEP: Confirm /status for "e2e-rq-status-mzxwb" resourceQuota via watch @ 04/24/23 04:40:50.169
  Apr 24 04:40:50.173: INFO: observed resourceQuota "e2e-rq-status-mzxwb" in namespace "resourcequota-7232" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 24 04:40:50.174: INFO: Found resourceQuota "e2e-rq-status-mzxwb" in namespace "resourcequota-7232" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 24 04:40:50.175: INFO: ResourceQuota "e2e-rq-status-mzxwb" /status was patched
  STEP: Get "e2e-rq-status-mzxwb" /status @ 04/24/23 04:40:50.175
  Apr 24 04:40:50.183: INFO: Resourcequota "e2e-rq-status-mzxwb" reports status: hard cpu of 1
  Apr 24 04:40:50.183: INFO: Resourcequota "e2e-rq-status-mzxwb" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-mzxwb" /status before checking Spec is unchanged @ 04/24/23 04:40:50.189
  Apr 24 04:40:50.199: INFO: Resourcequota "e2e-rq-status-mzxwb" reports status: hard cpu of 2
  Apr 24 04:40:50.200: INFO: Resourcequota "e2e-rq-status-mzxwb" reports status: hard memory of 2Gi
  Apr 24 04:40:50.202: INFO: Found resourceQuota "e2e-rq-status-mzxwb" in namespace "resourcequota-7232" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0424 04:40:50.960905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:51.961885      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:52.962591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:53.963735      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:54.964735      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:55.965240      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:56.965455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:57.966081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:58.966172      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:40:59.966603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:00.966764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:01.968213      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:02.968280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:03.969194      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:04.969408      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:05.969935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:06.970937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:07.971340      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:08.971963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:09.972472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:10.972558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:11.972672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:12.972797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:13.973183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:14.973527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:15.973510      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:16.973616      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:17.973804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:18.974190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:19.974566      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:20.974671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:21.974909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:22.974802      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:23.975581      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:24.976366      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:25.977326      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:26.977905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:27.978751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:28.980572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:29.980027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:30.980521      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:31.980739      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:32.982722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:33.982454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:34.983084      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:35.983280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:36.984236      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:37.984480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:38.985019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:39.985837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:40.986491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:41.987214      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:42.988187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:43.988235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:44.989022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:45.990268      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:46.990839      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:47.991185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:48.991629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:49.992089      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:50.991939      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:51.992602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:52.994350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:53.994835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:54.996061      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:55.995347      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:56.996096      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:57.996628      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:58.996634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:41:59.997420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:00.998382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:01.999181      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:02.999218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:03.999432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:04.999536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:06.000299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:07.000550      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:08.001222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:09.002048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:10.002868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:11.003001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:12.003780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:13.004413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:14.004637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:15.005161      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:16.006201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:17.006937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:18.007446      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:19.008280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:20.008512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:21.008688      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:22.008976      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:23.009683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:24.010034      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:25.010767      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:26.011490      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:27.011511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:28.012335      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:29.012858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:30.013236      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:31.013479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:32.013642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:33.013777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:34.014128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:35.014544      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:36.014656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:37.015321      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:38.015542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:39.016376      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:40.016454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:41.017491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:42.017769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:43.017904      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:44.021358      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:45.019758      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:46.020057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:47.020881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:48.022895      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:49.037130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:50.028791      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:51.030453      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:52.030455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:53.030795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:54.031323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:55.031292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:56.031814      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:57.032098      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:58.032479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:42:59.032743      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:00.032919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:01.033117      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:02.033217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:03.033449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:04.033588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:05.034128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:06.035668      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:07.036762      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:08.036942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:09.037884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:10.038050      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:11.038414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:12.038705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:13.039365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:14.039537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:15.040567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:16.040764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:17.041754      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:18.042054      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:19.042671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:20.043108      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:21.043386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:22.043827      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:23.044538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:24.044696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:25.045100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:26.045248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:27.046847      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:28.046147      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:29.046235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:30.046364      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:31.046492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:32.046479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:33.047197      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:34.047313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:35.047888      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:36.048114      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:37.048815      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:38.049671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:39.049803      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:40.049839      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:41.050733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:42.051529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:43.051963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:44.052788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:45.052926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:46.053421      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:47.053899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:48.054489      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:49.055384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:50.055967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:51.056169      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:52.057487      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:53.058247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:54.058802      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:55.059217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:56.059464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:57.060190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:58.060592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:43:59.060926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:00.061529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:01.061908      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:02.062890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:03.063292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:04.063819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:05.063967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:06.064066      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:07.066460      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:08.064728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:09.065605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:10.065939      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:11.066434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:12.066394      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:13.066760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:14.067443      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:15.067313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:16.067313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:17.068138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:18.068450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:19.068680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:20.069345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:21.069651      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:22.069680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:23.070008      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:24.070673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:25.071228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:26.071531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:27.072343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:28.072938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:29.072862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:30.073336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:31.074209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:32.075265      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:33.075311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:34.075974      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:35.076734      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:36.076291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:37.076177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:38.078299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:39.077092      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:40.077642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:41.078136      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:42.079399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:43.078646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:44.078682      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:45.078881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:46.079412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:47.080323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:48.080610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:49.080806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:50.080903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:51.081822      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:52.081969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:53.082318      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:54.082539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:55.082731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:56.082836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:57.084078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:58.084231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:44:59.084332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:00.084441      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:01.084605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:02.084826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:03.084942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:04.085130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:05.085272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:06.085890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:07.086309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:08.086864      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:09.087793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:10.088351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:11.088776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:12.089839      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:13.091702      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:14.091654      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:15.092407      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:16.093292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:17.094221      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:18.094720      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:19.099874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:20.099223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:21.099527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:22.099672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:23.099766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:24.099959      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:25.100226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:26.100361      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:27.100920      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:28.101120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:29.101233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:30.101425      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:31.101481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:32.102579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:33.102774      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:34.102944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:35.103182      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:36.103931      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:37.104652      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:38.105031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:39.105562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:40.106019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:45:40.215: INFO: ResourceQuota "e2e-rq-status-mzxwb" Spec was unchanged and /status reset
  Apr 24 04:45:40.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7232" for this suite. @ 04/24/23 04:45:40.23
• [290.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/24/23 04:45:40.252
  Apr 24 04:45:40.252: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-runtime @ 04/24/23 04:45:40.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:45:40.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:45:40.324
  STEP: create the container @ 04/24/23 04:45:40.328
  W0424 04:45:40.342909      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/24/23 04:45:40.343
  E0424 04:45:41.120747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:42.107245      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:43.107700      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/24/23 04:45:43.369
  STEP: the container should be terminated @ 04/24/23 04:45:43.373
  STEP: the termination message should be set @ 04/24/23 04:45:43.374
  Apr 24 04:45:43.374: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/24/23 04:45:43.374
  Apr 24 04:45:43.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6558" for this suite. @ 04/24/23 04:45:43.415
• [3.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/24/23 04:45:43.433
  Apr 24 04:45:43.434: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-preemption @ 04/24/23 04:45:43.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:45:43.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:45:43.493
  Apr 24 04:45:43.516: INFO: Waiting up to 1m0s for all nodes to be ready
  E0424 04:45:44.107943      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:45.108100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:46.108329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:47.109246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:48.109790      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:49.109848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:50.110716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:51.110846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:52.111625      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:53.111792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:54.111900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:55.112099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:56.112212      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:57.112275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:58.113105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:45:59.113274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:00.113406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:01.113534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:02.114446      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:03.114819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:04.114995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:05.115226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:06.115464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:07.116424      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:08.117154      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:09.117671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:10.118665      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:11.119204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:12.119472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:13.120240      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:14.120589      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:15.120775      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:16.121191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:17.121775      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:18.122305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:19.122414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:20.123390      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:21.123512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:22.124547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:23.125309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:24.125598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:25.125651      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:26.126687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:27.127136      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:28.127058      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:29.127328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:30.127556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:31.128093      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:32.128444      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:33.128529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:34.129081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:35.128808      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:36.129004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:37.130131      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:38.130306      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:39.130448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:40.130613      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:41.131262      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:42.132300      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:43.133141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:46:43.573: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/24/23 04:46:43.58
  Apr 24 04:46:43.581: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/24/23 04:46:43.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:46:43.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:46:43.637
  Apr 24 04:46:43.665: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Apr 24 04:46:43.671: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 24 04:46:43.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 04:46:43.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-1952" for this suite. @ 04/24/23 04:46:44
  STEP: Destroying namespace "sched-preemption-5103" for this suite. @ 04/24/23 04:46:44.073
  E0424 04:46:44.134600      14 retrywatcher.go:130] "Watch failed" err="context canceled"
• [60.704 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/24/23 04:46:44.145
  Apr 24 04:46:44.145: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 04:46:44.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:46:44.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:46:44.236
  STEP: Creating configMap configmap-1291/configmap-test-e99a99b1-331d-4c06-ae5c-4a8009d7bc37 @ 04/24/23 04:46:44.242
  STEP: Creating a pod to test consume configMaps @ 04/24/23 04:46:44.251
  E0424 04:46:45.161971      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:46.135137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:47.135710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:48.136511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:46:48.303
  Apr 24 04:46:48.308: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-05e1f8d5-92b2-4ec3-8001-9bac0daaccc2 container env-test: <nil>
  STEP: delete the pod @ 04/24/23 04:46:48.344
  Apr 24 04:46:48.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1291" for this suite. @ 04/24/23 04:46:48.436
• [4.311 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/24/23 04:46:48.461
  Apr 24 04:46:48.461: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename job @ 04/24/23 04:46:48.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:46:48.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:46:48.506
  STEP: Creating Indexed job @ 04/24/23 04:46:48.51
  STEP: Ensuring job reaches completions @ 04/24/23 04:46:48.524
  E0424 04:46:49.136866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:50.138608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:51.139349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:52.139810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:53.139798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:54.143129      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:55.143314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:56.143540      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:57.144065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:46:58.144267      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 04/24/23 04:46:58.535
  Apr 24 04:46:58.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6981" for this suite. @ 04/24/23 04:46:58.555
• [10.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/24/23 04:46:58.582
  Apr 24 04:46:58.582: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-watch @ 04/24/23 04:46:58.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:46:58.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:46:58.62
  Apr 24 04:46:58.624: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 04:46:59.145040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:00.146166      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:01.146480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 04/24/23 04:47:01.262
  Apr 24 04:47:01.269: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-24T04:47:01Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-24T04:47:01Z]] name:name1 resourceVersion:32870 uid:3ee662d5-26f8-4be9-90bf-802448e02805] num:map[num1:9223372036854775807 num2:1000000]]}
  E0424 04:47:02.146687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:03.147383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:04.148112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:05.148607      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:06.148959      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:07.149204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:08.149321      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:09.150250      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:10.150812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:11.151740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 04/24/23 04:47:11.27
  Apr 24 04:47:11.283: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-24T04:47:11Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-24T04:47:11Z]] name:name2 resourceVersion:32927 uid:b271428d-21ac-4681-af3c-4c6fdaec4346] num:map[num1:9223372036854775807 num2:1000000]]}
  E0424 04:47:12.152603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:13.152796      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:14.153619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:15.153750      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:16.154484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:17.154801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:18.155137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:19.155201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:20.155397      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:21.155536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 04/24/23 04:47:21.284
  Apr 24 04:47:21.295: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-24T04:47:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-24T04:47:21Z]] name:name1 resourceVersion:32950 uid:3ee662d5-26f8-4be9-90bf-802448e02805] num:map[num1:9223372036854775807 num2:1000000]]}
  E0424 04:47:22.156795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:23.156896      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:24.157074      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:25.157099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:26.157314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:27.158050      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:28.158424      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:29.158769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:30.159358      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:31.160065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 04/24/23 04:47:31.297
  Apr 24 04:47:31.312: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-24T04:47:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-24T04:47:31Z]] name:name2 resourceVersion:32974 uid:b271428d-21ac-4681-af3c-4c6fdaec4346] num:map[num1:9223372036854775807 num2:1000000]]}
  E0424 04:47:32.160810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:33.161602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:34.162168      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:35.162211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:36.162668      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:37.162753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:38.163717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:39.163229      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:40.163531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:41.163631      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 04/24/23 04:47:41.313
  Apr 24 04:47:41.329: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-24T04:47:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-24T04:47:21Z]] name:name1 resourceVersion:32997 uid:3ee662d5-26f8-4be9-90bf-802448e02805] num:map[num1:9223372036854775807 num2:1000000]]}
  E0424 04:47:42.163755      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:43.168261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:44.166872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:45.167316      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:46.168057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:47.169115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:48.176953      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:49.172564      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:50.184312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:51.178508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 04/24/23 04:47:51.332
  Apr 24 04:47:51.349: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-24T04:47:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-24T04:47:31Z]] name:name2 resourceVersion:33020 uid:b271428d-21ac-4681-af3c-4c6fdaec4346] num:map[num1:9223372036854775807 num2:1000000]]}
  E0424 04:47:52.177075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:53.177454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:54.178044      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:55.178385      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:56.178841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:57.178913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:58.179171      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:47:59.179263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:00.179391      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:01.179543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:48:01.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-1046" for this suite. @ 04/24/23 04:48:01.887
• [63.316 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/24/23 04:48:01.906
  Apr 24 04:48:01.906: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:48:01.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:48:01.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:48:01.95
  STEP: Creating projection with secret that has name projected-secret-test-f8163954-ba2c-4490-b1a4-9e07b1dda336 @ 04/24/23 04:48:01.954
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:48:01.96
  E0424 04:48:02.179761      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:03.181130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:04.181251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:05.181429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:48:05.999
  Apr 24 04:48:06.009: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-secrets-aefe94bb-80fd-4d46-92fe-7a784a8a9068 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:48:06.058
  Apr 24 04:48:06.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1352" for this suite. @ 04/24/23 04:48:06.118
• [4.225 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/24/23 04:48:06.133
  Apr 24 04:48:06.133: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 04:48:06.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:48:06.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:48:06.182
  E0424 04:48:06.182643      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test downward api env vars @ 04/24/23 04:48:06.188
  E0424 04:48:07.183718      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:08.183749      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:09.184060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:10.184674      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:48:10.224
  Apr 24 04:48:10.231: INFO: Trying to get logs from node aeveeng9ieph-3 pod downward-api-0a65f56b-9e97-453b-9802-86c287dbdc36 container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 04:48:10.246
  Apr 24 04:48:10.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3968" for this suite. @ 04/24/23 04:48:10.302
• [4.181 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/24/23 04:48:10.315
  Apr 24 04:48:10.315: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-runtime @ 04/24/23 04:48:10.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:48:10.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:48:10.348
  STEP: create the container @ 04/24/23 04:48:10.353
  W0424 04:48:10.369089      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/24/23 04:48:10.369
  E0424 04:48:11.201215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:12.187171      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:13.188123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/24/23 04:48:13.429
  STEP: the container should be terminated @ 04/24/23 04:48:13.437
  STEP: the termination message should be set @ 04/24/23 04:48:13.438
  Apr 24 04:48:13.438: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/24/23 04:48:13.438
  Apr 24 04:48:13.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4816" for this suite. @ 04/24/23 04:48:13.473
• [3.171 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/24/23 04:48:13.488
  Apr 24 04:48:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename limitrange @ 04/24/23 04:48:13.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:48:13.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:48:13.526
  STEP: Creating LimitRange "e2e-limitrange-97flj" in namespace "limitrange-3393" @ 04/24/23 04:48:13.531
  STEP: Creating another limitRange in another namespace @ 04/24/23 04:48:13.542
  Apr 24 04:48:13.565: INFO: Namespace "e2e-limitrange-97flj-4671" created
  Apr 24 04:48:13.565: INFO: Creating LimitRange "e2e-limitrange-97flj" in namespace "e2e-limitrange-97flj-4671"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-97flj" @ 04/24/23 04:48:13.572
  Apr 24 04:48:13.577: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-97flj" in "limitrange-3393" namespace @ 04/24/23 04:48:13.577
  Apr 24 04:48:13.585: INFO: LimitRange "e2e-limitrange-97flj" has been patched
  STEP: Delete LimitRange "e2e-limitrange-97flj" by Collection with labelSelector: "e2e-limitrange-97flj=patched" @ 04/24/23 04:48:13.585
  STEP: Confirm that the limitRange "e2e-limitrange-97flj" has been deleted @ 04/24/23 04:48:13.595
  Apr 24 04:48:13.595: INFO: Requesting list of LimitRange to confirm quantity
  Apr 24 04:48:13.610: INFO: Found 0 LimitRange with label "e2e-limitrange-97flj=patched"
  Apr 24 04:48:13.610: INFO: LimitRange "e2e-limitrange-97flj" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-97flj" @ 04/24/23 04:48:13.61
  Apr 24 04:48:13.615: INFO: Found 1 limitRange
  Apr 24 04:48:13.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-3393" for this suite. @ 04/24/23 04:48:13.621
  STEP: Destroying namespace "e2e-limitrange-97flj-4671" for this suite. @ 04/24/23 04:48:13.629
• [0.148 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/24/23 04:48:13.636
  Apr 24 04:48:13.636: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename dns @ 04/24/23 04:48:13.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:48:13.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:48:13.661
  STEP: Creating a test externalName service @ 04/24/23 04:48:13.664
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5867.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local; sleep 1; done
   @ 04/24/23 04:48:13.684
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5867.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local; sleep 1; done
   @ 04/24/23 04:48:13.684
  STEP: creating a pod to probe DNS @ 04/24/23 04:48:13.684
  STEP: submitting the pod to kubernetes @ 04/24/23 04:48:13.684
  E0424 04:48:14.188209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:15.188298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/24/23 04:48:15.709
  STEP: looking for the results for each expected name from probers @ 04/24/23 04:48:15.714
  Apr 24 04:48:15.747: INFO: File wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-ee2b39af-c934-4244-aad1-35ee4da503ee contains '' instead of 'foo.example.com.'
  Apr 24 04:48:15.755: INFO: Lookups using dns-5867/dns-test-ee2b39af-c934-4244-aad1-35ee4da503ee failed for: [wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local]

  E0424 04:48:16.188541      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:17.189337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:18.189534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:19.190598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:20.191113      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:48:20.769: INFO: DNS probes using dns-test-ee2b39af-c934-4244-aad1-35ee4da503ee succeeded

  STEP: changing the externalName to bar.example.com @ 04/24/23 04:48:20.769
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5867.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local; sleep 1; done
   @ 04/24/23 04:48:20.784
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5867.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local; sleep 1; done
   @ 04/24/23 04:48:20.784
  STEP: creating a second pod to probe DNS @ 04/24/23 04:48:20.784
  STEP: submitting the pod to kubernetes @ 04/24/23 04:48:20.784
  E0424 04:48:21.191393      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:22.193889      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:23.194180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:24.194370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/24/23 04:48:24.828
  STEP: looking for the results for each expected name from probers @ 04/24/23 04:48:24.837
  Apr 24 04:48:24.852: INFO: File wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:24.859: INFO: File jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:24.859: INFO: Lookups using dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 failed for: [wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local]

  E0424 04:48:25.194547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:26.194854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:27.195767      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:28.195988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:29.196098      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:48:29.874: INFO: File wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:29.882: INFO: File jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:29.883: INFO: Lookups using dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 failed for: [wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local]

  E0424 04:48:30.197016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:31.197109      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:32.197395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:33.197572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:34.200143      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:48:34.876: INFO: File wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:34.883: INFO: File jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:34.884: INFO: Lookups using dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 failed for: [wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local]

  E0424 04:48:35.198675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:36.198620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:37.199162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:38.199600      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:39.199726      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:48:39.875: INFO: File wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:39.885: INFO: File jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:39.885: INFO: Lookups using dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 failed for: [wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local]

  E0424 04:48:40.200051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:41.200153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:42.201024      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:43.201224      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:44.201736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:48:44.871: INFO: File wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:44.881: INFO: File jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local from pod  dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 24 04:48:44.881: INFO: Lookups using dns-5867/dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 failed for: [wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local]

  E0424 04:48:45.202368      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:46.202663      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:47.202847      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:48.203791      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:49.204291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:48:49.878: INFO: DNS probes using dns-test-8b1e5cdf-8f6f-4102-bc1c-d4b22c48a672 succeeded

  STEP: changing the service to type=ClusterIP @ 04/24/23 04:48:49.879
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5867.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5867.svc.cluster.local; sleep 1; done
   @ 04/24/23 04:48:49.906
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5867.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5867.svc.cluster.local; sleep 1; done
   @ 04/24/23 04:48:49.906
  STEP: creating a third pod to probe DNS @ 04/24/23 04:48:49.906
  STEP: submitting the pod to kubernetes @ 04/24/23 04:48:49.915
  E0424 04:48:50.206558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:51.207004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:52.207422      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:53.207625      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/24/23 04:48:53.98
  STEP: looking for the results for each expected name from probers @ 04/24/23 04:48:53.987
  Apr 24 04:48:54.004: INFO: DNS probes using dns-test-f810b882-25a7-46ca-84ab-d3b0575afaa9 succeeded

  Apr 24 04:48:54.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 04:48:54.015
  STEP: deleting the pod @ 04/24/23 04:48:54.039
  STEP: deleting the pod @ 04/24/23 04:48:54.056
  STEP: deleting the test externalName service @ 04/24/23 04:48:54.099
  STEP: Destroying namespace "dns-5867" for this suite. @ 04/24/23 04:48:54.13
• [40.507 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/24/23 04:48:54.152
  Apr 24 04:48:54.152: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 04:48:54.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:48:54.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:48:54.184
  STEP: set up a multi version CRD @ 04/24/23 04:48:54.189
  Apr 24 04:48:54.190: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 04:48:54.207813      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:55.208092      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:56.209339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:57.210277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:58.210596      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:48:59.212309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 04/24/23 04:48:59.362
  STEP: check the unserved version gets removed @ 04/24/23 04:48:59.406
  E0424 04:49:00.211845      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/24/23 04:49:01.19
  E0424 04:49:01.212705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:02.213492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:03.214321      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:04.215360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:49:04.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8335" for this suite. @ 04/24/23 04:49:04.974
• [10.834 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/24/23 04:49:04.989
  Apr 24 04:49:04.989: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename watch @ 04/24/23 04:49:04.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:05.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:05.024
  STEP: creating a watch on configmaps @ 04/24/23 04:49:05.027
  STEP: creating a new configmap @ 04/24/23 04:49:05.03
  STEP: modifying the configmap once @ 04/24/23 04:49:05.038
  STEP: closing the watch once it receives two notifications @ 04/24/23 04:49:05.047
  Apr 24 04:49:05.047: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8817  6fbbc0ed-e598-4e3f-87d8-85a15a05ede6 33427 0 2023-04-24 04:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:05.048: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8817  6fbbc0ed-e598-4e3f-87d8-85a15a05ede6 33428 0 2023-04-24 04:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/24/23 04:49:05.048
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/24/23 04:49:05.058
  STEP: deleting the configmap @ 04/24/23 04:49:05.06
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/24/23 04:49:05.068
  Apr 24 04:49:05.068: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8817  6fbbc0ed-e598-4e3f-87d8-85a15a05ede6 33429 0 2023-04-24 04:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:05.069: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8817  6fbbc0ed-e598-4e3f-87d8-85a15a05ede6 33430 0 2023-04-24 04:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:05.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8817" for this suite. @ 04/24/23 04:49:05.076
• [0.096 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/24/23 04:49:05.086
  Apr 24 04:49:05.086: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 04:49:05.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:05.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:05.12
  STEP: Creating configMap with name configmap-test-volume-ef512256-4b70-47fc-8790-79dcb57d29ea @ 04/24/23 04:49:05.123
  STEP: Creating a pod to test consume configMaps @ 04/24/23 04:49:05.128
  E0424 04:49:05.215788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:06.218258      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:07.218272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:08.218715      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:49:09.16
  Apr 24 04:49:09.167: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-4716cdb5-5aa0-4d28-a304-f8eacae6a427 container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 04:49:09.178
  Apr 24 04:49:09.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6471" for this suite. @ 04/24/23 04:49:09.21
  E0424 04:49:09.219037      14 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.134 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/24/23 04:49:09.22
  Apr 24 04:49:09.220: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 04:49:09.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:09.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:09.248
  Apr 24 04:49:09.253: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 04:49:10.220194      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:11.220834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:12.221022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/24/23 04:49:12.43
  Apr 24 04:49:12.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-2315 --namespace=crd-publish-openapi-2315 create -f -'
  E0424 04:49:13.221143      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:49:13.780: INFO: stderr: ""
  Apr 24 04:49:13.780: INFO: stdout: "e2e-test-crd-publish-openapi-3874-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 24 04:49:13.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-2315 --namespace=crd-publish-openapi-2315 delete e2e-test-crd-publish-openapi-3874-crds test-cr'
  Apr 24 04:49:14.004: INFO: stderr: ""
  Apr 24 04:49:14.004: INFO: stdout: "e2e-test-crd-publish-openapi-3874-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 24 04:49:14.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-2315 --namespace=crd-publish-openapi-2315 apply -f -'
  E0424 04:49:14.221637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:49:14.472: INFO: stderr: ""
  Apr 24 04:49:14.472: INFO: stdout: "e2e-test-crd-publish-openapi-3874-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 24 04:49:14.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-2315 --namespace=crd-publish-openapi-2315 delete e2e-test-crd-publish-openapi-3874-crds test-cr'
  Apr 24 04:49:14.628: INFO: stderr: ""
  Apr 24 04:49:14.628: INFO: stdout: "e2e-test-crd-publish-openapi-3874-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/24/23 04:49:14.628
  Apr 24 04:49:14.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=crd-publish-openapi-2315 explain e2e-test-crd-publish-openapi-3874-crds'
  Apr 24 04:49:15.202: INFO: stderr: ""
  Apr 24 04:49:15.202: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-3874-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0424 04:49:15.222737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:16.223468      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:49:16.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2315" for this suite. @ 04/24/23 04:49:16.932
• [7.723 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/24/23 04:49:16.944
  Apr 24 04:49:16.945: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename watch @ 04/24/23 04:49:16.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:16.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:16.971
  STEP: creating a new configmap @ 04/24/23 04:49:16.975
  STEP: modifying the configmap once @ 04/24/23 04:49:16.982
  STEP: modifying the configmap a second time @ 04/24/23 04:49:16.995
  STEP: deleting the configmap @ 04/24/23 04:49:17.007
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/24/23 04:49:17.015
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/24/23 04:49:17.017
  Apr 24 04:49:17.018: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9434  6233e354-ad9d-4267-8273-3b4d12696169 33517 0 2023-04-24 04:49:16 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:17.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9434  6233e354-ad9d-4267-8273-3b4d12696169 33518 0 2023-04-24 04:49:16 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:17.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9434" for this suite. @ 04/24/23 04:49:17.026
• [0.089 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/24/23 04:49:17.036
  Apr 24 04:49:17.036: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:49:17.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:17.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:17.071
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/24/23 04:49:17.075
  E0424 04:49:17.224400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:18.225502      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:19.226400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:20.227049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:49:21.108
  Apr 24 04:49:21.115: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-b4b40db1-9ab4-49a8-b8c8-1037cd7a0b18 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:49:21.148
  Apr 24 04:49:21.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6813" for this suite. @ 04/24/23 04:49:21.181
• [4.156 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/24/23 04:49:21.193
  Apr 24 04:49:21.193: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:49:21.208
  E0424 04:49:21.231145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:21.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:21.24
  STEP: validating api versions @ 04/24/23 04:49:21.246
  Apr 24 04:49:21.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-296 api-versions'
  Apr 24 04:49:21.458: INFO: stderr: ""
  Apr 24 04:49:21.458: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Apr 24 04:49:21.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-296" for this suite. @ 04/24/23 04:49:21.47
• [0.285 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/24/23 04:49:21.48
  Apr 24 04:49:21.480: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:49:21.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:21.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:21.515
  STEP: Creating configMap with name projected-configmap-test-volume-dd151c3f-26c4-4562-8464-fa02e7dc5452 @ 04/24/23 04:49:21.522
  STEP: Creating a pod to test consume configMaps @ 04/24/23 04:49:21.529
  E0424 04:49:22.231823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:23.232233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:24.232500      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:25.232720      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:49:25.569
  Apr 24 04:49:25.573: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-configmaps-9ec8a035-f737-4670-9884-a272c8321643 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:49:25.586
  Apr 24 04:49:25.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2839" for this suite. @ 04/24/23 04:49:25.612
• [4.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/24/23 04:49:25.624
  Apr 24 04:49:25.625: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename watch @ 04/24/23 04:49:25.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:25.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:25.66
  STEP: creating a watch on configmaps with a certain label @ 04/24/23 04:49:25.669
  STEP: creating a new configmap @ 04/24/23 04:49:25.671
  STEP: modifying the configmap once @ 04/24/23 04:49:25.682
  STEP: changing the label value of the configmap @ 04/24/23 04:49:25.701
  STEP: Expecting to observe a delete notification for the watched object @ 04/24/23 04:49:25.731
  Apr 24 04:49:25.731: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1240  1fd93a73-d6f9-46ca-b614-e23dfc01b96f 33591 0 2023-04-24 04:49:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:25.732: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1240  1fd93a73-d6f9-46ca-b614-e23dfc01b96f 33592 0 2023-04-24 04:49:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:25.732: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1240  1fd93a73-d6f9-46ca-b614-e23dfc01b96f 33593 0 2023-04-24 04:49:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/24/23 04:49:25.732
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/24/23 04:49:25.761
  E0424 04:49:26.233933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:27.233991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:28.234165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:29.234337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:30.234732      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:31.234899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:32.235560      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:33.235779      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:34.235986      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:35.236546      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 04/24/23 04:49:35.762
  STEP: modifying the configmap a third time @ 04/24/23 04:49:35.782
  STEP: deleting the configmap @ 04/24/23 04:49:35.794
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/24/23 04:49:35.804
  Apr 24 04:49:35.805: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1240  1fd93a73-d6f9-46ca-b614-e23dfc01b96f 33638 0 2023-04-24 04:49:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:35.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1240  1fd93a73-d6f9-46ca-b614-e23dfc01b96f 33639 0 2023-04-24 04:49:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:35.806: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1240  1fd93a73-d6f9-46ca-b614-e23dfc01b96f 33640 0 2023-04-24 04:49:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-24 04:49:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 04:49:35.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1240" for this suite. @ 04/24/23 04:49:35.817
• [10.206 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/24/23 04:49:35.835
  Apr 24 04:49:35.836: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 04:49:35.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:35.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:35.869
  STEP: creating an Endpoint @ 04/24/23 04:49:35.879
  STEP: waiting for available Endpoint @ 04/24/23 04:49:35.889
  STEP: listing all Endpoints @ 04/24/23 04:49:35.894
  STEP: updating the Endpoint @ 04/24/23 04:49:35.904
  STEP: fetching the Endpoint @ 04/24/23 04:49:35.914
  STEP: patching the Endpoint @ 04/24/23 04:49:35.924
  STEP: fetching the Endpoint @ 04/24/23 04:49:35.939
  STEP: deleting the Endpoint by Collection @ 04/24/23 04:49:35.945
  STEP: waiting for Endpoint deletion @ 04/24/23 04:49:35.962
  STEP: fetching the Endpoint @ 04/24/23 04:49:35.965
  Apr 24 04:49:35.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3492" for this suite. @ 04/24/23 04:49:35.981
• [0.179 seconds]
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/24/23 04:49:36.015
  Apr 24 04:49:36.015: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replication-controller @ 04/24/23 04:49:36.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:36.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:36.063
  Apr 24 04:49:36.070: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0424 04:49:36.237307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/24/23 04:49:37.093
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/24/23 04:49:37.104
  E0424 04:49:37.237787      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/24/23 04:49:38.118
  Apr 24 04:49:38.135: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/24/23 04:49:38.135
  E0424 04:49:38.238491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:49:39.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1749" for this suite. @ 04/24/23 04:49:39.159
• [3.154 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/24/23 04:49:39.171
  Apr 24 04:49:39.171: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:49:39.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:39.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:39.204
  STEP: Creating configMap with name projected-configmap-test-volume-map-a4a255a5-22a6-457f-98c8-bbaa5d796888 @ 04/24/23 04:49:39.207
  STEP: Creating a pod to test consume configMaps @ 04/24/23 04:49:39.213
  E0424 04:49:39.239203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:40.239251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:41.241064      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:42.241414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:43.241794      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:49:43.247
  Apr 24 04:49:43.252: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-configmaps-be81bb63-9b21-4154-8a5a-9b15ffa8b98d container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 04:49:43.265
  Apr 24 04:49:43.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9801" for this suite. @ 04/24/23 04:49:43.294
• [4.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/24/23 04:49:43.313
  Apr 24 04:49:43.313: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:49:43.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:43.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:43.339
  STEP: Setting up server cert @ 04/24/23 04:49:43.368
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:49:44.206
  STEP: Deploying the webhook pod @ 04/24/23 04:49:44.242
  E0424 04:49:44.243469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Wait for the deployment to be ready @ 04/24/23 04:49:44.274
  Apr 24 04:49:44.293: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0424 04:49:45.244328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:46.244615      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:49:46.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 49, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 49, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:49:47.245401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:48.245442      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:49:48.322
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:49:48.345
  E0424 04:49:49.245936      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:49:49.347: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/24/23 04:49:49.356
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/24/23 04:49:49.387
  STEP: Creating a dummy validating-webhook-configuration object @ 04/24/23 04:49:49.415
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/24/23 04:49:49.436
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/24/23 04:49:49.448
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/24/23 04:49:49.471
  Apr 24 04:49:49.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3948" for this suite. @ 04/24/23 04:49:49.663
  STEP: Destroying namespace "webhook-markers-3870" for this suite. @ 04/24/23 04:49:49.673
• [6.376 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/24/23 04:49:49.693
  Apr 24 04:49:49.693: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:49:49.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:49.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:49.737
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/24/23 04:49:49.742
  E0424 04:49:50.246019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:51.246275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:52.247481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:53.248473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:49:53.784
  Apr 24 04:49:53.792: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-2c258294-ee4c-448e-891a-fee591ef9304 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:49:53.805
  Apr 24 04:49:53.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9570" for this suite. @ 04/24/23 04:49:53.842
• [4.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/24/23 04:49:53.864
  Apr 24 04:49:53.864: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 04:49:53.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:49:53.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:49:53.898
  STEP: Create set of pods @ 04/24/23 04:49:53.903
  Apr 24 04:49:53.916: INFO: created test-pod-1
  Apr 24 04:49:53.927: INFO: created test-pod-2
  Apr 24 04:49:53.943: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/24/23 04:49:53.944
  E0424 04:49:54.248647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:55.248773      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:56.250597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:49:57.251079      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 04/24/23 04:49:58.139
  Apr 24 04:49:58.149: INFO: Pod quantity 3 is different from expected quantity 0
  E0424 04:49:58.251664      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:49:59.159: INFO: Pod quantity 3 is different from expected quantity 0
  E0424 04:49:59.253705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:50:00.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8782" for this suite. @ 04/24/23 04:50:00.165
• [6.315 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/24/23 04:50:00.179
  Apr 24 04:50:00.180: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:50:00.182
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:50:00.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:50:00.21
  E0424 04:50:00.254049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:01.255048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:02.255881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:03.256669      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:04.257191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:05.257374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:06.258363      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:07.259475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:08.260004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:09.260929      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:10.260764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:11.261494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:12.262350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:13.263585      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:14.264449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:15.264565      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:16.265711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/24/23 04:50:17.225
  E0424 04:50:17.268551      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:18.267620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:19.267757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:20.268749      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:21.269017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/24/23 04:50:22.235
  STEP: Ensuring resource quota status is calculated @ 04/24/23 04:50:22.246
  E0424 04:50:22.269189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:23.269465      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 04/24/23 04:50:24.253
  E0424 04:50:24.271081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status captures configMap creation @ 04/24/23 04:50:24.271
  E0424 04:50:25.271343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:26.271582      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 04/24/23 04:50:26.282
  STEP: Ensuring resource quota status released usage @ 04/24/23 04:50:26.292
  E0424 04:50:27.272261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:28.272414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:50:28.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-862" for this suite. @ 04/24/23 04:50:28.312
• [28.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/24/23 04:50:28.329
  Apr 24 04:50:28.329: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:50:28.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:50:28.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:50:28.438
  STEP: Discovering how many secrets are in namespace by default @ 04/24/23 04:50:28.442
  E0424 04:50:29.273097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:30.273625      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:31.274141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:32.275043      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:33.275616      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/24/23 04:50:33.46
  E0424 04:50:34.275958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:35.276137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:36.276979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:37.277620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:38.277841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/24/23 04:50:38.471
  STEP: Ensuring resource quota status is calculated @ 04/24/23 04:50:38.481
  E0424 04:50:39.278002      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:40.278778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 04/24/23 04:50:40.491
  STEP: Ensuring resource quota status captures secret creation @ 04/24/23 04:50:40.506
  E0424 04:50:41.279010      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:42.279858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 04/24/23 04:50:42.517
  STEP: Ensuring resource quota status released usage @ 04/24/23 04:50:42.53
  E0424 04:50:43.280342      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:44.280973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:50:44.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3981" for this suite. @ 04/24/23 04:50:44.56
• [16.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/24/23 04:50:44.574
  Apr 24 04:50:44.574: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:50:44.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:50:44.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:50:44.627
  STEP: Creating secret with name secret-test-f11db8b3-6b06-45c7-82f1-dda97dc8a47d @ 04/24/23 04:50:44.631
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:50:44.64
  E0424 04:50:45.281260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:46.281792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:47.281986      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:48.282207      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:50:48.683
  Apr 24 04:50:48.688: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-secrets-477f2fdd-c850-4d42-94f5-69bb30def97a container secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:50:49.064
  Apr 24 04:50:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5824" for this suite. @ 04/24/23 04:50:49.095
• [4.529 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/24/23 04:50:49.104
  Apr 24 04:50:49.104: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:50:49.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:50:49.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:50:49.129
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 04:50:49.134
  E0424 04:50:49.283124      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:50.283374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:51.284146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:52.285061      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:50:53.178
  Apr 24 04:50:53.186: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-dfbc92a2-bba2-4da1-9d2f-9dada980800d container client-container: <nil>
  STEP: delete the pod @ 04/24/23 04:50:53.199
  Apr 24 04:50:53.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-475" for this suite. @ 04/24/23 04:50:53.233
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/24/23 04:50:53.244
  Apr 24 04:50:53.244: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:50:53.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:50:53.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:50:53.278
  STEP: Creating projection with secret that has name projected-secret-test-map-e53dddc5-83e7-4ea8-bd3e-e9717b0e528f @ 04/24/23 04:50:53.281
  E0424 04:50:53.286206      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:50:53.298
  E0424 04:50:54.286443      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:55.287611      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:56.288709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:57.288936      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:50:57.35
  Apr 24 04:50:57.353: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-secrets-0001e3d3-1592-4fe6-bc9d-3622f2c7e6c7 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 04:50:57.366
  Apr 24 04:50:57.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6632" for this suite. @ 04/24/23 04:50:57.397
• [4.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/24/23 04:50:57.419
  Apr 24 04:50:57.419: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 04:50:57.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:50:57.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:50:57.475
  E0424 04:50:58.290041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:50:59.290139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:00.290364      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:01.290821      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:02.291392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:03.291523      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:04.291682      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:05.291937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:06.292443      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:07.292789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:08.293134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:09.294663      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:10.295042      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:11.303755      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:12.299015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:13.299272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:14.300358      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:15.306598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:16.302322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:17.302746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:18.303579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:19.304248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:51:19.613: INFO: Container started at 2023-04-24 04:50:58 +0000 UTC, pod became ready at 2023-04-24 04:51:17 +0000 UTC
  Apr 24 04:51:19.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-2101" for this suite. @ 04/24/23 04:51:19.622
• [22.220 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/24/23 04:51:19.641
  Apr 24 04:51:19.641: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 04:51:19.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:51:19.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:51:19.679
  STEP: Creating configMap with name configmap-test-volume-76966fdb-8937-4738-8106-32a8d0e73680 @ 04/24/23 04:51:19.683
  STEP: Creating a pod to test consume configMaps @ 04/24/23 04:51:19.69
  E0424 04:51:20.316331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:21.316652      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:22.317769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:23.317583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:51:23.731
  Apr 24 04:51:23.746: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-640b1870-5379-44fb-9a18-8dadc496c4bf container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 04:51:23.756
  Apr 24 04:51:23.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8348" for this suite. @ 04/24/23 04:51:23.788
• [4.159 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/24/23 04:51:23.799
  Apr 24 04:51:23.799: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:51:23.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:51:23.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:51:23.837
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/24/23 04:51:23.844
  E0424 04:51:24.317990      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:25.318941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:26.319619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:27.319886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:51:27.895
  Apr 24 04:51:27.904: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-ef53813a-f488-480c-beff-881967ac9eb2 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:51:27.929
  Apr 24 04:51:27.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2447" for this suite. @ 04/24/23 04:51:27.997
• [4.228 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/24/23 04:51:28.028
  Apr 24 04:51:28.028: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:51:28.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:51:28.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:51:28.056
  STEP: create deployment with httpd image @ 04/24/23 04:51:28.063
  Apr 24 04:51:28.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3498 create -f -'
  E0424 04:51:28.320812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:51:28.889: INFO: stderr: ""
  Apr 24 04:51:28.889: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/24/23 04:51:28.889
  Apr 24 04:51:28.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3498 diff -f -'
  E0424 04:51:29.320988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:51:29.520: INFO: rc: 1
  Apr 24 04:51:29.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-3498 delete -f -'
  Apr 24 04:51:29.731: INFO: stderr: ""
  Apr 24 04:51:29.731: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 24 04:51:29.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3498" for this suite. @ 04/24/23 04:51:29.739
• [1.721 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/24/23 04:51:29.751
  Apr 24 04:51:29.751: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-pred @ 04/24/23 04:51:29.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:51:29.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:51:29.793
  Apr 24 04:51:29.799: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 24 04:51:29.819: INFO: Waiting for terminating namespaces to be deleted...
  Apr 24 04:51:29.827: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-1 before test
  Apr 24 04:51:29.845: INFO: cilium-4hwfb from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.845: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:51:29.845: INFO: cilium-node-init-57zjr from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.845: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:51:29.846: INFO: coredns-5d78c9869d-dv4j2 from kube-system started at 2023-04-24 04:13:07 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.846: INFO: 	Container coredns ready: true, restart count 0
  Apr 24 04:51:29.846: INFO: kube-addon-manager-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.846: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 24 04:51:29.846: INFO: kube-apiserver-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.847: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 24 04:51:29.847: INFO: kube-controller-manager-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.847: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 24 04:51:29.847: INFO: kube-proxy-stmcm from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.847: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:51:29.847: INFO: kube-scheduler-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.848: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 24 04:51:29.848: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-9gjts from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:51:29.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:51:29.848: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 24 04:51:29.848: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-2 before test
  Apr 24 04:51:29.875: INFO: cilium-jjltm from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.876: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:51:29.877: INFO: cilium-node-init-8k2gd from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.878: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:51:29.878: INFO: coredns-5d78c9869d-7w9jr from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.879: INFO: 	Container coredns ready: true, restart count 0
  Apr 24 04:51:29.880: INFO: kube-addon-manager-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.880: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 24 04:51:29.881: INFO: kube-apiserver-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.882: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 24 04:51:29.882: INFO: kube-controller-manager-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.882: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 24 04:51:29.883: INFO: kube-proxy-w69d6 from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.884: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:51:29.884: INFO: kube-scheduler-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.885: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 24 04:51:29.885: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-fbwcp from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:51:29.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:51:29.886: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 24 04:51:29.887: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-3 before test
  Apr 24 04:51:29.904: INFO: cilium-c6lwz from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.905: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:51:29.905: INFO: cilium-node-init-gglwd from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.905: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:51:29.906: INFO: cilium-operator-85fcfcb8b4-tsmz8 from kube-system started at 2023-04-24 03:37:49 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.906: INFO: 	Container cilium-operator ready: true, restart count 0
  Apr 24 04:51:29.906: INFO: kube-proxy-q5ck5 from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.907: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:51:29.907: INFO: httpd-deployment-5cd84d4f9-xsmk5 from kubectl-3498 started at 2023-04-24 04:51:29 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.907: INFO: 	Container httpd ready: false, restart count 0
  Apr 24 04:51:29.908: INFO: sonobuoy from sonobuoy started at 2023-04-24 03:39:06 +0000 UTC (1 container statuses recorded)
  Apr 24 04:51:29.908: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 24 04:51:29.908: INFO: sonobuoy-e2e-job-32c32a5c56a94f9e from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:51:29.909: INFO: 	Container e2e ready: true, restart count 0
  Apr 24 04:51:29.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:51:29.910: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-c2fm7 from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:51:29.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:51:29.911: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node aeveeng9ieph-1 @ 04/24/23 04:51:29.972
  STEP: verifying the node has the label node aeveeng9ieph-2 @ 04/24/23 04:51:29.996
  STEP: verifying the node has the label node aeveeng9ieph-3 @ 04/24/23 04:51:30.037
  Apr 24 04:51:30.074: INFO: Pod cilium-4hwfb requesting resource cpu=0m on Node aeveeng9ieph-1
  Apr 24 04:51:30.180: INFO: Pod cilium-c6lwz requesting resource cpu=0m on Node aeveeng9ieph-3
  Apr 24 04:51:30.202: INFO: Pod cilium-jjltm requesting resource cpu=0m on Node aeveeng9ieph-2
  Apr 24 04:51:30.202: INFO: Pod cilium-node-init-57zjr requesting resource cpu=100m on Node aeveeng9ieph-1
  Apr 24 04:51:30.202: INFO: Pod cilium-node-init-8k2gd requesting resource cpu=100m on Node aeveeng9ieph-2
  Apr 24 04:51:30.202: INFO: Pod cilium-node-init-gglwd requesting resource cpu=100m on Node aeveeng9ieph-3
  Apr 24 04:51:30.202: INFO: Pod cilium-operator-85fcfcb8b4-tsmz8 requesting resource cpu=0m on Node aeveeng9ieph-3
  Apr 24 04:51:30.202: INFO: Pod coredns-5d78c9869d-7w9jr requesting resource cpu=100m on Node aeveeng9ieph-2
  Apr 24 04:51:30.202: INFO: Pod coredns-5d78c9869d-dv4j2 requesting resource cpu=100m on Node aeveeng9ieph-1
  Apr 24 04:51:30.202: INFO: Pod kube-addon-manager-aeveeng9ieph-1 requesting resource cpu=5m on Node aeveeng9ieph-1
  Apr 24 04:51:30.202: INFO: Pod kube-addon-manager-aeveeng9ieph-2 requesting resource cpu=5m on Node aeveeng9ieph-2
  Apr 24 04:51:30.202: INFO: Pod kube-apiserver-aeveeng9ieph-1 requesting resource cpu=250m on Node aeveeng9ieph-1
  Apr 24 04:51:30.202: INFO: Pod kube-apiserver-aeveeng9ieph-2 requesting resource cpu=250m on Node aeveeng9ieph-2
  Apr 24 04:51:30.202: INFO: Pod kube-controller-manager-aeveeng9ieph-1 requesting resource cpu=200m on Node aeveeng9ieph-1
  Apr 24 04:51:30.202: INFO: Pod kube-controller-manager-aeveeng9ieph-2 requesting resource cpu=200m on Node aeveeng9ieph-2
  Apr 24 04:51:30.202: INFO: Pod kube-proxy-q5ck5 requesting resource cpu=0m on Node aeveeng9ieph-3
  Apr 24 04:51:30.202: INFO: Pod kube-proxy-stmcm requesting resource cpu=0m on Node aeveeng9ieph-1
  Apr 24 04:51:30.202: INFO: Pod kube-proxy-w69d6 requesting resource cpu=0m on Node aeveeng9ieph-2
  Apr 24 04:51:30.202: INFO: Pod kube-scheduler-aeveeng9ieph-1 requesting resource cpu=100m on Node aeveeng9ieph-1
  Apr 24 04:51:30.202: INFO: Pod kube-scheduler-aeveeng9ieph-2 requesting resource cpu=100m on Node aeveeng9ieph-2
  Apr 24 04:51:30.202: INFO: Pod httpd-deployment-5cd84d4f9-xsmk5 requesting resource cpu=0m on Node aeveeng9ieph-3
  Apr 24 04:51:30.202: INFO: Pod sonobuoy requesting resource cpu=0m on Node aeveeng9ieph-3
  Apr 24 04:51:30.202: INFO: Pod sonobuoy-e2e-job-32c32a5c56a94f9e requesting resource cpu=0m on Node aeveeng9ieph-3
  Apr 24 04:51:30.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-9gjts requesting resource cpu=0m on Node aeveeng9ieph-1
  Apr 24 04:51:30.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-c2fm7 requesting resource cpu=0m on Node aeveeng9ieph-3
  Apr 24 04:51:30.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-fbwcp requesting resource cpu=0m on Node aeveeng9ieph-2
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/24/23 04:51:30.202
  Apr 24 04:51:30.202: INFO: Creating a pod which consumes cpu=591m on Node aeveeng9ieph-1
  Apr 24 04:51:30.237: INFO: Creating a pod which consumes cpu=591m on Node aeveeng9ieph-2
  E0424 04:51:30.335139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:51:30.444: INFO: Creating a pod which consumes cpu=1050m on Node aeveeng9ieph-3
  E0424 04:51:31.332383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:32.332907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:33.333468      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:34.333685      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/24/23 04:51:34.717
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-01a42ada-67f9-4679-bbfe-b0cc6b6a81b9.1758c5c03c31372c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4088/filler-pod-01a42ada-67f9-4679-bbfe-b0cc6b6a81b9 to aeveeng9ieph-1] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Warning], Name = [filler-pod-01a42ada-67f9-4679-bbfe-b0cc6b6a81b9.1758c5c083e9341d], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-qxz42" : failed to sync configmap cache: timed out waiting for the condition] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-01a42ada-67f9-4679-bbfe-b0cc6b6a81b9.1758c5c0c72059ca], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-01a42ada-67f9-4679-bbfe-b0cc6b6a81b9.1758c5c0d12f287c], Reason = [Created], Message = [Created container filler-pod-01a42ada-67f9-4679-bbfe-b0cc6b6a81b9] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-01a42ada-67f9-4679-bbfe-b0cc6b6a81b9.1758c5c0d2a6d325], Reason = [Started], Message = [Started container filler-pod-01a42ada-67f9-4679-bbfe-b0cc6b6a81b9] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-139a7bfc-7a1d-45b0-b1b7-5e3612a1f1c3.1758c5c04a87ddc5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4088/filler-pod-139a7bfc-7a1d-45b0-b1b7-5e3612a1f1c3 to aeveeng9ieph-3] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-139a7bfc-7a1d-45b0-b1b7-5e3612a1f1c3.1758c5c0b7b2141d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-139a7bfc-7a1d-45b0-b1b7-5e3612a1f1c3.1758c5c0dbc02b43], Reason = [Created], Message = [Created container filler-pod-139a7bfc-7a1d-45b0-b1b7-5e3612a1f1c3] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-139a7bfc-7a1d-45b0-b1b7-5e3612a1f1c3.1758c5c0e712a508], Reason = [Started], Message = [Started container filler-pod-139a7bfc-7a1d-45b0-b1b7-5e3612a1f1c3] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-66825aea-6763-41ad-b817-eec25c03219e.1758c5c03ed24aff], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4088/filler-pod-66825aea-6763-41ad-b817-eec25c03219e to aeveeng9ieph-2] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-66825aea-6763-41ad-b817-eec25c03219e.1758c5c071c93305], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-66825aea-6763-41ad-b817-eec25c03219e.1758c5c07b12bc58], Reason = [Created], Message = [Created container filler-pod-66825aea-6763-41ad-b817-eec25c03219e] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-66825aea-6763-41ad-b817-eec25c03219e.1758c5c07e064285], Reason = [Started], Message = [Started container filler-pod-66825aea-6763-41ad-b817-eec25c03219e] @ 04/24/23 04:51:34.724
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.1758c5c1475a34b7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 04/24/23 04:51:34.745
  E0424 04:51:35.333755      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node aeveeng9ieph-1 @ 04/24/23 04:51:35.744
  STEP: verifying the node doesn't have the label node @ 04/24/23 04:51:35.771
  STEP: removing the label node off the node aeveeng9ieph-2 @ 04/24/23 04:51:35.785
  STEP: verifying the node doesn't have the label node @ 04/24/23 04:51:35.823
  STEP: removing the label node off the node aeveeng9ieph-3 @ 04/24/23 04:51:35.838
  STEP: verifying the node doesn't have the label node @ 04/24/23 04:51:35.922
  Apr 24 04:51:35.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4088" for this suite. @ 04/24/23 04:51:35.954
• [6.327 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/24/23 04:51:36.092
  Apr 24 04:51:36.092: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 04:51:36.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:51:36.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:51:36.137
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/24/23 04:51:36.142
  Apr 24 04:51:36.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7061 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  E0424 04:51:36.374689      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:51:36.395: INFO: stderr: ""
  Apr 24 04:51:36.395: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/24/23 04:51:36.395
  Apr 24 04:51:36.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-7061 delete pods e2e-test-httpd-pod'
  E0424 04:51:37.455278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:38.455673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:39.455797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:51:39.985: INFO: stderr: ""
  Apr 24 04:51:39.985: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 24 04:51:39.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7061" for this suite. @ 04/24/23 04:51:40.002
• [3.920 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/24/23 04:51:40.012
  Apr 24 04:51:40.012: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 04:51:40.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:51:40.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:51:40.058
  STEP: Creating a pod to test downward api env vars @ 04/24/23 04:51:40.068
  E0424 04:51:40.456860      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:41.456984      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:42.457699      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:43.457809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:51:44.135
  Apr 24 04:51:44.152: INFO: Trying to get logs from node aeveeng9ieph-3 pod downward-api-a522dcbf-2a49-4f9b-add0-13bdd0255dfe container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 04:51:44.168
  Apr 24 04:51:44.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1910" for this suite. @ 04/24/23 04:51:44.205
• [4.210 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/24/23 04:51:44.223
  Apr 24 04:51:44.224: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 04:51:44.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:51:44.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:51:44.255
  STEP: Creating configMap with name cm-test-opt-del-5ccb5a3b-c972-47f4-944b-7daff8f20486 @ 04/24/23 04:51:44.264
  STEP: Creating configMap with name cm-test-opt-upd-3e5ad4a8-4651-4bd7-b7f0-403da94bf603 @ 04/24/23 04:51:44.273
  STEP: Creating the pod @ 04/24/23 04:51:44.281
  E0424 04:51:44.458273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:45.469182      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:46.460538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:47.460959      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-5ccb5a3b-c972-47f4-944b-7daff8f20486 @ 04/24/23 04:51:48.442
  STEP: Updating configmap cm-test-opt-upd-3e5ad4a8-4651-4bd7-b7f0-403da94bf603 @ 04/24/23 04:51:48.455
  E0424 04:51:48.461785      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating configMap with name cm-test-opt-create-a8f0a11b-4135-4713-9544-72294a09d2f4 @ 04/24/23 04:51:48.463
  STEP: waiting to observe update in volume @ 04/24/23 04:51:48.471
  E0424 04:51:49.462080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:50.463093      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:51.463493      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:52.463543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:53.463837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:54.464629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:55.464805      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:56.465062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:57.465856      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:58.466728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:51:59.466941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:00.467633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:01.467554      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:02.468195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:03.468168      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:04.468709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:05.468835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:06.469574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:07.469935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:08.470412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:09.471013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:10.471204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:11.471578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:12.472948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:13.473794      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:14.474046      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:15.475347      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:16.475457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:17.476342      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:18.476997      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:19.477438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:20.478179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:21.478235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:22.479546      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:23.480556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:24.480537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:25.481113      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:26.482326      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:27.483234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:28.483733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:29.493578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:30.487001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:31.487021      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:32.488811      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:33.489007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:34.489596      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:35.490038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:36.490676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:37.491246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:38.491828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:39.491974      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:40.492392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:41.492481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:42.493358      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:43.494184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:44.495195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:45.495237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:46.495583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:47.495955      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:48.496366      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:49.497126      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:50.497769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:51.498304      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:52.498515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:53.505040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:52:53.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5272" for this suite. @ 04/24/23 04:52:53.54
• [69.329 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/24/23 04:52:53.561
  Apr 24 04:52:53.561: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 04:52:53.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:52:53.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:52:53.596
  STEP: Counting existing ResourceQuota @ 04/24/23 04:52:53.601
  E0424 04:52:54.506488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:55.506427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:56.507101      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:57.507275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:52:58.507806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/24/23 04:52:58.609
  STEP: Ensuring resource quota status is calculated @ 04/24/23 04:52:58.624
  E0424 04:52:59.507954      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:00.508147      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 04/24/23 04:53:00.633
  STEP: Ensuring resource quota status captures replicaset creation @ 04/24/23 04:53:00.657
  E0424 04:53:01.508314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:02.509263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 04/24/23 04:53:02.67
  STEP: Ensuring resource quota status released usage @ 04/24/23 04:53:02.681
  E0424 04:53:03.510318      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:04.510623      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:04.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5613" for this suite. @ 04/24/23 04:53:04.699
• [11.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/24/23 04:53:04.713
  Apr 24 04:53:04.713: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename disruption @ 04/24/23 04:53:04.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:04.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:04.745
  STEP: Waiting for the pdb to be processed @ 04/24/23 04:53:04.756
  E0424 04:53:05.511164      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:06.511808      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/24/23 04:53:06.795
  Apr 24 04:53:06.804: INFO: running pods: 0 < 3
  E0424 04:53:07.512459      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:08.518463      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:08.822: INFO: running pods: 0 < 3
  E0424 04:53:09.519228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:10.519454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:10.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6307" for this suite. @ 04/24/23 04:53:10.833
• [6.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/24/23 04:53:10.856
  Apr 24 04:53:10.856: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename svcaccounts @ 04/24/23 04:53:10.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:10.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:10.888
  STEP: creating a ServiceAccount @ 04/24/23 04:53:10.892
  STEP: watching for the ServiceAccount to be added @ 04/24/23 04:53:10.904
  STEP: patching the ServiceAccount @ 04/24/23 04:53:10.91
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/24/23 04:53:10.919
  STEP: deleting the ServiceAccount @ 04/24/23 04:53:10.924
  Apr 24 04:53:10.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9489" for this suite. @ 04/24/23 04:53:10.947
• [0.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/24/23 04:53:10.959
  Apr 24 04:53:10.959: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pods @ 04/24/23 04:53:10.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:10.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:10.988
  STEP: creating the pod @ 04/24/23 04:53:10.991
  STEP: submitting the pod to kubernetes @ 04/24/23 04:53:10.992
  E0424 04:53:11.520364      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:12.520643      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/24/23 04:53:13.03
  STEP: updating the pod @ 04/24/23 04:53:13.036
  E0424 04:53:13.521431      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:13.557: INFO: Successfully updated pod "pod-update-2f031f5d-87c5-471a-b48d-221b9551d0d4"
  STEP: verifying the updated pod is in kubernetes @ 04/24/23 04:53:13.575
  Apr 24 04:53:13.581: INFO: Pod update OK
  Apr 24 04:53:13.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2739" for this suite. @ 04/24/23 04:53:13.592
• [2.649 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/24/23 04:53:13.616
  Apr 24 04:53:13.616: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename security-context @ 04/24/23 04:53:13.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:13.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:13.644
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/24/23 04:53:13.651
  E0424 04:53:14.521643      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:15.521913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:16.522138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:17.523293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:18.524092      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:19.523756      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:53:19.7
  Apr 24 04:53:19.704: INFO: Trying to get logs from node aeveeng9ieph-2 pod security-context-791fed38-df41-4aef-89c3-62af52abc23c container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:53:19.734
  Apr 24 04:53:19.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4210" for this suite. @ 04/24/23 04:53:19.78
• [6.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/24/23 04:53:19.801
  Apr 24 04:53:19.801: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 04:53:19.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:19.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:19.853
  STEP: set up a multi version CRD @ 04/24/23 04:53:19.856
  Apr 24 04:53:19.858: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 04:53:20.524548      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:21.525058      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:22.524856      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:23.525675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:24.525931      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 04/24/23 04:53:24.73
  STEP: check the new version name is served @ 04/24/23 04:53:24.753
  E0424 04:53:25.526944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:26.527939      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 04/24/23 04:53:26.65
  E0424 04:53:27.528925      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/24/23 04:53:27.688
  E0424 04:53:28.530555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:29.531469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:30.532155      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:31.533060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:31.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2829" for this suite. @ 04/24/23 04:53:31.962
• [12.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/24/23 04:53:31.982
  Apr 24 04:53:31.982: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:53:31.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:32.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:32.041
  STEP: Setting up server cert @ 04/24/23 04:53:32.075
  E0424 04:53:32.537469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:53:32.692
  STEP: Deploying the webhook pod @ 04/24/23 04:53:32.708
  STEP: Wait for the deployment to be ready @ 04/24/23 04:53:32.732
  Apr 24 04:53:32.743: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0424 04:53:33.536766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:34.537634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:34.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 53, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 53, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 53, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 53, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:53:35.538324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:36.538380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:53:36.774
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:53:36.79
  E0424 04:53:37.538516      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:37.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/24/23 04:53:37.799
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/24/23 04:53:37.843
  STEP: Creating a configMap that should not be mutated @ 04/24/23 04:53:37.852
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/24/23 04:53:37.869
  STEP: Creating a configMap that should be mutated @ 04/24/23 04:53:37.885
  Apr 24 04:53:37.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7000" for this suite. @ 04/24/23 04:53:38.066
  STEP: Destroying namespace "webhook-markers-3452" for this suite. @ 04/24/23 04:53:38.089
• [6.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/24/23 04:53:38.108
  Apr 24 04:53:38.108: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:53:38.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:38.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:38.141
  STEP: Setting up server cert @ 04/24/23 04:53:38.171
  E0424 04:53:38.539111      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:39.539262      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:53:39.736
  STEP: Deploying the webhook pod @ 04/24/23 04:53:39.742
  STEP: Wait for the deployment to be ready @ 04/24/23 04:53:39.76
  Apr 24 04:53:39.770: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0424 04:53:40.539649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:41.539812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:53:41.793
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:53:41.813
  E0424 04:53:42.540139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:42.815: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/24/23 04:53:42.826
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/24/23 04:53:42.875
  Apr 24 04:53:42.875: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:53:42.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2294" for this suite. @ 04/24/23 04:53:43.03
  STEP: Destroying namespace "webhook-markers-5819" for this suite. @ 04/24/23 04:53:43.043
• [4.942 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/24/23 04:53:43.052
  Apr 24 04:53:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename dns @ 04/24/23 04:53:43.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:43.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:43.085
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/24/23 04:53:43.089
  Apr 24 04:53:43.102: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1809  f99a8022-9c67-40ea-aa97-b3f8126ff3f2 35273 0 2023-04-24 04:53:43 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-24 04:53:43 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkjsr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkjsr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0424 04:53:43.541037      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:44.547277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:45.548105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:46.548529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/24/23 04:53:47.123
  Apr 24 04:53:47.123: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1809 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:53:47.123: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:53:47.125: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:53:47.125: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1809/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 04/24/23 04:53:47.295
  Apr 24 04:53:47.296: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1809 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 04:53:47.296: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 04:53:47.298: INFO: ExecWithOptions: Clientset creation
  Apr 24 04:53:47.298: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1809/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 24 04:53:47.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 04:53:47.488: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1809" for this suite. @ 04/24/23 04:53:47.511
• [4.470 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS  E0424 04:53:47.548780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 04/24/23 04:53:47.55
  Apr 24 04:53:47.550: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename daemonsets @ 04/24/23 04:53:47.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:53:47.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:53:47.58
  Apr 24 04:53:47.616: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/24/23 04:53:47.627
  Apr 24 04:53:47.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:53:47.638: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  E0424 04:53:48.549411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:48.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:53:48.789: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  E0424 04:53:49.550053      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:49.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 04:53:49.667: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/24/23 04:53:49.697
  STEP: Check that daemon pods images are updated. @ 04/24/23 04:53:49.724
  Apr 24 04:53:49.735: INFO: Wrong image for pod: daemon-set-9c9z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 24 04:53:49.737: INFO: Wrong image for pod: daemon-set-dckrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0424 04:53:50.551272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:50.756: INFO: Wrong image for pod: daemon-set-9c9z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 24 04:53:50.757: INFO: Wrong image for pod: daemon-set-dckrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0424 04:53:51.551266      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:51.756: INFO: Wrong image for pod: daemon-set-9c9z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 24 04:53:51.756: INFO: Wrong image for pod: daemon-set-dckrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 24 04:53:51.757: INFO: Pod daemon-set-lgxk9 is not available
  E0424 04:53:52.551923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:52.759: INFO: Wrong image for pod: daemon-set-9c9z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 24 04:53:52.760: INFO: Wrong image for pod: daemon-set-dckrd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 24 04:53:52.760: INFO: Pod daemon-set-lgxk9 is not available
  E0424 04:53:53.552781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:53.763: INFO: Wrong image for pod: daemon-set-9c9z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0424 04:53:54.553388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:54.757: INFO: Wrong image for pod: daemon-set-9c9z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 24 04:53:54.757: INFO: Pod daemon-set-dtnrx is not available
  E0424 04:53:55.553274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:55.759: INFO: Wrong image for pod: daemon-set-9c9z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 24 04:53:55.759: INFO: Pod daemon-set-dtnrx is not available
  E0424 04:53:56.553862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:53:57.554466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:57.758: INFO: Pod daemon-set-5nbz9 is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/24/23 04:53:57.766
  Apr 24 04:53:57.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 04:53:57.786: INFO: Node aeveeng9ieph-2 is running 0 daemon pod, expected 1
  E0424 04:53:58.555425      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:53:58.801: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 04:53:58.802: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/24/23 04:53:58.841
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3860, will wait for the garbage collector to delete the pods @ 04/24/23 04:53:58.841
  Apr 24 04:53:58.907: INFO: Deleting DaemonSet.extensions daemon-set took: 10.330599ms
  Apr 24 04:53:59.007: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.669677ms
  E0424 04:53:59.556057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:54:00.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 04:54:00.013: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 24 04:54:00.060: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35535"},"items":null}

  Apr 24 04:54:00.068: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35535"},"items":null}

  Apr 24 04:54:00.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3860" for this suite. @ 04/24/23 04:54:00.096
• [12.557 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/24/23 04:54:00.112
  Apr 24 04:54:00.112: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 04:54:00.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:00.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:00.146
  STEP: Creating secret with name secret-test-6d664c1b-b534-41d4-ab90-7a3d19ce0c54 @ 04/24/23 04:54:00.15
  STEP: Creating a pod to test consume secrets @ 04/24/23 04:54:00.157
  E0424 04:54:00.556950      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:01.557162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:02.558026      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:03.558320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:54:04.199
  Apr 24 04:54:04.210: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-secrets-5ef8326a-f047-4740-933d-458853a15f35 container secret-env-test: <nil>
  STEP: delete the pod @ 04/24/23 04:54:04.236
  Apr 24 04:54:04.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5777" for this suite. @ 04/24/23 04:54:04.269
• [4.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/24/23 04:54:04.299
  Apr 24 04:54:04.299: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:54:04.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:04.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:04.362
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/24/23 04:54:04.409
  E0424 04:54:04.558805      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:05.579509      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:06.565868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:07.566751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:54:08.478
  Apr 24 04:54:08.483: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-3352bd2b-35c8-4112-9e23-c61e59dd6582 container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:54:08.497
  Apr 24 04:54:08.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4490" for this suite. @ 04/24/23 04:54:08.538
• [4.253 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/24/23 04:54:08.555
  Apr 24 04:54:08.555: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 04:54:08.559
  E0424 04:54:08.567649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:08.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:08.596
  STEP: Creating the pod @ 04/24/23 04:54:08.599
  E0424 04:54:09.600369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:10.585760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:54:11.177: INFO: Successfully updated pod "labelsupdate68c7b986-1d63-40a2-b1a6-0b1c3b8b455d"
  E0424 04:54:11.585545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:12.588221      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:54:13.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6261" for this suite. @ 04/24/23 04:54:13.231
• [4.690 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/24/23 04:54:13.245
  Apr 24 04:54:13.245: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubelet-test @ 04/24/23 04:54:13.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:13.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:13.301
  Apr 24 04:54:13.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9292" for this suite. @ 04/24/23 04:54:13.367
• [0.139 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/24/23 04:54:13.386
  Apr 24 04:54:13.386: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename gc @ 04/24/23 04:54:13.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:13.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:13.42
  STEP: create the deployment @ 04/24/23 04:54:13.425
  W0424 04:54:13.434135      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/24/23 04:54:13.434
  E0424 04:54:13.588531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 04/24/23 04:54:13.953
  STEP: wait for all rs to be garbage collected @ 04/24/23 04:54:13.962
  STEP: expected 0 rs, got 1 rs @ 04/24/23 04:54:13.982
  STEP: expected 0 pods, got 2 pods @ 04/24/23 04:54:13.994
  STEP: Gathering metrics @ 04/24/23 04:54:14.549
  E0424 04:54:14.589289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:54:14.724: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 24 04:54:14.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2474" for this suite. @ 04/24/23 04:54:14.736
• [1.361 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/24/23 04:54:14.748
  Apr 24 04:54:14.749: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename runtimeclass @ 04/24/23 04:54:14.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:14.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:14.779
  E0424 04:54:15.589765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:16.590199      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:54:16.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8332" for this suite. @ 04/24/23 04:54:16.841
• [2.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/24/23 04:54:16.857
  Apr 24 04:54:16.857: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename svcaccounts @ 04/24/23 04:54:16.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:16.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:16.889
  Apr 24 04:54:16.897: INFO: Got root ca configmap in namespace "svcaccounts-3040"
  Apr 24 04:54:16.906: INFO: Deleted root ca configmap in namespace "svcaccounts-3040"
  STEP: waiting for a new root ca configmap created @ 04/24/23 04:54:17.406
  Apr 24 04:54:17.422: INFO: Recreated root ca configmap in namespace "svcaccounts-3040"
  Apr 24 04:54:17.432: INFO: Updated root ca configmap in namespace "svcaccounts-3040"
  E0424 04:54:17.591177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for the root ca configmap reconciled @ 04/24/23 04:54:17.933
  Apr 24 04:54:17.941: INFO: Reconciled root ca configmap in namespace "svcaccounts-3040"
  Apr 24 04:54:17.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3040" for this suite. @ 04/24/23 04:54:17.95
• [1.107 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/24/23 04:54:17.967
  Apr 24 04:54:17.967: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename deployment @ 04/24/23 04:54:17.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:17.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:17.998
  Apr 24 04:54:18.003: INFO: Creating deployment "test-recreate-deployment"
  Apr 24 04:54:18.022: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 24 04:54:18.042: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0424 04:54:18.591331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:19.593445      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:54:20.085: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 24 04:54:20.093: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 24 04:54:20.106: INFO: Updating deployment test-recreate-deployment
  Apr 24 04:54:20.106: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 24 04:54:20.260: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8855  1b419c1b-5d5d-4635-9446-b6027edebe54 35830 2 2023-04-24 04:54:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-24 04:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f498f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-24 04:54:20 +0000 UTC,LastTransitionTime:2023-04-24 04:54:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-04-24 04:54:20 +0000 UTC,LastTransitionTime:2023-04-24 04:54:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 24 04:54:20.266: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-8855  d72018e9-a562-4d3a-aab6-ca5696dc5226 35829 1 2023-04-24 04:54:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 1b419c1b-5d5d-4635-9446-b6027edebe54 0xc002f49cc7 0xc002f49cc8}] [] [{kube-controller-manager Update apps/v1 2023-04-24 04:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b419c1b-5d5d-4635-9446-b6027edebe54\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:54:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f49d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 04:54:20.266: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 24 04:54:20.266: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-8855  c627ac11-2060-488c-b0af-a05bfd3a0c10 35817 2 2023-04-24 04:54:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 1b419c1b-5d5d-4635-9446-b6027edebe54 0xc002f49de7 0xc002f49de8}] [] [{kube-controller-manager Update apps/v1 2023-04-24 04:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b419c1b-5d5d-4635-9446-b6027edebe54\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 04:54:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f49e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 04:54:20.273: INFO: Pod "test-recreate-deployment-54757ffd6c-ndczm" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-ndczm test-recreate-deployment-54757ffd6c- deployment-8855  a6252b1b-8cdd-4b57-b6f7-4455ce93e3af 35828 0 2023-04-24 04:54:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c d72018e9-a562-4d3a-aab6-ca5696dc5226 0xc0034a2cc7 0xc0034a2cc8}] [] [{kube-controller-manager Update v1 2023-04-24 04:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d72018e9-a562-4d3a-aab6-ca5696dc5226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 04:54:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsw86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsw86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:54:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:54:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:54:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 04:54:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 04:54:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 04:54:20.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8855" for this suite. @ 04/24/23 04:54:20.282
• [2.326 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/24/23 04:54:20.297
  Apr 24 04:54:20.297: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 04:54:20.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:20.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:20.347
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 04:54:20.36
  E0424 04:54:20.594341      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:21.595276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:22.596379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:23.598288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:54:24.498
  Apr 24 04:54:24.503: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-dfeb213b-b4fc-4bfd-892b-f70fb80d2607 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 04:54:24.515
  Apr 24 04:54:24.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5679" for this suite. @ 04/24/23 04:54:24.548
• [4.262 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/24/23 04:54:24.56
  Apr 24 04:54:24.560: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-pred @ 04/24/23 04:54:24.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:54:24.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:54:24.592
  Apr 24 04:54:24.595: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  E0424 04:54:24.597958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:54:24.607: INFO: Waiting for terminating namespaces to be deleted...
  Apr 24 04:54:24.613: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-1 before test
  Apr 24 04:54:24.629: INFO: cilium-4hwfb from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: cilium-node-init-57zjr from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: coredns-5d78c9869d-dv4j2 from kube-system started at 2023-04-24 04:13:07 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container coredns ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: kube-addon-manager-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: kube-apiserver-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: kube-controller-manager-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: kube-proxy-stmcm from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: kube-scheduler-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-9gjts from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:54:24.629: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 24 04:54:24.629: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-2 before test
  Apr 24 04:54:24.640: INFO: cilium-jjltm from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: cilium-node-init-8k2gd from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: coredns-5d78c9869d-7w9jr from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container coredns ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: kube-addon-manager-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: kube-apiserver-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: kube-controller-manager-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: kube-proxy-w69d6 from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: kube-scheduler-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-fbwcp from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:54:24.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 24 04:54:24.640: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-3 before test
  Apr 24 04:54:24.662: INFO: test-recreate-deployment-54757ffd6c-ndczm from deployment-8855 started at 2023-04-24 04:54:20 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.662: INFO: 	Container httpd ready: false, restart count 0
  Apr 24 04:54:24.662: INFO: cilium-c6lwz from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.662: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 04:54:24.662: INFO: cilium-node-init-gglwd from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.662: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 04:54:24.662: INFO: cilium-operator-85fcfcb8b4-tsmz8 from kube-system started at 2023-04-24 03:37:49 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.662: INFO: 	Container cilium-operator ready: true, restart count 0
  Apr 24 04:54:24.662: INFO: kube-proxy-q5ck5 from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.662: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 04:54:24.662: INFO: sonobuoy from sonobuoy started at 2023-04-24 03:39:06 +0000 UTC (1 container statuses recorded)
  Apr 24 04:54:24.662: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 24 04:54:24.662: INFO: sonobuoy-e2e-job-32c32a5c56a94f9e from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:54:24.662: INFO: 	Container e2e ready: true, restart count 0
  Apr 24 04:54:24.662: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:54:24.662: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-c2fm7 from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 04:54:24.662: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 04:54:24.662: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/24/23 04:54:24.662
  E0424 04:54:25.599045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:26.599512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/24/23 04:54:26.695
  STEP: Trying to apply a random label on the found node. @ 04/24/23 04:54:26.717
  STEP: verifying the node has the label kubernetes.io/e2e-02011a36-85a1-4515-b3b0-6bf99ed96219 95 @ 04/24/23 04:54:26.737
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/24/23 04:54:26.744
  E0424 04:54:27.601665      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:28.607655      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:29.605309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:30.606294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.18 on the node which pod4 resides and expect not scheduled @ 04/24/23 04:54:30.792
  E0424 04:54:31.607184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:32.607544      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:33.607587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:34.607941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:35.608180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:36.609426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:37.609459      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:38.609922      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:39.609969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:40.611175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:41.611190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:42.611303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:43.611427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:44.612466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:45.612710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:46.612879      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:47.614100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:48.614436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:49.615070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:50.615350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:51.615412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:52.615900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:53.616304      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:54.617415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:55.617587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:56.618078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:57.618243      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:58.618933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:54:59.619693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:00.620070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:01.620111      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:02.620660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:03.620728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:04.621378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:05.621714      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:06.622510      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:07.622664      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:08.623255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:09.623209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:10.623560      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:11.624327      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:12.625020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:13.625227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:14.625532      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:15.625903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:16.626515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:17.627394      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:18.628441      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:19.628764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:20.629336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:21.629283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:22.630355      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:23.631370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:24.632026      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:25.632469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:26.632754      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:27.632912      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:28.633674      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:29.633687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:30.634136      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:31.634884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:32.635338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:33.635341      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:34.637860      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:35.636942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:36.637166      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:37.637552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:38.638732      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:39.639434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:40.639703      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:41.639763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:42.640128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:43.640877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:44.641447      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:45.642599      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:46.642707      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:47.642782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:48.644106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:49.644269      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:50.648472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:51.644430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:52.644819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:53.645752      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:54.646247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:55.647263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:56.647435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:57.648339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:58.648681      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:55:59.649638      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:00.650002      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:01.650133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:02.651210      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:03.651645      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:04.652698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:05.653401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:06.653744      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:07.654610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:08.654864      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:09.655966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:10.656467      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:11.656541      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:12.657713      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:13.657933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:14.658618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:15.659309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:16.660551      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:17.660730      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:18.661385      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:19.661651      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:20.663536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:21.664659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:22.665952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:23.666455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:24.667322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:25.668282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:26.669106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:27.670201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:28.673134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:29.673320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:30.673958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:31.674653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:32.675431      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:33.676520      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:34.676728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:35.677797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:36.677920      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:37.679296      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:38.679324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:39.680283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:40.680611      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:41.681202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:42.681413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:43.682474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:44.682627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:45.683204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:46.683317      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:47.683807      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:48.684073      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:49.684874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:50.685153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:51.685270      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:52.685576      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:53.686127      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:54.686386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:55.686760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:56.687333      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:57.688360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:58.689142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:56:59.689215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:00.690009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:01.690435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:02.691505      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:03.692746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:04.693613      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:05.694020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:06.694743      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:07.695346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:08.696249      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:09.697020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:10.697077      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:11.697410      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:12.698109      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:13.698885      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:14.699060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:15.699459      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:16.699607      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:17.700080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:18.700175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:19.700947      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:20.700966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:21.701141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:22.701575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:23.702458      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:24.702583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:25.703244      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:26.703372      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:27.704482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:28.704694      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:29.705069      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:30.705322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:31.706245      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:32.706069      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:33.706675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:34.706757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:35.707855      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:36.708527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:37.708837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:38.709017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:39.710007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:40.710154      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:41.710487      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:42.710413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:43.710438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:44.710706      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:45.710734      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:46.711271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:47.711778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:48.712544      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:49.712897      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:50.715528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:51.715386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:52.716414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:53.717354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:54.718093      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:55.718861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:56.719287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:57.719605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:58.720255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:57:59.721164      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:00.722142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:01.722492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:02.722840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:03.723472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:04.724308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:05.725580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:06.726290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:07.726713      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:08.727348      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:09.728145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:10.728867      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:11.729727      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:12.730122      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:13.730485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:14.731343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:15.732053      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:16.732253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:17.732320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:18.733209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:19.734516      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:20.735346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:21.735656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:22.735877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:23.736765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:24.736821      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:25.737632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:26.737578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:27.737584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:28.737721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:29.738361      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:30.738116      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:31.738492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:32.738599      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:33.739996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:34.739883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:35.740066      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:36.740809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:37.752173      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:38.744279      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:39.746711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:40.747355      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:41.746798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:42.746747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:43.746981      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:44.747184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:45.747438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:46.748236      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:47.749067      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:48.749016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:49.749657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:50.750516      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:51.751346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:52.751298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:53.751931      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:54.751798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:55.752450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:56.752567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:57.752929      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:58.753383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:58:59.753536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:00.754178      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:01.754817      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:02.755049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:03.755746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:04.756514      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:05.756643      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:06.756851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:07.757186      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:08.757740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:09.758094      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:10.758552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:11.759225      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:12.759650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:13.760004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:14.760829      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:15.761015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:16.761157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:17.761535      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:18.761556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:19.761762      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:20.762125      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:21.762404      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:22.762477      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:23.762901      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:24.763859      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:25.764552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:26.765216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:27.765425      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:28.765780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:29.766082      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:30.766133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-02011a36-85a1-4515-b3b0-6bf99ed96219 off the node aeveeng9ieph-3 @ 04/24/23 04:59:30.805
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-02011a36-85a1-4515-b3b0-6bf99ed96219 @ 04/24/23 04:59:30.836
  Apr 24 04:59:30.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9489" for this suite. @ 04/24/23 04:59:30.864
• [306.316 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/24/23 04:59:30.877
  Apr 24 04:59:30.878: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:59:30.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:59:30.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:59:30.925
  STEP: Setting up server cert @ 04/24/23 04:59:30.967
  E0424 04:59:31.768228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:59:32.128
  STEP: Deploying the webhook pod @ 04/24/23 04:59:32.145
  STEP: Wait for the deployment to be ready @ 04/24/23 04:59:32.169
  Apr 24 04:59:32.180: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0424 04:59:32.815059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:33.804456      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:59:34.210: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 59, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 59, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:59:34.804647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:35.804588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:59:36.22
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:59:36.275
  E0424 04:59:36.804752      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:59:37.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/24/23 04:59:37.281
  STEP: create a pod that should be updated by the webhook @ 04/24/23 04:59:37.312
  Apr 24 04:59:37.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9591" for this suite. @ 04/24/23 04:59:37.46
  STEP: Destroying namespace "webhook-markers-6869" for this suite. @ 04/24/23 04:59:37.475
• [6.612 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/24/23 04:59:37.492
  Apr 24 04:59:37.492: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 04:59:37.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:59:37.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:59:37.54
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/24/23 04:59:37.544
  E0424 04:59:37.805552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:38.806620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:39.807162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:40.807302      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:59:41.58
  Apr 24 04:59:41.584: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-d5125ed4-1c63-4cca-bf29-751103f52add container test-container: <nil>
  STEP: delete the pod @ 04/24/23 04:59:41.61
  Apr 24 04:59:41.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1167" for this suite. @ 04/24/23 04:59:41.635
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/24/23 04:59:41.652
  Apr 24 04:59:41.652: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 04:59:41.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:59:41.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:59:41.688
  STEP: Creating configMap with name configmap-test-volume-map-d3135b6e-fa17-40ce-bc3d-b72136b6ebd9 @ 04/24/23 04:59:41.691
  STEP: Creating a pod to test consume configMaps @ 04/24/23 04:59:41.698
  E0424 04:59:41.807530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:42.808623      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:43.809337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:44.809515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 04:59:45.731
  Apr 24 04:59:45.736: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-90472fcf-fdf3-4d65-aff0-886d0d2de293 container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 04:59:45.747
  Apr 24 04:59:45.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9523" for this suite. @ 04/24/23 04:59:45.773
• [4.132 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/24/23 04:59:45.786
  Apr 24 04:59:45.786: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 04:59:45.788
  E0424 04:59:45.809499      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:59:45.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:59:45.817
  STEP: Setting up server cert @ 04/24/23 04:59:45.856
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 04:59:46.73
  STEP: Deploying the webhook pod @ 04/24/23 04:59:46.737
  STEP: Wait for the deployment to be ready @ 04/24/23 04:59:46.751
  Apr 24 04:59:46.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0424 04:59:46.809759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:47.809919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:59:48.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 4, 59, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 59, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 4, 59, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 4, 59, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 04:59:48.810631      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:49.810786      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 04:59:50.803
  E0424 04:59:50.812760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 04:59:50.826
  E0424 04:59:51.812873      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 04:59:51.827: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/24/23 04:59:51.834
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/24/23 04:59:51.869
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/24/23 04:59:51.893
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/24/23 04:59:51.911
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/24/23 04:59:51.926
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/24/23 04:59:51.937
  Apr 24 04:59:51.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5460" for this suite. @ 04/24/23 04:59:52.061
  STEP: Destroying namespace "webhook-markers-9294" for this suite. @ 04/24/23 04:59:52.073
• [6.295 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/24/23 04:59:52.084
  Apr 24 04:59:52.084: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename namespaces @ 04/24/23 04:59:52.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:59:52.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 04:59:52.12
  STEP: Creating a test namespace @ 04/24/23 04:59:52.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 04:59:52.147
  STEP: Creating a pod in the namespace @ 04/24/23 04:59:52.15
  STEP: Waiting for the pod to have running status @ 04/24/23 04:59:52.16
  E0424 04:59:52.837045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:53.837378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 04/24/23 04:59:54.172
  STEP: Waiting for the namespace to be removed. @ 04/24/23 04:59:54.186
  E0424 04:59:54.840770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:55.838840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:56.839993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:57.840354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:58.841152      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 04:59:59.841199      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:00.842038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:01.842429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:02.842464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:03.842917      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:04.843307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/24/23 05:00:05.195
  STEP: Verifying there are no pods in the namespace @ 04/24/23 05:00:05.224
  Apr 24 05:00:05.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-716" for this suite. @ 04/24/23 05:00:05.239
  STEP: Destroying namespace "nsdeletetest-6139" for this suite. @ 04/24/23 05:00:05.254
  Apr 24 05:00:05.259: INFO: Namespace nsdeletetest-6139 was already deleted
  STEP: Destroying namespace "nsdeletetest-5700" for this suite. @ 04/24/23 05:00:05.259
• [13.188 seconds]
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/24/23 05:00:05.272
  Apr 24 05:00:05.272: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 05:00:05.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:00:05.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:00:05.309
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 05:00:05.314
  E0424 05:00:05.843883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:06.845269      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:07.845013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:08.845586      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:00:09.358
  Apr 24 05:00:09.364: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-916b27f6-423a-4d8a-a4b4-5e1d444431d3 container client-container: <nil>
  STEP: delete the pod @ 04/24/23 05:00:09.375
  Apr 24 05:00:09.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2067" for this suite. @ 04/24/23 05:00:09.412
• [4.151 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/24/23 05:00:09.426
  Apr 24 05:00:09.426: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 05:00:09.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:00:09.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:00:09.47
  STEP: Creating configMap with name projected-configmap-test-volume-map-e4ed9a29-b563-4c56-97d0-52175ee9f0a1 @ 04/24/23 05:00:09.474
  STEP: Creating a pod to test consume configMaps @ 04/24/23 05:00:09.479
  E0424 05:00:09.846027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:10.846876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:11.847512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:12.848044      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:00:13.507
  Apr 24 05:00:13.511: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-configmaps-491fddea-ba25-4faa-b4ec-b8965b7d7f17 container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 05:00:13.524
  Apr 24 05:00:13.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5945" for this suite. @ 04/24/23 05:00:13.565
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/24/23 05:00:13.614
  Apr 24 05:00:13.615: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 05:00:13.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:00:13.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:00:13.652
  STEP: Creating pod liveness-207ccfe4-88d7-4580-b1b2-db342c243400 in namespace container-probe-7300 @ 04/24/23 05:00:13.656
  E0424 05:00:13.849006      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:14.849633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:00:15.692: INFO: Started pod liveness-207ccfe4-88d7-4580-b1b2-db342c243400 in namespace container-probe-7300
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/24/23 05:00:15.692
  Apr 24 05:00:15.697: INFO: Initial restart count of pod liveness-207ccfe4-88d7-4580-b1b2-db342c243400 is 0
  E0424 05:00:15.850755      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:16.851727      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:17.852631      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:18.853462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:19.853575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:20.854118      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:21.855620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:22.855743      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:23.856701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:24.857010      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:25.857945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:26.858534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:27.859409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:28.859558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:29.860048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:30.860636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:31.860515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:32.862908      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:33.864007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:34.864010      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:00:35.778: INFO: Restart count of pod container-probe-7300/liveness-207ccfe4-88d7-4580-b1b2-db342c243400 is now 1 (20.080769981s elapsed)
  E0424 05:00:35.864781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:36.865804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:37.866175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:38.866505      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:39.867259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:40.867426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:41.868044      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:42.868882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:43.869527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:44.869672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:45.870555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:46.871745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:47.872009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:48.872517      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:49.872822      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:50.873077      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:51.873571      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:52.874388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:53.874514      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:54.875219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:00:55.859: INFO: Restart count of pod container-probe-7300/liveness-207ccfe4-88d7-4580-b1b2-db342c243400 is now 2 (40.162176175s elapsed)
  E0424 05:00:55.875787      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:56.876574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:57.877637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:58.877806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:00:59.878748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:00.879022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:01.880051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:02.880592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:03.880666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:04.880907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:05.881569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:06.882069      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:07.882635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:08.882800      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:09.883958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:10.884466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:11.884910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:12.885301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:13.886373      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:14.887634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:15.887744      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:01:15.954: INFO: Restart count of pod container-probe-7300/liveness-207ccfe4-88d7-4580-b1b2-db342c243400 is now 3 (1m0.256429993s elapsed)
  E0424 05:01:16.888728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:17.890309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:18.909494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:19.898232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:20.898340      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:21.898686      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:22.899632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:23.900339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:24.901226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:25.901680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:26.901910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:27.902282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:28.902527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:29.902876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:30.903246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:31.903553      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:32.904353      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:33.905167      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:34.905352      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:35.906187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:01:36.052: INFO: Restart count of pod container-probe-7300/liveness-207ccfe4-88d7-4580-b1b2-db342c243400 is now 4 (1m20.35518999s elapsed)
  E0424 05:01:36.907393      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:37.907548      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:38.908291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:39.908308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:40.908956      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:41.909411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:42.909938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:43.910164      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:44.910377      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:45.911160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:46.912251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:47.912995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:48.912858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:49.913689      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:50.914462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:51.915459      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:52.915429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:53.915847      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:54.916141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:55.916724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:56.917915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:57.918462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:58.919130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:01:59.919445      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:00.920602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:01.921794      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:02.923054      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:03.923425      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:04.924263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:05.924575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:06.924710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:07.925705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:08.925931      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:09.926405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:10.927393      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:11.927899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:12.929031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:13.929163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:14.929338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:15.929653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:16.930650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:17.930938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:18.931485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:19.931681      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:20.931861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:21.932739      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:22.932848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:23.933102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:24.933281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:25.934025      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:26.934857      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:27.935251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:28.935788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:29.936785      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:30.937424      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:31.939172      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:32.939222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:33.940223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:34.940639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:35.940670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:36.941204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:37.941251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:38.942090      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:39.942636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:40.943179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:41.943644      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:42.944573      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:43.945823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:44.945905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:45.946857      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:46.948238      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:47.947680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:48.948261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:49.948876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:02:50.338: INFO: Restart count of pod container-probe-7300/liveness-207ccfe4-88d7-4580-b1b2-db342c243400 is now 5 (2m34.641054124s elapsed)
  Apr 24 05:02:50.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 05:02:50.347
  STEP: Destroying namespace "container-probe-7300" for this suite. @ 04/24/23 05:02:50.381
• [156.784 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/24/23 05:02:50.401
  Apr 24 05:02:50.401: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename field-validation @ 04/24/23 05:02:50.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:02:50.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:02:50.435
  Apr 24 05:02:50.440: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 05:02:50.949165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:51.950579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:52.950709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0424 05:02:53.205435      14 warnings.go:70] unknown field "alpha"
  W0424 05:02:53.205490      14 warnings.go:70] unknown field "beta"
  W0424 05:02:53.205508      14 warnings.go:70] unknown field "delta"
  W0424 05:02:53.205515      14 warnings.go:70] unknown field "epsilon"
  W0424 05:02:53.205573      14 warnings.go:70] unknown field "gamma"
  Apr 24 05:02:53.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4153" for this suite. @ 04/24/23 05:02:53.261
• [2.871 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/24/23 05:02:53.276
  Apr 24 05:02:53.276: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 05:02:53.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:02:53.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:02:53.312
  Apr 24 05:02:53.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7969" for this suite. @ 04/24/23 05:02:53.383
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/24/23 05:02:53.402
  Apr 24 05:02:53.402: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 05:02:53.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:02:53.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:02:53.43
  STEP: Creating a pod to test downward API volume plugin @ 04/24/23 05:02:53.436
  E0424 05:02:53.950436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:54.950924      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:55.950858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:56.951820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:02:57.487
  Apr 24 05:02:57.493: INFO: Trying to get logs from node aeveeng9ieph-3 pod downwardapi-volume-51c7fa13-8a83-454d-bf97-2510f4eef9db container client-container: <nil>
  STEP: delete the pod @ 04/24/23 05:02:57.519
  Apr 24 05:02:57.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5281" for this suite. @ 04/24/23 05:02:57.544
• [4.149 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/24/23 05:02:57.553
  Apr 24 05:02:57.553: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename subpath @ 04/24/23 05:02:57.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:02:57.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:02:57.587
  STEP: Setting up data @ 04/24/23 05:02:57.591
  STEP: Creating pod pod-subpath-test-configmap-ksv2 @ 04/24/23 05:02:57.606
  STEP: Creating a pod to test atomic-volume-subpath @ 04/24/23 05:02:57.606
  E0424 05:02:57.952840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:58.953569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:02:59.954237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:00.954110      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:01.954267      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:02.955066      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:03.955054      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:04.955220      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:05.956198      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:06.957367      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:07.957373      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:08.957608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:09.958439      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:10.959255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:11.959795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:12.959947      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:13.960163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:14.960241      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:15.960313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:16.960958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:17.961078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:18.962011      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:19.962207      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:20.962426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:03:21.722
  Apr 24 05:03:21.729: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-subpath-test-configmap-ksv2 container test-container-subpath-configmap-ksv2: <nil>
  STEP: delete the pod @ 04/24/23 05:03:21.744
  STEP: Deleting pod pod-subpath-test-configmap-ksv2 @ 04/24/23 05:03:21.763
  Apr 24 05:03:21.763: INFO: Deleting pod "pod-subpath-test-configmap-ksv2" in namespace "subpath-6793"
  Apr 24 05:03:21.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6793" for this suite. @ 04/24/23 05:03:21.777
• [24.236 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/24/23 05:03:21.792
  Apr 24 05:03:21.792: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename dns @ 04/24/23 05:03:21.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:03:21.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:03:21.841
  STEP: Creating a test headless service @ 04/24/23 05:03:21.846
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7769.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7769.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/24/23 05:03:21.857
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7769.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7769.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/24/23 05:03:21.857
  STEP: creating a pod to probe DNS @ 04/24/23 05:03:21.857
  STEP: submitting the pod to kubernetes @ 04/24/23 05:03:21.857
  E0424 05:03:21.963384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:22.963914      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:23.964398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:24.964877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/24/23 05:03:25.904
  STEP: looking for the results for each expected name from probers @ 04/24/23 05:03:25.911
  Apr 24 05:03:25.945: INFO: DNS probes using dns-7769/dns-test-3562a86b-290c-421a-aedb-1afa3eef30a1 succeeded

  Apr 24 05:03:25.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 05:03:25.953
  E0424 05:03:25.965464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the test headless service @ 04/24/23 05:03:25.973
  STEP: Destroying namespace "dns-7769" for this suite. @ 04/24/23 05:03:26.004
• [4.240 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/24/23 05:03:26.033
  Apr 24 05:03:26.033: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 05:03:26.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:03:26.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:03:26.079
  STEP: Creating projection with secret that has name projected-secret-test-df27d83e-83b6-4303-856d-69dbc3b52a54 @ 04/24/23 05:03:26.083
  STEP: Creating a pod to test consume secrets @ 04/24/23 05:03:26.091
  E0424 05:03:26.966508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:27.967273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:28.968605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:29.968959      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:03:30.124
  Apr 24 05:03:30.129: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-secrets-f5250edd-517f-4b65-a463-de95378c99d7 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 05:03:30.143
  Apr 24 05:03:30.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3768" for this suite. @ 04/24/23 05:03:30.188
• [4.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/24/23 05:03:30.206
  Apr 24 05:03:30.206: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 05:03:30.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:03:30.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:03:30.262
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-950737f1-8dea-40c7-b842-1685bf8ed70c @ 04/24/23 05:03:30.273
  STEP: Creating the pod @ 04/24/23 05:03:30.282
  E0424 05:03:30.971247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:31.972322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:32.972564      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:33.973172      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-950737f1-8dea-40c7-b842-1685bf8ed70c @ 04/24/23 05:03:34.365
  STEP: waiting to observe update in volume @ 04/24/23 05:03:34.38
  E0424 05:03:34.973587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:35.973543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:36.973899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:37.974714      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:38.974885      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:39.975392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:40.976123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:41.976970      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:42.977483      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:43.978241      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:44.979045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:45.979619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:46.980355      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:47.980492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:48.980672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:49.981190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:50.981315      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:51.983815      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:52.981985      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:53.982050      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:54.982882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:55.983284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:56.983497      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:57.984076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:58.984255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:03:59.984877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:00.985826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:01.986754      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:02.987273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:03.988164      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:04.988937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:05.989950      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:06.990618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:07.992918      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:08.992121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:09.992940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:10.993028      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:11.995605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:12.995669      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:14.007990      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:15.001239      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:15.999997      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:17.002258      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:18.002546      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:19.003000      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:20.003264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:21.004052      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:22.004421      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:23.004485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:24.007208      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:25.007547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:26.007826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:27.008872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:28.008983      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:29.009206      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:30.009347      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:31.009667      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:32.011379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:33.012051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:34.012648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:35.013269      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:36.013484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:37.014512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:38.014176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:39.014392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:40.015361      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:41.016742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:42.017852      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:43.017970      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:44.018141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:45.018536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:04:45.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7458" for this suite. @ 04/24/23 05:04:45.548
• [75.353 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/24/23 05:04:45.561
  Apr 24 05:04:45.561: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename deployment @ 04/24/23 05:04:45.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:04:45.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:04:45.601
  Apr 24 05:04:45.621: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0424 05:04:46.018821      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:47.019028      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:48.019477      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:49.019576      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:50.019827      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:04:50.628: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/24/23 05:04:50.629
  Apr 24 05:04:50.629: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/24/23 05:04:50.646
  Apr 24 05:04:50.666: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4587  44470f63-d4c6-4258-8bcf-8b0b0762a945 37894 1 2023-04-24 05:04:50 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-24 05:04:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030445e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 24 05:04:50.671: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Apr 24 05:04:50.671: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Apr 24 05:04:50.671: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4587  c00b2488-9231-44f2-9a08-9b9db973d33d 37897 1 2023-04-24 05:04:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 44470f63-d4c6-4258-8bcf-8b0b0762a945 0xc0044464a7 0xc0044464a8}] [] [{e2e.test Update apps/v1 2023-04-24 05:04:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 05:04:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-24 05:04:50 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"44470f63-d4c6-4258-8bcf-8b0b0762a945\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004446568 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 05:04:50.701: INFO: Pod "test-cleanup-controller-6gd42" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-6gd42 test-cleanup-controller- deployment-4587  00e90ddd-8ff9-4bae-86f4-c604ccc70802 37887 0 2023-04-24 05:04:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller c00b2488-9231-44f2-9a08-9b9db973d33d 0xc003044957 0xc003044958}] [] [{kube-controller-manager Update v1 2023-04-24 05:04:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c00b2488-9231-44f2-9a08-9b9db973d33d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:04:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9szx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9szx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:04:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:04:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:10.233.66.25,StartTime:2023-04-24 05:04:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:04:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2a96d103183ce497d21a415ca9270ede3f74aa53aba50340943996b7457782b6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.25,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:04:50.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4587" for this suite. @ 04/24/23 05:04:50.709
• [5.161 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/24/23 05:04:50.724
  Apr 24 05:04:50.724: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replication-controller @ 04/24/23 05:04:50.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:04:50.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:04:50.767
  STEP: Creating ReplicationController "e2e-rc-8jj4n" @ 04/24/23 05:04:50.771
  Apr 24 05:04:50.779: INFO: Get Replication Controller "e2e-rc-8jj4n" to confirm replicas
  E0424 05:04:51.020474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:04:51.784: INFO: Get Replication Controller "e2e-rc-8jj4n" to confirm replicas
  Apr 24 05:04:51.790: INFO: Found 1 replicas for "e2e-rc-8jj4n" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-8jj4n" @ 04/24/23 05:04:51.79
  STEP: Updating a scale subresource @ 04/24/23 05:04:51.804
  STEP: Verifying replicas where modified for replication controller "e2e-rc-8jj4n" @ 04/24/23 05:04:51.813
  Apr 24 05:04:51.813: INFO: Get Replication Controller "e2e-rc-8jj4n" to confirm replicas
  E0424 05:04:52.021273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:04:52.819: INFO: Get Replication Controller "e2e-rc-8jj4n" to confirm replicas
  Apr 24 05:04:52.827: INFO: Found 2 replicas for "e2e-rc-8jj4n" replication controller
  Apr 24 05:04:52.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3885" for this suite. @ 04/24/23 05:04:52.835
• [2.121 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/24/23 05:04:52.845
  Apr 24 05:04:52.845: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename subpath @ 04/24/23 05:04:52.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:04:52.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:04:52.877
  STEP: Setting up data @ 04/24/23 05:04:52.882
  STEP: Creating pod pod-subpath-test-secret-d254 @ 04/24/23 05:04:52.896
  STEP: Creating a pod to test atomic-volume-subpath @ 04/24/23 05:04:52.896
  E0424 05:04:53.022429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:54.022564      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:55.022948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:56.023216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:57.023538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:58.023782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:04:59.024127      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:00.024395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:01.025215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:02.025859      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:03.026369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:04.027035      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:05.028081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:06.029128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:07.029724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:08.030363      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:09.031001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:10.031745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:11.032306      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:12.032501      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:13.033176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:14.033402      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:15.033700      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:16.034160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:05:17.012
  Apr 24 05:05:17.016: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-subpath-test-secret-d254 container test-container-subpath-secret-d254: <nil>
  E0424 05:05:17.034205      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/24/23 05:05:17.034
  STEP: Deleting pod pod-subpath-test-secret-d254 @ 04/24/23 05:05:17.056
  Apr 24 05:05:17.057: INFO: Deleting pod "pod-subpath-test-secret-d254" in namespace "subpath-7217"
  Apr 24 05:05:17.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7217" for this suite. @ 04/24/23 05:05:17.072
• [24.236 seconds]
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/24/23 05:05:17.081
  Apr 24 05:05:17.081: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename prestop @ 04/24/23 05:05:17.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:05:17.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:05:17.116
  STEP: Creating server pod server in namespace prestop-4765 @ 04/24/23 05:05:17.121
  STEP: Waiting for pods to come up. @ 04/24/23 05:05:17.13
  E0424 05:05:18.039338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:19.035771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:20.035798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:21.037055      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-4765 @ 04/24/23 05:05:21.156
  E0424 05:05:22.036814      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:23.037227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:24.038348      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:25.038283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 04/24/23 05:05:25.19
  E0424 05:05:26.038875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:27.041955      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:28.041572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:29.040543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:30.040679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:05:30.225: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 24 05:05:30.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/24/23 05:05:30.235
  STEP: Destroying namespace "prestop-4765" for this suite. @ 04/24/23 05:05:30.269
• [13.208 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/24/23 05:05:30.29
  Apr 24 05:05:30.290: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename deployment @ 04/24/23 05:05:30.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:05:30.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:05:30.334
  Apr 24 05:05:30.339: INFO: Creating deployment "webserver-deployment"
  Apr 24 05:05:30.353: INFO: Waiting for observed generation 1
  E0424 05:05:31.041489      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:32.068512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:05:32.514: INFO: Waiting for all required pods to come up
  Apr 24 05:05:32.722: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/24/23 05:05:32.722
  E0424 05:05:33.048165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:34.048289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:05:34.803: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 24 05:05:34.817: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Apr 24 05:05:34.845: INFO: Updating deployment webserver-deployment
  Apr 24 05:05:34.845: INFO: Waiting for observed generation 2
  E0424 05:05:35.048824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:36.048951      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:05:36.914: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 24 05:05:36.919: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 24 05:05:36.927: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 24 05:05:36.945: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 24 05:05:36.945: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 24 05:05:36.951: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 24 05:05:36.958: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 24 05:05:36.959: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 24 05:05:36.980: INFO: Updating deployment webserver-deployment
  Apr 24 05:05:36.980: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Apr 24 05:05:36.992: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Apr 24 05:05:36.995: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0424 05:05:37.049834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:38.050864      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:39.052161      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:05:39.054: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-8372  b9824e18-f6fc-474e-aed3-a42ffe6c548d 38442 3 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-24 05:05:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041308c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-24 05:05:37 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-04-24 05:05:37 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Apr 24 05:05:39.064: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-8372  4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 38438 3 2023-04-24 05:05:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b9824e18-f6fc-474e-aed3-a42ffe6c548d 0xc004130e47 0xc004130e48}] [] [{kube-controller-manager Update apps/v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9824e18-f6fc-474e-aed3-a42ffe6c548d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004130ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 05:05:39.064: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 24 05:05:39.065: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-8372  a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 38436 3 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b9824e18-f6fc-474e-aed3-a42ffe6c548d 0xc004130d27 0xc004130d28}] [] [{kube-controller-manager Update apps/v1 2023-04-24 05:05:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9824e18-f6fc-474e-aed3-a42ffe6c548d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004130db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 05:05:39.081: INFO: Pod "webserver-deployment-67bd4bf6dc-5tjfq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5tjfq webserver-deployment-67bd4bf6dc- deployment-8372  92dc5df6-ff90-4720-9e68-2eeb0ff30356 38405 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc004131e87 0xc004131e88}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42cs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42cs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.081: INFO: Pod "webserver-deployment-67bd4bf6dc-6b5jx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6b5jx webserver-deployment-67bd4bf6dc- deployment-8372  16a05e2b-9613-437b-963e-04afa6140dc0 38449 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277e057 0xc00277e058}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8xmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8xmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.082: INFO: Pod "webserver-deployment-67bd4bf6dc-8jd98" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8jd98 webserver-deployment-67bd4bf6dc- deployment-8372  c062352c-0ed5-4e6f-8cdd-989b7690dac3 38396 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277e237 0xc00277e238}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqkr2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqkr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.085: INFO: Pod "webserver-deployment-67bd4bf6dc-9fp2p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9fp2p webserver-deployment-67bd4bf6dc- deployment-8372  01c06103-5834-4dfe-978f-df74989a89b2 38439 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277e437 0xc00277e438}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-znm8l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-znm8l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.086: INFO: Pod "webserver-deployment-67bd4bf6dc-dvqdh" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dvqdh webserver-deployment-67bd4bf6dc- deployment-8372  acb89ebe-3f87-45e0-9ecf-962226943da9 38276 0 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277e637 0xc00277e638}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tsdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tsdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:10.233.64.191,StartTime:2023-04-24 05:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:05:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://cf30713ad3fd18879db2ad8a90dbd1f2fa8acdf96f3b493192066a6526cf8857,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.191,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.087: INFO: Pod "webserver-deployment-67bd4bf6dc-hggv8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hggv8 webserver-deployment-67bd4bf6dc- deployment-8372  5d42fb01-83ce-48df-9324-89760ee76733 38222 0 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277e847 0xc00277e848}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b7wt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b7wt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:10.233.65.191,StartTime:2023-04-24 05:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:05:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://f81893a7f1af01c4546afc84d30a7cab95388a1279b5ffed0e7d3c9073335c05,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.191,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.088: INFO: Pod "webserver-deployment-67bd4bf6dc-jthvx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jthvx webserver-deployment-67bd4bf6dc- deployment-8372  60ad02e2-802b-4720-beb0-0e3c772c27a9 38260 0 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277ea87 0xc00277ea88}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2ghm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2ghm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:10.233.66.63,StartTime:2023-04-24 05:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:05:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b73cb9d153eb4617a217f4e928c6781f27bf6d3abc3d24a59c87379ffb04aa81,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.63,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.088: INFO: Pod "webserver-deployment-67bd4bf6dc-m92k7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m92k7 webserver-deployment-67bd4bf6dc- deployment-8372  90cb98b8-be8d-432b-be49-dca4a82ef657 38403 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277ec87 0xc00277ec88}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkzrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkzrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.089: INFO: Pod "webserver-deployment-67bd4bf6dc-mnbfm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mnbfm webserver-deployment-67bd4bf6dc- deployment-8372  354d3db5-43c7-41a8-8d78-14c8897f18ee 38479 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277f1d7 0xc00277f1d8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zx57n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zx57n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.090: INFO: Pod "webserver-deployment-67bd4bf6dc-r6znk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-r6znk webserver-deployment-67bd4bf6dc- deployment-8372  ee4aad62-40f0-4c0b-96a9-a4a992806c9c 38219 0 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277f3e7 0xc00277f3e8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7sfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7sfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:10.233.65.142,StartTime:2023-04-24 05:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:05:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0eff115963ae57abf9884071aabcc9471ea154a8ac30f24e88bc46e34ae8dfcb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.142,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.090: INFO: Pod "webserver-deployment-67bd4bf6dc-rblbn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rblbn webserver-deployment-67bd4bf6dc- deployment-8372  baa7d5d7-a272-41a9-8fc3-f8ec9a728730 38457 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277f617 0xc00277f618}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlbqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlbqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.094: INFO: Pod "webserver-deployment-67bd4bf6dc-rfdwc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rfdwc webserver-deployment-67bd4bf6dc- deployment-8372  de889a26-3e86-46ef-8602-3631c5cfa531 38461 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277f7e7 0xc00277f7e8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghjq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghjq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.096: INFO: Pod "webserver-deployment-67bd4bf6dc-s4d46" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s4d46 webserver-deployment-67bd4bf6dc- deployment-8372  3c787eec-c643-4ff6-852e-8766f1bced35 38273 0 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277f9d7 0xc00277f9d8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jhb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jhb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:10.233.64.17,StartTime:2023-04-24 05:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:05:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d2c7be9917f3b6b987329bc3fa9468c49d4dafaf018a7079568141a2c6628fcb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.17,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.102: INFO: Pod "webserver-deployment-67bd4bf6dc-s9z7s" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s9z7s webserver-deployment-67bd4bf6dc- deployment-8372  e26c2039-f15e-459e-9ea1-36a82bd49c9c 38435 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277fbe7 0xc00277fbe8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hmrj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hmrj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.103: INFO: Pod "webserver-deployment-67bd4bf6dc-slbtx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-slbtx webserver-deployment-67bd4bf6dc- deployment-8372  c94263f0-1cba-4c80-b82b-085ad0206907 38263 0 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00277fdd7 0xc00277fdd8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gk79m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gk79m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:10.233.66.159,StartTime:2023-04-24 05:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:05:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://f19c2fc4e0bfabddaf88ed9f838437f17ec43f64345b515de62c4b172c37c628,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.159,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.104: INFO: Pod "webserver-deployment-67bd4bf6dc-v9p77" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-v9p77 webserver-deployment-67bd4bf6dc- deployment-8372  e0f62f16-6914-45c7-9716-618ea982b180 38244 0 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00433c097 0xc00433c098}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcfdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcfdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:10.233.64.172,StartTime:2023-04-24 05:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:05:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8995693be99b0b2ae07b13851df05169068e73653613f8255724ed01c43e7ea2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.172,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.105: INFO: Pod "webserver-deployment-67bd4bf6dc-vbjgh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vbjgh webserver-deployment-67bd4bf6dc- deployment-8372  21c38641-3b60-43a5-941e-4c00dff3bc95 38428 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00433c297 0xc00433c298}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x5qvt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x5qvt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.106: INFO: Pod "webserver-deployment-67bd4bf6dc-vdcrt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vdcrt webserver-deployment-67bd4bf6dc- deployment-8372  7031ba38-5b16-4c0e-93b6-7c72a8bd5d49 38466 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00433c477 0xc00433c478}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xxw6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xxw6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.108: INFO: Pod "webserver-deployment-67bd4bf6dc-vmzcw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vmzcw webserver-deployment-67bd4bf6dc- deployment-8372  d40c963f-a7a1-44b4-a7b0-6d8180a9efaf 38441 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00433c647 0xc00433c648}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdfkl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdfkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.109: INFO: Pod "webserver-deployment-67bd4bf6dc-x46xs" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x46xs webserver-deployment-67bd4bf6dc- deployment-8372  60c866bc-7e2b-41d6-a13c-f63d3a1a492f 38240 0 2023-04-24 05:05:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a82309a2-22f9-49b7-bbf0-4ff8fb2f5208 0xc00433c827 0xc00433c828}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a82309a2-22f9-49b7-bbf0-4ff8fb2f5208\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58fpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58fpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:10.233.65.113,StartTime:2023-04-24 05:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:05:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d49008a651ebf1c82895680781f53799dbec36dd27fbe2c853f1bb8effba7ad5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.113,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.110: INFO: Pod "webserver-deployment-7b75d79cf5-2wjkf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2wjkf webserver-deployment-7b75d79cf5- deployment-8372  968cdfe4-66cb-42d6-9da0-573704456c5f 38434 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc00433ca17 0xc00433ca18}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6cdm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6cdm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.110: INFO: Pod "webserver-deployment-7b75d79cf5-9h8tb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9h8tb webserver-deployment-7b75d79cf5- deployment-8372  5ae08c6b-9462-46f6-825d-445baa53204f 38417 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc00433cc17 0xc00433cc18}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9jt5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9jt5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.115: INFO: Pod "webserver-deployment-7b75d79cf5-d8l8l" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-d8l8l webserver-deployment-7b75d79cf5- deployment-8372  1e9506f0-a121-459e-927f-52cdf73ecc9b 38399 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc00433ce07 0xc00433ce08}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2dtms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2dtms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.115: INFO: Pod "webserver-deployment-7b75d79cf5-dnn57" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dnn57 webserver-deployment-7b75d79cf5- deployment-8372  f5f8a9f3-88e9-4d67-8e99-66fe130fbe2c 38330 0 2023-04-24 05:05:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc00433cff7 0xc00433cff8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pk9dl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pk9dl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:,StartTime:2023-04-24 05:05:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.116: INFO: Pod "webserver-deployment-7b75d79cf5-j5qpn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-j5qpn webserver-deployment-7b75d79cf5- deployment-8372  90587f59-c6be-49fb-aca0-2aa33747d5b7 38444 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc00433d1e7 0xc00433d1e8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6v68q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6v68q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.117: INFO: Pod "webserver-deployment-7b75d79cf5-jz69v" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jz69v webserver-deployment-7b75d79cf5- deployment-8372  9f73f588-5703-47a6-80b9-8711dbbff224 38380 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc00433d3d7 0xc00433d3d8}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jtfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jtfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.117: INFO: Pod "webserver-deployment-7b75d79cf5-qr5nh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qr5nh webserver-deployment-7b75d79cf5- deployment-8372  a08660b7-3047-47e4-b1b4-9953723be4c5 38475 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc00433d957 0xc00433d958}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcm2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcm2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.118: INFO: Pod "webserver-deployment-7b75d79cf5-r2tv4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-r2tv4 webserver-deployment-7b75d79cf5- deployment-8372  8e947356-9740-47ee-a088-b7aed872b4a2 38331 0 2023-04-24 05:05:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc002514127 0xc002514128}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbm8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbm8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.118: INFO: Pod "webserver-deployment-7b75d79cf5-sp5dw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-sp5dw webserver-deployment-7b75d79cf5- deployment-8372  a608719f-88ee-4d43-9fc0-2dbea128dfbe 38453 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc002514347 0xc002514348}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvcvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvcvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.128: INFO: Pod "webserver-deployment-7b75d79cf5-vqkgm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vqkgm webserver-deployment-7b75d79cf5- deployment-8372  46c8f631-8d4d-4173-a163-a14719f9bde7 38305 0 2023-04-24 05:05:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc002514547 0xc002514548}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gb4mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gb4mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2023-04-24 05:05:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.129: INFO: Pod "webserver-deployment-7b75d79cf5-xnxkt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xnxkt webserver-deployment-7b75d79cf5- deployment-8372  1b5dcbe4-7372-41f2-bdf9-b0b2d5863615 38422 0 2023-04-24 05:05:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc002514767 0xc002514768}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6pcm9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6pcm9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:,StartTime:2023-04-24 05:05:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.130: INFO: Pod "webserver-deployment-7b75d79cf5-zf6x5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zf6x5 webserver-deployment-7b75d79cf5- deployment-8372  29fb21c9-6b40-4fbe-ac91-99c38ea75452 38302 0 2023-04-24 05:05:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc002514967 0xc002514968}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dswgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dswgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.252,PodIP:,StartTime:2023-04-24 05:05:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.130: INFO: Pod "webserver-deployment-7b75d79cf5-zlhdp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zlhdp webserver-deployment-7b75d79cf5- deployment-8372  a972d83b-0298-4d21-b08c-02340c8cae83 38333 0 2023-04-24 05:05:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 4f1d37dc-20b3-4cc1-a3c1-557fe1167a19 0xc002514b57 0xc002514b58}] [] [{kube-controller-manager Update v1 2023-04-24 05:05:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1d37dc-20b3-4cc1-a3c1-557fe1167a19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:05:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwrc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwrc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:05:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:,StartTime:2023-04-24 05:05:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:05:39.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8372" for this suite. @ 04/24/23 05:05:39.155
• [8.919 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/24/23 05:05:39.212
  Apr 24 05:05:39.212: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename cronjob @ 04/24/23 05:05:39.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:05:39.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:05:39.32
  STEP: Creating a suspended cronjob @ 04/24/23 05:05:39.325
  STEP: Ensuring no jobs are scheduled @ 04/24/23 05:05:39.356
  E0424 05:05:40.095688      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:41.212951      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:42.143442      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:43.143639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:44.143840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:45.144320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:46.144572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:47.145275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:48.145376      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:49.146237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:50.146691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:51.147824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:52.150405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:53.150455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:54.150809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:55.151248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:56.151661      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:57.152331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:58.153159      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:05:59.153941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:00.154124      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:01.155040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:02.155485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:03.156307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:04.157383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:05.157920      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:06.158147      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:07.158939      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:08.159117      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:09.159249      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:10.159440      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:11.159557      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:12.160435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:13.160622      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:14.160931      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:15.161038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:16.161333      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:17.162427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:18.162816      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:19.163317      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:20.163749      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:21.166239      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:22.165985      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:23.166940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:24.167118      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:25.169973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:26.168382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:27.171066      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:28.170435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:29.170682      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:30.171607      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:31.171749      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:32.171772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:33.172180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:34.173336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:35.173765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:36.174102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:37.175371      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:38.175553      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:39.176314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:40.176940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:41.177365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:42.178320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:43.178811      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:44.179280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:45.179448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:46.180569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:47.181256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:48.181479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:49.182056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:50.182673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:51.183292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:52.184272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:53.184454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:54.184607      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:55.184837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:56.185846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:57.187159      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:58.188081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:06:59.188190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:00.188398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:01.188930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:02.189488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:03.189648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:04.190568      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:05.190680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:06.191273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:07.191985      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:08.192142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:09.192345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:10.192555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:11.192680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:12.192724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:13.192850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:14.193915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:15.194039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:16.194283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:17.194943      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:18.195291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:19.196311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:20.196908      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:21.197116      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:22.197894      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:23.197975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:24.198142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:25.199307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:26.199322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:27.199712      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:28.199905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:29.200095      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:30.200296      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:31.200494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:32.200636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:33.200841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:34.200894      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:35.201948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:36.202444      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:37.202606      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:38.202804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:39.203288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:40.203323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:41.203463      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:42.203537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:43.203701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:44.204639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:45.204925      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:46.205439      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:47.205922      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:48.205878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:49.205963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:50.206496      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:51.206677      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:52.208232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:53.209236      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:54.208480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:55.208378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:56.208571      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:57.208724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:58.209395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:07:59.209647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:00.210035      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:01.210070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:02.210201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:03.210702      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:04.211427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:05.211778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:06.212144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:07.213011      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:08.213382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:09.214246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:10.215022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:11.214665      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:12.215112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:13.215277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:14.215555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:15.215850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:16.216013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:17.217150      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:18.217830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:19.218893      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:20.219418      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:21.219928      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:22.220860      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:23.221321      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:24.221912      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:25.221823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:26.222934      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:27.223102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:28.223316      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:29.223457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:30.223679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:31.224630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:32.225605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:33.225733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:34.226156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:35.227547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:36.227035      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:37.227971      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:38.228152      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:39.228545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:40.228872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:41.228952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:42.229183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:43.229294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:44.229502      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:45.229956      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:46.230076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:47.230493      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:48.230868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:49.231207      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:50.231445      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:51.231450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:52.231650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:53.231579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:54.231802      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:55.231975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:56.232139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:57.232392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:58.233197      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:08:59.233669      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:00.234432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:01.235331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:02.235212      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:03.235745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:04.235721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:05.235937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:06.236099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:07.237695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:08.238253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:09.238574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:10.239100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:11.239338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:12.240365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:13.240586      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:14.240717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:15.240903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:16.241448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:17.242314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:18.242861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:19.243346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:20.243858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:21.243969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:22.244802      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:23.244932      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:24.245141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:25.245291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:26.246335      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:27.246915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:28.247228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:29.247177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:30.247474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:31.248324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:32.248648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:33.248976      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:34.249821      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:35.249998      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:36.250396      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:37.251216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:38.252043      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:39.252941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:40.252877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:41.252994      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:42.253822      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:43.253945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:44.254068      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:45.254634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:46.254818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:47.254932      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:48.255106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:49.255223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:50.255396      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:51.255816      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:52.256797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:53.257079      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:54.257179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:55.257242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:56.257372      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:57.258091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:58.258569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:09:59.259122      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:00.259275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:01.259404      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:02.259699      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:03.260029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:04.260266      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:05.260413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:06.260510      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:07.260806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:08.261660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:09.261866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:10.262040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:11.263266      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:12.263938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:13.264105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:14.264198      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:15.264672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:16.264815      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:17.264921      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:18.265060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:19.265191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:20.265382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:21.265968      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:22.266801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:23.267209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:24.267531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:25.267723      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:26.267865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:27.268304      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:28.268464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:29.268603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:30.269311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:31.269545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:32.270322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:33.270758      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:34.270918      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:35.271192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:36.271349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:37.272423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:38.272628      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:39.273147      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/24/23 05:10:39.373
  STEP: Removing cronjob @ 04/24/23 05:10:39.379
  Apr 24 05:10:39.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9184" for this suite. @ 04/24/23 05:10:39.402
• [300.199 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/24/23 05:10:39.416
  Apr 24 05:10:39.417: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename var-expansion @ 04/24/23 05:10:39.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:10:39.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:10:39.454
  STEP: Creating a pod to test substitution in container's command @ 04/24/23 05:10:39.458
  E0424 05:10:40.274398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:41.275336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:42.275598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:43.276330      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:10:43.494
  Apr 24 05:10:43.498: INFO: Trying to get logs from node aeveeng9ieph-3 pod var-expansion-8e9a8181-ceab-46db-a83e-9a7d683882f9 container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 05:10:43.529
  Apr 24 05:10:43.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8827" for this suite. @ 04/24/23 05:10:43.567
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/24/23 05:10:43.581
  Apr 24 05:10:43.581: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubelet-test @ 04/24/23 05:10:43.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:10:43.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:10:43.617
  E0424 05:10:44.276717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:45.286661      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:10:45.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2559" for this suite. @ 04/24/23 05:10:45.677
• [2.108 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/24/23 05:10:45.689
  Apr 24 05:10:45.689: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename hostport @ 04/24/23 05:10:45.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:10:45.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:10:45.738
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/24/23 05:10:45.753
  E0424 05:10:46.280056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:47.284952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:48.283842      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:49.283392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.43 on the node which pod1 resides and expect scheduled @ 04/24/23 05:10:49.818
  E0424 05:10:50.284004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:51.284077      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:52.284992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:53.285153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.43 but use UDP protocol on the node which pod2 resides @ 04/24/23 05:10:53.848
  E0424 05:10:54.285746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:55.286019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:56.286930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:57.287140      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:58.288112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:10:59.288015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/24/23 05:10:59.94
  Apr 24 05:10:59.941: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.43 http://127.0.0.1:54323/hostname] Namespace:hostport-2904 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 05:10:59.941: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 05:10:59.945: INFO: ExecWithOptions: Clientset creation
  Apr 24 05:10:59.945: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2904/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.43+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.43, port: 54323 @ 04/24/23 05:11:00.147
  Apr 24 05:11:00.147: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.43:54323/hostname] Namespace:hostport-2904 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 05:11:00.147: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 05:11:00.148: INFO: ExecWithOptions: Clientset creation
  Apr 24 05:11:00.148: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2904/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.43%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0424 05:11:00.288645      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.43, port: 54323 UDP @ 04/24/23 05:11:00.32
  Apr 24 05:11:00.320: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.43 54323] Namespace:hostport-2904 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 05:11:00.321: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 05:11:00.323: INFO: ExecWithOptions: Clientset creation
  Apr 24 05:11:00.323: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-2904/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.43+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0424 05:11:01.288881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:02.289743      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:03.290066      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:04.290450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:05.290588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:11:05.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-2904" for this suite. @ 04/24/23 05:11:05.498
• [19.818 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/24/23 05:11:05.511
  Apr 24 05:11:05.511: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename cronjob @ 04/24/23 05:11:05.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:11:05.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:11:05.546
  STEP: Creating a cronjob @ 04/24/23 05:11:05.551
  STEP: creating @ 04/24/23 05:11:05.551
  STEP: getting @ 04/24/23 05:11:05.559
  STEP: listing @ 04/24/23 05:11:05.565
  STEP: watching @ 04/24/23 05:11:05.571
  Apr 24 05:11:05.571: INFO: starting watch
  STEP: cluster-wide listing @ 04/24/23 05:11:05.572
  STEP: cluster-wide watching @ 04/24/23 05:11:05.577
  Apr 24 05:11:05.577: INFO: starting watch
  STEP: patching @ 04/24/23 05:11:05.578
  STEP: updating @ 04/24/23 05:11:05.588
  Apr 24 05:11:05.611: INFO: waiting for watch events with expected annotations
  Apr 24 05:11:05.612: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/24/23 05:11:05.612
  STEP: updating /status @ 04/24/23 05:11:05.622
  STEP: get /status @ 04/24/23 05:11:05.64
  STEP: deleting @ 04/24/23 05:11:05.648
  STEP: deleting a collection @ 04/24/23 05:11:05.67
  Apr 24 05:11:05.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-169" for this suite. @ 04/24/23 05:11:05.693
• [0.191 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/24/23 05:11:05.702
  Apr 24 05:11:05.702: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 05:11:05.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:11:05.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:11:05.739
  STEP: Creating the pod @ 04/24/23 05:11:05.743
  E0424 05:11:06.301331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:07.296272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:08.296575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:11:08.324: INFO: Successfully updated pod "annotationupdate8ebe21e6-1049-4f8b-971e-27a10b86cf59"
  E0424 05:11:09.296716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:10.297453      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:11.297717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:12.298568      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:11:12.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-195" for this suite. @ 04/24/23 05:11:12.497
• [6.822 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 04/24/23 05:11:12.526
  Apr 24 05:11:12.526: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename field-validation @ 04/24/23 05:11:12.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:11:12.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:11:12.576
  Apr 24 05:11:12.581: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 05:11:13.299267      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:14.299084      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:15.298978      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:11:15.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9828" for this suite. @ 04/24/23 05:11:15.362
• [2.858 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/24/23 05:11:15.384
  Apr 24 05:11:15.384: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 05:11:15.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:11:15.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:11:15.43
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/24/23 05:11:15.435
  Apr 24 05:11:15.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-2759 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 24 05:11:15.633: INFO: stderr: ""
  Apr 24 05:11:15.633: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/24/23 05:11:15.633
  E0424 05:11:16.299934      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:17.300703      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:18.301087      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:19.301240      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:20.301503      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/24/23 05:11:20.685
  Apr 24 05:11:20.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-2759 get pod e2e-test-httpd-pod -o json'
  Apr 24 05:11:20.845: INFO: stderr: ""
  Apr 24 05:11:20.845: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-04-24T05:11:15Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2759\",\n        \"resourceVersion\": \"39832\",\n        \"uid\": \"19d7cd28-9e81-4b92-ba5e-f332ab397846\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qqrp7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"aeveeng9ieph-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qqrp7\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-24T05:11:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-24T05:11:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-24T05:11:16Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-24T05:11:15Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://136f09ceee32db01d644623b2d0e2d766a8c0596832671f8576f96c49cd996c5\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-24T05:11:16Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.18\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.69\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.69\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-24T05:11:15Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/24/23 05:11:20.845
  Apr 24 05:11:20.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-2759 replace -f -'
  E0424 05:11:21.302466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:22.302797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:11:22.677: INFO: stderr: ""
  Apr 24 05:11:22.677: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/24/23 05:11:22.677
  Apr 24 05:11:22.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-2759 delete pods e2e-test-httpd-pod'
  E0424 05:11:23.303090      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:24.303356      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:25.303501      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:11:26.024: INFO: stderr: ""
  Apr 24 05:11:26.024: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 24 05:11:26.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2759" for this suite. @ 04/24/23 05:11:26.031
• [10.655 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/24/23 05:11:26.04
  Apr 24 05:11:26.040: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 05:11:26.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:11:26.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:11:26.069
  STEP: Creating secret with name s-test-opt-del-e1d80d70-e999-4022-908c-38567cf4c194 @ 04/24/23 05:11:26.08
  STEP: Creating secret with name s-test-opt-upd-b3861aab-54ae-461d-a016-e63fde37091c @ 04/24/23 05:11:26.087
  STEP: Creating the pod @ 04/24/23 05:11:26.093
  E0424 05:11:26.304131      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:27.304461      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:28.305357      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:29.305657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-e1d80d70-e999-4022-908c-38567cf4c194 @ 04/24/23 05:11:30.183
  STEP: Updating secret s-test-opt-upd-b3861aab-54ae-461d-a016-e63fde37091c @ 04/24/23 05:11:30.193
  STEP: Creating secret with name s-test-opt-create-f3529d4a-7b14-4711-99c0-04ddff4fabe1 @ 04/24/23 05:11:30.205
  STEP: waiting to observe update in volume @ 04/24/23 05:11:30.218
  E0424 05:11:30.305860      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:31.306104      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:32.307016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:33.307235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:34.307349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:35.307626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:36.308438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:37.308715      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:38.309045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:39.309579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:40.310180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:41.310606      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:42.310610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:43.311876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:44.311884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:45.312194      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:46.312327      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:47.312974      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:48.313294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:49.313887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:50.314659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:51.314276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:52.314535      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:53.314980      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:54.317253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:55.317445      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:56.317587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:57.318656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:58.319007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:11:59.319074      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:00.319769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:01.320047      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:02.321191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:03.321940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:04.322554      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:05.322591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:06.323250      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:07.323440      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:08.324144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:09.323940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:10.324428      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:11.324693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:12.325462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:13.325600      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:14.326418      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:15.327454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:16.327561      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:17.328448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:18.329627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:19.330105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:20.330303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:21.331400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:22.331319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:23.331975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:24.332701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:25.333029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:26.333415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:27.335603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:28.334511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:29.334837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:30.335183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:31.336493      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:32.336338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:33.336562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:34.337750      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:35.338384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:36.338894      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:37.339262      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:12:37.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3406" for this suite. @ 04/24/23 05:12:37.428
• [71.399 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/24/23 05:12:37.457
  Apr 24 05:12:37.457: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename job @ 04/24/23 05:12:37.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:12:37.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:12:37.498
  STEP: Creating a job @ 04/24/23 05:12:37.502
  STEP: Ensuring active pods == parallelism @ 04/24/23 05:12:37.512
  E0424 05:12:38.339866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:39.340035      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 04/24/23 05:12:39.52
  STEP: deleting Job.batch foo in namespace job-8274, will wait for the garbage collector to delete the pods @ 04/24/23 05:12:39.52
  Apr 24 05:12:39.593: INFO: Deleting Job.batch foo took: 17.302119ms
  Apr 24 05:12:39.694: INFO: Terminating Job.batch foo pods took: 101.031398ms
  E0424 05:12:40.339967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:41.340405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:42.341390      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:43.341695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:44.341887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:45.342627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:46.342943      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:47.343059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:48.343696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:49.344740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:50.345982      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:51.346160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:52.348199      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:53.348971      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:54.349480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:55.350346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:56.350496      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:57.351109      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:58.351307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:12:59.351847      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:00.352204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:01.352945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:02.353921      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:03.354232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:04.354652      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:05.355900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:06.356413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:07.357383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:08.357878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:09.358828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:10.359341      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:11.360032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 04/24/23 05:13:11.696
  Apr 24 05:13:11.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8274" for this suite. @ 04/24/23 05:13:11.709
• [34.265 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/24/23 05:13:11.722
  Apr 24 05:13:11.722: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename init-container @ 04/24/23 05:13:11.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:13:11.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:13:11.756
  STEP: creating the pod @ 04/24/23 05:13:11.76
  Apr 24 05:13:11.761: INFO: PodSpec: initContainers in spec.initContainers
  E0424 05:13:12.360182      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:13.360788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:14.361956      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:15.363219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:16.363368      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:17.363887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:18.364809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:19.364880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:20.364882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:21.365115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:22.365900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:23.366022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:24.366184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:25.366311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:26.366687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:27.367300      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:28.367480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:29.367877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:30.367920      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:31.368601      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:32.368570      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:33.368665      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:34.369070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:35.369960      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:36.369731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:37.370756      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:38.371859      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:39.371794      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:40.372519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:41.373938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:42.374254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:43.374528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:44.375220      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:45.375329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:46.375982      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:47.377459      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:48.377527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:49.377887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:50.378637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:51.379298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:52.379642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:53.380316      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:54.380745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:55.382208      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:56.382933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:13:56.463: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-50ce7f6b-19c1-4b97-8fd3-6e703f103884", GenerateName:"", Namespace:"init-container-8159", SelfLink:"", UID:"ac0ae215-9da3-42c2-9a26-788851821b53", ResourceVersion:"40373", Generation:0, CreationTimestamp:time.Date(2023, time.April, 24, 5, 13, 11, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"761115625"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 24, 5, 13, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0060640a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 24, 5, 13, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0060640f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-hmnl5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005ed6040), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hmnl5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hmnl5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hmnl5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005ffe0d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"aeveeng9ieph-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc005cc6000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005ffe160)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005ffe180)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005ffe188), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005ffe18c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004f8c020), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 24, 5, 13, 11, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 24, 5, 13, 11, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 24, 5, 13, 11, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 24, 5, 13, 11, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.18", PodIP:"10.233.66.56", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.56"}}, StartTime:time.Date(2023, time.April, 24, 5, 13, 11, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc005cc60e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc005cc6150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://6143d63469c93f1d3582f04a75f72bbcb7352cec88542a11c80933f4fcc2d5fe", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005ed60c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005ed60a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc005ffe20f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 24 05:13:56.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8159" for this suite. @ 04/24/23 05:13:56.473
• [44.772 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/24/23 05:13:56.509
  Apr 24 05:13:56.509: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 05:13:56.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:13:56.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:13:56.542
  STEP: Creating pod liveness-50559e55-6d24-4fdc-a474-0b96ed6fc482 in namespace container-probe-1342 @ 04/24/23 05:13:56.547
  E0424 05:13:57.383346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:13:58.384202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:13:58.583: INFO: Started pod liveness-50559e55-6d24-4fdc-a474-0b96ed6fc482 in namespace container-probe-1342
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/24/23 05:13:58.584
  Apr 24 05:13:58.591: INFO: Initial restart count of pod liveness-50559e55-6d24-4fdc-a474-0b96ed6fc482 is 0
  E0424 05:13:59.384490      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:00.385510      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:01.385839      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:02.386706      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:03.387267      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:04.387617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:05.388406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:06.389125      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:07.389274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:08.389591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:09.389819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:10.390508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:11.390391      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:12.391123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:13.391553      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:14.392553      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:15.392710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:16.392795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:17.393501      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:18.394254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:19.394211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:20.394395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:21.394910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:22.395149      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:23.395238      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:24.395871      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:25.395975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:26.396794      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:27.397075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:28.397328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:29.397962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:30.398695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:31.398632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:32.399071      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:33.400030      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:34.400612      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:35.400553      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:36.401214      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:37.402036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:38.402637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:39.402766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:40.403225      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:41.404477      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:42.405533      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:43.405844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:44.406435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:45.406542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:46.407131      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:47.408110      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:48.408215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:49.408295      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:50.408920      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:51.410091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:52.410067      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:53.410747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:54.411535      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:55.411766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:56.413062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:57.413705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:58.414128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:14:59.416216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:00.416317      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:01.416504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:02.417757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:03.418312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:04.418259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:05.418680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:06.419586      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:07.420502      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:08.420757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:09.421823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:10.422664      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:11.422599      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:12.422830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:13.423656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:14.424386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:15.425038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:16.425596      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:17.426913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:18.426756      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:19.427004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:20.428000      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:21.428555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:22.428650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:23.428973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:24.429574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:25.429886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:26.430907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:27.430596      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:28.431293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:29.432270      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:30.433492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:31.433985      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:32.434603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:33.435400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:34.436007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:35.436888      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:36.436889      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:37.437219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:38.437829      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:39.438169      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:40.438679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:41.439654      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:42.440539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:43.440518      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:44.441094      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:45.441697      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:46.442843      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:47.443479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:48.444051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:49.444173      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:50.444312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:51.445260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:52.445797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:53.446646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:54.447632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:55.447846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:56.451374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:57.451826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:58.452184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:15:59.452282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:00.452463      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:01.453149      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:02.453911      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:03.454614      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:04.455331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:05.455860      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:06.456637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:07.457033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:08.457610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:09.458083      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:10.458629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:11.461858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:12.462188      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:13.462753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:14.463126      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:15.463788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:16.475404      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:17.468039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:18.472062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:19.468966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:20.469312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:21.470024      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:22.470433      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:23.473902      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:24.474174      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:25.475038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:26.475952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:27.476850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:28.477532      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:29.478782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:30.479247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:31.479200      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:32.479755      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:33.480045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:34.480913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:35.480944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:36.481751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:37.482233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:38.482550      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:39.482577      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:40.483223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:41.484342      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:42.485224      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:43.486067      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:44.486160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:45.489444      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:46.488398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:47.488777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:48.489580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:49.490151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:50.490650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:51.491100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:52.491313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:53.492356      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:54.493070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:55.493447      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:56.494219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:57.494851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:58.496040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:16:59.496740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:00.497254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:01.497926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:02.499395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:03.499912      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:04.500175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:05.500959      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:06.501511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:07.502252      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:08.503199      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:09.503253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:10.504033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:11.504600      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:12.505917      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:13.506448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:14.507185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:15.507588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:16.508006      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:17.508356      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:18.508643      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:19.508780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:20.509212      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:21.509256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:22.509825      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:23.510555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:24.510925      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:25.511329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:26.513608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:27.512674      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:28.513696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:29.513755      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:30.519861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:31.514339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:32.516423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:33.515276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:34.516213      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:35.516368      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:36.517163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:37.517679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:38.518670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:39.519177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:40.519507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:41.520294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:42.520563      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:43.520556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:44.522379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:45.522232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:46.523276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:47.523288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:48.524007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:49.524056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:50.524683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:51.524853      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:52.525387      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:53.525608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:54.525986      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:55.526139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:56.527986      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:57.527104      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:58.527172      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:17:59.527221      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:17:59.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 05:17:59.67
  STEP: Destroying namespace "container-probe-1342" for this suite. @ 04/24/23 05:17:59.694
• [243.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/24/23 05:17:59.72
  Apr 24 05:17:59.720: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename namespaces @ 04/24/23 05:17:59.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:17:59.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:17:59.768
  STEP: creating a Namespace @ 04/24/23 05:17:59.771
  STEP: patching the Namespace @ 04/24/23 05:17:59.795
  STEP: get the Namespace and ensuring it has the label @ 04/24/23 05:17:59.803
  Apr 24 05:17:59.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3096" for this suite. @ 04/24/23 05:17:59.814
  STEP: Destroying namespace "nspatchtest-b93956eb-074b-4a4b-8b65-488de1258a1c-7135" for this suite. @ 04/24/23 05:17:59.824
• [0.119 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 04/24/23 05:17:59.84
  Apr 24 05:17:59.840: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename daemonsets @ 04/24/23 05:17:59.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:17:59.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:17:59.882
  STEP: Creating simple DaemonSet "daemon-set" @ 04/24/23 05:17:59.927
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/24/23 05:17:59.936
  Apr 24 05:17:59.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 05:17:59.956: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  E0424 05:18:00.528381      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:18:00.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 24 05:18:00.971: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  E0424 05:18:01.529471      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:18:01.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 24 05:18:01.995: INFO: Node aeveeng9ieph-3 is running 0 daemon pod, expected 1
  E0424 05:18:02.530033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:18:02.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 24 05:18:02.977: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/24/23 05:18:02.983
  STEP: DeleteCollection of the DaemonSets @ 04/24/23 05:18:02.994
  STEP: Verify that ReplicaSets have been deleted @ 04/24/23 05:18:03.009
  Apr 24 05:18:03.034: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41004"},"items":null}

  Apr 24 05:18:03.061: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41009"},"items":[{"metadata":{"name":"daemon-set-95w8f","generateName":"daemon-set-","namespace":"daemonsets-5472","uid":"856ada2e-8be1-4975-bdd3-7e0a4247882b","resourceVersion":"41004","creationTimestamp":"2023-04-24T05:17:59Z","deletionTimestamp":"2023-04-24T05:18:33Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"48a15f31-a017-4ae5-bcdf-aa4eed511d53","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-24T05:17:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48a15f31-a017-4ae5-bcdf-aa4eed511d53\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-24T05:18:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l7q6z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l7q6z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aeveeng9ieph-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aeveeng9ieph-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:01Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:01Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:00Z"}],"hostIP":"192.168.121.43","podIP":"10.233.65.100","podIPs":[{"ip":"10.233.65.100"}],"startTime":"2023-04-24T05:18:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-24T05:18:01Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://9fdd95b02cdf515694a8a1c23650254ba4d3a2c9d4d82185be022af316ea67b5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fcbl9","generateName":"daemon-set-","namespace":"daemonsets-5472","uid":"9a18e30d-e241-4686-8898-044511a1b458","resourceVersion":"41006","creationTimestamp":"2023-04-24T05:17:59Z","deletionTimestamp":"2023-04-24T05:18:33Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"48a15f31-a017-4ae5-bcdf-aa4eed511d53","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-24T05:17:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48a15f31-a017-4ae5-bcdf-aa4eed511d53\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-24T05:18:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lg444","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lg444","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aeveeng9ieph-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aeveeng9ieph-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:17:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:17:59Z"}],"hostIP":"192.168.121.18","podIP":"10.233.66.42","podIPs":[{"ip":"10.233.66.42"}],"startTime":"2023-04-24T05:17:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-24T05:18:01Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://e7f2c923ae9ddc7ee4a937afe66c8cc206275536a0472077d81ec2d2926530db","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nf9hv","generateName":"daemon-set-","namespace":"daemonsets-5472","uid":"5cd18f0b-e156-4e63-bd48-53cbc8f6e0ed","resourceVersion":"41007","creationTimestamp":"2023-04-24T05:17:59Z","deletionTimestamp":"2023-04-24T05:18:33Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"48a15f31-a017-4ae5-bcdf-aa4eed511d53","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-24T05:17:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48a15f31-a017-4ae5-bcdf-aa4eed511d53\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-24T05:18:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bv6kq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bv6kq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aeveeng9ieph-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aeveeng9ieph-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:01Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:18:01Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-24T05:17:59Z"}],"hostIP":"192.168.121.252","podIP":"10.233.64.151","podIPs":[{"ip":"10.233.64.151"}],"startTime":"2023-04-24T05:18:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-24T05:18:01Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://3ae73bde3057c9cc8e8ec92a4d956959e0ac3c0889b5e075c886fbd1d5a90557","started":true}],"qosClass":"BestEffort"}}]}

  Apr 24 05:18:03.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5472" for this suite. @ 04/24/23 05:18:03.105
• [3.277 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/24/23 05:18:03.121
  Apr 24 05:18:03.121: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename runtimeclass @ 04/24/23 05:18:03.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:18:03.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:18:03.191
  STEP: Deleting RuntimeClass runtimeclass-1639-delete-me @ 04/24/23 05:18:03.22
  STEP: Waiting for the RuntimeClass to disappear @ 04/24/23 05:18:03.235
  Apr 24 05:18:03.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1639" for this suite. @ 04/24/23 05:18:03.313
• [0.219 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/24/23 05:18:03.342
  Apr 24 05:18:03.342: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename controllerrevisions @ 04/24/23 05:18:03.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:18:03.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:18:03.395
  STEP: Creating DaemonSet "e2e-xbgx9-daemon-set" @ 04/24/23 05:18:03.466
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/24/23 05:18:03.476
  Apr 24 05:18:03.494: INFO: Number of nodes with available pods controlled by daemonset e2e-xbgx9-daemon-set: 0
  Apr 24 05:18:03.494: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  E0424 05:18:03.530875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:04.531060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:18:04.582: INFO: Number of nodes with available pods controlled by daemonset e2e-xbgx9-daemon-set: 0
  Apr 24 05:18:04.591: INFO: Node aeveeng9ieph-1 is running 0 daemon pod, expected 1
  Apr 24 05:18:05.505: INFO: Number of nodes with available pods controlled by daemonset e2e-xbgx9-daemon-set: 3
  Apr 24 05:18:05.505: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-xbgx9-daemon-set
  STEP: Confirm DaemonSet "e2e-xbgx9-daemon-set" successfully created with "daemonset-name=e2e-xbgx9-daemon-set" label @ 04/24/23 05:18:05.51
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-xbgx9-daemon-set" @ 04/24/23 05:18:05.523
  Apr 24 05:18:05.530: INFO: Located ControllerRevision: "e2e-xbgx9-daemon-set-5fdbc995c7"
  E0424 05:18:05.531056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching ControllerRevision "e2e-xbgx9-daemon-set-5fdbc995c7" @ 04/24/23 05:18:05.534
  Apr 24 05:18:05.545: INFO: e2e-xbgx9-daemon-set-5fdbc995c7 has been patched
  STEP: Create a new ControllerRevision @ 04/24/23 05:18:05.545
  Apr 24 05:18:05.555: INFO: Created ControllerRevision: e2e-xbgx9-daemon-set-5b4f8f94bf
  STEP: Confirm that there are two ControllerRevisions @ 04/24/23 05:18:05.555
  Apr 24 05:18:05.555: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 24 05:18:05.560: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-xbgx9-daemon-set-5fdbc995c7" @ 04/24/23 05:18:05.56
  STEP: Confirm that there is only one ControllerRevision @ 04/24/23 05:18:05.568
  Apr 24 05:18:05.568: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 24 05:18:05.573: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-xbgx9-daemon-set-5b4f8f94bf" @ 04/24/23 05:18:05.578
  Apr 24 05:18:05.591: INFO: e2e-xbgx9-daemon-set-5b4f8f94bf has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/24/23 05:18:05.591
  W0424 05:18:05.600042      14 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/24/23 05:18:05.6
  Apr 24 05:18:05.600: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0424 05:18:06.531882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:18:06.606: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 24 05:18:06.617: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-xbgx9-daemon-set-5b4f8f94bf=updated" @ 04/24/23 05:18:06.618
  STEP: Confirm that there is only one ControllerRevision @ 04/24/23 05:18:06.634
  Apr 24 05:18:06.634: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 24 05:18:06.639: INFO: Found 1 ControllerRevisions
  Apr 24 05:18:06.643: INFO: ControllerRevision "e2e-xbgx9-daemon-set-84b87d987b" has revision 3
  STEP: Deleting DaemonSet "e2e-xbgx9-daemon-set" @ 04/24/23 05:18:06.648
  STEP: deleting DaemonSet.extensions e2e-xbgx9-daemon-set in namespace controllerrevisions-6747, will wait for the garbage collector to delete the pods @ 04/24/23 05:18:06.648
  Apr 24 05:18:06.715: INFO: Deleting DaemonSet.extensions e2e-xbgx9-daemon-set took: 9.543344ms
  Apr 24 05:18:06.815: INFO: Terminating DaemonSet.extensions e2e-xbgx9-daemon-set pods took: 100.407901ms
  E0424 05:18:07.532086      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:18:08.021: INFO: Number of nodes with available pods controlled by daemonset e2e-xbgx9-daemon-set: 0
  Apr 24 05:18:08.021: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-xbgx9-daemon-set
  Apr 24 05:18:08.025: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41136"},"items":null}

  Apr 24 05:18:08.044: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41136"},"items":null}

  Apr 24 05:18:08.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-6747" for this suite. @ 04/24/23 05:18:08.072
• [4.742 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/24/23 05:18:08.085
  Apr 24 05:18:08.085: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-probe @ 04/24/23 05:18:08.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:18:08.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:18:08.113
  STEP: Creating pod busybox-b370a669-fd7b-439e-a020-05213bac7f63 in namespace container-probe-9960 @ 04/24/23 05:18:08.117
  E0424 05:18:08.532951      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:09.533373      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:10.533792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:11.533519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:18:12.148: INFO: Started pod busybox-b370a669-fd7b-439e-a020-05213bac7f63 in namespace container-probe-9960
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/24/23 05:18:12.148
  Apr 24 05:18:12.157: INFO: Initial restart count of pod busybox-b370a669-fd7b-439e-a020-05213bac7f63 is 0
  E0424 05:18:12.534580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:13.534767      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:14.535978      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:15.535838      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:16.536245      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:17.536419      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:18.537992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:19.537744      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:20.538650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:21.539312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:22.539807      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:23.539809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:24.540279      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:25.540310      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:26.541210      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:27.541176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:28.542235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:29.542636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:30.543852      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:31.543750      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:32.544891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:33.544672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:34.545746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:35.545565      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:36.545776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:37.546139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:38.547141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:39.547338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:40.548404      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:41.548718      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:42.549335      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:43.549543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:44.549880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:45.550130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:46.551017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:47.552148      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:48.553578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:49.553514      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:50.553781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:51.554508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:52.555663      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:53.557734      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:54.557624      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:55.557753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:56.557838      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:57.558047      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:58.559166      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:18:59.559299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:00.559336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:01.559615      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:02.559718      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:03.559913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:04.560930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:05.561163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:06.561550      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:07.561620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:08.562194      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:09.562914      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:10.564032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:11.563731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:12.564845      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:13.564778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:14.565760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:15.565770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:16.567053      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:17.566881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:18.567356      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:19.568529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:20.569347      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:21.569488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:22.570676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:23.571211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:24.571817      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:25.572761      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:26.573282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:27.573439      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:28.574116      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:29.574728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:30.575569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:31.575824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:32.576717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:33.576827      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:34.578308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:35.577721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:36.578686      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:37.579263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:38.580618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:39.580735      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:40.581188      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:41.581384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:42.582541      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:43.582217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:44.582558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:45.582526      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:46.582895      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:47.583209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:48.583550      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:49.583722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:50.584123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:51.584439      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:52.585597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:53.587062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:54.586784      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:55.586575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:56.591694      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:57.591180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:58.592323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:19:59.592351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:00.592865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:01.593047      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:02.598322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:03.595515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:04.596471      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:05.595427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:06.596072      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:07.596237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:08.597163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:09.597009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:10.597291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:11.597441      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:12.597823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:13.598168      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:14.598398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:15.598670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:16.599175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:17.599385      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:18.599693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:19.599740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:20.600165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:21.600331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:22.601398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:23.603176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:24.601933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:25.602370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:26.602844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:27.602854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:28.603716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:29.603753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:30.604176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:31.604353      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:32.604765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:33.604785      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:34.604996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:35.605451      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:36.605921      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:37.606076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:38.606884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:39.607310      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:40.608033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:41.608392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:42.608768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:43.608970      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:44.609634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:45.609946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:46.610166      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:47.610164      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:48.610619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:49.611455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:50.612060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:51.612648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:52.613323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:53.613331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:54.613542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:55.613655      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:56.613836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:57.614168      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:58.614400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:20:59.614670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:00.615383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:01.616537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:02.616017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:03.616103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:04.616344      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:05.616747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:06.617419      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:07.617511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:08.618484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:09.619412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:10.619541      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:11.620345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:12.621053      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:13.621449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:14.621552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:15.622865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:16.623328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:17.624690      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:18.624137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:19.624323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:20.624762      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:21.625219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:22.626009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:23.626331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:24.627208      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:25.627205      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:26.627499      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:27.628284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:28.628629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:29.629071      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:30.629415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:31.629970      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:32.630218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:33.630425      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:34.630725      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:35.631180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:36.631405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:37.631711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:38.632414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:39.632766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:40.633027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:41.633386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:42.633641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:43.634392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:44.635305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:45.636488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:46.636657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:47.637175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:48.637778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:49.637766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:50.638583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:51.638594      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:52.639133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:53.640412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:54.640724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:55.640893      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:56.641255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:57.641562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:58.641940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:21:59.644227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:00.645592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:01.646419      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:02.646465      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:03.646513      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:04.647488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:05.647454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:06.647659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:07.648248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:08.648693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:09.648952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:10.649226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:11.649049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:12.649420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:13.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 05:22:13.238
  STEP: Destroying namespace "container-probe-9960" for this suite. @ 04/24/23 05:22:13.262
• [245.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 04/24/23 05:22:13.28
  Apr 24 05:22:13.281: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename field-validation @ 04/24/23 05:22:13.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:22:13.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:22:13.313
  Apr 24 05:22:13.318: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 05:22:13.649546      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:14.650064      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:15.650687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0424 05:22:16.060166      14 warnings.go:70] unknown field "alpha"
  W0424 05:22:16.060208      14 warnings.go:70] unknown field "beta"
  W0424 05:22:16.060217      14 warnings.go:70] unknown field "delta"
  W0424 05:22:16.060225      14 warnings.go:70] unknown field "epsilon"
  W0424 05:22:16.060232      14 warnings.go:70] unknown field "gamma"
  Apr 24 05:22:16.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-172" for this suite. @ 04/24/23 05:22:16.12
• [2.849 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/24/23 05:22:16.149
  Apr 24 05:22:16.149: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 05:22:16.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:22:16.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:22:16.237
  STEP: Creating a pod to test downward api env vars @ 04/24/23 05:22:16.241
  E0424 05:22:16.651643      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:17.651679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:18.652649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:19.653137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:22:20.281
  Apr 24 05:22:20.287: INFO: Trying to get logs from node aeveeng9ieph-3 pod downward-api-09e8339b-c8c7-40da-81a8-0f8cd50ffa29 container dapi-container: <nil>
  STEP: delete the pod @ 04/24/23 05:22:20.321
  Apr 24 05:22:20.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7031" for this suite. @ 04/24/23 05:22:20.385
• [4.325 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 04/24/23 05:22:20.48
  Apr 24 05:22:20.480: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename statefulset @ 04/24/23 05:22:20.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:22:20.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:22:20.51
  STEP: Creating service test in namespace statefulset-9977 @ 04/24/23 05:22:20.514
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/24/23 05:22:20.521
  STEP: Creating stateful set ss in namespace statefulset-9977 @ 04/24/23 05:22:20.525
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9977 @ 04/24/23 05:22:20.535
  Apr 24 05:22:20.543: INFO: Found 0 stateful pods, waiting for 1
  E0424 05:22:20.655036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:21.655985      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:22.657151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:23.657999      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:24.658380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:25.658437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:26.658747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:27.659080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:28.659786      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:29.659861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:30.555: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/24/23 05:22:30.556
  Apr 24 05:22:30.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0424 05:22:30.660531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:31.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 05:22:31.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 05:22:31.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 24 05:22:31.261: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0424 05:22:31.660690      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:32.661046      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:33.661516      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:34.661583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:35.661702      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:36.662426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:37.662799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:38.667282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:39.663548      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:40.664387      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:41.278: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 24 05:22:41.279: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 05:22:41.312: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999609s
  E0424 05:22:41.665094      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:42.319: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992650546s
  E0424 05:22:42.666058      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:43.327: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98612145s
  E0424 05:22:43.666021      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:44.334: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978620373s
  E0424 05:22:44.668523      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:45.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971370485s
  E0424 05:22:45.668867      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:46.354: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961318012s
  E0424 05:22:46.669398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:47.362: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951196269s
  E0424 05:22:47.669459      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:48.432: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.943025859s
  E0424 05:22:48.669663      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:49.439: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.872739082s
  E0424 05:22:49.670409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:50.450: INFO: Verifying statefulset ss doesn't scale past 1 for another 865.580928ms
  E0424 05:22:50.672730      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9977 @ 04/24/23 05:22:51.451
  Apr 24 05:22:51.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0424 05:22:51.673175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:22:51.764: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 24 05:22:51.765: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 24 05:22:51.765: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 24 05:22:51.771: INFO: Found 1 stateful pods, waiting for 3
  E0424 05:22:52.673905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:53.674008      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:54.674193      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:55.674823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:56.675280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:57.675388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:58.676555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:22:59.677534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:00.677828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:01.678103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:01.783: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 05:23:01.783: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 05:23:01.783: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/24/23 05:23:01.783
  STEP: Scale down will halt with unhealthy stateful pod @ 04/24/23 05:23:01.784
  Apr 24 05:23:01.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 24 05:23:02.043: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 05:23:02.043: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 05:23:02.043: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 24 05:23:02.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 24 05:23:02.263: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 05:23:02.263: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 05:23:02.263: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 24 05:23:02.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 24 05:23:02.541: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 24 05:23:02.541: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 24 05:23:02.541: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 24 05:23:02.541: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 05:23:02.546: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E0424 05:23:02.678681      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:03.679197      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:04.679332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:05.679528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:06.679858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:07.680006      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:08.680159      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:09.680340      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:10.680539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:11.680641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:12.572: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 24 05:23:12.573: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 24 05:23:12.574: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 24 05:23:12.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999704s
  E0424 05:23:12.682751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:13.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993135456s
  E0424 05:23:13.683841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:14.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984094922s
  E0424 05:23:14.684592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:15.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976069873s
  E0424 05:23:15.685189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:16.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970083847s
  E0424 05:23:16.685967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:17.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961331429s
  E0424 05:23:17.686269      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:18.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954139079s
  E0424 05:23:18.687192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:19.660: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.943063252s
  E0424 05:23:19.687911      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:20.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.93572204s
  E0424 05:23:20.688797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:21.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 924.176305ms
  E0424 05:23:21.695080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9977 @ 04/24/23 05:23:22.682
  Apr 24 05:23:22.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0424 05:23:22.695661      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:23.151: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 24 05:23:23.151: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 24 05:23:23.151: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 24 05:23:23.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:23:23.599: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 24 05:23:23.600: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 24 05:23:23.600: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 24 05:23:23.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0424 05:23:23.696134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:23.918: INFO: rc: 1
  Apr 24 05:23:23.918: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  error: unable to upgrade connection: container not found ("webserver")

  error:
  exit status 1
  E0424 05:23:24.696855      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:25.697038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:26.697997      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:27.698308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:28.698372      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:29.698608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:30.698796      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:31.699228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:32.700260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:33.701252      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:33.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:23:34.057: INFO: rc: 1
  Apr 24 05:23:34.057: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:23:34.701411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:35.702025      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:36.702369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:37.703157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:38.703176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:39.703429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:40.704270      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:41.705495      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:42.705903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:43.706696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:44.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:23:44.169: INFO: rc: 1
  Apr 24 05:23:44.170: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:23:44.707169      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:45.708283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:46.709467      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:47.710170      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:48.710998      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:49.711247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:50.711504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:51.711620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:52.711726      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:53.712032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:23:54.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:23:54.288: INFO: rc: 1
  Apr 24 05:23:54.288: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:23:54.712059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:55.712753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:56.713086      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:57.714371      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:58.714562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:23:59.715542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:00.716264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:01.716691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:02.717728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:03.718462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:24:04.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:24:04.445: INFO: rc: 1
  Apr 24 05:24:04.445: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:24:04.719151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:05.719180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:06.719327      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:07.719518      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:08.720033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:09.720518      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:10.721804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:11.721109      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:12.722169      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:13.722442      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:24:14.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:24:14.588: INFO: rc: 1
  Apr 24 05:24:14.588: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:24:14.723516      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:15.723629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:16.724360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:17.724832      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:18.725212      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:19.726121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:20.726422      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:21.726992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:22.727203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:23.727234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:24:24.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0424 05:24:24.727971      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:24:24.743: INFO: rc: 1
  Apr 24 05:24:24.743: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:24:25.728796      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:26.728907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:27.728977      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:28.729188      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:29.730266      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:30.730384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:31.730516      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:32.730771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:33.730937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:34.731229      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:24:34.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:24:34.879: INFO: rc: 1
  Apr 24 05:24:34.879: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:24:35.731397      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:36.732312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:37.732390      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:38.733440      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:39.733963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:40.734293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:41.735297      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:42.735421      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:43.735541      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:44.735641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:24:44.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:24:45.074: INFO: rc: 1
  Apr 24 05:24:45.074: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:24:45.735810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:46.736302      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:47.736500      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:48.736589      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:49.736767      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:50.736875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:51.737166      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:52.737232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:53.737516      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:54.737688      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:24:55.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:24:55.359: INFO: rc: 1
  Apr 24 05:24:55.359: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:24:55.737878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:56.737997      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:57.738116      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:58.738350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:24:59.738534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:00.738618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:01.739384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:02.739494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:03.739648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:04.740750      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:25:05.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:25:05.529: INFO: rc: 1
  Apr 24 05:25:05.529: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:25:05.740765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:06.740801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:07.741941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:08.742312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:09.742832      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:10.743218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:11.743349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:12.743476      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:13.743591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:14.747292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:25:15.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:25:15.676: INFO: rc: 1
  Apr 24 05:25:15.676: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:25:15.748083      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:16.748517      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:17.749405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:18.748769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:19.749038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:20.749413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:21.750404      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:22.751227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:23.751464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:24.752556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:25:25.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0424 05:25:25.753233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:25:25.836: INFO: rc: 1
  Apr 24 05:25:25.836: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:25:26.753344      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:27.753562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:28.754032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:29.755268      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:30.755165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:31.755355      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:32.756237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:33.756688      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:34.757699      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:35.758073      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:25:35.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:25:36.043: INFO: rc: 1
  Apr 24 05:25:36.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:25:36.758175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:37.759139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:38.759225      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:39.759307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:40.760327      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:41.760664      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:42.760797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:43.760915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:44.761054      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:45.761213      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:25:46.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:25:46.161: INFO: rc: 1
  Apr 24 05:25:46.161: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:25:46.761811      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:47.761818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:48.762868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:49.762909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:50.763218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:51.763337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:52.763558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:53.763686      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:54.763787      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:55.763977      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:25:56.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:25:56.277: INFO: rc: 1
  Apr 24 05:25:56.278: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:25:56.764445      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:57.764509      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:58.764714      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:25:59.764947      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:00.765065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:01.765322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:02.765451      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:03.765524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:04.765711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:05.765866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:26:06.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:26:06.410: INFO: rc: 1
  Apr 24 05:26:06.410: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:26:06.765962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:07.766764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:08.767183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:09.767286      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:10.767438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:11.767634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:12.768330      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:13.768907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:14.769179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:15.769343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:26:16.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:26:16.536: INFO: rc: 1
  Apr 24 05:26:16.536: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:26:16.769791      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:17.770332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:18.770409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:19.771089      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:20.771239      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:21.771346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:22.771483      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:23.771630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:24.771754      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:25.772152      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:26:26.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:26:26.662: INFO: rc: 1
  Apr 24 05:26:26.662: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:26:26.772631      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:27.772766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:28.772927      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:29.773055      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:30.773234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:31.773374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:32.773481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:33.773729      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:34.774078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:35.774253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:26:36.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0424 05:26:36.774543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:26:36.819: INFO: rc: 1
  Apr 24 05:26:36.819: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:26:37.774572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:38.774835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:39.775330      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:40.775420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:41.775528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:42.775886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:43.776298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:44.776392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:45.776542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:46.776679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:26:46.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:26:47.013: INFO: rc: 1
  Apr 24 05:26:47.013: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:26:47.777403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:48.777549      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:49.777719      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:50.777957      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:51.778132      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:52.778812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:53.779850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:54.780612      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:55.780905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:56.781935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:26:57.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:26:57.190: INFO: rc: 1
  Apr 24 05:26:57.190: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:26:57.783085      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:58.783234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:26:59.783319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:00.783480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:01.784334      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:02.785231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:03.785652      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:04.785706      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:05.785818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:06.785942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:27:07.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:27:07.366: INFO: rc: 1
  Apr 24 05:27:07.367: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:27:07.787570      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:08.787277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:09.787331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:10.787487      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:11.788789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:12.788104      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:13.789260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:14.789655      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:15.790052      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:16.790368      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:27:17.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:27:17.519: INFO: rc: 1
  Apr 24 05:27:17.519: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:27:17.790757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:18.791289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:19.792419      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:20.792833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:21.793198      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:22.793672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:23.793997      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:24.794293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:25.794649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:26.794900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:27:27.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:27:27.702: INFO: rc: 1
  Apr 24 05:27:27.702: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:27:27.795691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:28.796351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:29.796730      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:30.796891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:31.797503      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:32.798405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:33.799055      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:34.799171      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:35.800329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:36.800427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:27:37.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0424 05:27:37.801032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:27:37.867: INFO: rc: 1
  Apr 24 05:27:37.867: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:27:38.801198      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:39.801415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:40.801786      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:41.801870      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:42.802017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:43.802185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:44.802369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:45.802498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:46.802628      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:47.802783      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:27:47.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:27:48.006: INFO: rc: 1
  Apr 24 05:27:48.006: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:27:48.802876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:49.803190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:50.803342      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:51.803808      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:52.803902      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:53.804003      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:54.804160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:55.805420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:56.805576      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:57.806288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:27:58.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:27:58.155: INFO: rc: 1
  Apr 24 05:27:58.155: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:27:58.807381      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:27:59.811354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:00.811465      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:01.812351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:02.813318      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:03.813437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:04.813627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:05.813983      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:06.814369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:07.814762      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:08.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:28:08.279: INFO: rc: 1
  Apr 24 05:28:08.279: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:28:08.815206      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:09.815318      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:10.815491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:11.815670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:12.816418      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:13.816511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:14.816741      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:15.817063      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:16.817540      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:17.817771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:18.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:28:18.401: INFO: rc: 1
  Apr 24 05:28:18.401: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0424 05:28:18.818056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:19.818343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:20.818634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:21.819006      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:22.819293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:23.819783      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:24.820271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:25.820574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:26.821360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:27.821810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:28.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=statefulset-9977 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 24 05:28:28.519: INFO: rc: 1
  Apr 24 05:28:28.519: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
  Apr 24 05:28:28.519: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/24/23 05:28:28.544
  Apr 24 05:28:28.545: INFO: Deleting all statefulset in ns statefulset-9977
  Apr 24 05:28:28.551: INFO: Scaling statefulset ss to 0
  Apr 24 05:28:28.566: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 05:28:28.572: INFO: Deleting statefulset ss
  Apr 24 05:28:28.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9977" for this suite. @ 04/24/23 05:28:28.627
• [368.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/24/23 05:28:28.644
  Apr 24 05:28:28.644: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 05:28:28.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:28:28.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:28:28.672
  STEP: creating Agnhost RC @ 04/24/23 05:28:28.676
  Apr 24 05:28:28.676: INFO: namespace kubectl-4364
  Apr 24 05:28:28.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-4364 create -f -'
  E0424 05:28:28.822408      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:29.824844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:30.429: INFO: stderr: ""
  Apr 24 05:28:30.429: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/24/23 05:28:30.429
  E0424 05:28:30.823534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:31.437: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 05:28:31.437: INFO: Found 0 / 1
  E0424 05:28:31.955296      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:32.454: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 05:28:32.454: INFO: Found 1 / 1
  Apr 24 05:28:32.454: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 24 05:28:32.468: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 05:28:32.468: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 24 05:28:32.468: INFO: wait on agnhost-primary startup in kubectl-4364 
  Apr 24 05:28:32.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-4364 logs agnhost-primary-mk888 agnhost-primary'
  Apr 24 05:28:32.653: INFO: stderr: ""
  Apr 24 05:28:32.653: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/24/23 05:28:32.653
  Apr 24 05:28:32.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-4364 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 24 05:28:32.860: INFO: stderr: ""
  Apr 24 05:28:32.860: INFO: stdout: "service/rm2 exposed\n"
  Apr 24 05:28:32.873: INFO: Service rm2 in namespace kubectl-4364 found.
  E0424 05:28:32.955966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:33.956102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 04/24/23 05:28:34.888
  Apr 24 05:28:34.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-4364 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  E0424 05:28:34.956982      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:35.222: INFO: stderr: ""
  Apr 24 05:28:35.222: INFO: stdout: "service/rm3 exposed\n"
  Apr 24 05:28:35.231: INFO: Service rm3 in namespace kubectl-4364 found.
  E0424 05:28:35.957415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:36.958185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:37.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4364" for this suite. @ 04/24/23 05:28:37.256
• [8.623 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/24/23 05:28:37.268
  Apr 24 05:28:37.268: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename endpointslice @ 04/24/23 05:28:37.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:28:37.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:28:37.296
  STEP: getting /apis @ 04/24/23 05:28:37.301
  STEP: getting /apis/discovery.k8s.io @ 04/24/23 05:28:37.309
  STEP: getting /apis/discovery.k8s.iov1 @ 04/24/23 05:28:37.31
  STEP: creating @ 04/24/23 05:28:37.312
  STEP: getting @ 04/24/23 05:28:37.335
  STEP: listing @ 04/24/23 05:28:37.34
  STEP: watching @ 04/24/23 05:28:37.345
  Apr 24 05:28:37.345: INFO: starting watch
  STEP: cluster-wide listing @ 04/24/23 05:28:37.347
  STEP: cluster-wide watching @ 04/24/23 05:28:37.353
  Apr 24 05:28:37.353: INFO: starting watch
  STEP: patching @ 04/24/23 05:28:37.355
  STEP: updating @ 04/24/23 05:28:37.364
  Apr 24 05:28:37.379: INFO: waiting for watch events with expected annotations
  Apr 24 05:28:37.380: INFO: saw patched and updated annotations
  STEP: deleting @ 04/24/23 05:28:37.38
  STEP: deleting a collection @ 04/24/23 05:28:37.398
  Apr 24 05:28:37.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8228" for this suite. @ 04/24/23 05:28:37.43
• [0.171 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/24/23 05:28:37.44
  Apr 24 05:28:37.440: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/24/23 05:28:37.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:28:37.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:28:37.483
  STEP: create the container to handle the HTTPGet hook request. @ 04/24/23 05:28:37.494
  E0424 05:28:37.958742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:38.959328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/24/23 05:28:39.531
  E0424 05:28:39.959660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:40.960110      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/24/23 05:28:41.559
  E0424 05:28:41.960633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:42.962498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:43.962778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:44.963259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/24/23 05:28:45.587
  Apr 24 05:28:45.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2104" for this suite. @ 04/24/23 05:28:45.675
• [8.243 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/24/23 05:28:45.684
  Apr 24 05:28:45.684: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 05:28:45.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:28:45.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:28:45.712
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/24/23 05:28:45.716
  Apr 24 05:28:45.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-4066 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 24 05:28:45.936: INFO: stderr: ""
  Apr 24 05:28:45.936: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/24/23 05:28:45.936
  Apr 24 05:28:45.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-4066 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  E0424 05:28:45.963336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:46.076: INFO: stderr: ""
  Apr 24 05:28:46.076: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/24/23 05:28:46.076
  Apr 24 05:28:46.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-4066 delete pods e2e-test-httpd-pod'
  E0424 05:28:46.963563      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:47.964242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:48.386: INFO: stderr: ""
  Apr 24 05:28:48.387: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 24 05:28:48.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4066" for this suite. @ 04/24/23 05:28:48.436
• [2.805 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/24/23 05:28:48.49
  Apr 24 05:28:48.490: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename webhook @ 04/24/23 05:28:48.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:28:48.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:28:48.52
  STEP: Setting up server cert @ 04/24/23 05:28:48.56
  E0424 05:28:48.964649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/24/23 05:28:49.569
  STEP: Deploying the webhook pod @ 04/24/23 05:28:49.584
  STEP: Wait for the deployment to be ready @ 04/24/23 05:28:49.604
  Apr 24 05:28:49.618: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0424 05:28:49.965217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:50.965399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 05:28:51.636
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 05:28:51.65
  E0424 05:28:51.966496      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:28:52.650: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/24/23 05:28:52.658
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/24/23 05:28:52.658
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/24/23 05:28:52.691
  E0424 05:28:52.967146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/24/23 05:28:53.713
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/24/23 05:28:53.713
  E0424 05:28:53.967738      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 04/24/23 05:28:54.759
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/24/23 05:28:54.76
  E0424 05:28:54.976569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:55.976590      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:56.977498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:57.977937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:28:58.978010      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/24/23 05:28:59.817
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/24/23 05:28:59.817
  E0424 05:28:59.978442      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:00.978566      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:01.979465      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:02.979717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:03.979770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:29:04.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0424 05:29:04.980420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-5291" for this suite. @ 04/24/23 05:29:04.998
  STEP: Destroying namespace "webhook-markers-4224" for this suite. @ 04/24/23 05:29:05.018
• [16.537 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/24/23 05:29:05.033
  Apr 24 05:29:05.033: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename var-expansion @ 04/24/23 05:29:05.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:29:05.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:29:05.068
  E0424 05:29:05.980793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:06.980858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:07.981049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:08.981995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:29:09.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 24 05:29:09.114: INFO: Deleting pod "var-expansion-2480093b-f810-4783-b0b7-f60fb86e0df3" in namespace "var-expansion-5531"
  Apr 24 05:29:09.125: INFO: Wait up to 5m0s for pod "var-expansion-2480093b-f810-4783-b0b7-f60fb86e0df3" to be fully deleted
  E0424 05:29:09.982274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:10.983166      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-5531" for this suite. @ 04/24/23 05:29:11.14
• [6.116 seconds]
------------------------------
S
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/24/23 05:29:11.15
  Apr 24 05:29:11.150: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename disruption @ 04/24/23 05:29:11.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:29:11.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:29:11.18
  STEP: creating the pdb @ 04/24/23 05:29:11.183
  STEP: Waiting for the pdb to be processed @ 04/24/23 05:29:11.19
  E0424 05:29:11.987257      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:12.987880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 04/24/23 05:29:13.207
  STEP: Waiting for the pdb to be processed @ 04/24/23 05:29:13.229
  E0424 05:29:13.988278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:14.989079      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 04/24/23 05:29:15.246
  STEP: Waiting for the pdb to be processed @ 04/24/23 05:29:15.263
  E0424 05:29:15.989390      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:16.990302      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 04/24/23 05:29:17.288
  Apr 24 05:29:17.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-39" for this suite. @ 04/24/23 05:29:17.3
• [6.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/24/23 05:29:17.323
  Apr 24 05:29:17.323: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename watch @ 04/24/23 05:29:17.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:29:17.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:29:17.358
  STEP: creating a watch on configmaps with label A @ 04/24/23 05:29:17.361
  STEP: creating a watch on configmaps with label B @ 04/24/23 05:29:17.363
  STEP: creating a watch on configmaps with label A or B @ 04/24/23 05:29:17.365
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/24/23 05:29:17.366
  Apr 24 05:29:17.372: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-54  40ed3255-9fa3-4d0f-a1fa-899fe530d877 43324 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 05:29:17.372: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-54  40ed3255-9fa3-4d0f-a1fa-899fe530d877 43324 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/24/23 05:29:17.373
  Apr 24 05:29:17.384: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-54  40ed3255-9fa3-4d0f-a1fa-899fe530d877 43325 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 05:29:17.384: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-54  40ed3255-9fa3-4d0f-a1fa-899fe530d877 43325 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/24/23 05:29:17.384
  Apr 24 05:29:17.394: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-54  40ed3255-9fa3-4d0f-a1fa-899fe530d877 43326 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 05:29:17.395: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-54  40ed3255-9fa3-4d0f-a1fa-899fe530d877 43326 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/24/23 05:29:17.395
  Apr 24 05:29:17.403: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-54  40ed3255-9fa3-4d0f-a1fa-899fe530d877 43327 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 05:29:17.403: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-54  40ed3255-9fa3-4d0f-a1fa-899fe530d877 43327 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/24/23 05:29:17.403
  Apr 24 05:29:17.415: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-54  68392914-4a8e-4df8-adec-488ea0841cf0 43328 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 05:29:17.416: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-54  68392914-4a8e-4df8-adec-488ea0841cf0 43328 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0424 05:29:17.990372      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:18.991104      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:19.991603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:20.991862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:21.992393      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:22.992486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:23.992744      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:24.992940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:25.993074      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:26.993781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/24/23 05:29:27.416
  Apr 24 05:29:27.431: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-54  68392914-4a8e-4df8-adec-488ea0841cf0 43356 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 24 05:29:27.432: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-54  68392914-4a8e-4df8-adec-488ea0841cf0 43356 0 2023-04-24 05:29:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-24 05:29:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0424 05:29:27.993846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:28.994579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:29.994666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:30.995030      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:31.995945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:32.996197      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:33.996506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:34.997475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:35.997506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:36.997913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:29:37.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-54" for this suite. @ 04/24/23 05:29:37.444
• [20.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/24/23 05:29:37.459
  Apr 24 05:29:37.459: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename proxy @ 04/24/23 05:29:37.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:29:37.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:29:37.499
  Apr 24 05:29:37.503: INFO: Creating pod...
  E0424 05:29:38.001511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:39.001570      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:29:39.533: INFO: Creating service...
  Apr 24 05:29:39.551: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/pods/agnhost/proxy?method=DELETE
  Apr 24 05:29:39.583: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 24 05:29:39.585: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/pods/agnhost/proxy?method=OPTIONS
  Apr 24 05:29:39.599: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 24 05:29:39.599: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/pods/agnhost/proxy?method=PATCH
  Apr 24 05:29:39.606: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 24 05:29:39.606: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/pods/agnhost/proxy?method=POST
  Apr 24 05:29:39.613: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 24 05:29:39.614: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/pods/agnhost/proxy?method=PUT
  Apr 24 05:29:39.622: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 24 05:29:39.622: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 24 05:29:39.629: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 24 05:29:39.630: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 24 05:29:39.639: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 24 05:29:39.639: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 24 05:29:39.651: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 24 05:29:39.651: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/services/e2e-proxy-test-service/proxy?method=POST
  Apr 24 05:29:39.658: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 24 05:29:39.658: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 24 05:29:39.665: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 24 05:29:39.666: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/pods/agnhost/proxy?method=GET
  Apr 24 05:29:39.670: INFO: http.Client request:GET StatusCode:301
  Apr 24 05:29:39.670: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/services/e2e-proxy-test-service/proxy?method=GET
  Apr 24 05:29:39.680: INFO: http.Client request:GET StatusCode:301
  Apr 24 05:29:39.680: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/pods/agnhost/proxy?method=HEAD
  Apr 24 05:29:39.688: INFO: http.Client request:HEAD StatusCode:301
  Apr 24 05:29:39.688: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7812/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 24 05:29:39.693: INFO: http.Client request:HEAD StatusCode:301
  Apr 24 05:29:39.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7812" for this suite. @ 04/24/23 05:29:39.703
• [2.254 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/24/23 05:29:39.718
  Apr 24 05:29:39.718: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename configmap @ 04/24/23 05:29:39.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:29:39.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:29:39.758
  STEP: Creating configMap configmap-9072/configmap-test-24a5d75a-1c7a-4ee8-b493-61c8e4b0d8da @ 04/24/23 05:29:39.761
  STEP: Creating a pod to test consume configMaps @ 04/24/23 05:29:39.776
  E0424 05:29:40.001613      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:41.008160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:42.008628      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:43.009547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:29:43.817
  Apr 24 05:29:43.825: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-configmaps-bd2c8f4f-d862-487d-8863-4c03bf841ed0 container env-test: <nil>
  STEP: delete the pod @ 04/24/23 05:29:43.86
  Apr 24 05:29:43.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9072" for this suite. @ 04/24/23 05:29:43.931
• [4.233 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/24/23 05:29:43.952
  Apr 24 05:29:43.952: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-webhook @ 04/24/23 05:29:43.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:29:43.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:29:44.004
  E0424 05:29:44.009157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 04/24/23 05:29:44.014
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/24/23 05:29:44.67
  STEP: Deploying the custom resource conversion webhook pod @ 04/24/23 05:29:44.679
  STEP: Wait for the deployment to be ready @ 04/24/23 05:29:44.703
  Apr 24 05:29:44.718: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0424 05:29:45.010399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:46.010942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:29:46.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 5, 29, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 5, 29, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 5, 29, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 5, 29, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 05:29:47.012276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:48.012653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/24/23 05:29:48.751
  STEP: Verifying the service has paired with the endpoint @ 04/24/23 05:29:48.765
  E0424 05:29:49.013136      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:29:49.767: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 24 05:29:49.776: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 05:29:50.013946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:51.015046      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:52.015672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/24/23 05:29:52.725
  STEP: Create a v2 custom resource @ 04/24/23 05:29:52.75
  STEP: List CRs in v1 @ 04/24/23 05:29:52.962
  STEP: List CRs in v2 @ 04/24/23 05:29:52.97
  Apr 24 05:29:52.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0424 05:29:53.016742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-2701" for this suite. @ 04/24/23 05:29:53.578
• [9.639 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/24/23 05:29:53.593
  Apr 24 05:29:53.594: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 05:29:53.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:29:53.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:29:53.643
  STEP: Creating the pod @ 04/24/23 05:29:53.663
  E0424 05:29:54.020747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:55.019286      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:56.020311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:29:56.240: INFO: Successfully updated pod "annotationupdate90d869a5-d279-40af-b05c-9afb55cf4ff3"
  E0424 05:29:57.021155      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:29:58.021446      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:29:58.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8680" for this suite. @ 04/24/23 05:29:58.294
• [4.717 seconds]
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/24/23 05:29:58.311
  Apr 24 05:29:58.311: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename endpointslice @ 04/24/23 05:29:58.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:29:58.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:29:58.344
  E0424 05:29:59.022158      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:00.022081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:00.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9623" for this suite. @ 04/24/23 05:30:00.495
• [2.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/24/23 05:30:00.511
  Apr 24 05:30:00.511: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename pod-network-test @ 04/24/23 05:30:00.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:30:00.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:30:00.542
  STEP: Performing setup for networking test in namespace pod-network-test-7544 @ 04/24/23 05:30:00.546
  STEP: creating a selector @ 04/24/23 05:30:00.546
  STEP: Creating the service pods in kubernetes @ 04/24/23 05:30:00.547
  Apr 24 05:30:00.547: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0424 05:30:01.022853      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:02.023556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:03.024816      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:04.025038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:05.025994      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:06.025891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:07.026682      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:08.027360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:09.028271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:10.028444      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:11.029064      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:12.029575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:13.030281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:14.030830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:15.031717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:16.032398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:17.033281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:18.033936      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:19.034538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:20.034797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:21.035504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:22.036365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/24/23 05:30:22.741
  E0424 05:30:23.036810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:24.060961      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:25.042543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:26.042278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:26.808: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 24 05:30:26.808: INFO: Going to poll 10.233.64.120 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 24 05:30:26.814: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.120 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 05:30:26.814: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 05:30:26.816: INFO: ExecWithOptions: Clientset creation
  Apr 24 05:30:26.816: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.120+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0424 05:30:27.043671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:28.043808      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:28.247: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 24 05:30:28.247: INFO: Going to poll 10.233.65.153 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 24 05:30:28.257: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 05:30:28.257: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 05:30:28.259: INFO: ExecWithOptions: Clientset creation
  Apr 24 05:30:28.259: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.153+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0424 05:30:29.043918      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:29.346: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 24 05:30:29.346: INFO: Going to poll 10.233.66.81 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 24 05:30:29.355: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 24 05:30:29.355: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  Apr 24 05:30:29.357: INFO: ExecWithOptions: Clientset creation
  Apr 24 05:30:29.357: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.81+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0424 05:30:30.044377      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:30.484: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 24 05:30:30.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7544" for this suite. @ 04/24/23 05:30:30.496
• [29.997 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/24/23 05:30:30.511
  Apr 24 05:30:30.511: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename deployment @ 04/24/23 05:30:30.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:30:30.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:30:30.54
  STEP: creating a Deployment @ 04/24/23 05:30:30.553
  Apr 24 05:30:30.553: INFO: Creating simple deployment test-deployment-rdf5n
  Apr 24 05:30:30.580: INFO: deployment "test-deployment-rdf5n" doesn't have the required revision set
  E0424 05:30:31.045705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:32.114039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:32.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 24, 5, 30, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 5, 30, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 5, 30, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 5, 30, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-rdf5n-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0424 05:30:33.114186      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:34.114817      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 04/24/23 05:30:34.615
  Apr 24 05:30:34.624: INFO: Deployment test-deployment-rdf5n has Conditions: [{Available True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdf5n-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/24/23 05:30:34.625
  Apr 24 05:30:34.647: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 5, 30, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 5, 30, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 24, 5, 30, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 24, 5, 30, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-rdf5n-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/24/23 05:30:34.647
  Apr 24 05:30:34.652: INFO: Observed &Deployment event: ADDED
  Apr 24 05:30:34.652: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdf5n-5994cf9475"}
  Apr 24 05:30:34.652: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.652: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdf5n-5994cf9475"}
  Apr 24 05:30:34.652: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 24 05:30:34.653: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.653: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 24 05:30:34.653: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rdf5n-5994cf9475" is progressing.}
  Apr 24 05:30:34.653: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.653: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 24 05:30:34.653: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdf5n-5994cf9475" has successfully progressed.}
  Apr 24 05:30:34.653: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.653: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 24 05:30:34.653: INFO: Observed Deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdf5n-5994cf9475" has successfully progressed.}
  Apr 24 05:30:34.653: INFO: Found Deployment test-deployment-rdf5n in namespace deployment-3616 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 24 05:30:34.653: INFO: Deployment test-deployment-rdf5n has an updated status
  STEP: patching the Statefulset Status @ 04/24/23 05:30:34.653
  Apr 24 05:30:34.653: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 24 05:30:34.669: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/24/23 05:30:34.67
  Apr 24 05:30:34.677: INFO: Observed &Deployment event: ADDED
  Apr 24 05:30:34.677: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdf5n-5994cf9475"}
  Apr 24 05:30:34.677: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.678: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdf5n-5994cf9475"}
  Apr 24 05:30:34.678: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 24 05:30:34.678: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.679: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 24 05:30:34.679: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:30 +0000 UTC 2023-04-24 05:30:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rdf5n-5994cf9475" is progressing.}
  Apr 24 05:30:34.679: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.679: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 24 05:30:34.680: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdf5n-5994cf9475" has successfully progressed.}
  Apr 24 05:30:34.680: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.681: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 24 05:30:34.681: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-24 05:30:33 +0000 UTC 2023-04-24 05:30:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdf5n-5994cf9475" has successfully progressed.}
  Apr 24 05:30:34.688: INFO: Observed deployment test-deployment-rdf5n in namespace deployment-3616 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 24 05:30:34.688: INFO: Observed &Deployment event: MODIFIED
  Apr 24 05:30:34.689: INFO: Found deployment test-deployment-rdf5n in namespace deployment-3616 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 24 05:30:34.689: INFO: Deployment test-deployment-rdf5n has a patched status
  Apr 24 05:30:34.696: INFO: Deployment "test-deployment-rdf5n":
  &Deployment{ObjectMeta:{test-deployment-rdf5n  deployment-3616  c1f9a006-a773-4bf3-87de-37686f0ed0e6 43773 1 2023-04-24 05:30:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-24 05:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-24 05:30:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-24 05:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004269468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-24 05:30:34 +0000 UTC,LastTransitionTime:2023-04-24 05:30:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-rdf5n-5994cf9475" has successfully progressed.,LastUpdateTime:2023-04-24 05:30:34 +0000 UTC,LastTransitionTime:2023-04-24 05:30:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 24 05:30:34.703: INFO: New ReplicaSet "test-deployment-rdf5n-5994cf9475" of Deployment "test-deployment-rdf5n":
  &ReplicaSet{ObjectMeta:{test-deployment-rdf5n-5994cf9475  deployment-3616  4c2eaccf-421a-47f2-89dc-13eec4da82d1 43764 1 2023-04-24 05:30:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-rdf5n c1f9a006-a773-4bf3-87de-37686f0ed0e6 0xc004269887 0xc004269888}] [] [{kube-controller-manager Update apps/v1 2023-04-24 05:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1f9a006-a773-4bf3-87de-37686f0ed0e6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-24 05:30:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004269938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 24 05:30:34.709: INFO: Pod "test-deployment-rdf5n-5994cf9475-q56pw" is available:
  &Pod{ObjectMeta:{test-deployment-rdf5n-5994cf9475-q56pw test-deployment-rdf5n-5994cf9475- deployment-3616  8c9c101c-cc31-48ec-929f-25c4e86bed0f 43763 0 2023-04-24 05:30:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-rdf5n-5994cf9475 4c2eaccf-421a-47f2-89dc-13eec4da82d1 0xc004269cb0 0xc004269cb1}] [] [{kube-controller-manager Update v1 2023-04-24 05:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c2eaccf-421a-47f2-89dc-13eec4da82d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-24 05:30:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqftq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqftq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aeveeng9ieph-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:30:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:30:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-24 05:30:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.18,PodIP:10.233.66.146,StartTime:2023-04-24 05:30:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-24 05:30:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b34eb6d8c286695ffafdd491e5eb2148f9f6c98fced929e4a5340535b7ea73e8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.146,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 24 05:30:34.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3616" for this suite. @ 04/24/23 05:30:34.717
• [4.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/24/23 05:30:34.732
  Apr 24 05:30:34.732: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/24/23 05:30:34.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:30:34.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:30:34.759
  STEP: fetching the /apis discovery document @ 04/24/23 05:30:34.763
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/24/23 05:30:34.765
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/24/23 05:30:34.765
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/24/23 05:30:34.765
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/24/23 05:30:34.766
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/24/23 05:30:34.766
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/24/23 05:30:34.768
  Apr 24 05:30:34.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-596" for this suite. @ 04/24/23 05:30:34.778
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/24/23 05:30:34.791
  Apr 24 05:30:34.792: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename dns @ 04/24/23 05:30:34.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:30:34.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:30:34.826
  STEP: Creating a test headless service @ 04/24/23 05:30:34.829
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3957 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3957;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3957 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3957;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3957.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3957.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3957.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3957.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3957.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3957.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3957.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3957.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3957.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3957.svc;check="$$(dig +notcp +noall +answer +search 104.11.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.11.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.11.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.11.104_tcp@PTR;sleep 1; done
   @ 04/24/23 05:30:34.858
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3957 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3957;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3957 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3957;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3957.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3957.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3957.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3957.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3957.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3957.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3957.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3957.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3957.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3957.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3957.svc;check="$$(dig +notcp +noall +answer +search 104.11.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.11.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.11.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.11.104_tcp@PTR;sleep 1; done
   @ 04/24/23 05:30:34.858
  STEP: creating a pod to probe DNS @ 04/24/23 05:30:34.859
  STEP: submitting the pod to kubernetes @ 04/24/23 05:30:34.859
  E0424 05:30:35.115898      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:36.116524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:37.117143      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:38.117190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/24/23 05:30:38.913
  STEP: looking for the results for each expected name from probers @ 04/24/23 05:30:38.919
  Apr 24 05:30:38.927: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.933: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.938: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.943: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.948: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.952: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.956: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.964: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.987: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.992: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:38.996: INFO: Unable to read jessie_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:39.000: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:39.005: INFO: Unable to read jessie_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:39.009: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:39.013: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:39.018: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:39.041: INFO: Lookups using dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3957 wheezy_tcp@dns-test-service.dns-3957 wheezy_udp@dns-test-service.dns-3957.svc wheezy_tcp@dns-test-service.dns-3957.svc wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3957 jessie_tcp@dns-test-service.dns-3957 jessie_udp@dns-test-service.dns-3957.svc jessie_tcp@dns-test-service.dns-3957.svc jessie_udp@_http._tcp.dns-test-service.dns-3957.svc jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc]

  E0424 05:30:39.117680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:40.117759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:41.118088      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:42.118449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:43.118683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:44.055: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.063: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.068: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.074: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.083: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.092: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.099: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.106: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  E0424 05:30:44.119392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:44.143: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.149: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.155: INFO: Unable to read jessie_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.164: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.171: INFO: Unable to read jessie_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.179: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.188: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.192: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:44.215: INFO: Lookups using dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3957 wheezy_tcp@dns-test-service.dns-3957 wheezy_udp@dns-test-service.dns-3957.svc wheezy_tcp@dns-test-service.dns-3957.svc wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3957 jessie_tcp@dns-test-service.dns-3957 jessie_udp@dns-test-service.dns-3957.svc jessie_tcp@dns-test-service.dns-3957.svc jessie_udp@_http._tcp.dns-test-service.dns-3957.svc jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc]

  E0424 05:30:45.119508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:46.119768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:47.122253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:48.122721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:49.051: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.056: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.061: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.066: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.075: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.080: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.084: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.107: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.113: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.119: INFO: Unable to read jessie_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  E0424 05:30:49.123556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:49.125: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.130: INFO: Unable to read jessie_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.141: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.146: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:49.172: INFO: Lookups using dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3957 wheezy_tcp@dns-test-service.dns-3957 wheezy_udp@dns-test-service.dns-3957.svc wheezy_tcp@dns-test-service.dns-3957.svc wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3957 jessie_tcp@dns-test-service.dns-3957 jessie_udp@dns-test-service.dns-3957.svc jessie_tcp@dns-test-service.dns-3957.svc jessie_udp@_http._tcp.dns-test-service.dns-3957.svc jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc]

  E0424 05:30:50.124914      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:51.124949      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:52.125819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:53.126580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:54.052: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.067: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.083: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.093: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.103: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  E0424 05:30:54.127595      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:54.128: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.134: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.142: INFO: Unable to read jessie_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.148: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.153: INFO: Unable to read jessie_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.159: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.165: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.170: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:54.197: INFO: Lookups using dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3957 wheezy_tcp@dns-test-service.dns-3957 wheezy_udp@dns-test-service.dns-3957.svc wheezy_tcp@dns-test-service.dns-3957.svc wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3957 jessie_tcp@dns-test-service.dns-3957 jessie_udp@dns-test-service.dns-3957.svc jessie_tcp@dns-test-service.dns-3957.svc jessie_udp@_http._tcp.dns-test-service.dns-3957.svc jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc]

  E0424 05:30:55.128292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:56.128591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:57.128789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:30:58.128955      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:59.051: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.060: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.072: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.082: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.095: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.103: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.108: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  E0424 05:30:59.129307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:30:59.141: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.146: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.152: INFO: Unable to read jessie_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.156: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.161: INFO: Unable to read jessie_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.167: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.174: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.180: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:30:59.198: INFO: Lookups using dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3957 wheezy_tcp@dns-test-service.dns-3957 wheezy_udp@dns-test-service.dns-3957.svc wheezy_tcp@dns-test-service.dns-3957.svc wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3957 jessie_tcp@dns-test-service.dns-3957 jessie_udp@dns-test-service.dns-3957.svc jessie_tcp@dns-test-service.dns-3957.svc jessie_udp@_http._tcp.dns-test-service.dns-3957.svc jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc]

  E0424 05:31:00.129573      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:01.129690      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:02.129878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:03.130022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:31:04.051: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.057: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.099: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.107: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.112: INFO: Unable to read wheezy_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.117: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.122: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  E0424 05:31:04.132824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:31:04.133: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.172: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.182: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.189: INFO: Unable to read jessie_udp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.198: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957 from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.204: INFO: Unable to read jessie_udp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.213: INFO: Unable to read jessie_tcp@dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.221: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.229: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc from pod dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271: the server could not find the requested resource (get pods dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271)
  Apr 24 05:31:04.269: INFO: Lookups using dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3957 wheezy_tcp@dns-test-service.dns-3957 wheezy_udp@dns-test-service.dns-3957.svc wheezy_tcp@dns-test-service.dns-3957.svc wheezy_udp@_http._tcp.dns-test-service.dns-3957.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3957.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3957 jessie_tcp@dns-test-service.dns-3957 jessie_udp@dns-test-service.dns-3957.svc jessie_tcp@dns-test-service.dns-3957.svc jessie_udp@_http._tcp.dns-test-service.dns-3957.svc jessie_tcp@_http._tcp.dns-test-service.dns-3957.svc]

  E0424 05:31:05.131824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:06.131997      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:07.133119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:08.133428      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:09.134097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:31:09.171: INFO: DNS probes using dns-3957/dns-test-ee2d60e9-31a0-466b-9c60-954bd70d0271 succeeded

  Apr 24 05:31:09.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/24/23 05:31:09.179
  STEP: deleting the test service @ 04/24/23 05:31:09.208
  STEP: deleting the test headless service @ 04/24/23 05:31:09.268
  STEP: Destroying namespace "dns-3957" for this suite. @ 04/24/23 05:31:09.305
• [34.528 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/24/23 05:31:09.32
  Apr 24 05:31:09.320: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename containers @ 04/24/23 05:31:09.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:31:09.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:31:09.356
  STEP: Creating a pod to test override command @ 04/24/23 05:31:09.365
  E0424 05:31:10.134845      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:11.135411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:12.135716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:13.136352      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:31:13.413
  Apr 24 05:31:13.419: INFO: Trying to get logs from node aeveeng9ieph-3 pod client-containers-9857fc4a-97d1-4319-ac54-e8691f8b39bf container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 05:31:13.43
  Apr 24 05:31:13.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-14" for this suite. @ 04/24/23 05:31:13.482
• [4.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/24/23 05:31:13.494
  Apr 24 05:31:13.494: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename containers @ 04/24/23 05:31:13.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:31:13.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:31:13.522
  E0424 05:31:14.136509      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:15.137569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:31:15.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6784" for this suite. @ 04/24/23 05:31:15.572
• [2.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/24/23 05:31:15.584
  Apr 24 05:31:15.585: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename init-container @ 04/24/23 05:31:15.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:31:15.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:31:15.607
  STEP: creating the pod @ 04/24/23 05:31:15.61
  Apr 24 05:31:15.611: INFO: PodSpec: initContainers in spec.initContainers
  E0424 05:31:16.138232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:17.139080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:18.139982      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:31:18.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7145" for this suite. @ 04/24/23 05:31:18.692
• [3.124 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 04/24/23 05:31:18.709
  Apr 24 05:31:18.709: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename statefulset @ 04/24/23 05:31:18.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:31:18.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:31:18.744
  STEP: Creating service test in namespace statefulset-3781 @ 04/24/23 05:31:18.748
  STEP: Creating a new StatefulSet @ 04/24/23 05:31:18.755
  Apr 24 05:31:18.771: INFO: Found 0 stateful pods, waiting for 3
  E0424 05:31:19.140529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:20.140966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:21.141098      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:22.141841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:23.141894      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:24.142009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:25.142156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:26.142826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:27.143751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:28.144000      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:31:28.781: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 05:31:28.782: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 05:31:28.782: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/24/23 05:31:28.8
  Apr 24 05:31:28.829: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/24/23 05:31:28.829
  E0424 05:31:29.144707      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:30.144998      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:31.145157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:32.145733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:33.146028      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:34.147049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:35.147725      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:36.148150      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:37.148138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:38.148954      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/24/23 05:31:38.859
  STEP: Performing a canary update @ 04/24/23 05:31:38.859
  Apr 24 05:31:38.892: INFO: Updating stateful set ss2
  Apr 24 05:31:38.918: INFO: Waiting for Pod statefulset-3781/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0424 05:31:39.149613      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:40.149958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:41.150173      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:42.150346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:43.150496      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:44.151141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:45.151157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:46.151552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:47.152281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:48.152732      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/24/23 05:31:48.935
  Apr 24 05:31:49.015: INFO: Found 1 stateful pods, waiting for 3
  E0424 05:31:49.153632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:50.153936      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:51.154307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:52.154946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:53.155231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:54.155656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:55.155923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:56.156658      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:57.156829      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:31:58.157465      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:31:59.048: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 05:31:59.048: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 24 05:31:59.048: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/24/23 05:31:59.067
  Apr 24 05:31:59.107: INFO: Updating stateful set ss2
  E0424 05:31:59.161674      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:31:59.164: INFO: Waiting for Pod statefulset-3781/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0424 05:32:00.161776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:01.162071      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:02.162348      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:03.162876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:04.163277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:05.163430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:06.164635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:07.165427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:08.165207      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:09.166192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:09.209: INFO: Updating stateful set ss2
  Apr 24 05:32:09.219: INFO: Waiting for StatefulSet statefulset-3781/ss2 to complete update
  Apr 24 05:32:09.220: INFO: Waiting for Pod statefulset-3781/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0424 05:32:10.165991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:11.166374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:12.166813      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:13.167468      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:14.168286      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:15.168705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:16.169325      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:17.169927      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:18.170461      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:19.171216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:19.234: INFO: Deleting all statefulset in ns statefulset-3781
  Apr 24 05:32:19.243: INFO: Scaling statefulset ss2 to 0
  E0424 05:32:20.171596      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:21.172371      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:22.172536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:23.173617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:24.173764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:25.173865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:26.174106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:27.174807      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:28.175366      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:29.175555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:29.285: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 24 05:32:29.292: INFO: Deleting statefulset ss2
  Apr 24 05:32:29.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3781" for this suite. @ 04/24/23 05:32:29.322
• [70.623 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/24/23 05:32:29.333
  Apr 24 05:32:29.333: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl @ 04/24/23 05:32:29.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:32:29.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:32:29.381
  Apr 24 05:32:29.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-5931 create -f -'
  E0424 05:32:30.176324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:30.188: INFO: stderr: ""
  Apr 24 05:32:30.188: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 24 05:32:30.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-5931 create -f -'
  Apr 24 05:32:31.008: INFO: stderr: ""
  Apr 24 05:32:31.009: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/24/23 05:32:31.009
  E0424 05:32:31.177284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:32.017: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 05:32:32.017: INFO: Found 1 / 1
  Apr 24 05:32:32.017: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 24 05:32:32.022: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 24 05:32:32.022: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 24 05:32:32.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-5931 describe pod agnhost-primary-h8sqf'
  E0424 05:32:32.177959      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:32.315: INFO: stderr: ""
  Apr 24 05:32:32.315: INFO: stdout: "Name:             agnhost-primary-h8sqf\nNamespace:        kubectl-5931\nPriority:         0\nService Account:  default\nNode:             aeveeng9ieph-3/192.168.121.18\nStart Time:       Mon, 24 Apr 2023 05:32:30 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.37\nIPs:\n  IP:           10.233.66.37\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://a0c8afaa4e1e99dfdaa56a781f7718ac38a5e4653df2dfae864025f84fea278f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 24 Apr 2023 05:32:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bkzwf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bkzwf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5931/agnhost-primary-h8sqf to aeveeng9ieph-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Apr 24 05:32:32.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-5931 describe rc agnhost-primary'
  Apr 24 05:32:32.497: INFO: stderr: ""
  Apr 24 05:32:32.497: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5931\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-h8sqf\n"
  Apr 24 05:32:32.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-5931 describe service agnhost-primary'
  Apr 24 05:32:32.644: INFO: stderr: ""
  Apr 24 05:32:32.644: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5931\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.20.30\nIPs:               10.233.20.30\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.37:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 24 05:32:32.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-5931 describe node aeveeng9ieph-1'
  Apr 24 05:32:32.936: INFO: stderr: ""
  Apr 24 05:32:32.936: INFO: stdout: "Name:               aeveeng9ieph-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=aeveeng9ieph-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 24 Apr 2023 03:05:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  aeveeng9ieph-1\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 24 Apr 2023 05:32:23 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 24 Apr 2023 03:07:44 +0000   Mon, 24 Apr 2023 03:07:44 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Mon, 24 Apr 2023 05:28:11 +0000   Mon, 24 Apr 2023 03:05:14 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 24 Apr 2023 05:28:11 +0000   Mon, 24 Apr 2023 03:05:14 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 24 Apr 2023 05:28:11 +0000   Mon, 24 Apr 2023 03:05:14 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 24 Apr 2023 05:28:11 +0000   Mon, 24 Apr 2023 03:08:49 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.252\n  Hostname:    aeveeng9ieph-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  122749536Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8140188Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  119410748528\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3290524Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 43fbeda9f90440c88f1cbb04d8febf15\n  System UUID:                43fbeda9-f904-40c8-8f1c-bb04d8febf15\n  Boot ID:                    8e51ccaa-e0cd-4abe-a8b6-02b979ea6d65\n  Kernel Version:             5.19.0-40-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.0\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-4hwfb                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         114m\n  kube-system                 cilium-node-init-57zjr                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         114m\n  kube-system                 coredns-5d78c9869d-dv4j2                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     79m\n  kube-system                 kube-addon-manager-aeveeng9ieph-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         114m\n  kube-system                 kube-apiserver-aeveeng9ieph-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 kube-controller-manager-aeveeng9ieph-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 kube-proxy-stmcm                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 kube-scheduler-aeveeng9ieph-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         114m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-9gjts    0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                855m (53%)  0 (0%)\n  memory             320Mi (9%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  Apr 24 05:32:32.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-5931 describe namespace kubectl-5931'
  Apr 24 05:32:33.113: INFO: stderr: ""
  Apr 24 05:32:33.113: INFO: stdout: "Name:         kubectl-5931\nLabels:       e2e-framework=kubectl\n              e2e-run=48ff365e-287c-4da3-a1bd-7e72510f2298\n              kubernetes.io/metadata.name=kubectl-5931\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 24 05:32:33.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5931" for this suite. @ 04/24/23 05:32:33.121
• [3.798 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/24/23 05:32:33.134
  Apr 24 05:32:33.134: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 05:32:33.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:32:33.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:32:33.164
  STEP: Creating secret with name secret-test-4a87dc76-eebf-4996-9837-7d07326dff53 @ 04/24/23 05:32:33.168
  STEP: Creating a pod to test consume secrets @ 04/24/23 05:32:33.175
  E0424 05:32:33.178013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:34.178222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:35.178663      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:36.178834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:37.179877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:32:37.218
  Apr 24 05:32:37.223: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-secrets-a1c67fdf-3020-4302-b33a-54c2508864c2 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 05:32:37.237
  Apr 24 05:32:37.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4660" for this suite. @ 04/24/23 05:32:37.269
• [4.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/24/23 05:32:37.282
  Apr 24 05:32:37.282: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename namespaces @ 04/24/23 05:32:37.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:32:37.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:32:37.308
  STEP: Creating a test namespace @ 04/24/23 05:32:37.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:32:37.33
  STEP: Creating a service in the namespace @ 04/24/23 05:32:37.334
  STEP: Deleting the namespace @ 04/24/23 05:32:37.349
  STEP: Waiting for the namespace to be removed. @ 04/24/23 05:32:37.368
  E0424 05:32:38.180256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:39.180447      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:40.182146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:41.181935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:42.182569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:43.182727      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/24/23 05:32:43.374
  STEP: Verifying there is no service in the namespace @ 04/24/23 05:32:43.402
  Apr 24 05:32:43.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4552" for this suite. @ 04/24/23 05:32:43.42
  STEP: Destroying namespace "nsdeletetest-576" for this suite. @ 04/24/23 05:32:43.428
  Apr 24 05:32:43.435: INFO: Namespace nsdeletetest-576 was already deleted
  STEP: Destroying namespace "nsdeletetest-5908" for this suite. @ 04/24/23 05:32:43.435
• [6.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/24/23 05:32:43.45
  Apr 24 05:32:43.450: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename sched-pred @ 04/24/23 05:32:43.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:32:43.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:32:43.501
  Apr 24 05:32:43.507: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 24 05:32:43.523: INFO: Waiting for terminating namespaces to be deleted...
  Apr 24 05:32:43.528: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-1 before test
  Apr 24 05:32:43.540: INFO: cilium-4hwfb from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.540: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 05:32:43.541: INFO: cilium-node-init-57zjr from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.541: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 05:32:43.541: INFO: coredns-5d78c9869d-dv4j2 from kube-system started at 2023-04-24 04:13:07 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.541: INFO: 	Container coredns ready: true, restart count 0
  Apr 24 05:32:43.541: INFO: kube-addon-manager-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.541: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 24 05:32:43.541: INFO: kube-apiserver-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.541: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 24 05:32:43.542: INFO: kube-controller-manager-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.542: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 24 05:32:43.542: INFO: kube-proxy-stmcm from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.542: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 05:32:43.542: INFO: kube-scheduler-aeveeng9ieph-1 from kube-system started at 2023-04-24 03:08:50 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.543: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 24 05:32:43.543: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-9gjts from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 05:32:43.543: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 05:32:43.543: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 24 05:32:43.544: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-2 before test
  Apr 24 05:32:43.560: INFO: cilium-jjltm from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.560: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 05:32:43.560: INFO: cilium-node-init-8k2gd from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.560: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 05:32:43.560: INFO: coredns-5d78c9869d-7w9jr from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.560: INFO: 	Container coredns ready: true, restart count 0
  Apr 24 05:32:43.560: INFO: kube-addon-manager-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.560: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Apr 24 05:32:43.560: INFO: kube-apiserver-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.560: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 24 05:32:43.561: INFO: kube-controller-manager-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.561: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 24 05:32:43.561: INFO: kube-proxy-w69d6 from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.561: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 05:32:43.561: INFO: kube-scheduler-aeveeng9ieph-2 from kube-system started at 2023-04-24 03:09:10 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.561: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 24 05:32:43.561: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-fbwcp from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 05:32:43.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 05:32:43.561: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 24 05:32:43.561: INFO: 
  Logging pods the apiserver thinks is on node aeveeng9ieph-3 before test
  Apr 24 05:32:43.571: INFO: cilium-c6lwz from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.571: INFO: 	Container cilium-agent ready: true, restart count 0
  Apr 24 05:32:43.571: INFO: cilium-node-init-gglwd from kube-system started at 2023-04-24 03:37:50 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.571: INFO: 	Container node-init ready: true, restart count 0
  Apr 24 05:32:43.571: INFO: cilium-operator-85fcfcb8b4-tsmz8 from kube-system started at 2023-04-24 03:37:49 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.571: INFO: 	Container cilium-operator ready: true, restart count 0
  Apr 24 05:32:43.571: INFO: kube-proxy-q5ck5 from kube-system started at 2023-04-24 03:37:51 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.571: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 24 05:32:43.571: INFO: sonobuoy from sonobuoy started at 2023-04-24 03:39:06 +0000 UTC (1 container statuses recorded)
  Apr 24 05:32:43.571: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 24 05:32:43.571: INFO: sonobuoy-e2e-job-32c32a5c56a94f9e from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 05:32:43.571: INFO: 	Container e2e ready: true, restart count 0
  Apr 24 05:32:43.571: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 05:32:43.571: INFO: sonobuoy-systemd-logs-daemon-set-80221b1b79ae4299-c2fm7 from sonobuoy started at 2023-04-24 03:39:15 +0000 UTC (2 container statuses recorded)
  Apr 24 05:32:43.571: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 24 05:32:43.571: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/24/23 05:32:43.571
  E0424 05:32:44.187926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:45.189301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/24/23 05:32:45.611
  STEP: Trying to apply a random label on the found node. @ 04/24/23 05:32:45.647
  STEP: verifying the node has the label kubernetes.io/e2e-7bfd2610-80e0-4b3d-9816-08c0d6ade8c3 42 @ 04/24/23 05:32:45.683
  STEP: Trying to relaunch the pod, now with labels. @ 04/24/23 05:32:45.69
  E0424 05:32:46.189739      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:47.190696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-7bfd2610-80e0-4b3d-9816-08c0d6ade8c3 off the node aeveeng9ieph-3 @ 04/24/23 05:32:47.73
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-7bfd2610-80e0-4b3d-9816-08c0d6ade8c3 @ 04/24/23 05:32:47.75
  Apr 24 05:32:47.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2114" for this suite. @ 04/24/23 05:32:47.85
• [4.411 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/24/23 05:32:47.872
  Apr 24 05:32:47.873: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/24/23 05:32:47.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:32:47.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:32:47.943
  STEP: getting /apis @ 04/24/23 05:32:47.957
  STEP: getting /apis/storage.k8s.io @ 04/24/23 05:32:47.964
  STEP: getting /apis/storage.k8s.io/v1 @ 04/24/23 05:32:47.966
  STEP: creating @ 04/24/23 05:32:47.969
  STEP: watching @ 04/24/23 05:32:48.06
  Apr 24 05:32:48.060: INFO: starting watch
  STEP: getting @ 04/24/23 05:32:48.14
  STEP: listing in namespace @ 04/24/23 05:32:48.145
  STEP: listing across namespaces @ 04/24/23 05:32:48.151
  STEP: patching @ 04/24/23 05:32:48.158
  STEP: updating @ 04/24/23 05:32:48.175
  Apr 24 05:32:48.181: INFO: waiting for watch events with expected annotations in namespace
  Apr 24 05:32:48.181: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/24/23 05:32:48.182
  E0424 05:32:48.191154      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting a collection @ 04/24/23 05:32:48.199
  Apr 24 05:32:48.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-8006" for this suite. @ 04/24/23 05:32:48.237
• [0.374 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/24/23 05:32:48.262
  Apr 24 05:32:48.262: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replicaset @ 04/24/23 05:32:48.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:32:48.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:32:48.294
  Apr 24 05:32:48.324: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0424 05:32:49.191902      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:50.192040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:51.192599      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:52.193201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:53.193308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:53.330: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/24/23 05:32:53.33
  STEP: Scaling up "test-rs" replicaset  @ 04/24/23 05:32:53.33
  Apr 24 05:32:53.369: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/24/23 05:32:53.37
  W0424 05:32:53.408015      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 24 05:32:53.419: INFO: observed ReplicaSet test-rs in namespace replicaset-7245 with ReadyReplicas 1, AvailableReplicas 1
  Apr 24 05:32:53.457: INFO: observed ReplicaSet test-rs in namespace replicaset-7245 with ReadyReplicas 1, AvailableReplicas 1
  Apr 24 05:32:53.535: INFO: observed ReplicaSet test-rs in namespace replicaset-7245 with ReadyReplicas 1, AvailableReplicas 1
  Apr 24 05:32:53.564: INFO: observed ReplicaSet test-rs in namespace replicaset-7245 with ReadyReplicas 1, AvailableReplicas 1
  E0424 05:32:54.194472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:55.029: INFO: observed ReplicaSet test-rs in namespace replicaset-7245 with ReadyReplicas 2, AvailableReplicas 2
  E0424 05:32:55.194899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:32:55.714: INFO: observed Replicaset test-rs in namespace replicaset-7245 with ReadyReplicas 3 found true
  Apr 24 05:32:55.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7245" for this suite. @ 04/24/23 05:32:55.72
• [7.467 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/24/23 05:32:55.73
  Apr 24 05:32:55.730: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename container-runtime @ 04/24/23 05:32:55.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:32:55.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:32:55.77
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/24/23 05:32:55.808
  E0424 05:32:56.195944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:57.196179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:58.197122      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:32:59.197294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:00.197379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:01.197539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:02.198362      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:03.198616      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:04.199434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:05.199375      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:06.199670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:07.200124      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:08.200970      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:09.200987      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:10.202061      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:11.202521      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:12.203255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:13.203324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/24/23 05:33:14.042
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/24/23 05:33:14.046
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/24/23 05:33:14.058
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/24/23 05:33:14.059
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/24/23 05:33:14.106
  E0424 05:33:14.204207      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:15.204836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:16.205349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:17.206128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/24/23 05:33:18.144
  E0424 05:33:18.206694      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/24/23 05:33:19.161
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/24/23 05:33:19.173
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/24/23 05:33:19.173
  E0424 05:33:19.207336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/24/23 05:33:19.208
  E0424 05:33:20.208313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/24/23 05:33:20.229
  E0424 05:33:21.208534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:22.208763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:23.208879      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/24/23 05:33:23.272
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/24/23 05:33:23.284
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/24/23 05:33:23.284
  Apr 24 05:33:23.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9599" for this suite. @ 04/24/23 05:33:23.328
• [27.610 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/24/23 05:33:23.341
  Apr 24 05:33:23.341: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename secrets @ 04/24/23 05:33:23.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:33:23.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:33:23.372
  STEP: Creating secret with name secret-test-23deb028-f1d8-4079-8b57-d5c3c95af1d0 @ 04/24/23 05:33:23.408
  STEP: Creating a pod to test consume secrets @ 04/24/23 05:33:23.413
  E0424 05:33:24.209076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:25.209308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:26.209534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:27.210332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:33:27.447
  Apr 24 05:33:27.453: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-secrets-c31fe758-5488-4e92-b86b-3a1aef6ad630 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 05:33:27.477
  Apr 24 05:33:27.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6393" for this suite. @ 04/24/23 05:33:27.512
  STEP: Destroying namespace "secret-namespace-3520" for this suite. @ 04/24/23 05:33:27.523
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/24/23 05:33:27.538
  Apr 24 05:33:27.538: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 05:33:27.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:33:27.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:33:27.573
  STEP: fetching services @ 04/24/23 05:33:27.58
  Apr 24 05:33:27.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8879" for this suite. @ 04/24/23 05:33:27.591
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/24/23 05:33:27.605
  Apr 24 05:33:27.605: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replicaset @ 04/24/23 05:33:27.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:33:27.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:33:27.63
  STEP: Create a Replicaset @ 04/24/23 05:33:27.639
  STEP: Verify that the required pods have come up. @ 04/24/23 05:33:27.647
  Apr 24 05:33:27.652: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0424 05:33:28.210592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:29.210840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:30.211244      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:31.211453      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:32.211733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:33:32.658: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/24/23 05:33:32.658
  STEP: Getting /status @ 04/24/23 05:33:32.658
  Apr 24 05:33:32.667: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/24/23 05:33:32.668
  Apr 24 05:33:32.692: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/24/23 05:33:32.692
  Apr 24 05:33:32.698: INFO: Observed &ReplicaSet event: ADDED
  Apr 24 05:33:32.698: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 24 05:33:32.699: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 24 05:33:32.699: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 24 05:33:32.699: INFO: Found replicaset test-rs in namespace replicaset-2892 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 24 05:33:32.699: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/24/23 05:33:32.699
  Apr 24 05:33:32.699: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 24 05:33:32.712: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/24/23 05:33:32.712
  Apr 24 05:33:32.718: INFO: Observed &ReplicaSet event: ADDED
  Apr 24 05:33:32.718: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 24 05:33:32.719: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 24 05:33:32.719: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 24 05:33:32.719: INFO: Observed replicaset test-rs in namespace replicaset-2892 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 24 05:33:32.719: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 24 05:33:32.719: INFO: Found replicaset test-rs in namespace replicaset-2892 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 24 05:33:32.720: INFO: Replicaset test-rs has a patched status
  Apr 24 05:33:32.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2892" for this suite. @ 04/24/23 05:33:32.73
• [5.139 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/24/23 05:33:32.746
  Apr 24 05:33:32.746: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename kubectl-logs @ 04/24/23 05:33:32.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:33:32.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:33:32.793
  STEP: creating an pod @ 04/24/23 05:33:32.8
  Apr 24 05:33:32.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-logs-7104 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Apr 24 05:33:33.034: INFO: stderr: ""
  Apr 24 05:33:33.034: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/24/23 05:33:33.034
  Apr 24 05:33:33.034: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0424 05:33:33.212840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:34.213223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:35.213718      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:36.213847      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:33:37.067: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/24/23 05:33:37.067
  Apr 24 05:33:37.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-logs-7104 logs logs-generator logs-generator'
  E0424 05:33:37.215241      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:33:37.371: INFO: stderr: ""
  Apr 24 05:33:37.371: INFO: stdout: "I0424 05:33:34.769068       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/k6v 425\nI0424 05:33:34.969155       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/rhgm 364\nI0424 05:33:35.169565       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/s89 222\nI0424 05:33:35.368981       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/n97 392\nI0424 05:33:35.569345       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/qbnn 510\nI0424 05:33:35.768755       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/ksh 398\nI0424 05:33:35.969137       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/zqj 215\nI0424 05:33:36.169560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/lx9c 201\nI0424 05:33:36.368756       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/vs79 434\nI0424 05:33:36.569384       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/bjhc 525\nI0424 05:33:36.768752       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/vgg 496\nI0424 05:33:36.969309       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/qc7 396\nI0424 05:33:37.168882       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/4v8 258\n"
  STEP: limiting log lines @ 04/24/23 05:33:37.371
  Apr 24 05:33:37.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-logs-7104 logs logs-generator logs-generator --tail=1'
  Apr 24 05:33:37.540: INFO: stderr: ""
  Apr 24 05:33:37.540: INFO: stdout: "I0424 05:33:37.369065       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/bjkn 567\n"
  Apr 24 05:33:37.540: INFO: got output "I0424 05:33:37.369065       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/bjkn 567\n"
  STEP: limiting log bytes @ 04/24/23 05:33:37.54
  Apr 24 05:33:37.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-logs-7104 logs logs-generator logs-generator --limit-bytes=1'
  Apr 24 05:33:37.690: INFO: stderr: ""
  Apr 24 05:33:37.690: INFO: stdout: "I"
  Apr 24 05:33:37.690: INFO: got output "I"
  STEP: exposing timestamps @ 04/24/23 05:33:37.692
  Apr 24 05:33:37.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-logs-7104 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 24 05:33:37.900: INFO: stderr: ""
  Apr 24 05:33:37.900: INFO: stdout: "2023-04-24T05:33:37.769137818Z I0424 05:33:37.769068       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/642 410\n"
  Apr 24 05:33:37.900: INFO: got output "2023-04-24T05:33:37.769137818Z I0424 05:33:37.769068       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/642 410\n"
  STEP: restricting to a time range @ 04/24/23 05:33:37.9
  E0424 05:33:38.216100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:39.216669      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:40.217146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:33:40.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-logs-7104 logs logs-generator logs-generator --since=1s'
  Apr 24 05:33:40.552: INFO: stderr: ""
  Apr 24 05:33:40.552: INFO: stdout: "I0424 05:33:39.569552       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/l97t 491\nI0424 05:33:39.768834       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/vkp 314\nI0424 05:33:39.969234       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/nx8n 224\nI0424 05:33:40.171123       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/wfxz 452\nI0424 05:33:40.369621       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/t6b 278\n"
  Apr 24 05:33:40.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-logs-7104 logs logs-generator logs-generator --since=24h'
  Apr 24 05:33:41.061: INFO: stderr: ""
  Apr 24 05:33:41.061: INFO: stdout: "I0424 05:33:34.769068       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/k6v 425\nI0424 05:33:34.969155       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/rhgm 364\nI0424 05:33:35.169565       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/s89 222\nI0424 05:33:35.368981       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/n97 392\nI0424 05:33:35.569345       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/qbnn 510\nI0424 05:33:35.768755       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/ksh 398\nI0424 05:33:35.969137       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/zqj 215\nI0424 05:33:36.169560       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/lx9c 201\nI0424 05:33:36.368756       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/vs79 434\nI0424 05:33:36.569384       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/bjhc 525\nI0424 05:33:36.768752       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/vgg 496\nI0424 05:33:36.969309       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/qc7 396\nI0424 05:33:37.168882       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/4v8 258\nI0424 05:33:37.369065       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/bjkn 567\nI0424 05:33:37.569630       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/dmpr 513\nI0424 05:33:37.769068       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/642 410\nI0424 05:33:37.969477       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/vds 375\nI0424 05:33:38.168754       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/4qf 321\nI0424 05:33:38.369263       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/2j8q 301\nI0424 05:33:38.568802       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/p9d 422\nI0424 05:33:38.769279       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/grk 336\nI0424 05:33:38.968718       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/w5n2 574\nI0424 05:33:39.168927       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/ghk9 466\nI0424 05:33:39.369250       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/snl 451\nI0424 05:33:39.569552       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/l97t 491\nI0424 05:33:39.768834       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/vkp 314\nI0424 05:33:39.969234       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/nx8n 224\nI0424 05:33:40.171123       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/wfxz 452\nI0424 05:33:40.369621       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/t6b 278\nI0424 05:33:40.569130       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/n5bh 379\nI0424 05:33:40.769608       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/xgh 218\nI0424 05:33:40.969101       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/w2d 333\n"
  Apr 24 05:33:41.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=kubectl-logs-7104 delete pod logs-generator'
  E0424 05:33:41.217893      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:33:42.056: INFO: stderr: ""
  Apr 24 05:33:42.056: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 24 05:33:42.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-7104" for this suite. @ 04/24/23 05:33:42.066
• [9.334 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/24/23 05:33:42.083
  Apr 24 05:33:42.083: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 05:33:42.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:33:42.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:33:42.137
  STEP: Creating secret with name projected-secret-test-f822e24f-dd9e-4626-a19d-b13cb386701e @ 04/24/23 05:33:42.14
  STEP: Creating a pod to test consume secrets @ 04/24/23 05:33:42.147
  E0424 05:33:42.218736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:43.219765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:44.219803      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:45.220799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:33:46.183
  Apr 24 05:33:46.189: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-secrets-a6823bf1-930e-49eb-951b-3707fe1b3783 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/24/23 05:33:46.204
  E0424 05:33:46.221388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:33:46.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8318" for this suite. @ 04/24/23 05:33:46.242
• [4.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/24/23 05:33:46.26
  Apr 24 05:33:46.260: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/24/23 05:33:46.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:33:46.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:33:46.289
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/24/23 05:33:46.293
  Apr 24 05:33:46.294: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 05:33:47.221792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:48.222457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:33:48.312: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  E0424 05:33:49.222567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:50.223163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:51.223897      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:52.224182      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:53.224502      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:54.224827      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:55.225793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:56.226513      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:33:56.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9897" for this suite. @ 04/24/23 05:33:56.379
• [10.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/24/23 05:33:56.473
  Apr 24 05:33:56.473: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 05:33:56.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:33:56.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:33:56.538
  STEP: Counting existing ResourceQuota @ 04/24/23 05:33:56.543
  E0424 05:33:57.226675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:58.227793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:33:59.228180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:00.228830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:01.229266      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/24/23 05:34:01.549
  STEP: Ensuring resource quota status is calculated @ 04/24/23 05:34:01.561
  E0424 05:34:02.229757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:03.230380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 04/24/23 05:34:03.571
  STEP: Creating a NodePort Service @ 04/24/23 05:34:03.593
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/24/23 05:34:03.638
  STEP: Ensuring resource quota status captures service creation @ 04/24/23 05:34:03.666
  E0424 05:34:04.230156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:05.230642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 04/24/23 05:34:05.674
  STEP: Ensuring resource quota status released usage @ 04/24/23 05:34:05.792
  E0424 05:34:06.231319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:07.231308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:07.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8266" for this suite. @ 04/24/23 05:34:07.806
• [11.343 seconds]
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/24/23 05:34:07.819
  Apr 24 05:34:07.819: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename replicaset @ 04/24/23 05:34:07.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:34:07.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:34:07.855
  STEP: Create a ReplicaSet @ 04/24/23 05:34:07.859
  STEP: Verify that the required pods have come up @ 04/24/23 05:34:07.868
  Apr 24 05:34:07.884: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0424 05:34:08.232399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:09.233287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:10.234370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:11.234711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:12.234925      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:12.896: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/24/23 05:34:12.896
  Apr 24 05:34:12.903: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/24/23 05:34:12.904
  STEP: DeleteCollection of the ReplicaSets @ 04/24/23 05:34:12.909
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/24/23 05:34:12.921
  Apr 24 05:34:12.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9698" for this suite. @ 04/24/23 05:34:12.933
• [5.122 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/24/23 05:34:12.942
  Apr 24 05:34:12.942: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename subpath @ 04/24/23 05:34:12.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:34:12.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:34:13.002
  STEP: Setting up data @ 04/24/23 05:34:13.008
  STEP: Creating pod pod-subpath-test-configmap-z5lm @ 04/24/23 05:34:13.025
  STEP: Creating a pod to test atomic-volume-subpath @ 04/24/23 05:34:13.025
  E0424 05:34:13.235421      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:14.236360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:15.238225      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:16.238420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:17.239687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:18.239988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:19.241011      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:20.241609      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:21.241748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:22.242054      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:23.243080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:24.245254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:25.243547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:26.244091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:27.244325      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:28.245250      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:29.245938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:30.246193      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:31.246203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:32.247036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:33.247698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:34.248292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:35.248584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:36.249320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:34:37.171
  Apr 24 05:34:37.175: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-subpath-test-configmap-z5lm container test-container-subpath-configmap-z5lm: <nil>
  STEP: delete the pod @ 04/24/23 05:34:37.195
  STEP: Deleting pod pod-subpath-test-configmap-z5lm @ 04/24/23 05:34:37.221
  Apr 24 05:34:37.221: INFO: Deleting pod "pod-subpath-test-configmap-z5lm" in namespace "subpath-4238"
  Apr 24 05:34:37.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4238" for this suite. @ 04/24/23 05:34:37.234
• [24.302 seconds]
------------------------------
SSSSSSSSSSS  E0424 05:34:37.249432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/24/23 05:34:37.251
  Apr 24 05:34:37.251: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename emptydir @ 04/24/23 05:34:37.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:34:37.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:34:37.29
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/24/23 05:34:37.292
  E0424 05:34:38.249696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:39.250623      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:40.250536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:41.251511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:34:41.321
  Apr 24 05:34:41.327: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-a6a25dca-0d77-4eca-a1ad-d1e564da40bc container test-container: <nil>
  STEP: delete the pod @ 04/24/23 05:34:41.339
  Apr 24 05:34:41.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8353" for this suite. @ 04/24/23 05:34:41.365
• [4.122 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/24/23 05:34:41.374
  Apr 24 05:34:41.374: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename svcaccounts @ 04/24/23 05:34:41.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:34:41.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:34:41.404
  STEP: Creating ServiceAccount "e2e-sa-lvnjb"  @ 04/24/23 05:34:41.407
  Apr 24 05:34:41.413: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-lvnjb"  @ 04/24/23 05:34:41.413
  Apr 24 05:34:41.427: INFO: AutomountServiceAccountToken: true
  Apr 24 05:34:41.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4161" for this suite. @ 04/24/23 05:34:41.433
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/24/23 05:34:41.446
  Apr 24 05:34:41.446: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 05:34:41.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:34:41.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:34:41.51
  STEP: Creating configMap with name projected-configmap-test-volume-map-4c5f9116-8268-42da-a3bd-de7185622316 @ 04/24/23 05:34:41.514
  STEP: Creating a pod to test consume configMaps @ 04/24/23 05:34:41.52
  E0424 05:34:42.252823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:43.252871      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:44.252991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:45.253107      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:34:45.564
  Apr 24 05:34:45.571: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-configmaps-8d9db1c2-4bbc-4b57-8a41-c2ab7246d4d6 container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 05:34:45.583
  Apr 24 05:34:45.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5925" for this suite. @ 04/24/23 05:34:45.616
• [4.179 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/24/23 05:34:45.629
  Apr 24 05:34:45.629: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename gc @ 04/24/23 05:34:45.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:34:45.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:34:45.664
  STEP: create the rc @ 04/24/23 05:34:45.673
  W0424 05:34:45.681335      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0424 05:34:46.253428      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:47.319296      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:48.299277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:49.300301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/24/23 05:34:49.819
  STEP: wait for the rc to be deleted @ 04/24/23 05:34:49.964
  E0424 05:34:50.300962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:34:51.539238      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:51.743: INFO: 83 pods remaining
  Apr 24 05:34:51.743: INFO: 77 pods has nil DeletionTimestamp
  Apr 24 05:34:51.743: INFO: 
  E0424 05:34:52.539687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:52.560: INFO: 75 pods remaining
  Apr 24 05:34:52.560: INFO: 68 pods has nil DeletionTimestamp
  Apr 24 05:34:52.560: INFO: 
  Apr 24 05:34:53.278: INFO: 73 pods remaining
  Apr 24 05:34:53.279: INFO: 64 pods has nil DeletionTimestamp
  Apr 24 05:34:53.280: INFO: 
  E0424 05:34:53.540492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:54.171: INFO: 65 pods remaining
  Apr 24 05:34:54.172: INFO: 52 pods has nil DeletionTimestamp
  Apr 24 05:34:54.172: INFO: 
  E0424 05:34:54.540795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:55.050: INFO: 53 pods remaining
  Apr 24 05:34:55.050: INFO: 29 pods has nil DeletionTimestamp
  Apr 24 05:34:55.050: INFO: 
  E0424 05:34:55.541204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:56.272: INFO: 46 pods remaining
  Apr 24 05:34:56.273: INFO: 14 pods has nil DeletionTimestamp
  Apr 24 05:34:56.273: INFO: 
  E0424 05:34:56.541677      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:57.452: INFO: 40 pods remaining
  Apr 24 05:34:57.452: INFO: 2 pods has nil DeletionTimestamp
  Apr 24 05:34:57.452: INFO: 
  E0424 05:34:57.543413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:58.221: INFO: 33 pods remaining
  Apr 24 05:34:58.222: INFO: 0 pods has nil DeletionTimestamp
  Apr 24 05:34:58.222: INFO: 
  E0424 05:34:58.544379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:34:59.048: INFO: 26 pods remaining
  Apr 24 05:34:59.048: INFO: 0 pods has nil DeletionTimestamp
  Apr 24 05:34:59.048: INFO: 
  E0424 05:34:59.544693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:35:00.256: INFO: 19 pods remaining
  Apr 24 05:35:00.256: INFO: 0 pods has nil DeletionTimestamp
  Apr 24 05:35:00.256: INFO: 
  E0424 05:35:00.545015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:35:01.060: INFO: 15 pods remaining
  Apr 24 05:35:01.060: INFO: 0 pods has nil DeletionTimestamp
  Apr 24 05:35:01.060: INFO: 
  E0424 05:35:01.545369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:35:02.010: INFO: 9 pods remaining
  Apr 24 05:35:02.010: INFO: 0 pods has nil DeletionTimestamp
  Apr 24 05:35:02.010: INFO: 
  E0424 05:35:02.545417      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:35:03.007: INFO: 2 pods remaining
  Apr 24 05:35:03.007: INFO: 0 pods has nil DeletionTimestamp
  Apr 24 05:35:03.007: INFO: 
  E0424 05:35:03.546250      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/24/23 05:35:04.014
  E0424 05:35:04.546534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:35:05.049: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 24 05:35:05.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4730" for this suite. @ 04/24/23 05:35:05.067
• [19.541 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/24/23 05:35:05.174
  Apr 24 05:35:05.174: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 05:35:05.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:35:05.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:35:05.299
  Apr 24 05:35:05.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5033" for this suite. @ 04/24/23 05:35:05.33
• [0.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/24/23 05:35:05.369
  Apr 24 05:35:05.369: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/24/23 05:35:05.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:35:05.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:35:05.407
  STEP: mirroring a new custom Endpoint @ 04/24/23 05:35:05.523
  E0424 05:35:05.547546      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:35:05.587: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0424 05:35:06.548623      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:07.549283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 04/24/23 05:35:07.594
  Apr 24 05:35:07.639: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0424 05:35:08.549966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:09.550278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 04/24/23 05:35:09.643
  Apr 24 05:35:09.666: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0424 05:35:10.551283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:11.551404      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:35:11.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-1023" for this suite. @ 04/24/23 05:35:11.681
• [6.323 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/24/23 05:35:11.693
  Apr 24 05:35:11.693: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename server-version @ 04/24/23 05:35:11.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:35:11.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:35:11.737
  STEP: Request ServerVersion @ 04/24/23 05:35:11.744
  STEP: Confirm major version @ 04/24/23 05:35:11.746
  Apr 24 05:35:11.746: INFO: Major version: 1
  STEP: Confirm minor version @ 04/24/23 05:35:11.746
  Apr 24 05:35:11.746: INFO: cleanMinorVersion: 27
  Apr 24 05:35:11.746: INFO: Minor version: 27
  Apr 24 05:35:11.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-9853" for this suite. @ 04/24/23 05:35:11.753
• [0.073 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/24/23 05:35:11.767
  Apr 24 05:35:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename resourcequota @ 04/24/23 05:35:11.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:35:11.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:35:11.822
  STEP: Counting existing ResourceQuota @ 04/24/23 05:35:11.827
  E0424 05:35:12.551702      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:13.552022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:14.552208      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:15.552656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:16.556692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/24/23 05:35:16.834
  STEP: Ensuring resource quota status is calculated @ 04/24/23 05:35:16.93
  E0424 05:35:17.579173      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:18.557288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:35:18.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-331" for this suite. @ 04/24/23 05:35:19.128
• [7.398 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/24/23 05:35:19.176
  Apr 24 05:35:19.176: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename svcaccounts @ 04/24/23 05:35:19.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:35:19.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:35:19.443
  E0424 05:35:19.567619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:20.577427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:21.569524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:22.582150      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:23.574348      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:24.574747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:25.575664      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:26.578429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:27.579267      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:28.579683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:29.579846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:30.580377      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:31.580496      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:32.580656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:33.581251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:34.582047      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:35.582707      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:36.583145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:37.583770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:38.584185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:39.584753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:40.585582      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:41.586287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:42.586554      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:43.586992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:44.587108      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:45.587187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:46.587283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:47.589649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/24/23 05:35:47.666
  Apr 24 05:35:47.666: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9642 pod-service-account-f4096049-2e45-4290-81bd-f60ec720409f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 04/24/23 05:35:48.017
  Apr 24 05:35:48.018: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9642 pod-service-account-f4096049-2e45-4290-81bd-f60ec720409f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 04/24/23 05:35:48.289
  Apr 24 05:35:48.290: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9642 pod-service-account-f4096049-2e45-4290-81bd-f60ec720409f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Apr 24 05:35:48.580: INFO: Got root ca configmap in namespace "svcaccounts-9642"
  Apr 24 05:35:48.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9642" for this suite. @ 04/24/23 05:35:48.588
  E0424 05:35:48.589216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
• [29.423 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/24/23 05:35:48.599
  Apr 24 05:35:48.599: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename projected @ 04/24/23 05:35:48.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:35:48.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:35:48.629
  STEP: Creating configMap with name projected-configmap-test-volume-7108c144-a6c8-4b3f-82cb-29813e42b13a @ 04/24/23 05:35:48.632
  STEP: Creating a pod to test consume configMaps @ 04/24/23 05:35:48.637
  E0424 05:35:49.589788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:50.590169      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:51.591075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:52.592509      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:53.593003      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:54.593588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:55.593807      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:56.597507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/24/23 05:35:56.726
  Apr 24 05:35:56.732: INFO: Trying to get logs from node aeveeng9ieph-3 pod pod-projected-configmaps-cfda3cef-abb2-4f6e-8f71-ec75479cc3cf container agnhost-container: <nil>
  STEP: delete the pod @ 04/24/23 05:35:57.086
  Apr 24 05:35:57.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7023" for this suite. @ 04/24/23 05:35:57.123
• [8.539 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/24/23 05:35:57.143
  Apr 24 05:35:57.144: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename downward-api @ 04/24/23 05:35:57.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:35:57.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:35:57.175
  STEP: Creating the pod @ 04/24/23 05:35:57.178
  E0424 05:35:57.598647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:58.598736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:35:59.598833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:00.599331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:01.599339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:36:01.760: INFO: Successfully updated pod "labelsupdated7f3b538-cfbb-49dc-acd8-7805b4bc3256"
  E0424 05:36:02.599866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:03.599884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:36:03.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6715" for this suite. @ 04/24/23 05:36:03.796
• [6.663 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/24/23 05:36:03.809
  Apr 24 05:36:03.809: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename podtemplate @ 04/24/23 05:36:03.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:36:03.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:36:03.847
  STEP: Create set of pod templates @ 04/24/23 05:36:03.851
  Apr 24 05:36:03.859: INFO: created test-podtemplate-1
  Apr 24 05:36:03.868: INFO: created test-podtemplate-2
  Apr 24 05:36:03.876: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/24/23 05:36:03.876
  STEP: delete collection of pod templates @ 04/24/23 05:36:03.883
  Apr 24 05:36:03.883: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/24/23 05:36:03.914
  Apr 24 05:36:03.914: INFO: requesting list of pod templates to confirm quantity
  Apr 24 05:36:03.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2613" for this suite. @ 04/24/23 05:36:03.924
• [0.123 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/24/23 05:36:03.935
  Apr 24 05:36:03.935: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename services @ 04/24/23 05:36:03.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:36:03.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:36:03.983
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-7514 @ 04/24/23 05:36:03.986
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/24/23 05:36:04.013
  STEP: creating service externalsvc in namespace services-7514 @ 04/24/23 05:36:04.013
  STEP: creating replication controller externalsvc in namespace services-7514 @ 04/24/23 05:36:04.074
  I0424 05:36:04.082896      14 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7514, replica count: 2
  E0424 05:36:04.600019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:05.600861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:06.601231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0424 05:36:07.134797      14 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/24/23 05:36:07.141
  Apr 24 05:36:07.166: INFO: Creating new exec pod
  E0424 05:36:07.601939      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:08.602195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:09.602865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:10.603962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:36:11.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-358926346 --namespace=services-7514 exec execpod7v2mr -- /bin/sh -x -c nslookup nodeport-service.services-7514.svc.cluster.local'
  E0424 05:36:11.603899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:36:11.688: INFO: stderr: "+ nslookup nodeport-service.services-7514.svc.cluster.local\n"
  Apr 24 05:36:11.688: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-7514.svc.cluster.local\tcanonical name = externalsvc.services-7514.svc.cluster.local.\nName:\texternalsvc.services-7514.svc.cluster.local\nAddress: 10.233.14.129\n\n"
  Apr 24 05:36:11.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7514, will wait for the garbage collector to delete the pods @ 04/24/23 05:36:11.696
  Apr 24 05:36:11.760: INFO: Deleting ReplicationController externalsvc took: 9.013066ms
  Apr 24 05:36:11.860: INFO: Terminating ReplicationController externalsvc pods took: 100.502686ms
  E0424 05:36:12.604391      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:13.604981      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 24 05:36:14.500: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-7514" for this suite. @ 04/24/23 05:36:14.523
• [10.603 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/24/23 05:36:14.545
  Apr 24 05:36:14.545: INFO: >>> kubeConfig: /tmp/kubeconfig-358926346
  STEP: Building a namespace api object, basename job @ 04/24/23 05:36:14.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/24/23 05:36:14.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/24/23 05:36:14.579
  STEP: Creating a job @ 04/24/23 05:36:14.584
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/24/23 05:36:14.594
  E0424 05:36:14.606010      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:15.637572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:16.606556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:17.607283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0424 05:36:18.607303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 04/24/23 05:36:18.607
  STEP: updating /status @ 04/24/23 05:36:18.625
  STEP: get /status @ 04/24/23 05:36:18.641
  Apr 24 05:36:18.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9870" for this suite. @ 04/24/23 05:36:18.659
• [4.139 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 24 05:36:18.698: INFO: Running AfterSuite actions on node 1
  Apr 24 05:36:18.699: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.001 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.085 seconds]
------------------------------

Ran 378 of 7207 Specs in 6981.421 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h56m22.392214125s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

