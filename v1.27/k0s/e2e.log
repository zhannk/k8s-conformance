  I0508 11:39:55.805998      23 e2e.go:117] Starting e2e run "928fc166-9554-4a46-90b9-1821fb86d69c" on Ginkgo node 1
  May  8 11:39:55.832: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683545995 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May  8 11:39:56.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:39:56.002: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May  8 11:39:56.025: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May  8 11:39:56.028: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
  May  8 11:39:56.028: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  May  8 11:39:56.028: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-router' (0 seconds elapsed)
  May  8 11:39:56.028: INFO: e2e test version: v1.27.1
  May  8 11:39:56.029: INFO: kube-apiserver version: v1.27.1+k0s
  May  8 11:39:56.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:39:56.031: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.030 seconds]
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/08/23 11:39:56.315
  May  8 11:39:56.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/08/23 11:39:56.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:39:56.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:39:56.327
  STEP: mirroring a new custom Endpoint @ 05/08/23 11:39:56.341
  May  8 11:39:56.347: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 05/08/23 11:39:58.35
  May  8 11:39:58.355: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 05/08/23 11:40:00.359
  May  8 11:40:00.365: INFO: Waiting for 0 EndpointSlices to exist, got 1
  May  8 11:40:02.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-7457" for this suite. @ 05/08/23 11:40:02.37
• [6.060 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/08/23 11:40:02.376
  May  8 11:40:02.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 11:40:02.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:40:02.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:40:02.386
  STEP: Creating secret with name secret-test-99a2a080-9a6d-4c3e-9d12-05faad2f61ac @ 05/08/23 11:40:02.4
  STEP: Creating a pod to test consume secrets @ 05/08/23 11:40:02.403
  STEP: Saw pod success @ 05/08/23 11:40:12.428
  May  8 11:40:12.430: INFO: Trying to get logs from node worker-0 pod pod-secrets-b6a412aa-bab5-4658-b36a-f310f2ffdff0 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 11:40:12.444
  May  8 11:40:12.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1100" for this suite. @ 05/08/23 11:40:12.456
  STEP: Destroying namespace "secret-namespace-1876" for this suite. @ 05/08/23 11:40:12.46
• [10.089 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/08/23 11:40:12.465
  May  8 11:40:12.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename security-context-test @ 05/08/23 11:40:12.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:40:12.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:40:12.476
  May  8 11:40:16.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6430" for this suite. @ 05/08/23 11:40:16.504
• [4.048 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/08/23 11:40:16.513
  May  8 11:40:16.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 11:40:16.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:40:16.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:40:16.524
  STEP: Counting existing ResourceQuota @ 05/08/23 11:40:16.526
  STEP: Creating a ResourceQuota @ 05/08/23 11:40:21.529
  STEP: Ensuring resource quota status is calculated @ 05/08/23 11:40:21.533
  STEP: Creating a Pod that fits quota @ 05/08/23 11:40:23.537
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/08/23 11:40:23.549
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/08/23 11:40:25.552
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/08/23 11:40:25.553
  STEP: Ensuring a pod cannot update its resource requirements @ 05/08/23 11:40:25.555
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/08/23 11:40:25.558
  STEP: Deleting the pod @ 05/08/23 11:40:27.562
  STEP: Ensuring resource quota status released the pod usage @ 05/08/23 11:40:27.568
  May  8 11:40:29.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5524" for this suite. @ 05/08/23 11:40:29.572
• [13.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/08/23 11:40:29.579
  May  8 11:40:29.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename svcaccounts @ 05/08/23 11:40:29.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:40:29.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:40:29.59
  May  8 11:40:29.601: INFO: created pod
  STEP: Saw pod success @ 05/08/23 11:40:33.613
  May  8 11:41:03.615: INFO: polling logs
  May  8 11:41:03.622: INFO: Pod logs: 
  I0508 11:40:30.152922       1 log.go:198] OK: Got token
  I0508 11:40:30.152972       1 log.go:198] validating with in-cluster discovery
  I0508 11:40:30.153285       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0508 11:40:30.153315       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4915:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683546629, NotBefore:1683546029, IssuedAt:1683546029, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4915", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4dd8a89f-3ad4-4cb0-8438-11e4957b0a24"}}}
  I0508 11:40:30.161515       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0508 11:40:30.162784       1 log.go:198] OK: Validated signature on JWT
  I0508 11:40:30.162864       1 log.go:198] OK: Got valid claims from token!
  I0508 11:40:30.162985       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4915:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683546629, NotBefore:1683546029, IssuedAt:1683546029, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4915", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4dd8a89f-3ad4-4cb0-8438-11e4957b0a24"}}}

  May  8 11:41:03.622: INFO: completed pod
  May  8 11:41:03.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4915" for this suite. @ 05/08/23 11:41:03.628
• [34.053 seconds]
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/08/23 11:41:03.633
  May  8 11:41:03.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename endpointslice @ 05/08/23 11:41:03.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:41:03.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:41:03.645
  STEP: getting /apis @ 05/08/23 11:41:03.648
  STEP: getting /apis/discovery.k8s.io @ 05/08/23 11:41:03.652
  STEP: getting /apis/discovery.k8s.iov1 @ 05/08/23 11:41:03.653
  STEP: creating @ 05/08/23 11:41:03.655
  STEP: getting @ 05/08/23 11:41:03.665
  STEP: listing @ 05/08/23 11:41:03.671
  STEP: watching @ 05/08/23 11:41:03.673
  May  8 11:41:03.673: INFO: starting watch
  STEP: cluster-wide listing @ 05/08/23 11:41:03.674
  STEP: cluster-wide watching @ 05/08/23 11:41:03.676
  May  8 11:41:03.676: INFO: starting watch
  STEP: patching @ 05/08/23 11:41:03.677
  STEP: updating @ 05/08/23 11:41:03.681
  May  8 11:41:03.685: INFO: waiting for watch events with expected annotations
  May  8 11:41:03.685: INFO: saw patched and updated annotations
  STEP: deleting @ 05/08/23 11:41:03.685
  STEP: deleting a collection @ 05/08/23 11:41:03.691
  May  8 11:41:03.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1056" for this suite. @ 05/08/23 11:41:03.701
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/08/23 11:41:03.71
  May  8 11:41:03.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename dns @ 05/08/23 11:41:03.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:41:03.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:41:03.721
  STEP: Creating a test headless service @ 05/08/23 11:41:03.724
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4918.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4918.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local;sleep 1; done
   @ 05/08/23 11:41:03.727
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4918.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4918.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local;sleep 1; done
   @ 05/08/23 11:41:03.727
  STEP: creating a pod to probe DNS @ 05/08/23 11:41:03.727
  STEP: submitting the pod to kubernetes @ 05/08/23 11:41:03.727
  STEP: retrieving the pod @ 05/08/23 11:41:11.755
  STEP: looking for the results for each expected name from probers @ 05/08/23 11:41:11.757
  May  8 11:41:11.762: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:11.764: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:11.767: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:11.769: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:11.772: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:11.774: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:11.777: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:11.779: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:11.779: INFO: Lookups using dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local]

  May  8 11:41:16.785: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:16.787: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:16.790: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:16.792: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:16.795: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:16.797: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:16.799: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:16.802: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:16.802: INFO: Lookups using dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local]

  May  8 11:41:21.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:21.786: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:21.788: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:21.791: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:21.793: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:21.796: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:21.798: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:21.803: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:21.803: INFO: Lookups using dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local]

  May  8 11:41:26.783: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:26.786: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:26.789: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:26.791: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:26.794: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:26.796: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:26.799: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:26.801: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:26.801: INFO: Lookups using dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local]

  May  8 11:41:31.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:31.786: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:31.789: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:31.791: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:31.793: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:31.796: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:31.799: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:31.801: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local from pod dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf: the server could not find the requested resource (get pods dns-test-21c4a209-0440-4925-b890-5cedff8d60cf)
  May  8 11:41:31.801: INFO: Lookups using dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4918.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4918.svc.cluster.local jessie_udp@dns-test-service-2.dns-4918.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4918.svc.cluster.local]

  May  8 11:41:36.803: INFO: DNS probes using dns-4918/dns-test-21c4a209-0440-4925-b890-5cedff8d60cf succeeded

  May  8 11:41:36.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 11:41:36.805
  STEP: deleting the test headless service @ 05/08/23 11:41:36.815
  STEP: Destroying namespace "dns-4918" for this suite. @ 05/08/23 11:41:36.828
• [33.122 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/08/23 11:41:36.833
  May  8 11:41:36.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 11:41:36.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:41:36.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:41:36.845
  STEP: creating a replication controller @ 05/08/23 11:41:36.847
  May  8 11:41:36.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 create -f -'
  May  8 11:41:37.505: INFO: stderr: ""
  May  8 11:41:37.505: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/08/23 11:41:37.505
  May  8 11:41:37.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  8 11:41:37.576: INFO: stderr: ""
  May  8 11:41:37.576: INFO: stdout: "update-demo-nautilus-7wq9t update-demo-nautilus-pfdng "
  May  8 11:41:37.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-7wq9t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:41:37.638: INFO: stderr: ""
  May  8 11:41:37.638: INFO: stdout: ""
  May  8 11:41:37.638: INFO: update-demo-nautilus-7wq9t is created but not running
  May  8 11:41:42.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  8 11:41:42.702: INFO: stderr: ""
  May  8 11:41:42.702: INFO: stdout: "update-demo-nautilus-7wq9t update-demo-nautilus-pfdng "
  May  8 11:41:42.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-7wq9t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:41:42.763: INFO: stderr: ""
  May  8 11:41:42.763: INFO: stdout: "true"
  May  8 11:41:42.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-7wq9t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  8 11:41:42.822: INFO: stderr: ""
  May  8 11:41:42.822: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  8 11:41:42.822: INFO: validating pod update-demo-nautilus-7wq9t
  May  8 11:41:42.828: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  8 11:41:42.828: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  8 11:41:42.828: INFO: update-demo-nautilus-7wq9t is verified up and running
  May  8 11:41:42.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-pfdng -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:41:42.888: INFO: stderr: ""
  May  8 11:41:42.888: INFO: stdout: "true"
  May  8 11:41:42.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-pfdng -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  8 11:41:42.947: INFO: stderr: ""
  May  8 11:41:42.947: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  8 11:41:42.947: INFO: validating pod update-demo-nautilus-pfdng
  May  8 11:41:42.952: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  8 11:41:42.952: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  8 11:41:42.952: INFO: update-demo-nautilus-pfdng is verified up and running
  STEP: scaling down the replication controller @ 05/08/23 11:41:42.952
  May  8 11:41:42.953: INFO: scanned /root for discovery docs: <nil>
  May  8 11:41:42.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  May  8 11:41:44.025: INFO: stderr: ""
  May  8 11:41:44.025: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/08/23 11:41:44.025
  May  8 11:41:44.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  8 11:41:44.087: INFO: stderr: ""
  May  8 11:41:44.087: INFO: stdout: "update-demo-nautilus-7wq9t update-demo-nautilus-pfdng "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 05/08/23 11:41:44.087
  May  8 11:41:49.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  8 11:41:49.148: INFO: stderr: ""
  May  8 11:41:49.148: INFO: stdout: "update-demo-nautilus-7wq9t "
  May  8 11:41:49.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-7wq9t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:41:49.207: INFO: stderr: ""
  May  8 11:41:49.207: INFO: stdout: "true"
  May  8 11:41:49.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-7wq9t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  8 11:41:49.266: INFO: stderr: ""
  May  8 11:41:49.266: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  8 11:41:49.266: INFO: validating pod update-demo-nautilus-7wq9t
  May  8 11:41:49.269: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  8 11:41:49.269: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  8 11:41:49.269: INFO: update-demo-nautilus-7wq9t is verified up and running
  STEP: scaling up the replication controller @ 05/08/23 11:41:49.269
  May  8 11:41:49.270: INFO: scanned /root for discovery docs: <nil>
  May  8 11:41:49.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  May  8 11:41:50.342: INFO: stderr: ""
  May  8 11:41:50.342: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/08/23 11:41:50.342
  May  8 11:41:50.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  8 11:41:50.407: INFO: stderr: ""
  May  8 11:41:50.407: INFO: stdout: "update-demo-nautilus-7wq9t update-demo-nautilus-fbclz "
  May  8 11:41:50.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-7wq9t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:41:50.466: INFO: stderr: ""
  May  8 11:41:50.466: INFO: stdout: "true"
  May  8 11:41:50.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-7wq9t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  8 11:41:50.526: INFO: stderr: ""
  May  8 11:41:50.526: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  8 11:41:50.526: INFO: validating pod update-demo-nautilus-7wq9t
  May  8 11:41:50.529: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  8 11:41:50.529: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  8 11:41:50.529: INFO: update-demo-nautilus-7wq9t is verified up and running
  May  8 11:41:50.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-fbclz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:41:50.587: INFO: stderr: ""
  May  8 11:41:50.587: INFO: stdout: "true"
  May  8 11:41:50.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods update-demo-nautilus-fbclz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  8 11:41:50.647: INFO: stderr: ""
  May  8 11:41:50.647: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  8 11:41:50.647: INFO: validating pod update-demo-nautilus-fbclz
  May  8 11:41:50.653: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  8 11:41:50.653: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  8 11:41:50.653: INFO: update-demo-nautilus-fbclz is verified up and running
  STEP: using delete to clean up resources @ 05/08/23 11:41:50.653
  May  8 11:41:50.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 delete --grace-period=0 --force -f -'
  May  8 11:41:50.713: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 11:41:50.713: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May  8 11:41:50.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get rc,svc -l name=update-demo --no-headers'
  May  8 11:41:50.785: INFO: stderr: "No resources found in kubectl-2881 namespace.\n"
  May  8 11:41:50.785: INFO: stdout: ""
  May  8 11:41:50.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-2881 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  8 11:41:50.849: INFO: stderr: ""
  May  8 11:41:50.849: INFO: stdout: ""
  May  8 11:41:50.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2881" for this suite. @ 05/08/23 11:41:50.852
• [14.023 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/08/23 11:41:50.856
  May  8 11:41:50.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubelet-test @ 05/08/23 11:41:50.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:41:50.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:41:50.87
  May  8 11:41:50.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2694" for this suite. @ 05/08/23 11:41:50.885
• [0.035 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/08/23 11:41:50.891
  May  8 11:41:50.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 11:41:50.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:41:50.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:41:50.904
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/08/23 11:41:50.907
  STEP: Saw pod success @ 05/08/23 11:41:54.922
  May  8 11:41:54.924: INFO: Trying to get logs from node worker-0 pod pod-ce5a38fa-8fd6-4b3b-9453-c20959f84911 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 11:41:54.929
  May  8 11:41:54.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2871" for this suite. @ 05/08/23 11:41:54.942
• [4.054 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/08/23 11:41:54.946
  May  8 11:41:54.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/08/23 11:41:54.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:41:54.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:41:54.958
  STEP: create the container to handle the HTTPGet hook request. @ 05/08/23 11:41:54.964
  STEP: create the pod with lifecycle hook @ 05/08/23 11:41:56.978
  STEP: delete the pod with lifecycle hook @ 05/08/23 11:42:00.993
  STEP: check prestop hook @ 05/08/23 11:42:05.007
  May  8 11:42:05.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4483" for this suite. @ 05/08/23 11:42:05.014
• [10.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/08/23 11:42:05.021
  May  8 11:42:05.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename limitrange @ 05/08/23 11:42:05.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:05.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:05.034
  STEP: Creating LimitRange "e2e-limitrange-gvpk6" in namespace "limitrange-5152" @ 05/08/23 11:42:05.036
  STEP: Creating another limitRange in another namespace @ 05/08/23 11:42:05.04
  May  8 11:42:05.049: INFO: Namespace "e2e-limitrange-gvpk6-4641" created
  May  8 11:42:05.049: INFO: Creating LimitRange "e2e-limitrange-gvpk6" in namespace "e2e-limitrange-gvpk6-4641"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-gvpk6" @ 05/08/23 11:42:05.052
  May  8 11:42:05.054: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-gvpk6" in "limitrange-5152" namespace @ 05/08/23 11:42:05.054
  May  8 11:42:05.060: INFO: LimitRange "e2e-limitrange-gvpk6" has been patched
  STEP: Delete LimitRange "e2e-limitrange-gvpk6" by Collection with labelSelector: "e2e-limitrange-gvpk6=patched" @ 05/08/23 11:42:05.06
  STEP: Confirm that the limitRange "e2e-limitrange-gvpk6" has been deleted @ 05/08/23 11:42:05.064
  May  8 11:42:05.064: INFO: Requesting list of LimitRange to confirm quantity
  May  8 11:42:05.066: INFO: Found 0 LimitRange with label "e2e-limitrange-gvpk6=patched"
  May  8 11:42:05.066: INFO: LimitRange "e2e-limitrange-gvpk6" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-gvpk6" @ 05/08/23 11:42:05.066
  May  8 11:42:05.068: INFO: Found 1 limitRange
  May  8 11:42:05.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5152" for this suite. @ 05/08/23 11:42:05.07
  STEP: Destroying namespace "e2e-limitrange-gvpk6-4641" for this suite. @ 05/08/23 11:42:05.073
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/08/23 11:42:05.078
  May  8 11:42:05.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl-logs @ 05/08/23 11:42:05.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:05.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:05.088
  STEP: creating an pod @ 05/08/23 11:42:05.09
  May  8 11:42:05.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-logs-9141 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May  8 11:42:05.161: INFO: stderr: ""
  May  8 11:42:05.161: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/08/23 11:42:05.161
  May  8 11:42:05.161: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  May  8 11:42:07.167: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/08/23 11:42:07.167
  May  8 11:42:07.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-logs-9141 logs logs-generator logs-generator'
  May  8 11:42:07.241: INFO: stderr: ""
  May  8 11:42:07.241: INFO: stdout: "I0508 11:42:05.702287       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5d6 454\nI0508 11:42:05.902481       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/rwq 562\nI0508 11:42:06.102981       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/j78k 540\nI0508 11:42:06.303412       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/l88 367\nI0508 11:42:06.502843       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/pg6 213\nI0508 11:42:06.703285       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/dlr 394\nI0508 11:42:06.902715       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/ttrm 438\nI0508 11:42:07.103137       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xlrx 511\n"
  STEP: limiting log lines @ 05/08/23 11:42:07.241
  May  8 11:42:07.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-logs-9141 logs logs-generator logs-generator --tail=1'
  May  8 11:42:07.305: INFO: stderr: ""
  May  8 11:42:07.305: INFO: stdout: "I0508 11:42:07.302490       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/75pp 448\n"
  May  8 11:42:07.305: INFO: got output "I0508 11:42:07.302490       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/75pp 448\n"
  STEP: limiting log bytes @ 05/08/23 11:42:07.305
  May  8 11:42:07.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-logs-9141 logs logs-generator logs-generator --limit-bytes=1'
  May  8 11:42:07.370: INFO: stderr: ""
  May  8 11:42:07.370: INFO: stdout: "I"
  May  8 11:42:07.370: INFO: got output "I"
  STEP: exposing timestamps @ 05/08/23 11:42:07.37
  May  8 11:42:07.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-logs-9141 logs logs-generator logs-generator --tail=1 --timestamps'
  May  8 11:42:07.433: INFO: stderr: ""
  May  8 11:42:07.433: INFO: stdout: "2023-05-08T11:42:07.302633331Z I0508 11:42:07.302490       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/75pp 448\n"
  May  8 11:42:07.433: INFO: got output "2023-05-08T11:42:07.302633331Z I0508 11:42:07.302490       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/75pp 448\n"
  STEP: restricting to a time range @ 05/08/23 11:42:07.433
  May  8 11:42:09.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-logs-9141 logs logs-generator logs-generator --since=1s'
  May  8 11:42:09.998: INFO: stderr: ""
  May  8 11:42:09.998: INFO: stdout: "I0508 11:42:09.103194       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/8m4 472\nI0508 11:42:09.302502       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7w22 476\nI0508 11:42:09.502934       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vh5c 483\nI0508 11:42:09.703368       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/95jn 551\nI0508 11:42:09.902799       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/8fp7 578\n"
  May  8 11:42:09.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-logs-9141 logs logs-generator logs-generator --since=24h'
  May  8 11:42:10.062: INFO: stderr: ""
  May  8 11:42:10.063: INFO: stdout: "I0508 11:42:05.702287       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5d6 454\nI0508 11:42:05.902481       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/rwq 562\nI0508 11:42:06.102981       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/j78k 540\nI0508 11:42:06.303412       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/l88 367\nI0508 11:42:06.502843       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/pg6 213\nI0508 11:42:06.703285       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/dlr 394\nI0508 11:42:06.902715       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/ttrm 438\nI0508 11:42:07.103137       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xlrx 511\nI0508 11:42:07.302490       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/75pp 448\nI0508 11:42:07.502910       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/mmq 352\nI0508 11:42:07.703365       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/ptm 558\nI0508 11:42:07.902805       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/t7vk 503\nI0508 11:42:08.103245       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/zs8f 477\nI0508 11:42:08.302450       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/jd7 330\nI0508 11:42:08.502890       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/qltc 323\nI0508 11:42:08.703325       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/qpb 519\nI0508 11:42:08.902764       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/r46 572\nI0508 11:42:09.103194       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/8m4 472\nI0508 11:42:09.302502       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7w22 476\nI0508 11:42:09.502934       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vh5c 483\nI0508 11:42:09.703368       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/95jn 551\nI0508 11:42:09.902799       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/8fp7 578\n"
  May  8 11:42:10.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-logs-9141 delete pod logs-generator'
  May  8 11:42:10.840: INFO: stderr: ""
  May  8 11:42:10.840: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May  8 11:42:10.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-9141" for this suite. @ 05/08/23 11:42:10.842
• [5.768 seconds]
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/08/23 11:42:10.846
  May  8 11:42:10.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replicaset @ 05/08/23 11:42:10.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:10.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:10.858
  May  8 11:42:10.861: INFO: Creating ReplicaSet my-hostname-basic-c58aef03-9dd5-4601-b9ee-b8ca0d4c442a
  May  8 11:42:10.868: INFO: Pod name my-hostname-basic-c58aef03-9dd5-4601-b9ee-b8ca0d4c442a: Found 0 pods out of 1
  May  8 11:42:15.871: INFO: Pod name my-hostname-basic-c58aef03-9dd5-4601-b9ee-b8ca0d4c442a: Found 1 pods out of 1
  May  8 11:42:15.871: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c58aef03-9dd5-4601-b9ee-b8ca0d4c442a" is running
  May  8 11:42:15.873: INFO: Pod "my-hostname-basic-c58aef03-9dd5-4601-b9ee-b8ca0d4c442a-69qv2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-08 11:42:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-08 11:42:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-08 11:42:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-08 11:42:10 +0000 UTC Reason: Message:}])
  May  8 11:42:15.873: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/08/23 11:42:15.873
  May  8 11:42:15.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-162" for this suite. @ 05/08/23 11:42:15.885
• [5.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/08/23 11:42:15.891
  May  8 11:42:15.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 11:42:15.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:15.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:15.905
  STEP: fetching services @ 05/08/23 11:42:15.907
  May  8 11:42:15.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2098" for this suite. @ 05/08/23 11:42:15.912
• [0.025 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/08/23 11:42:15.917
  May  8 11:42:15.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename security-context-test @ 05/08/23 11:42:15.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:15.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:15.933
  May  8 11:42:19.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2809" for this suite. @ 05/08/23 11:42:19.953
• [4.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/08/23 11:42:19.96
  May  8 11:42:19.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename dns @ 05/08/23 11:42:19.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:19.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:19.97
  STEP: Creating a test headless service @ 05/08/23 11:42:19.973
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1469.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1469.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/08/23 11:42:19.975
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1469.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1469.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/08/23 11:42:19.976
  STEP: creating a pod to probe DNS @ 05/08/23 11:42:19.976
  STEP: submitting the pod to kubernetes @ 05/08/23 11:42:19.976
  STEP: retrieving the pod @ 05/08/23 11:42:21.99
  STEP: looking for the results for each expected name from probers @ 05/08/23 11:42:21.992
  May  8 11:42:22.005: INFO: DNS probes using dns-1469/dns-test-e579c4a4-814d-4c05-9d13-67d8a8a394f9 succeeded

  May  8 11:42:22.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 11:42:22.007
  STEP: deleting the test headless service @ 05/08/23 11:42:22.016
  STEP: Destroying namespace "dns-1469" for this suite. @ 05/08/23 11:42:22.027
• [2.075 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/08/23 11:42:22.035
  May  8 11:42:22.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 11:42:22.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:22.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:22.046
  STEP: Setting up server cert @ 05/08/23 11:42:22.06
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 11:42:22.371
  STEP: Deploying the webhook pod @ 05/08/23 11:42:22.378
  STEP: Wait for the deployment to be ready @ 05/08/23 11:42:22.386
  May  8 11:42:22.392: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 11:42:24.399
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 11:42:24.407
  May  8 11:42:25.407: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/08/23 11:42:25.41
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/08/23 11:42:25.411
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/08/23 11:42:25.411
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/08/23 11:42:25.411
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/08/23 11:42:25.412
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/08/23 11:42:25.412
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/08/23 11:42:25.413
  May  8 11:42:25.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2941" for this suite. @ 05/08/23 11:42:25.44
  STEP: Destroying namespace "webhook-markers-801" for this suite. @ 05/08/23 11:42:25.445
• [3.415 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/08/23 11:42:25.451
  May  8 11:42:25.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 11:42:25.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:25.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:25.463
  May  8 11:42:25.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/08/23 11:42:26.712
  May  8 11:42:26.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-8795 --namespace=crd-publish-openapi-8795 create -f -'
  May  8 11:42:27.382: INFO: stderr: ""
  May  8 11:42:27.382: INFO: stdout: "e2e-test-crd-publish-openapi-1355-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May  8 11:42:27.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-8795 --namespace=crd-publish-openapi-8795 delete e2e-test-crd-publish-openapi-1355-crds test-cr'
  May  8 11:42:27.446: INFO: stderr: ""
  May  8 11:42:27.446: INFO: stdout: "e2e-test-crd-publish-openapi-1355-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May  8 11:42:27.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-8795 --namespace=crd-publish-openapi-8795 apply -f -'
  May  8 11:42:27.685: INFO: stderr: ""
  May  8 11:42:27.685: INFO: stdout: "e2e-test-crd-publish-openapi-1355-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May  8 11:42:27.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-8795 --namespace=crd-publish-openapi-8795 delete e2e-test-crd-publish-openapi-1355-crds test-cr'
  May  8 11:42:27.745: INFO: stderr: ""
  May  8 11:42:27.745: INFO: stdout: "e2e-test-crd-publish-openapi-1355-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/08/23 11:42:27.745
  May  8 11:42:27.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-8795 explain e2e-test-crd-publish-openapi-1355-crds'
  May  8 11:42:27.988: INFO: stderr: ""
  May  8 11:42:27.988: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-1355-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May  8 11:42:29.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8795" for this suite. @ 05/08/23 11:42:29.239
• [3.792 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/08/23 11:42:29.244
  May  8 11:42:29.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename deployment @ 05/08/23 11:42:29.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:29.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:29.257
  May  8 11:42:29.259: INFO: Creating deployment "webserver-deployment"
  May  8 11:42:29.262: INFO: Waiting for observed generation 1
  May  8 11:42:31.267: INFO: Waiting for all required pods to come up
  May  8 11:42:31.281: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/08/23 11:42:31.281
  May  8 11:42:35.298: INFO: Waiting for deployment "webserver-deployment" to complete
  May  8 11:42:35.302: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May  8 11:42:35.309: INFO: Updating deployment webserver-deployment
  May  8 11:42:35.309: INFO: Waiting for observed generation 2
  May  8 11:42:37.315: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May  8 11:42:37.317: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May  8 11:42:37.318: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May  8 11:42:37.323: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May  8 11:42:37.323: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May  8 11:42:37.325: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May  8 11:42:37.328: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May  8 11:42:37.328: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May  8 11:42:37.335: INFO: Updating deployment webserver-deployment
  May  8 11:42:37.335: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May  8 11:42:37.339: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May  8 11:42:37.341: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  May  8 11:42:37.357: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-5469  4fcb8105-b609-470b-8939-aac1e4b50bc6 2090 3 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041a8cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-08 11:42:34 +0000 UTC,LastTransitionTime:2023-05-08 11:42:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-08 11:42:35 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  May  8 11:42:37.374: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-5469  60d062ed-9145-4cbe-bfb3-707ee0d20fed 2094 3 2023-05-08 11:42:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4fcb8105-b609-470b-8939-aac1e4b50bc6 0xc003aa2e97 0xc003aa2e98}] [] [{kube-controller-manager Update apps/v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4fcb8105-b609-470b-8939-aac1e4b50bc6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aa2f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  8 11:42:37.374: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May  8 11:42:37.374: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-5469  eaeae815-7dfd-4b27-8b53-f7500727de29 2091 3 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4fcb8105-b609-470b-8939-aac1e4b50bc6 0xc003aa2da7 0xc003aa2da8}] [] [{kube-controller-manager Update apps/v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4fcb8105-b609-470b-8939-aac1e4b50bc6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aa2e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  May  8 11:42:37.397: INFO: Pod "webserver-deployment-67bd4bf6dc-494qd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-494qd webserver-deployment-67bd4bf6dc- deployment-5469  fc0b5f2d-175c-43ee-9ab1-029a7626e0b4 2120 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc003aa3430 0xc003aa3431}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttdh9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttdh9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.398: INFO: Pod "webserver-deployment-67bd4bf6dc-6pmsf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6pmsf webserver-deployment-67bd4bf6dc- deployment-5469  51ed3a5d-7cdd-434b-95ff-8f2e0ad93ca3 2101 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc003aa3567 0xc003aa3568}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bv5m7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bv5m7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.398: INFO: Pod "webserver-deployment-67bd4bf6dc-9hcj2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9hcj2 webserver-deployment-67bd4bf6dc- deployment-5469  2a0ae0dd-37ae-4f62-b032-a3850e63d856 2126 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc003aa36d0 0xc003aa36d1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hr9n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hr9n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.398: INFO: Pod "webserver-deployment-67bd4bf6dc-9jhnp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9jhnp webserver-deployment-67bd4bf6dc- deployment-5469  be63e689-cac8-42cc-a4b1-d661cfcc5e04 1994 0 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc003aa3807 0xc003aa3808}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4d79n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4d79n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:10.244.1.21,StartTime:2023-05-08 11:42:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 11:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c28a3d37cebbbe1359125c8cdaabb23bcc05b24a4396c50b5a2bc646a6676eff,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.21,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.399: INFO: Pod "webserver-deployment-67bd4bf6dc-b95w6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-b95w6 webserver-deployment-67bd4bf6dc- deployment-5469  4652efd8-3ba7-4848-89be-20b301a612b8 2119 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc003aa39f0 0xc003aa39f1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t4wt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t4wt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.399: INFO: Pod "webserver-deployment-67bd4bf6dc-f4xx5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-f4xx5 webserver-deployment-67bd4bf6dc- deployment-5469  19a03a8b-f50b-4434-b353-3a008b1cbb90 2121 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc003aa3b50 0xc003aa3b51}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5mbg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5mbg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.399: INFO: Pod "webserver-deployment-67bd4bf6dc-fnlls" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fnlls webserver-deployment-67bd4bf6dc- deployment-5469  41d4573a-a102-4d29-adf2-8f4e77196844 1979 0 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc003aa3c87 0xc003aa3c88}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vhrxb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vhrxb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.53.117,PodIP:10.244.0.9,StartTime:2023-05-08 11:42:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 11:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://87678aa3663dd4adbe5e91f359be21d5c306b97ae5c8c781ca5c46b76c84074d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.9,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.400: INFO: Pod "webserver-deployment-67bd4bf6dc-gm967" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gm967 webserver-deployment-67bd4bf6dc- deployment-5469  64f5d9ca-d559-409a-a134-162ce1d89daa 2002 0 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc003aa3e70 0xc003aa3e71}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lgxrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lgxrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:10.244.1.18,StartTime:2023-05-08 11:42:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 11:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://822b85b29b37a039ffa547ddd7da67c734713bda2ff5042dee492f3d2998cb2a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.400: INFO: Pod "webserver-deployment-67bd4bf6dc-jfn6n" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jfn6n webserver-deployment-67bd4bf6dc- deployment-5469  81d71232-7cd3-44fb-81f5-6d216c81a747 1981 0 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043ca050 0xc0043ca051}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jb7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jb7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.53.117,PodIP:10.244.0.12,StartTime:2023-05-08 11:42:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 11:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c73e51932d62ae5ecee45c860de09c39bc49b829fb0c721d3edb13c301a463e8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.12,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.401: INFO: Pod "webserver-deployment-67bd4bf6dc-p675r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-p675r webserver-deployment-67bd4bf6dc- deployment-5469  7dd4081e-b879-4c67-86c4-73345623007a 2116 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043ca240 0xc0043ca241}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26khs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26khs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.401: INFO: Pod "webserver-deployment-67bd4bf6dc-pvlx7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pvlx7 webserver-deployment-67bd4bf6dc- deployment-5469  21e8a03d-24d7-43b0-af99-aaa9cc621105 1987 0 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043ca3a0 0xc0043ca3a1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfpw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfpw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.53.117,PodIP:10.244.0.11,StartTime:2023-05-08 11:42:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 11:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://13cb48cc5dc05009f526c2536b52f9885d78425ab8875c26dabb01cf07a4a887,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.11,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.402: INFO: Pod "webserver-deployment-67bd4bf6dc-qlp7w" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qlp7w webserver-deployment-67bd4bf6dc- deployment-5469  767f9b4b-c6c9-454f-a024-d1c6ff109c06 2118 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043ca5b0 0xc0043ca5b1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-snnsj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-snnsj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.403: INFO: Pod "webserver-deployment-67bd4bf6dc-r7w67" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-r7w67 webserver-deployment-67bd4bf6dc- deployment-5469  49dbdea4-a8a6-4259-86bc-e0c158f22b15 1990 0 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043ca720 0xc0043ca721}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tlpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tlpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.53.117,PodIP:10.244.0.13,StartTime:2023-05-08 11:42:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 11:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f0883a2d01262ff02bff58ad8ca8b559dc82203d41d59e50cc3ba4b8c6b08d5c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.13,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.404: INFO: Pod "webserver-deployment-67bd4bf6dc-rhw2j" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rhw2j webserver-deployment-67bd4bf6dc- deployment-5469  bbc8ff51-4417-4a42-ac1b-7bc4f0fcdfff 2115 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043ca920 0xc0043ca921}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-thnkl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-thnkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:,StartTime:2023-05-08 11:42:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.404: INFO: Pod "webserver-deployment-67bd4bf6dc-sk287" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sk287 webserver-deployment-67bd4bf6dc- deployment-5469  6fc3ad82-e101-40f6-8723-43f97a0459cf 1946 0 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043cab10 0xc0043cab11}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd7cx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd7cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:10.244.1.17,StartTime:2023-05-08 11:42:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 11:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0923ba15e4fa0b6bac9027e4616672df01ed8519ddc9f6b27e26b7a054439bdf,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.17,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.404: INFO: Pod "webserver-deployment-67bd4bf6dc-t77rq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-t77rq webserver-deployment-67bd4bf6dc- deployment-5469  48086528-311f-478f-ac75-9091c6e182da 1984 0 2023-05-08 11:42:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043cad00 0xc0043cad01}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qz6jr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qz6jr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.53.117,PodIP:10.244.0.10,StartTime:2023-05-08 11:42:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 11:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5eb38d3566412c68d9afaafe95e9e51c5c54a8368d678de7f9a3ced7add6f6c0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.10,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.405: INFO: Pod "webserver-deployment-67bd4bf6dc-xtjrx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xtjrx webserver-deployment-67bd4bf6dc- deployment-5469  80499993-ee23-4098-ab67-0912dd34b94f 2122 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043caf10 0xc0043caf11}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvfpj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvfpj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.406: INFO: Pod "webserver-deployment-67bd4bf6dc-z4nqt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z4nqt webserver-deployment-67bd4bf6dc- deployment-5469  23446657-7595-47ed-8152-a01af713cc80 2117 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043cb057 0xc0043cb058}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gjttr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gjttr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.407: INFO: Pod "webserver-deployment-67bd4bf6dc-z6qv8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z6qv8 webserver-deployment-67bd4bf6dc- deployment-5469  5803ffe4-b506-46c5-9ca2-5474901dc957 2130 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043cb1d0 0xc0043cb1d1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l6pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l6pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.411: INFO: Pod "webserver-deployment-67bd4bf6dc-z6wtb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z6wtb webserver-deployment-67bd4bf6dc- deployment-5469  04cac53f-5aba-4b21-a771-ffa903319c19 2103 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc eaeae815-7dfd-4b27-8b53-f7500727de29 0xc0043cb337 0xc0043cb338}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaeae815-7dfd-4b27-8b53-f7500727de29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6pw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6pw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.412: INFO: Pod "webserver-deployment-7b75d79cf5-5x924" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5x924 webserver-deployment-7b75d79cf5- deployment-5469  aeb4b3ec-058b-45f2-bc28-3c54f9d90563 2131 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc0043cb4a0 0xc0043cb4a1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zb66f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zb66f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.413: INFO: Pod "webserver-deployment-7b75d79cf5-9d7gm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9d7gm webserver-deployment-7b75d79cf5- deployment-5469  9403276c-cb0a-46df-bea5-907441d19185 2077 0 2023-05-08 11:42:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc0043cb5f7 0xc0043cb5f8}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5bplw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bplw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.53.117,PodIP:10.244.0.15,StartTime:2023-05-08 11:42:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.15,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.415: INFO: Pod "webserver-deployment-7b75d79cf5-bblmv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bblmv webserver-deployment-7b75d79cf5- deployment-5469  ee256b85-f77d-4b6e-b5de-9fdc44b3b196 2125 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc0043cb830 0xc0043cb831}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gwzq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gwzq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.416: INFO: Pod "webserver-deployment-7b75d79cf5-bxxqd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bxxqd webserver-deployment-7b75d79cf5- deployment-5469  93c6abaa-75b3-49fb-a22a-fb4f9b2b6b94 2082 0 2023-05-08 11:42:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc0043cb9a0 0xc0043cb9a1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9tjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9tjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.53.117,PodIP:10.244.0.14,StartTime:2023-05-08 11:42:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.14,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.417: INFO: Pod "webserver-deployment-7b75d79cf5-chglm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-chglm webserver-deployment-7b75d79cf5- deployment-5469  2ee3648d-3bcf-4f5b-8dc6-905feddda43d 2058 0 2023-05-08 11:42:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc0043cbbb0 0xc0043cbbb1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4f78v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4f78v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:,StartTime:2023-05-08 11:42:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.419: INFO: Pod "webserver-deployment-7b75d79cf5-clf24" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-clf24 webserver-deployment-7b75d79cf5- deployment-5469  63498829-a85f-42fa-8d07-b6333a1c4ba9 2129 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc0043cbd90 0xc0043cbd91}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjwpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjwpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.419: INFO: Pod "webserver-deployment-7b75d79cf5-gbdxd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gbdxd webserver-deployment-7b75d79cf5- deployment-5469  fddadc95-765f-4ae8-9edd-16f0d4d025a1 2088 0 2023-05-08 11:42:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc0043cbed7 0xc0043cbed8}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-27vhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-27vhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:10.244.1.23,StartTime:2023-05-08 11:42:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.23,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.420: INFO: Pod "webserver-deployment-7b75d79cf5-n9q8m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-n9q8m webserver-deployment-7b75d79cf5- deployment-5469  a465eabc-a132-40ee-b446-b6af4ad8797a 2128 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc00236a0f0 0xc00236a0f1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcfft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcfft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.420: INFO: Pod "webserver-deployment-7b75d79cf5-q87b8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-q87b8 webserver-deployment-7b75d79cf5- deployment-5469  e519c3fc-7fdf-4461-9cd2-6b972cef5c5b 2085 0 2023-05-08 11:42:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc00236a267 0xc00236a268}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hr66,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hr66,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:10.244.1.22,StartTime:2023-05-08 11:42:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.22,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.420: INFO: Pod "webserver-deployment-7b75d79cf5-xpdgb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xpdgb webserver-deployment-7b75d79cf5- deployment-5469  b5c6c898-c94a-48de-bfdc-60b8c393039a 2109 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc00236a480 0xc00236a481}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fcwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.421: INFO: Pod "webserver-deployment-7b75d79cf5-xw8nb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xw8nb webserver-deployment-7b75d79cf5- deployment-5469  bdb54cbc-2ff7-40d9-97d6-fc683194fe33 2127 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc00236a5f0 0xc00236a5f1}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cw7tk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cw7tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:42:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.421: INFO: Pod "webserver-deployment-7b75d79cf5-ztncv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ztncv webserver-deployment-7b75d79cf5- deployment-5469  16a468b8-a6c4-4a67-8910-20fcda87de7d 2132 0 2023-05-08 11:42:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 60d062ed-9145-4cbe-bfb3-707ee0d20fed 0xc00236a760 0xc00236a761}] [] [{kube-controller-manager Update v1 2023-05-08 11:42:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60d062ed-9145-4cbe-bfb3-707ee0d20fed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4xk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4xk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:42:37.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5469" for this suite. @ 05/08/23 11:42:37.431
• [8.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/08/23 11:42:37.443
  May  8 11:42:37.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename var-expansion @ 05/08/23 11:42:37.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:37.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:37.457
  STEP: Creating a pod to test substitution in container's args @ 05/08/23 11:42:37.46
  STEP: Saw pod success @ 05/08/23 11:42:41.475
  May  8 11:42:41.477: INFO: Trying to get logs from node worker-1 pod var-expansion-17f09a1a-b8fb-43f1-bb1b-74499f79ce89 container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 11:42:41.482
  May  8 11:42:41.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-324" for this suite. @ 05/08/23 11:42:41.493
• [4.054 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/08/23 11:42:41.498
  May  8 11:42:41.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 11:42:41.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:41.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:41.509
  STEP: creating a secret @ 05/08/23 11:42:41.512
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/08/23 11:42:41.516
  STEP: patching the secret @ 05/08/23 11:42:41.518
  STEP: deleting the secret using a LabelSelector @ 05/08/23 11:42:41.523
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/08/23 11:42:41.527
  May  8 11:42:41.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5985" for this suite. @ 05/08/23 11:42:41.531
• [0.036 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/08/23 11:42:41.535
  May  8 11:42:41.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 11:42:41.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:41.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:41.545
  STEP: Setting up server cert @ 05/08/23 11:42:41.558
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 11:42:42.141
  STEP: Deploying the webhook pod @ 05/08/23 11:42:42.145
  STEP: Wait for the deployment to be ready @ 05/08/23 11:42:42.154
  May  8 11:42:42.160: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 11:42:44.167
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 11:42:44.175
  May  8 11:42:45.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/08/23 11:42:45.178
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/08/23 11:42:45.2
  STEP: Creating a configMap that should not be mutated @ 05/08/23 11:42:45.207
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/08/23 11:42:45.228
  STEP: Creating a configMap that should be mutated @ 05/08/23 11:42:45.237
  May  8 11:42:45.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2673" for this suite. @ 05/08/23 11:42:45.283
  STEP: Destroying namespace "webhook-markers-4355" for this suite. @ 05/08/23 11:42:45.29
• [3.759 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/08/23 11:42:45.295
  May  8 11:42:45.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-runtime @ 05/08/23 11:42:45.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:45.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:45.307
  STEP: create the container @ 05/08/23 11:42:45.309
  W0508 11:42:45.316481      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/08/23 11:42:45.316
  STEP: get the container status @ 05/08/23 11:42:47.326
  STEP: the container should be terminated @ 05/08/23 11:42:47.328
  STEP: the termination message should be set @ 05/08/23 11:42:47.328
  May  8 11:42:47.328: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/08/23 11:42:47.328
  May  8 11:42:47.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2396" for this suite. @ 05/08/23 11:42:47.337
• [2.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/08/23 11:42:47.345
  May  8 11:42:47.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-preemption @ 05/08/23 11:42:47.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:42:47.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:42:47.355
  May  8 11:42:47.367: INFO: Waiting up to 1m0s for all nodes to be ready
  May  8 11:43:47.381: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/08/23 11:43:47.383
  May  8 11:43:47.399: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May  8 11:43:47.407: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May  8 11:43:47.421: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May  8 11:43:47.432: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/08/23 11:43:47.432
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/08/23 11:43:49.443
  May  8 11:43:55.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-3924" for this suite. @ 05/08/23 11:43:55.5
• [68.159 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/08/23 11:43:55.505
  May  8 11:43:55.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 11:43:55.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:43:55.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:43:55.519
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 11:43:55.521
  STEP: Saw pod success @ 05/08/23 11:43:57.535
  May  8 11:43:57.537: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-9bdfba72-990d-4600-9ad5-132a96a1ed04 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 11:43:57.553
  May  8 11:43:57.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-356" for this suite. @ 05/08/23 11:43:57.564
• [2.063 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/08/23 11:43:57.569
  May  8 11:43:57.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename proxy @ 05/08/23 11:43:57.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:43:57.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:43:57.579
  STEP: starting an echo server on multiple ports @ 05/08/23 11:43:57.589
  STEP: creating replication controller proxy-service-sskq2 in namespace proxy-4186 @ 05/08/23 11:43:57.59
  I0508 11:43:57.597162      23 runners.go:194] Created replication controller with name: proxy-service-sskq2, namespace: proxy-4186, replica count: 1
  I0508 11:43:58.648376      23 runners.go:194] proxy-service-sskq2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0508 11:43:59.649601      23 runners.go:194] proxy-service-sskq2 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 11:43:59.652: INFO: setup took 2.070761669s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/08/23 11:43:59.652
  May  8 11:43:59.662: INFO: (0) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 9.119111ms)
  May  8 11:43:59.663: INFO: (0) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 10.164195ms)
  May  8 11:43:59.664: INFO: (0) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 11.646026ms)
  May  8 11:43:59.664: INFO: (0) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 11.656285ms)
  May  8 11:43:59.664: INFO: (0) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 11.737684ms)
  May  8 11:43:59.665: INFO: (0) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 12.1482ms)
  May  8 11:43:59.665: INFO: (0) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 12.530387ms)
  May  8 11:43:59.667: INFO: (0) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 14.420281ms)
  May  8 11:43:59.668: INFO: (0) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 14.919675ms)
  May  8 11:43:59.668: INFO: (0) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 15.36657ms)
  May  8 11:43:59.668: INFO: (0) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 15.915195ms)
  May  8 11:43:59.670: INFO: (0) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 18.020631ms)
  May  8 11:43:59.674: INFO: (0) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 21.100064ms)
  May  8 11:43:59.674: INFO: (0) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 21.125971ms)
  May  8 11:43:59.677: INFO: (0) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 24.315607ms)
  May  8 11:43:59.677: INFO: (0) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 24.027132ms)
  May  8 11:43:59.680: INFO: (1) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 3.677045ms)
  May  8 11:43:59.683: INFO: (1) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 6.140948ms)
  May  8 11:43:59.683: INFO: (1) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 6.69777ms)
  May  8 11:43:59.684: INFO: (1) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 7.209519ms)
  May  8 11:43:59.684: INFO: (1) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 7.475362ms)
  May  8 11:43:59.684: INFO: (1) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 7.78858ms)
  May  8 11:43:59.685: INFO: (1) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 7.547762ms)
  May  8 11:43:59.685: INFO: (1) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.391113ms)
  May  8 11:43:59.686: INFO: (1) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 8.494878ms)
  May  8 11:43:59.686: INFO: (1) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 8.639144ms)
  May  8 11:43:59.686: INFO: (1) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 8.506112ms)
  May  8 11:43:59.686: INFO: (1) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 9.254244ms)
  May  8 11:43:59.686: INFO: (1) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 9.260556ms)
  May  8 11:43:59.686: INFO: (1) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 9.411614ms)
  May  8 11:43:59.687: INFO: (1) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 9.907123ms)
  May  8 11:43:59.687: INFO: (1) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 10.062434ms)
  May  8 11:43:59.692: INFO: (2) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 5.376479ms)
  May  8 11:43:59.692: INFO: (2) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 5.294269ms)
  May  8 11:43:59.697: INFO: (2) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.77664ms)
  May  8 11:43:59.697: INFO: (2) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 9.746011ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 10.857682ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 10.318964ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 10.526122ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 10.416303ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 10.51494ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 10.637762ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 10.554975ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 11.14898ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 11.18092ms)
  May  8 11:43:59.698: INFO: (2) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 10.367925ms)
  May  8 11:43:59.700: INFO: (2) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 12.361017ms)
  May  8 11:43:59.700: INFO: (2) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 12.602489ms)
  May  8 11:43:59.706: INFO: (3) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 5.930517ms)
  May  8 11:43:59.707: INFO: (3) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 6.215319ms)
  May  8 11:43:59.707: INFO: (3) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 6.559771ms)
  May  8 11:43:59.707: INFO: (3) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 6.369408ms)
  May  8 11:43:59.707: INFO: (3) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 6.347071ms)
  May  8 11:43:59.708: INFO: (3) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 7.758976ms)
  May  8 11:43:59.708: INFO: (3) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 7.718622ms)
  May  8 11:43:59.708: INFO: (3) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 8.055642ms)
  May  8 11:43:59.708: INFO: (3) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 7.901079ms)
  May  8 11:43:59.709: INFO: (3) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 8.855175ms)
  May  8 11:43:59.709: INFO: (3) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 9.221338ms)
  May  8 11:43:59.710: INFO: (3) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 9.271652ms)
  May  8 11:43:59.710: INFO: (3) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.761682ms)
  May  8 11:43:59.710: INFO: (3) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 9.797117ms)
  May  8 11:43:59.710: INFO: (3) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 10.061583ms)
  May  8 11:43:59.711: INFO: (3) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 10.074766ms)
  May  8 11:43:59.715: INFO: (4) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 4.355678ms)
  May  8 11:43:59.715: INFO: (4) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 4.733048ms)
  May  8 11:43:59.718: INFO: (4) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 7.146793ms)
  May  8 11:43:59.718: INFO: (4) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 7.523837ms)
  May  8 11:43:59.719: INFO: (4) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 7.751559ms)
  May  8 11:43:59.719: INFO: (4) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 8.306153ms)
  May  8 11:43:59.719: INFO: (4) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.221754ms)
  May  8 11:43:59.719: INFO: (4) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 8.534193ms)
  May  8 11:43:59.719: INFO: (4) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 8.638427ms)
  May  8 11:43:59.719: INFO: (4) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 8.753016ms)
  May  8 11:43:59.719: INFO: (4) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 8.536952ms)
  May  8 11:43:59.720: INFO: (4) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.085618ms)
  May  8 11:43:59.720: INFO: (4) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 9.427737ms)
  May  8 11:43:59.720: INFO: (4) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 9.590992ms)
  May  8 11:43:59.721: INFO: (4) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 10.528941ms)
  May  8 11:43:59.721: INFO: (4) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 10.349271ms)
  May  8 11:43:59.725: INFO: (5) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 4.207619ms)
  May  8 11:43:59.726: INFO: (5) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 4.425322ms)
  May  8 11:43:59.727: INFO: (5) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 5.108114ms)
  May  8 11:43:59.727: INFO: (5) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 5.677635ms)
  May  8 11:43:59.727: INFO: (5) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 5.672768ms)
  May  8 11:43:59.728: INFO: (5) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 5.969176ms)
  May  8 11:43:59.728: INFO: (5) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 6.504278ms)
  May  8 11:43:59.728: INFO: (5) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 6.873356ms)
  May  8 11:43:59.730: INFO: (5) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 7.902522ms)
  May  8 11:43:59.730: INFO: (5) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 8.152938ms)
  May  8 11:43:59.730: INFO: (5) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 8.940623ms)
  May  8 11:43:59.730: INFO: (5) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 8.809694ms)
  May  8 11:43:59.732: INFO: (5) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 10.068824ms)
  May  8 11:43:59.732: INFO: (5) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 10.398431ms)
  May  8 11:43:59.732: INFO: (5) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 10.306819ms)
  May  8 11:43:59.732: INFO: (5) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 10.516473ms)
  May  8 11:43:59.738: INFO: (6) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 5.832749ms)
  May  8 11:43:59.740: INFO: (6) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 7.045897ms)
  May  8 11:43:59.740: INFO: (6) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 7.533146ms)
  May  8 11:43:59.740: INFO: (6) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 7.661758ms)
  May  8 11:43:59.740: INFO: (6) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 8.031933ms)
  May  8 11:43:59.740: INFO: (6) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 7.995541ms)
  May  8 11:43:59.740: INFO: (6) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 7.95513ms)
  May  8 11:43:59.740: INFO: (6) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 8.15318ms)
  May  8 11:43:59.741: INFO: (6) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 9.067173ms)
  May  8 11:43:59.742: INFO: (6) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 9.163793ms)
  May  8 11:43:59.742: INFO: (6) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 9.692536ms)
  May  8 11:43:59.742: INFO: (6) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 9.786612ms)
  May  8 11:43:59.742: INFO: (6) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.547419ms)
  May  8 11:43:59.742: INFO: (6) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 9.635839ms)
  May  8 11:43:59.743: INFO: (6) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 10.389256ms)
  May  8 11:43:59.743: INFO: (6) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 10.74951ms)
  May  8 11:43:59.747: INFO: (7) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 3.959084ms)
  May  8 11:43:59.747: INFO: (7) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 3.909072ms)
  May  8 11:43:59.749: INFO: (7) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 5.467315ms)
  May  8 11:43:59.749: INFO: (7) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 5.947144ms)
  May  8 11:43:59.750: INFO: (7) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 6.17427ms)
  May  8 11:43:59.750: INFO: (7) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 6.375023ms)
  May  8 11:43:59.751: INFO: (7) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 7.475322ms)
  May  8 11:43:59.751: INFO: (7) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 7.01875ms)
  May  8 11:43:59.752: INFO: (7) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 7.684272ms)
  May  8 11:43:59.752: INFO: (7) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 7.754308ms)
  May  8 11:43:59.752: INFO: (7) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.455699ms)
  May  8 11:43:59.753: INFO: (7) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.638012ms)
  May  8 11:43:59.754: INFO: (7) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 10.077024ms)
  May  8 11:43:59.754: INFO: (7) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 9.952004ms)
  May  8 11:43:59.754: INFO: (7) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 10.558046ms)
  May  8 11:43:59.754: INFO: (7) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 10.463166ms)
  May  8 11:43:59.760: INFO: (8) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 5.666985ms)
  May  8 11:43:59.760: INFO: (8) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 5.912241ms)
  May  8 11:43:59.761: INFO: (8) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 5.75013ms)
  May  8 11:43:59.762: INFO: (8) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 7.571658ms)
  May  8 11:43:59.762: INFO: (8) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 7.695223ms)
  May  8 11:43:59.762: INFO: (8) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 7.681319ms)
  May  8 11:43:59.762: INFO: (8) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 8.077272ms)
  May  8 11:43:59.763: INFO: (8) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 7.976265ms)
  May  8 11:43:59.763: INFO: (8) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 7.824286ms)
  May  8 11:43:59.763: INFO: (8) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 8.156296ms)
  May  8 11:43:59.763: INFO: (8) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 8.220957ms)
  May  8 11:43:59.763: INFO: (8) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 8.362519ms)
  May  8 11:43:59.763: INFO: (8) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.786996ms)
  May  8 11:43:59.763: INFO: (8) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 8.612931ms)
  May  8 11:43:59.763: INFO: (8) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 8.517777ms)
  May  8 11:43:59.764: INFO: (8) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 9.226447ms)
  May  8 11:43:59.768: INFO: (9) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 3.768823ms)
  May  8 11:43:59.769: INFO: (9) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 4.682663ms)
  May  8 11:43:59.771: INFO: (9) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 6.194368ms)
  May  8 11:43:59.771: INFO: (9) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 6.39016ms)
  May  8 11:43:59.771: INFO: (9) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 6.656693ms)
  May  8 11:43:59.771: INFO: (9) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 6.657222ms)
  May  8 11:43:59.772: INFO: (9) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 7.604707ms)
  May  8 11:43:59.772: INFO: (9) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 7.760142ms)
  May  8 11:43:59.772: INFO: (9) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.063187ms)
  May  8 11:43:59.773: INFO: (9) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 8.896291ms)
  May  8 11:43:59.774: INFO: (9) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.112065ms)
  May  8 11:43:59.774: INFO: (9) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 9.434248ms)
  May  8 11:43:59.774: INFO: (9) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 9.101999ms)
  May  8 11:43:59.774: INFO: (9) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 9.969452ms)
  May  8 11:43:59.775: INFO: (9) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 10.155719ms)
  May  8 11:43:59.775: INFO: (9) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 10.201468ms)
  May  8 11:43:59.778: INFO: (10) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 3.675477ms)
  May  8 11:43:59.781: INFO: (10) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 6.221598ms)
  May  8 11:43:59.782: INFO: (10) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 6.925403ms)
  May  8 11:43:59.782: INFO: (10) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 7.32753ms)
  May  8 11:43:59.783: INFO: (10) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 7.826656ms)
  May  8 11:43:59.783: INFO: (10) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 7.843855ms)
  May  8 11:43:59.783: INFO: (10) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 7.896177ms)
  May  8 11:43:59.783: INFO: (10) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 8.278733ms)
  May  8 11:43:59.783: INFO: (10) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 8.463993ms)
  May  8 11:43:59.784: INFO: (10) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 9.020201ms)
  May  8 11:43:59.784: INFO: (10) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 9.586793ms)
  May  8 11:43:59.785: INFO: (10) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 9.734903ms)
  May  8 11:43:59.785: INFO: (10) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 10.043734ms)
  May  8 11:43:59.785: INFO: (10) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 10.119414ms)
  May  8 11:43:59.785: INFO: (10) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 10.573303ms)
  May  8 11:43:59.786: INFO: (10) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 10.653229ms)
  May  8 11:43:59.791: INFO: (11) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 4.968771ms)
  May  8 11:43:59.793: INFO: (11) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 7.207928ms)
  May  8 11:43:59.794: INFO: (11) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 7.476455ms)
  May  8 11:43:59.794: INFO: (11) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 6.693669ms)
  May  8 11:43:59.794: INFO: (11) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 7.247672ms)
  May  8 11:43:59.794: INFO: (11) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 8.037507ms)
  May  8 11:43:59.795: INFO: (11) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 8.083986ms)
  May  8 11:43:59.795: INFO: (11) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 8.307531ms)
  May  8 11:43:59.796: INFO: (11) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 8.586757ms)
  May  8 11:43:59.796: INFO: (11) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.57581ms)
  May  8 11:43:59.796: INFO: (11) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 8.671606ms)
  May  8 11:43:59.796: INFO: (11) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 8.939265ms)
  May  8 11:43:59.796: INFO: (11) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 9.314387ms)
  May  8 11:43:59.796: INFO: (11) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.007343ms)
  May  8 11:43:59.796: INFO: (11) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 8.834512ms)
  May  8 11:43:59.797: INFO: (11) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 9.50853ms)
  May  8 11:43:59.800: INFO: (12) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 3.059355ms)
  May  8 11:43:59.804: INFO: (12) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 6.405543ms)
  May  8 11:43:59.804: INFO: (12) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 7.042655ms)
  May  8 11:43:59.806: INFO: (12) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 8.814542ms)
  May  8 11:43:59.806: INFO: (12) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 9.170804ms)
  May  8 11:43:59.807: INFO: (12) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 9.38327ms)
  May  8 11:43:59.807: INFO: (12) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 9.433711ms)
  May  8 11:43:59.807: INFO: (12) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 9.451141ms)
  May  8 11:43:59.807: INFO: (12) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 9.321313ms)
  May  8 11:43:59.807: INFO: (12) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.435484ms)
  May  8 11:43:59.807: INFO: (12) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 9.338552ms)
  May  8 11:43:59.807: INFO: (12) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 9.835561ms)
  May  8 11:43:59.808: INFO: (12) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 10.371219ms)
  May  8 11:43:59.808: INFO: (12) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 10.248224ms)
  May  8 11:43:59.808: INFO: (12) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 10.157491ms)
  May  8 11:43:59.808: INFO: (12) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 10.763049ms)
  May  8 11:43:59.813: INFO: (13) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 4.771442ms)
  May  8 11:43:59.815: INFO: (13) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 7.128334ms)
  May  8 11:43:59.815: INFO: (13) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 6.874999ms)
  May  8 11:43:59.816: INFO: (13) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 7.907008ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 8.308212ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 7.982818ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 8.221944ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.244116ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 8.198915ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 8.108581ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 8.305634ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 8.525513ms)
  May  8 11:43:59.817: INFO: (13) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 9.144035ms)
  May  8 11:43:59.818: INFO: (13) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 9.14558ms)
  May  8 11:43:59.818: INFO: (13) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 9.214183ms)
  May  8 11:43:59.818: INFO: (13) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 9.368684ms)
  May  8 11:43:59.822: INFO: (14) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 3.607579ms)
  May  8 11:43:59.822: INFO: (14) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 4.061261ms)
  May  8 11:43:59.831: INFO: (14) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 12.475689ms)
  May  8 11:43:59.831: INFO: (14) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 12.631649ms)
  May  8 11:43:59.831: INFO: (14) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 12.574293ms)
  May  8 11:43:59.831: INFO: (14) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 12.625676ms)
  May  8 11:43:59.831: INFO: (14) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 12.809848ms)
  May  8 11:43:59.831: INFO: (14) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 12.606812ms)
  May  8 11:43:59.832: INFO: (14) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 13.513026ms)
  May  8 11:43:59.832: INFO: (14) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 13.375615ms)
  May  8 11:43:59.832: INFO: (14) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 13.575566ms)
  May  8 11:43:59.832: INFO: (14) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 13.790895ms)
  May  8 11:43:59.832: INFO: (14) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 13.695641ms)
  May  8 11:43:59.832: INFO: (14) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 14.018227ms)
  May  8 11:43:59.833: INFO: (14) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 14.423976ms)
  May  8 11:43:59.833: INFO: (14) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 14.663024ms)
  May  8 11:43:59.837: INFO: (15) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 3.851879ms)
  May  8 11:43:59.840: INFO: (15) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 6.673754ms)
  May  8 11:43:59.840: INFO: (15) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 6.828315ms)
  May  8 11:43:59.841: INFO: (15) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 7.38521ms)
  May  8 11:43:59.841: INFO: (15) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 7.638803ms)
  May  8 11:43:59.841: INFO: (15) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 7.861175ms)
  May  8 11:43:59.841: INFO: (15) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 8.287186ms)
  May  8 11:43:59.842: INFO: (15) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 8.294219ms)
  May  8 11:43:59.842: INFO: (15) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 8.281962ms)
  May  8 11:43:59.842: INFO: (15) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 8.536236ms)
  May  8 11:43:59.842: INFO: (15) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 8.698416ms)
  May  8 11:43:59.842: INFO: (15) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 9.097644ms)
  May  8 11:43:59.843: INFO: (15) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 9.956627ms)
  May  8 11:43:59.844: INFO: (15) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 10.467226ms)
  May  8 11:43:59.844: INFO: (15) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 10.216141ms)
  May  8 11:43:59.844: INFO: (15) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 10.752263ms)
  May  8 11:43:59.848: INFO: (16) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 3.727128ms)
  May  8 11:43:59.848: INFO: (16) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 3.910856ms)
  May  8 11:43:59.849: INFO: (16) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 4.75755ms)
  May  8 11:43:59.849: INFO: (16) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 4.802226ms)
  May  8 11:43:59.852: INFO: (16) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 7.411099ms)
  May  8 11:43:59.853: INFO: (16) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 8.702764ms)
  May  8 11:43:59.853: INFO: (16) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 8.678287ms)
  May  8 11:43:59.853: INFO: (16) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.733424ms)
  May  8 11:43:59.853: INFO: (16) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 8.663944ms)
  May  8 11:43:59.853: INFO: (16) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 8.795677ms)
  May  8 11:43:59.853: INFO: (16) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 8.789768ms)
  May  8 11:43:59.853: INFO: (16) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 8.955023ms)
  May  8 11:43:59.853: INFO: (16) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 8.863959ms)
  May  8 11:43:59.854: INFO: (16) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 9.244093ms)
  May  8 11:43:59.854: INFO: (16) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 10.021097ms)
  May  8 11:43:59.854: INFO: (16) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 9.99158ms)
  May  8 11:43:59.857: INFO: (17) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 2.955763ms)
  May  8 11:43:59.861: INFO: (17) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 6.494053ms)
  May  8 11:43:59.861: INFO: (17) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 7.022658ms)
  May  8 11:43:59.862: INFO: (17) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 7.73318ms)
  May  8 11:43:59.864: INFO: (17) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 9.288278ms)
  May  8 11:43:59.864: INFO: (17) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 9.301459ms)
  May  8 11:43:59.864: INFO: (17) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 9.653836ms)
  May  8 11:43:59.864: INFO: (17) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 9.477845ms)
  May  8 11:43:59.864: INFO: (17) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 9.377477ms)
  May  8 11:43:59.864: INFO: (17) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 9.80164ms)
  May  8 11:43:59.864: INFO: (17) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 9.781476ms)
  May  8 11:43:59.865: INFO: (17) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 10.489781ms)
  May  8 11:43:59.865: INFO: (17) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 10.615986ms)
  May  8 11:43:59.866: INFO: (17) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 11.292932ms)
  May  8 11:43:59.866: INFO: (17) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 11.352135ms)
  May  8 11:43:59.866: INFO: (17) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 11.530491ms)
  May  8 11:43:59.871: INFO: (18) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 4.683483ms)
  May  8 11:43:59.872: INFO: (18) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 5.226746ms)
  May  8 11:43:59.872: INFO: (18) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 6.175228ms)
  May  8 11:43:59.873: INFO: (18) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 6.341648ms)
  May  8 11:43:59.873: INFO: (18) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 6.194138ms)
  May  8 11:43:59.873: INFO: (18) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 6.195719ms)
  May  8 11:43:59.874: INFO: (18) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 8.060275ms)
  May  8 11:43:59.874: INFO: (18) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 7.800323ms)
  May  8 11:43:59.875: INFO: (18) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 8.036939ms)
  May  8 11:43:59.875: INFO: (18) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 8.30191ms)
  May  8 11:43:59.875: INFO: (18) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 8.67192ms)
  May  8 11:43:59.875: INFO: (18) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 8.818077ms)
  May  8 11:43:59.875: INFO: (18) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 8.846581ms)
  May  8 11:43:59.876: INFO: (18) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 9.153605ms)
  May  8 11:43:59.876: INFO: (18) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 9.392347ms)
  May  8 11:43:59.876: INFO: (18) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 9.449331ms)
  May  8 11:43:59.882: INFO: (19) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:462/proxy/: tls qux (200; 5.562235ms)
  May  8 11:43:59.882: INFO: (19) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:162/proxy/: bar (200; 5.755802ms)
  May  8 11:43:59.882: INFO: (19) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:160/proxy/: foo (200; 5.918009ms)
  May  8 11:43:59.882: INFO: (19) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:1080/proxy/rewriteme">... (200; 6.08164ms)
  May  8 11:43:59.883: INFO: (19) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:162/proxy/: bar (200; 6.386736ms)
  May  8 11:43:59.883: INFO: (19) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465:1080/proxy/rewriteme">test<... (200; 6.594383ms)
  May  8 11:43:59.883: INFO: (19) /api/v1/namespaces/proxy-4186/pods/http:proxy-service-sskq2-9b465:160/proxy/: foo (200; 7.042456ms)
  May  8 11:43:59.883: INFO: (19) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:443/proxy/tlsrewritem... (200; 6.759555ms)
  May  8 11:43:59.883: INFO: (19) /api/v1/namespaces/proxy-4186/pods/https:proxy-service-sskq2-9b465:460/proxy/: tls baz (200; 6.997655ms)
  May  8 11:43:59.884: INFO: (19) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname2/proxy/: tls qux (200; 7.311688ms)
  May  8 11:43:59.884: INFO: (19) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname1/proxy/: foo (200; 7.820761ms)
  May  8 11:43:59.884: INFO: (19) /api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/: <a href="/api/v1/namespaces/proxy-4186/pods/proxy-service-sskq2-9b465/proxy/rewriteme">test</a> (200; 7.891986ms)
  May  8 11:43:59.885: INFO: (19) /api/v1/namespaces/proxy-4186/services/http:proxy-service-sskq2:portname2/proxy/: bar (200; 8.240069ms)
  May  8 11:43:59.885: INFO: (19) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname2/proxy/: bar (200; 8.633296ms)
  May  8 11:43:59.885: INFO: (19) /api/v1/namespaces/proxy-4186/services/https:proxy-service-sskq2:tlsportname1/proxy/: tls baz (200; 8.639181ms)
  May  8 11:43:59.885: INFO: (19) /api/v1/namespaces/proxy-4186/services/proxy-service-sskq2:portname1/proxy/: foo (200; 8.965409ms)
  May  8 11:43:59.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-sskq2 in namespace proxy-4186, will wait for the garbage collector to delete the pods @ 05/08/23 11:43:59.888
  May  8 11:43:59.944: INFO: Deleting ReplicationController proxy-service-sskq2 took: 4.001106ms
  May  8 11:44:00.044: INFO: Terminating ReplicationController proxy-service-sskq2 pods took: 100.625228ms
  STEP: Destroying namespace "proxy-4186" for this suite. @ 05/08/23 11:44:02.345
• [4.780 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/08/23 11:44:02.35
  May  8 11:44:02.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 11:44:02.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:44:02.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:44:02.362
  STEP: Setting up server cert @ 05/08/23 11:44:02.375
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 11:44:02.704
  STEP: Deploying the webhook pod @ 05/08/23 11:44:02.708
  STEP: Wait for the deployment to be ready @ 05/08/23 11:44:02.716
  May  8 11:44:02.722: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 11:44:04.73
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 11:44:04.742
  May  8 11:44:05.742: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/08/23 11:44:05.745
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/08/23 11:44:05.745
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/08/23 11:44:05.76
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/08/23 11:44:06.769
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/08/23 11:44:06.769
  STEP: Having no error when timeout is longer than webhook latency @ 05/08/23 11:44:07.79
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/08/23 11:44:07.79
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/08/23 11:44:12.818
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/08/23 11:44:12.818
  May  8 11:44:17.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1633" for this suite. @ 05/08/23 11:44:17.874
  STEP: Destroying namespace "webhook-markers-1756" for this suite. @ 05/08/23 11:44:17.878
• [15.532 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/08/23 11:44:17.883
  May  8 11:44:17.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 11:44:17.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:44:17.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:44:17.895
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/08/23 11:44:17.898
  STEP: Saw pod success @ 05/08/23 11:44:21.91
  May  8 11:44:21.912: INFO: Trying to get logs from node worker-0 pod pod-e53790e6-f8ca-4f35-acf0-c48cccc19f76 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 11:44:21.917
  May  8 11:44:21.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8692" for this suite. @ 05/08/23 11:44:21.929
• [4.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/08/23 11:44:21.942
  May  8 11:44:21.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 11:44:21.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:44:21.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:44:21.954
  May  8 11:45:21.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5589" for this suite. @ 05/08/23 11:45:21.969
• [60.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/08/23 11:45:21.976
  May  8 11:45:21.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/08/23 11:45:21.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:45:21.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:45:21.989
  STEP: create the container to handle the HTTPGet hook request. @ 05/08/23 11:45:21.994
  STEP: create the pod with lifecycle hook @ 05/08/23 11:45:24.006
  STEP: delete the pod with lifecycle hook @ 05/08/23 11:45:26.018
  STEP: check prestop hook @ 05/08/23 11:45:30.029
  May  8 11:45:30.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4533" for this suite. @ 05/08/23 11:45:30.038
• [8.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/08/23 11:45:30.046
  May  8 11:45:30.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename subjectreview @ 05/08/23 11:45:30.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:45:30.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:45:30.058
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-7172" @ 05/08/23 11:45:30.06
  May  8 11:45:30.063: INFO: saUsername: "system:serviceaccount:subjectreview-7172:e2e"
  May  8 11:45:30.063: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-7172"}
  May  8 11:45:30.063: INFO: saUID: "51172c12-8473-426b-9a39-040c1bd80df6"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-7172:e2e" @ 05/08/23 11:45:30.063
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-7172:e2e" @ 05/08/23 11:45:30.063
  May  8 11:45:30.064: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-7172:e2e" api 'list' configmaps in "subjectreview-7172" namespace @ 05/08/23 11:45:30.064
  May  8 11:45:30.066: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-7172:e2e" @ 05/08/23 11:45:30.066
  May  8 11:45:30.067: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May  8 11:45:30.067: INFO: LocalSubjectAccessReview has been verified
  May  8 11:45:30.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-7172" for this suite. @ 05/08/23 11:45:30.07
• [0.027 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/08/23 11:45:30.074
  May  8 11:45:30.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 11:45:30.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:45:30.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:45:30.085
  STEP: Creating the pod @ 05/08/23 11:45:30.087
  May  8 11:45:32.622: INFO: Successfully updated pod "annotationupdateb861e0cb-41b0-4836-847b-c5a6aae2ab7c"
  May  8 11:45:36.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8946" for this suite. @ 05/08/23 11:45:36.643
• [6.572 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/08/23 11:45:36.647
  May  8 11:45:36.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-runtime @ 05/08/23 11:45:36.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:45:36.656
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:45:36.66
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/08/23 11:45:36.669
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/08/23 11:45:50.707
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/08/23 11:45:50.709
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/08/23 11:45:50.712
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/08/23 11:45:50.712
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/08/23 11:45:50.724
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/08/23 11:45:52.731
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/08/23 11:45:53.736
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/08/23 11:45:53.74
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/08/23 11:45:53.74
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/08/23 11:45:53.751
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/08/23 11:45:54.755
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/08/23 11:45:55.76
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/08/23 11:45:55.764
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/08/23 11:45:55.764
  May  8 11:45:55.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2584" for this suite. @ 05/08/23 11:45:55.78
• [19.136 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/08/23 11:45:55.784
  May  8 11:45:55.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 11:45:55.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:45:55.792
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:45:55.794
  STEP: Creating configMap that has name configmap-test-emptyKey-5242dc08-eb0a-441e-b177-7cc42ccefb30 @ 05/08/23 11:45:55.797
  May  8 11:45:55.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7808" for this suite. @ 05/08/23 11:45:55.8
• [0.019 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/08/23 11:45:55.804
  May  8 11:45:55.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 11:45:55.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:45:55.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:45:55.814
  STEP: creating service nodeport-test with type=NodePort in namespace services-2532 @ 05/08/23 11:45:55.817
  STEP: creating replication controller nodeport-test in namespace services-2532 @ 05/08/23 11:45:55.828
  I0508 11:45:55.833824      23 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-2532, replica count: 2
  I0508 11:45:58.886919      23 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 11:45:58.886: INFO: Creating new exec pod
  May  8 11:46:01.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-2532 exec execpodg6t28 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May  8 11:46:02.026: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May  8 11:46:02.026: INFO: stdout: "nodeport-test-cb9xx"
  May  8 11:46:02.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-2532 exec execpodg6t28 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.105.172 80'
  May  8 11:46:02.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.105.172 80\nConnection to 10.109.105.172 80 port [tcp/http] succeeded!\n"
  May  8 11:46:02.149: INFO: stdout: ""
  May  8 11:46:03.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-2532 exec execpodg6t28 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.105.172 80'
  May  8 11:46:03.264: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.105.172 80\nConnection to 10.109.105.172 80 port [tcp/http] succeeded!\n"
  May  8 11:46:03.264: INFO: stdout: "nodeport-test-9h72g"
  May  8 11:46:03.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-2532 exec execpodg6t28 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.39.71 30977'
  May  8 11:46:03.393: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.39.71 30977\nConnection to 10.0.39.71 30977 port [tcp/*] succeeded!\n"
  May  8 11:46:03.393: INFO: stdout: "nodeport-test-cb9xx"
  May  8 11:46:03.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-2532 exec execpodg6t28 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.53.117 30977'
  May  8 11:46:03.522: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.53.117 30977\nConnection to 10.0.53.117 30977 port [tcp/*] succeeded!\n"
  May  8 11:46:03.522: INFO: stdout: ""
  May  8 11:46:04.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-2532 exec execpodg6t28 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.53.117 30977'
  May  8 11:46:04.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.53.117 30977\nConnection to 10.0.53.117 30977 port [tcp/*] succeeded!\n"
  May  8 11:46:04.641: INFO: stdout: "nodeport-test-9h72g"
  May  8 11:46:04.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2532" for this suite. @ 05/08/23 11:46:04.644
• [8.843 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/08/23 11:46:04.652
  May  8 11:46:04.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 11:46:04.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:46:04.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:46:04.663
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/08/23 11:46:04.665
  May  8 11:46:04.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8295 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May  8 11:46:04.731: INFO: stderr: ""
  May  8 11:46:04.731: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/08/23 11:46:04.731
  May  8 11:46:04.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8295 delete pods e2e-test-httpd-pod'
  May  8 11:46:06.363: INFO: stderr: ""
  May  8 11:46:06.363: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  8 11:46:06.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8295" for this suite. @ 05/08/23 11:46:06.366
• [1.717 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/08/23 11:46:06.371
  May  8 11:46:06.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 11:46:06.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:46:06.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:46:06.384
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/08/23 11:46:06.386
  May  8 11:46:06.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:46:07.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:46:13.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5946" for this suite. @ 05/08/23 11:46:13.411
• [7.044 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/08/23 11:46:13.415
  May  8 11:46:13.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename gc @ 05/08/23 11:46:13.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:46:13.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:46:13.427
  STEP: create the rc @ 05/08/23 11:46:13.432
  W0508 11:46:13.435429      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/08/23 11:46:19.438
  STEP: wait for the rc to be deleted @ 05/08/23 11:46:19.443
  May  8 11:46:20.455: INFO: 80 pods remaining
  May  8 11:46:20.456: INFO: 80 pods has nil DeletionTimestamp
  May  8 11:46:20.456: INFO: 
  May  8 11:46:21.456: INFO: 71 pods remaining
  May  8 11:46:21.456: INFO: 70 pods has nil DeletionTimestamp
  May  8 11:46:21.456: INFO: 
  May  8 11:46:22.455: INFO: 60 pods remaining
  May  8 11:46:22.455: INFO: 60 pods has nil DeletionTimestamp
  May  8 11:46:22.455: INFO: 
  May  8 11:46:23.453: INFO: 40 pods remaining
  May  8 11:46:23.453: INFO: 40 pods has nil DeletionTimestamp
  May  8 11:46:23.453: INFO: 
  May  8 11:46:24.459: INFO: 30 pods remaining
  May  8 11:46:24.459: INFO: 30 pods has nil DeletionTimestamp
  May  8 11:46:24.459: INFO: 
  May  8 11:46:25.452: INFO: 20 pods remaining
  May  8 11:46:25.452: INFO: 20 pods has nil DeletionTimestamp
  May  8 11:46:25.452: INFO: 
  STEP: Gathering metrics @ 05/08/23 11:46:26.446
  W0508 11:46:26.449463      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  8 11:46:26.449: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  8 11:46:26.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7045" for this suite. @ 05/08/23 11:46:26.451
• [13.040 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/08/23 11:46:26.455
  May  8 11:46:26.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 11:46:26.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:46:26.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:46:26.469
  STEP: validating cluster-info @ 05/08/23 11:46:26.471
  May  8 11:46:26.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5895 cluster-info'
  May  8 11:46:26.539: INFO: stderr: ""
  May  8 11:46:26.539: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May  8 11:46:26.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5895" for this suite. @ 05/08/23 11:46:26.542
• [0.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/08/23 11:46:26.548
  May  8 11:46:26.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename job @ 05/08/23 11:46:26.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:46:26.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:46:26.559
  STEP: Creating a job @ 05/08/23 11:46:26.561
  STEP: Ensuring active pods == parallelism @ 05/08/23 11:46:26.565
  STEP: Orphaning one of the Job's Pods @ 05/08/23 11:46:28.568
  May  8 11:46:29.078: INFO: Successfully updated pod "adopt-release-7mwvz"
  STEP: Checking that the Job readopts the Pod @ 05/08/23 11:46:29.078
  STEP: Removing the labels from the Job's Pod @ 05/08/23 11:46:31.083
  May  8 11:46:31.592: INFO: Successfully updated pod "adopt-release-7mwvz"
  STEP: Checking that the Job releases the Pod @ 05/08/23 11:46:31.592
  May  8 11:46:33.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1972" for this suite. @ 05/08/23 11:46:33.601
• [7.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/08/23 11:46:33.607
  May  8 11:46:33.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-preemption @ 05/08/23 11:46:33.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:46:33.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:46:33.62
  May  8 11:46:33.630: INFO: Waiting up to 1m0s for all nodes to be ready
  May  8 11:47:33.646: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/08/23 11:47:33.649
  May  8 11:47:33.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/08/23 11:47:33.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:47:33.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:47:33.663
  STEP: Finding an available node @ 05/08/23 11:47:33.666
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/08/23 11:47:33.666
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/08/23 11:47:35.677
  May  8 11:47:35.684: INFO: found a healthy node: worker-0
  May  8 11:47:41.733: INFO: pods created so far: [1 1 1]
  May  8 11:47:41.733: INFO: length of pods created so far: 3
  May  8 11:47:43.739: INFO: pods created so far: [2 2 1]
  May  8 11:47:50.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 11:47:50.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-3953" for this suite. @ 05/08/23 11:47:50.79
  STEP: Destroying namespace "sched-preemption-3051" for this suite. @ 05/08/23 11:47:50.793
• [77.191 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/08/23 11:47:50.798
  May  8 11:47:50.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 11:47:50.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:47:50.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:47:50.809
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 11:47:50.812
  STEP: Saw pod success @ 05/08/23 11:47:52.824
  May  8 11:47:52.825: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-c33f4a04-9859-4c67-85f9-02acbe2d9725 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 11:47:52.84
  May  8 11:47:52.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9703" for this suite. @ 05/08/23 11:47:52.851
• [2.056 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/08/23 11:47:52.856
  May  8 11:47:52.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename var-expansion @ 05/08/23 11:47:52.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:47:52.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:47:52.866
  STEP: Creating a pod to test env composition @ 05/08/23 11:47:52.871
  STEP: Saw pod success @ 05/08/23 11:47:56.884
  May  8 11:47:56.886: INFO: Trying to get logs from node worker-1 pod var-expansion-f0358e91-8138-482c-940d-332d08f3d651 container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 11:47:56.892
  May  8 11:47:56.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1750" for this suite. @ 05/08/23 11:47:56.904
• [4.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/08/23 11:47:56.91
  May  8 11:47:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename cronjob @ 05/08/23 11:47:56.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:47:56.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:47:56.921
  STEP: Creating a cronjob @ 05/08/23 11:47:56.924
  STEP: Ensuring more than one job is running at a time @ 05/08/23 11:47:56.927
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/08/23 11:49:00.93
  STEP: Removing cronjob @ 05/08/23 11:49:00.932
  May  8 11:49:00.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4218" for this suite. @ 05/08/23 11:49:00.938
• [64.032 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/08/23 11:49:00.942
  May  8 11:49:00.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 11:49:00.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:00.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:00.961
  STEP: Creating configMap with name projected-configmap-test-volume-9dd190f0-08a2-4776-b776-1ebdf5232c27 @ 05/08/23 11:49:00.963
  STEP: Creating a pod to test consume configMaps @ 05/08/23 11:49:00.966
  STEP: Saw pod success @ 05/08/23 11:49:02.979
  May  8 11:49:02.981: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-5bd2cfd6-e537-42fd-b633-04a84b1a96ee container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 11:49:02.986
  May  8 11:49:02.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9664" for this suite. @ 05/08/23 11:49:02.999
• [2.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/08/23 11:49:03.008
  May  8 11:49:03.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename dns @ 05/08/23 11:49:03.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:03.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:03.021
  STEP: Creating a test externalName service @ 05/08/23 11:49:03.023
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8771.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local; sleep 1; done
   @ 05/08/23 11:49:03.026
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8771.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local; sleep 1; done
   @ 05/08/23 11:49:03.026
  STEP: creating a pod to probe DNS @ 05/08/23 11:49:03.026
  STEP: submitting the pod to kubernetes @ 05/08/23 11:49:03.026
  STEP: retrieving the pod @ 05/08/23 11:49:05.038
  STEP: looking for the results for each expected name from probers @ 05/08/23 11:49:05.04
  May  8 11:49:05.048: INFO: DNS probes using dns-test-6b9acad1-6957-428f-ba0c-f0476587fbc5 succeeded

  STEP: changing the externalName to bar.example.com @ 05/08/23 11:49:05.048
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8771.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local; sleep 1; done
   @ 05/08/23 11:49:05.052
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8771.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local; sleep 1; done
   @ 05/08/23 11:49:05.053
  STEP: creating a second pod to probe DNS @ 05/08/23 11:49:05.053
  STEP: submitting the pod to kubernetes @ 05/08/23 11:49:05.053
  STEP: retrieving the pod @ 05/08/23 11:49:13.074
  STEP: looking for the results for each expected name from probers @ 05/08/23 11:49:13.075
  May  8 11:49:13.080: INFO: File wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:13.083: INFO: File jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:13.083: INFO: Lookups using dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f failed for: [wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local]

  May  8 11:49:18.087: INFO: File wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:18.090: INFO: File jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:18.090: INFO: Lookups using dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f failed for: [wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local]

  May  8 11:49:23.088: INFO: File wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:23.091: INFO: File jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:23.091: INFO: Lookups using dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f failed for: [wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local]

  May  8 11:49:28.088: INFO: File wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:28.091: INFO: File jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:28.091: INFO: Lookups using dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f failed for: [wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local]

  May  8 11:49:33.087: INFO: File wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:33.091: INFO: File jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local from pod  dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  8 11:49:33.091: INFO: Lookups using dns-8771/dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f failed for: [wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local]

  May  8 11:49:38.091: INFO: DNS probes using dns-test-9d101dd2-8923-47d5-99fd-347a5760be0f succeeded

  STEP: changing the service to type=ClusterIP @ 05/08/23 11:49:38.091
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8771.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8771.svc.cluster.local; sleep 1; done
   @ 05/08/23 11:49:38.103
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8771.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8771.svc.cluster.local; sleep 1; done
   @ 05/08/23 11:49:38.103
  STEP: creating a third pod to probe DNS @ 05/08/23 11:49:38.103
  STEP: submitting the pod to kubernetes @ 05/08/23 11:49:38.105
  STEP: retrieving the pod @ 05/08/23 11:49:40.115
  STEP: looking for the results for each expected name from probers @ 05/08/23 11:49:40.117
  May  8 11:49:40.125: INFO: DNS probes using dns-test-66b73a0b-94b5-49a7-83d9-8738db3126e0 succeeded

  May  8 11:49:40.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 11:49:40.127
  STEP: deleting the pod @ 05/08/23 11:49:40.134
  STEP: deleting the pod @ 05/08/23 11:49:40.142
  STEP: deleting the test externalName service @ 05/08/23 11:49:40.151
  STEP: Destroying namespace "dns-8771" for this suite. @ 05/08/23 11:49:40.162
• [37.158 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/08/23 11:49:40.166
  May  8 11:49:40.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename field-validation @ 05/08/23 11:49:40.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:40.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:40.177
  May  8 11:49:40.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  W0508 11:49:42.716664      23 warnings.go:70] unknown field "alpha"
  W0508 11:49:42.716690      23 warnings.go:70] unknown field "beta"
  W0508 11:49:42.716696      23 warnings.go:70] unknown field "delta"
  W0508 11:49:42.716702      23 warnings.go:70] unknown field "epsilon"
  W0508 11:49:42.716708      23 warnings.go:70] unknown field "gamma"
  May  8 11:49:42.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8600" for this suite. @ 05/08/23 11:49:42.733
• [2.570 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/08/23 11:49:42.737
  May  8 11:49:42.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 11:49:42.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:42.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:42.747
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/08/23 11:49:42.749
  STEP: Saw pod success @ 05/08/23 11:49:46.762
  May  8 11:49:46.763: INFO: Trying to get logs from node worker-0 pod pod-5e10db11-fc71-45ae-af05-cf15e6dad05f container test-container: <nil>
  STEP: delete the pod @ 05/08/23 11:49:46.775
  May  8 11:49:46.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4463" for this suite. @ 05/08/23 11:49:46.79
• [4.056 seconds]
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/08/23 11:49:46.793
  May  8 11:49:46.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 11:49:46.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:46.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:46.805
  STEP: Creating configMap with name configmap-projected-all-test-volume-919635e1-7d45-43e3-9e73-ef52342860b8 @ 05/08/23 11:49:46.807
  STEP: Creating secret with name secret-projected-all-test-volume-a16b9aaf-5bc6-4b9a-b6fb-96ff39cc7f2a @ 05/08/23 11:49:46.81
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/08/23 11:49:46.813
  STEP: Saw pod success @ 05/08/23 11:49:50.827
  May  8 11:49:50.828: INFO: Trying to get logs from node worker-0 pod projected-volume-ed9d6428-5436-4ec0-8681-c09535c333fd container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 11:49:50.833
  May  8 11:49:50.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6459" for this suite. @ 05/08/23 11:49:50.842
• [4.054 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/08/23 11:49:50.848
  May  8 11:49:50.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename deployment @ 05/08/23 11:49:50.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:50.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:50.859
  May  8 11:49:50.862: INFO: Creating deployment "test-recreate-deployment"
  May  8 11:49:50.865: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May  8 11:49:50.870: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  May  8 11:49:52.874: INFO: Waiting deployment "test-recreate-deployment" to complete
  May  8 11:49:52.876: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May  8 11:49:52.883: INFO: Updating deployment test-recreate-deployment
  May  8 11:49:52.883: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May  8 11:49:52.939: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7842  6dfd4dd9-3f66-4e25-9875-d0a8a246904f 6215 2 2023-05-08 11:49:50 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-08 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043cbb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-08 11:49:52 +0000 UTC,LastTransitionTime:2023-05-08 11:49:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-08 11:49:52 +0000 UTC,LastTransitionTime:2023-05-08 11:49:50 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May  8 11:49:52.941: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-7842  0e4dbfad-44d4-4e1a-ba59-4a19e648449a 6211 1 2023-05-08 11:49:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6dfd4dd9-3f66-4e25-9875-d0a8a246904f 0xc003aa2517 0xc003aa2518}] [] [{kube-controller-manager Update apps/v1 2023-05-08 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dfd4dd9-3f66-4e25-9875-d0a8a246904f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 11:49:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aa25b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  8 11:49:52.942: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May  8 11:49:52.942: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-7842  638fe300-5097-4a9e-a410-f1afef1d47b8 6203 2 2023-05-08 11:49:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6dfd4dd9-3f66-4e25-9875-d0a8a246904f 0xc003aa2747 0xc003aa2748}] [] [{kube-controller-manager Update apps/v1 2023-05-08 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6dfd4dd9-3f66-4e25-9875-d0a8a246904f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 11:49:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003aa2848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  8 11:49:52.944: INFO: Pod "test-recreate-deployment-54757ffd6c-qwr7g" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-qwr7g test-recreate-deployment-54757ffd6c- deployment-7842  457a140f-6663-4350-a6a4-b79fff286e23 6214 0 2023-05-08 11:49:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 0e4dbfad-44d4-4e1a-ba59-4a19e648449a 0xc0060650f7 0xc0060650f8}] [] [{kube-controller-manager Update v1 2023-05-08 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e4dbfad-44d4-4e1a-ba59-4a19e648449a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 11:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tt5n5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tt5n5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:49:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:49:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 11:49:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:,StartTime:2023-05-08 11:49:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 11:49:52.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7842" for this suite. @ 05/08/23 11:49:52.946
• [2.101 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/08/23 11:49:52.95
  May  8 11:49:52.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/08/23 11:49:52.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:52.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:52.962
  May  8 11:49:52.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:49:56.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1282" for this suite. @ 05/08/23 11:49:56.039
• [3.093 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/08/23 11:49:56.043
  May  8 11:49:56.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename init-container @ 05/08/23 11:49:56.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:56.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:56.054
  STEP: creating the pod @ 05/08/23 11:49:56.057
  May  8 11:49:56.057: INFO: PodSpec: initContainers in spec.initContainers
  May  8 11:49:59.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8016" for this suite. @ 05/08/23 11:49:59.046
• [3.008 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/08/23 11:49:59.054
  May  8 11:49:59.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename security-context @ 05/08/23 11:49:59.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:49:59.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:49:59.064
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/08/23 11:49:59.066
  STEP: Saw pod success @ 05/08/23 11:50:01.077
  May  8 11:50:01.079: INFO: Trying to get logs from node worker-1 pod security-context-717c9c48-f5c3-44e3-80c7-8822e58918e3 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 11:50:01.085
  May  8 11:50:01.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-6727" for this suite. @ 05/08/23 11:50:01.105
• [2.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/08/23 11:50:01.111
  May  8 11:50:01.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename var-expansion @ 05/08/23 11:50:01.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:01.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:01.124
  STEP: Creating a pod to test substitution in volume subpath @ 05/08/23 11:50:01.127
  STEP: Saw pod success @ 05/08/23 11:50:03.139
  May  8 11:50:03.141: INFO: Trying to get logs from node worker-1 pod var-expansion-46a72844-86f6-42e0-96d0-1d2b223f97b5 container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 11:50:03.146
  May  8 11:50:03.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5311" for this suite. @ 05/08/23 11:50:03.157
• [2.050 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/08/23 11:50:03.161
  May  8 11:50:03.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 11:50:03.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:03.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:03.172
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/08/23 11:50:03.174
  STEP: Saw pod success @ 05/08/23 11:50:07.187
  May  8 11:50:07.188: INFO: Trying to get logs from node worker-1 pod pod-5251ab7a-6346-4550-8d2f-40a5844f5422 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 11:50:07.193
  May  8 11:50:07.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-998" for this suite. @ 05/08/23 11:50:07.205
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/08/23 11:50:07.21
  May  8 11:50:07.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 11:50:07.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:07.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:07.223
  STEP: Creating configMap with name configmap-test-volume-fdc99232-f212-4f34-a7cc-1028b4fb810a @ 05/08/23 11:50:07.225
  STEP: Creating a pod to test consume configMaps @ 05/08/23 11:50:07.228
  STEP: Saw pod success @ 05/08/23 11:50:09.239
  May  8 11:50:09.241: INFO: Trying to get logs from node worker-0 pod pod-configmaps-450ef7b5-32a3-48bf-9c31-243cd9ccfb86 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 11:50:09.246
  May  8 11:50:09.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6768" for this suite. @ 05/08/23 11:50:09.258
• [2.054 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/08/23 11:50:09.265
  May  8 11:50:09.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 11:50:09.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:09.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:09.277
  STEP: creating a replication controller @ 05/08/23 11:50:09.28
  May  8 11:50:09.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 create -f -'
  May  8 11:50:10.043: INFO: stderr: ""
  May  8 11:50:10.043: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/08/23 11:50:10.043
  May  8 11:50:10.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  8 11:50:10.113: INFO: stderr: ""
  May  8 11:50:10.113: INFO: stdout: "update-demo-nautilus-lf4z6 update-demo-nautilus-nq8pc "
  May  8 11:50:10.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get pods update-demo-nautilus-lf4z6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:50:10.175: INFO: stderr: ""
  May  8 11:50:10.175: INFO: stdout: ""
  May  8 11:50:10.176: INFO: update-demo-nautilus-lf4z6 is created but not running
  May  8 11:50:15.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  8 11:50:15.239: INFO: stderr: ""
  May  8 11:50:15.240: INFO: stdout: "update-demo-nautilus-lf4z6 update-demo-nautilus-nq8pc "
  May  8 11:50:15.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get pods update-demo-nautilus-lf4z6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:50:15.297: INFO: stderr: ""
  May  8 11:50:15.297: INFO: stdout: "true"
  May  8 11:50:15.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get pods update-demo-nautilus-lf4z6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  8 11:50:15.356: INFO: stderr: ""
  May  8 11:50:15.356: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  8 11:50:15.356: INFO: validating pod update-demo-nautilus-lf4z6
  May  8 11:50:15.361: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  8 11:50:15.361: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  8 11:50:15.361: INFO: update-demo-nautilus-lf4z6 is verified up and running
  May  8 11:50:15.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get pods update-demo-nautilus-nq8pc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  8 11:50:15.420: INFO: stderr: ""
  May  8 11:50:15.420: INFO: stdout: "true"
  May  8 11:50:15.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get pods update-demo-nautilus-nq8pc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  8 11:50:15.480: INFO: stderr: ""
  May  8 11:50:15.480: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  8 11:50:15.480: INFO: validating pod update-demo-nautilus-nq8pc
  May  8 11:50:15.485: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  8 11:50:15.485: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  8 11:50:15.485: INFO: update-demo-nautilus-nq8pc is verified up and running
  STEP: using delete to clean up resources @ 05/08/23 11:50:15.485
  May  8 11:50:15.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 delete --grace-period=0 --force -f -'
  May  8 11:50:15.546: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 11:50:15.546: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May  8 11:50:15.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get rc,svc -l name=update-demo --no-headers'
  May  8 11:50:15.618: INFO: stderr: "No resources found in kubectl-8324 namespace.\n"
  May  8 11:50:15.618: INFO: stdout: ""
  May  8 11:50:15.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8324 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  8 11:50:15.694: INFO: stderr: ""
  May  8 11:50:15.694: INFO: stdout: ""
  May  8 11:50:15.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8324" for this suite. @ 05/08/23 11:50:15.696
• [6.435 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/08/23 11:50:15.7
  May  8 11:50:15.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/08/23 11:50:15.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:15.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:15.713
  STEP: Setting up the test @ 05/08/23 11:50:15.716
  STEP: Creating hostNetwork=false pod @ 05/08/23 11:50:15.716
  STEP: Creating hostNetwork=true pod @ 05/08/23 11:50:17.728
  STEP: Running the test @ 05/08/23 11:50:19.739
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/08/23 11:50:19.739
  May  8 11:50:19.739: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:19.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:19.740: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:19.740: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  8 11:50:19.799: INFO: Exec stderr: ""
  May  8 11:50:19.799: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:19.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:19.799: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:19.799: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  8 11:50:19.859: INFO: Exec stderr: ""
  May  8 11:50:19.859: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:19.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:19.859: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:19.859: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  8 11:50:19.900: INFO: Exec stderr: ""
  May  8 11:50:19.901: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:19.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:19.901: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:19.901: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  8 11:50:19.971: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/08/23 11:50:19.971
  May  8 11:50:19.971: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:19.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:19.972: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:19.972: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May  8 11:50:20.014: INFO: Exec stderr: ""
  May  8 11:50:20.014: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:20.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:20.015: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:20.015: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May  8 11:50:20.074: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/08/23 11:50:20.074
  May  8 11:50:20.074: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:20.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:20.075: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:20.075: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  8 11:50:20.142: INFO: Exec stderr: ""
  May  8 11:50:20.142: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:20.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:20.143: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:20.143: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  8 11:50:20.184: INFO: Exec stderr: ""
  May  8 11:50:20.185: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:20.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:20.185: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:20.185: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  8 11:50:20.249: INFO: Exec stderr: ""
  May  8 11:50:20.249: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2414 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:50:20.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:50:20.250: INFO: ExecWithOptions: Clientset creation
  May  8 11:50:20.250: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2414/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  8 11:50:20.309: INFO: Exec stderr: ""
  May  8 11:50:20.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2414" for this suite. @ 05/08/23 11:50:20.312
• [4.615 seconds]
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/08/23 11:50:20.316
  May  8 11:50:20.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 11:50:20.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:20.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:20.328
  STEP: creating the pod @ 05/08/23 11:50:20.331
  STEP: submitting the pod to kubernetes @ 05/08/23 11:50:20.331
  STEP: verifying QOS class is set on the pod @ 05/08/23 11:50:20.336
  May  8 11:50:20.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7905" for this suite. @ 05/08/23 11:50:20.342
• [0.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/08/23 11:50:20.348
  May  8 11:50:20.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename subpath @ 05/08/23 11:50:20.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:20.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:20.36
  STEP: Setting up data @ 05/08/23 11:50:20.362
  STEP: Creating pod pod-subpath-test-configmap-5grl @ 05/08/23 11:50:20.373
  STEP: Creating a pod to test atomic-volume-subpath @ 05/08/23 11:50:20.373
  STEP: Saw pod success @ 05/08/23 11:50:44.421
  May  8 11:50:44.423: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-configmap-5grl container test-container-subpath-configmap-5grl: <nil>
  STEP: delete the pod @ 05/08/23 11:50:44.429
  STEP: Deleting pod pod-subpath-test-configmap-5grl @ 05/08/23 11:50:44.438
  May  8 11:50:44.438: INFO: Deleting pod "pod-subpath-test-configmap-5grl" in namespace "subpath-8455"
  May  8 11:50:44.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8455" for this suite. @ 05/08/23 11:50:44.443
• [24.098 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/08/23 11:50:44.447
  May  8 11:50:44.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename field-validation @ 05/08/23 11:50:44.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:44.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:44.459
  May  8 11:50:44.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  W0508 11:50:46.998608      23 warnings.go:70] unknown field "alpha"
  W0508 11:50:46.998635      23 warnings.go:70] unknown field "beta"
  W0508 11:50:46.998641      23 warnings.go:70] unknown field "delta"
  W0508 11:50:46.998647      23 warnings.go:70] unknown field "epsilon"
  W0508 11:50:46.998652      23 warnings.go:70] unknown field "gamma"
  May  8 11:50:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3631" for this suite. @ 05/08/23 11:50:47.017
• [2.575 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/08/23 11:50:47.022
  May  8 11:50:47.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 11:50:47.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:50:47.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:50:47.032
  STEP: Discovering how many secrets are in namespace by default @ 05/08/23 11:50:47.035
  STEP: Counting existing ResourceQuota @ 05/08/23 11:50:52.038
  STEP: Creating a ResourceQuota @ 05/08/23 11:50:57.041
  STEP: Ensuring resource quota status is calculated @ 05/08/23 11:50:57.044
  STEP: Creating a Secret @ 05/08/23 11:50:59.048
  STEP: Ensuring resource quota status captures secret creation @ 05/08/23 11:50:59.056
  STEP: Deleting a secret @ 05/08/23 11:51:01.059
  STEP: Ensuring resource quota status released usage @ 05/08/23 11:51:01.063
  May  8 11:51:03.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9944" for this suite. @ 05/08/23 11:51:03.068
• [16.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/08/23 11:51:03.077
  May  8 11:51:03.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 11:51:03.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:51:03.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:51:03.091
  STEP: creating a Service @ 05/08/23 11:51:03.097
  STEP: watching for the Service to be added @ 05/08/23 11:51:03.106
  May  8 11:51:03.107: INFO: Found Service test-service-2xdb6 in namespace services-3627 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May  8 11:51:03.107: INFO: Service test-service-2xdb6 created
  STEP: Getting /status @ 05/08/23 11:51:03.108
  May  8 11:51:03.110: INFO: Service test-service-2xdb6 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/08/23 11:51:03.111
  STEP: watching for the Service to be patched @ 05/08/23 11:51:03.115
  May  8 11:51:03.117: INFO: observed Service test-service-2xdb6 in namespace services-3627 with annotations: map[] & LoadBalancer: {[]}
  May  8 11:51:03.117: INFO: Found Service test-service-2xdb6 in namespace services-3627 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May  8 11:51:03.117: INFO: Service test-service-2xdb6 has service status patched
  STEP: updating the ServiceStatus @ 05/08/23 11:51:03.118
  May  8 11:51:03.124: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/08/23 11:51:03.124
  May  8 11:51:03.125: INFO: Observed Service test-service-2xdb6 in namespace services-3627 with annotations: map[] & Conditions: {[]}
  May  8 11:51:03.125: INFO: Observed event: &Service{ObjectMeta:{test-service-2xdb6  services-3627  65afd231-267b-4602-8692-62230bf66aa2 6731 0 2023-05-08 11:51:03 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-08 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-08 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.169.183,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.169.183],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May  8 11:51:03.126: INFO: Found Service test-service-2xdb6 in namespace services-3627 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  8 11:51:03.126: INFO: Service test-service-2xdb6 has service status updated
  STEP: patching the service @ 05/08/23 11:51:03.126
  STEP: watching for the Service to be patched @ 05/08/23 11:51:03.135
  May  8 11:51:03.136: INFO: observed Service test-service-2xdb6 in namespace services-3627 with labels: map[test-service-static:true]
  May  8 11:51:03.136: INFO: observed Service test-service-2xdb6 in namespace services-3627 with labels: map[test-service-static:true]
  May  8 11:51:03.136: INFO: observed Service test-service-2xdb6 in namespace services-3627 with labels: map[test-service-static:true]
  May  8 11:51:03.136: INFO: Found Service test-service-2xdb6 in namespace services-3627 with labels: map[test-service:patched test-service-static:true]
  May  8 11:51:03.136: INFO: Service test-service-2xdb6 patched
  STEP: deleting the service @ 05/08/23 11:51:03.136
  STEP: watching for the Service to be deleted @ 05/08/23 11:51:03.145
  May  8 11:51:03.147: INFO: Observed event: ADDED
  May  8 11:51:03.147: INFO: Observed event: MODIFIED
  May  8 11:51:03.147: INFO: Observed event: MODIFIED
  May  8 11:51:03.147: INFO: Observed event: MODIFIED
  May  8 11:51:03.147: INFO: Found Service test-service-2xdb6 in namespace services-3627 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May  8 11:51:03.147: INFO: Service test-service-2xdb6 deleted
  May  8 11:51:03.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3627" for this suite. @ 05/08/23 11:51:03.149
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/08/23 11:51:03.154
  May  8 11:51:03.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pod-network-test @ 05/08/23 11:51:03.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:51:03.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:51:03.167
  STEP: Performing setup for networking test in namespace pod-network-test-6969 @ 05/08/23 11:51:03.17
  STEP: creating a selector @ 05/08/23 11:51:03.17
  STEP: Creating the service pods in kubernetes @ 05/08/23 11:51:03.17
  May  8 11:51:03.170: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/08/23 11:51:15.213
  May  8 11:51:17.233: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May  8 11:51:17.233: INFO: Going to poll 10.244.1.109 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May  8 11:51:17.235: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.109:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6969 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:51:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:51:17.235: INFO: ExecWithOptions: Clientset creation
  May  8 11:51:17.235: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6969/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.109%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  8 11:51:17.296: INFO: Found all 1 expected endpoints: [netserver-0]
  May  8 11:51:17.296: INFO: Going to poll 10.244.0.83 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May  8 11:51:17.299: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.83:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6969 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 11:51:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 11:51:17.299: INFO: ExecWithOptions: Clientset creation
  May  8 11:51:17.299: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6969/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.83%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  8 11:51:17.342: INFO: Found all 1 expected endpoints: [netserver-1]
  May  8 11:51:17.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6969" for this suite. @ 05/08/23 11:51:17.344
• [14.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/08/23 11:51:17.353
  May  8 11:51:17.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename gc @ 05/08/23 11:51:17.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:51:17.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:51:17.365
  STEP: create the rc @ 05/08/23 11:51:17.369
  W0508 11:51:17.374294      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/08/23 11:51:23.377
  STEP: wait for the rc to be deleted @ 05/08/23 11:51:23.382
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/08/23 11:51:28.387
  STEP: Gathering metrics @ 05/08/23 11:51:58.398
  W0508 11:51:58.401943      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  8 11:51:58.401: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  8 11:51:58.402: INFO: Deleting pod "simpletest.rc-2224p" in namespace "gc-2238"
  May  8 11:51:58.412: INFO: Deleting pod "simpletest.rc-2k9lz" in namespace "gc-2238"
  May  8 11:51:58.419: INFO: Deleting pod "simpletest.rc-2mkdp" in namespace "gc-2238"
  May  8 11:51:58.428: INFO: Deleting pod "simpletest.rc-2skrm" in namespace "gc-2238"
  May  8 11:51:58.434: INFO: Deleting pod "simpletest.rc-2whsh" in namespace "gc-2238"
  May  8 11:51:58.443: INFO: Deleting pod "simpletest.rc-424tn" in namespace "gc-2238"
  May  8 11:51:58.451: INFO: Deleting pod "simpletest.rc-4h774" in namespace "gc-2238"
  May  8 11:51:58.461: INFO: Deleting pod "simpletest.rc-4hpbl" in namespace "gc-2238"
  May  8 11:51:58.469: INFO: Deleting pod "simpletest.rc-5h72g" in namespace "gc-2238"
  May  8 11:51:58.478: INFO: Deleting pod "simpletest.rc-5kxrc" in namespace "gc-2238"
  May  8 11:51:58.486: INFO: Deleting pod "simpletest.rc-5mxcf" in namespace "gc-2238"
  May  8 11:51:58.495: INFO: Deleting pod "simpletest.rc-5rnb8" in namespace "gc-2238"
  May  8 11:51:58.505: INFO: Deleting pod "simpletest.rc-5vq9b" in namespace "gc-2238"
  May  8 11:51:58.513: INFO: Deleting pod "simpletest.rc-5xnz6" in namespace "gc-2238"
  May  8 11:51:58.521: INFO: Deleting pod "simpletest.rc-66zrg" in namespace "gc-2238"
  May  8 11:51:58.531: INFO: Deleting pod "simpletest.rc-68dt6" in namespace "gc-2238"
  May  8 11:51:58.553: INFO: Deleting pod "simpletest.rc-6zffb" in namespace "gc-2238"
  May  8 11:51:58.559: INFO: Deleting pod "simpletest.rc-76c4r" in namespace "gc-2238"
  May  8 11:51:58.569: INFO: Deleting pod "simpletest.rc-78dt9" in namespace "gc-2238"
  May  8 11:51:58.579: INFO: Deleting pod "simpletest.rc-7nmhr" in namespace "gc-2238"
  May  8 11:51:58.589: INFO: Deleting pod "simpletest.rc-7vhh9" in namespace "gc-2238"
  May  8 11:51:58.598: INFO: Deleting pod "simpletest.rc-7vkdr" in namespace "gc-2238"
  May  8 11:51:58.609: INFO: Deleting pod "simpletest.rc-82w2h" in namespace "gc-2238"
  May  8 11:51:58.630: INFO: Deleting pod "simpletest.rc-8665z" in namespace "gc-2238"
  May  8 11:51:58.641: INFO: Deleting pod "simpletest.rc-882dg" in namespace "gc-2238"
  May  8 11:51:58.652: INFO: Deleting pod "simpletest.rc-9246m" in namespace "gc-2238"
  May  8 11:51:58.665: INFO: Deleting pod "simpletest.rc-95vc8" in namespace "gc-2238"
  May  8 11:51:58.678: INFO: Deleting pod "simpletest.rc-9f42m" in namespace "gc-2238"
  May  8 11:51:58.688: INFO: Deleting pod "simpletest.rc-9lhlm" in namespace "gc-2238"
  May  8 11:51:58.697: INFO: Deleting pod "simpletest.rc-9pt7x" in namespace "gc-2238"
  May  8 11:51:58.707: INFO: Deleting pod "simpletest.rc-b7d8n" in namespace "gc-2238"
  May  8 11:51:58.718: INFO: Deleting pod "simpletest.rc-bb4nx" in namespace "gc-2238"
  May  8 11:51:58.729: INFO: Deleting pod "simpletest.rc-bgg5q" in namespace "gc-2238"
  May  8 11:51:58.740: INFO: Deleting pod "simpletest.rc-bqq5n" in namespace "gc-2238"
  May  8 11:51:58.749: INFO: Deleting pod "simpletest.rc-c55rr" in namespace "gc-2238"
  May  8 11:51:58.761: INFO: Deleting pod "simpletest.rc-cqzr8" in namespace "gc-2238"
  May  8 11:51:58.768: INFO: Deleting pod "simpletest.rc-d2bmg" in namespace "gc-2238"
  May  8 11:51:58.778: INFO: Deleting pod "simpletest.rc-d2s25" in namespace "gc-2238"
  May  8 11:51:58.784: INFO: Deleting pod "simpletest.rc-d788h" in namespace "gc-2238"
  May  8 11:51:58.791: INFO: Deleting pod "simpletest.rc-d87r6" in namespace "gc-2238"
  May  8 11:51:58.801: INFO: Deleting pod "simpletest.rc-dbb95" in namespace "gc-2238"
  May  8 11:51:58.810: INFO: Deleting pod "simpletest.rc-dmmdl" in namespace "gc-2238"
  May  8 11:51:58.818: INFO: Deleting pod "simpletest.rc-fbvld" in namespace "gc-2238"
  May  8 11:51:58.826: INFO: Deleting pod "simpletest.rc-flcq2" in namespace "gc-2238"
  May  8 11:51:58.833: INFO: Deleting pod "simpletest.rc-frr6w" in namespace "gc-2238"
  May  8 11:51:58.847: INFO: Deleting pod "simpletest.rc-gjrd4" in namespace "gc-2238"
  May  8 11:51:58.855: INFO: Deleting pod "simpletest.rc-h4498" in namespace "gc-2238"
  May  8 11:51:58.870: INFO: Deleting pod "simpletest.rc-hn2k5" in namespace "gc-2238"
  May  8 11:51:58.880: INFO: Deleting pod "simpletest.rc-htfjd" in namespace "gc-2238"
  May  8 11:51:58.889: INFO: Deleting pod "simpletest.rc-jcn24" in namespace "gc-2238"
  May  8 11:51:58.896: INFO: Deleting pod "simpletest.rc-jgln8" in namespace "gc-2238"
  May  8 11:51:58.906: INFO: Deleting pod "simpletest.rc-jh2sw" in namespace "gc-2238"
  May  8 11:51:58.915: INFO: Deleting pod "simpletest.rc-jqcwd" in namespace "gc-2238"
  May  8 11:51:58.922: INFO: Deleting pod "simpletest.rc-jrcnj" in namespace "gc-2238"
  May  8 11:51:58.928: INFO: Deleting pod "simpletest.rc-js7vv" in namespace "gc-2238"
  May  8 11:51:58.937: INFO: Deleting pod "simpletest.rc-jsdx7" in namespace "gc-2238"
  May  8 11:51:58.948: INFO: Deleting pod "simpletest.rc-k5zf8" in namespace "gc-2238"
  May  8 11:51:58.957: INFO: Deleting pod "simpletest.rc-kb7sp" in namespace "gc-2238"
  May  8 11:51:58.964: INFO: Deleting pod "simpletest.rc-khv5b" in namespace "gc-2238"
  May  8 11:51:58.974: INFO: Deleting pod "simpletest.rc-krqv5" in namespace "gc-2238"
  May  8 11:51:59.002: INFO: Deleting pod "simpletest.rc-kszth" in namespace "gc-2238"
  May  8 11:51:59.049: INFO: Deleting pod "simpletest.rc-kx6b9" in namespace "gc-2238"
  May  8 11:51:59.100: INFO: Deleting pod "simpletest.rc-m85hn" in namespace "gc-2238"
  May  8 11:51:59.154: INFO: Deleting pod "simpletest.rc-mp4vf" in namespace "gc-2238"
  May  8 11:51:59.197: INFO: Deleting pod "simpletest.rc-mvhrx" in namespace "gc-2238"
  May  8 11:51:59.250: INFO: Deleting pod "simpletest.rc-mwkkv" in namespace "gc-2238"
  May  8 11:51:59.297: INFO: Deleting pod "simpletest.rc-n7zmx" in namespace "gc-2238"
  May  8 11:51:59.347: INFO: Deleting pod "simpletest.rc-p6ksk" in namespace "gc-2238"
  May  8 11:51:59.404: INFO: Deleting pod "simpletest.rc-p7ccr" in namespace "gc-2238"
  May  8 11:51:59.450: INFO: Deleting pod "simpletest.rc-pkvkq" in namespace "gc-2238"
  May  8 11:51:59.498: INFO: Deleting pod "simpletest.rc-prsbd" in namespace "gc-2238"
  May  8 11:51:59.549: INFO: Deleting pod "simpletest.rc-q5fl2" in namespace "gc-2238"
  May  8 11:51:59.599: INFO: Deleting pod "simpletest.rc-r77ks" in namespace "gc-2238"
  May  8 11:51:59.650: INFO: Deleting pod "simpletest.rc-rh9q4" in namespace "gc-2238"
  May  8 11:51:59.699: INFO: Deleting pod "simpletest.rc-s79qr" in namespace "gc-2238"
  May  8 11:51:59.747: INFO: Deleting pod "simpletest.rc-sdgvl" in namespace "gc-2238"
  May  8 11:51:59.797: INFO: Deleting pod "simpletest.rc-sj2mx" in namespace "gc-2238"
  May  8 11:51:59.849: INFO: Deleting pod "simpletest.rc-smqn4" in namespace "gc-2238"
  May  8 11:51:59.899: INFO: Deleting pod "simpletest.rc-tgfxj" in namespace "gc-2238"
  May  8 11:51:59.947: INFO: Deleting pod "simpletest.rc-trd9h" in namespace "gc-2238"
  May  8 11:51:59.998: INFO: Deleting pod "simpletest.rc-tt6jl" in namespace "gc-2238"
  May  8 11:52:00.051: INFO: Deleting pod "simpletest.rc-tz8sh" in namespace "gc-2238"
  May  8 11:52:00.099: INFO: Deleting pod "simpletest.rc-v6nbm" in namespace "gc-2238"
  May  8 11:52:00.147: INFO: Deleting pod "simpletest.rc-vgrxm" in namespace "gc-2238"
  May  8 11:52:00.196: INFO: Deleting pod "simpletest.rc-vmpx4" in namespace "gc-2238"
  May  8 11:52:00.247: INFO: Deleting pod "simpletest.rc-vrcmt" in namespace "gc-2238"
  May  8 11:52:00.297: INFO: Deleting pod "simpletest.rc-whrbc" in namespace "gc-2238"
  May  8 11:52:00.353: INFO: Deleting pod "simpletest.rc-wkg2q" in namespace "gc-2238"
  May  8 11:52:00.405: INFO: Deleting pod "simpletest.rc-wkvhp" in namespace "gc-2238"
  May  8 11:52:00.448: INFO: Deleting pod "simpletest.rc-wr75d" in namespace "gc-2238"
  May  8 11:52:00.501: INFO: Deleting pod "simpletest.rc-x5spn" in namespace "gc-2238"
  May  8 11:52:00.552: INFO: Deleting pod "simpletest.rc-x887b" in namespace "gc-2238"
  May  8 11:52:00.598: INFO: Deleting pod "simpletest.rc-xcrw9" in namespace "gc-2238"
  May  8 11:52:00.646: INFO: Deleting pod "simpletest.rc-xkstx" in namespace "gc-2238"
  May  8 11:52:00.702: INFO: Deleting pod "simpletest.rc-xr7t9" in namespace "gc-2238"
  May  8 11:52:00.751: INFO: Deleting pod "simpletest.rc-xx65x" in namespace "gc-2238"
  May  8 11:52:00.796: INFO: Deleting pod "simpletest.rc-zbcrd" in namespace "gc-2238"
  May  8 11:52:00.852: INFO: Deleting pod "simpletest.rc-zdrnj" in namespace "gc-2238"
  May  8 11:52:00.896: INFO: Deleting pod "simpletest.rc-zfhzq" in namespace "gc-2238"
  May  8 11:52:00.949: INFO: Deleting pod "simpletest.rc-zkz9v" in namespace "gc-2238"
  May  8 11:52:00.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2238" for this suite. @ 05/08/23 11:52:01.05
• [43.741 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/08/23 11:52:01.118
  May  8 11:52:01.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename controllerrevisions @ 05/08/23 11:52:01.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:01.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:01.137
  STEP: Creating DaemonSet "e2e-xc9rh-daemon-set" @ 05/08/23 11:52:01.157
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/08/23 11:52:01.164
  May  8 11:52:01.172: INFO: Number of nodes with available pods controlled by daemonset e2e-xc9rh-daemon-set: 0
  May  8 11:52:01.172: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 11:52:02.177: INFO: Number of nodes with available pods controlled by daemonset e2e-xc9rh-daemon-set: 0
  May  8 11:52:02.177: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 11:52:03.177: INFO: Number of nodes with available pods controlled by daemonset e2e-xc9rh-daemon-set: 2
  May  8 11:52:03.177: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-xc9rh-daemon-set
  STEP: Confirm DaemonSet "e2e-xc9rh-daemon-set" successfully created with "daemonset-name=e2e-xc9rh-daemon-set" label @ 05/08/23 11:52:03.179
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-xc9rh-daemon-set" @ 05/08/23 11:52:03.182
  May  8 11:52:03.184: INFO: Located ControllerRevision: "e2e-xc9rh-daemon-set-f77dbf5f9"
  STEP: Patching ControllerRevision "e2e-xc9rh-daemon-set-f77dbf5f9" @ 05/08/23 11:52:03.186
  May  8 11:52:03.191: INFO: e2e-xc9rh-daemon-set-f77dbf5f9 has been patched
  STEP: Create a new ControllerRevision @ 05/08/23 11:52:03.191
  May  8 11:52:03.195: INFO: Created ControllerRevision: e2e-xc9rh-daemon-set-596ff6697f
  STEP: Confirm that there are two ControllerRevisions @ 05/08/23 11:52:03.195
  May  8 11:52:03.195: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  8 11:52:03.196: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-xc9rh-daemon-set-f77dbf5f9" @ 05/08/23 11:52:03.197
  STEP: Confirm that there is only one ControllerRevision @ 05/08/23 11:52:03.2
  May  8 11:52:03.200: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  8 11:52:03.201: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-xc9rh-daemon-set-596ff6697f" @ 05/08/23 11:52:03.203
  May  8 11:52:03.209: INFO: e2e-xc9rh-daemon-set-596ff6697f has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/08/23 11:52:03.209
  W0508 11:52:03.217125      23 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/08/23 11:52:03.217
  May  8 11:52:03.217: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  8 11:52:04.219: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  8 11:52:04.221: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-xc9rh-daemon-set-596ff6697f=updated" @ 05/08/23 11:52:04.221
  STEP: Confirm that there is only one ControllerRevision @ 05/08/23 11:52:04.225
  May  8 11:52:04.225: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  8 11:52:04.227: INFO: Found 1 ControllerRevisions
  May  8 11:52:04.229: INFO: ControllerRevision "e2e-xc9rh-daemon-set-7fbd6b854" has revision 3
  STEP: Deleting DaemonSet "e2e-xc9rh-daemon-set" @ 05/08/23 11:52:04.23
  STEP: deleting DaemonSet.extensions e2e-xc9rh-daemon-set in namespace controllerrevisions-552, will wait for the garbage collector to delete the pods @ 05/08/23 11:52:04.23
  May  8 11:52:04.286: INFO: Deleting DaemonSet.extensions e2e-xc9rh-daemon-set took: 3.29177ms
  May  8 11:52:04.387: INFO: Terminating DaemonSet.extensions e2e-xc9rh-daemon-set pods took: 100.778395ms
  May  8 11:52:04.789: INFO: Number of nodes with available pods controlled by daemonset e2e-xc9rh-daemon-set: 0
  May  8 11:52:04.789: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-xc9rh-daemon-set
  May  8 11:52:04.792: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8287"},"items":null}

  May  8 11:52:04.794: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8287"},"items":null}

  May  8 11:52:04.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-552" for this suite. @ 05/08/23 11:52:04.802
• [3.687 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/08/23 11:52:04.808
  May  8 11:52:04.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 11:52:04.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:04.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:04.818
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 11:52:04.82
  STEP: Saw pod success @ 05/08/23 11:52:06.83
  May  8 11:52:06.832: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-7069b04c-e1ba-4676-97f6-237b7edcc6a2 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 11:52:06.851
  May  8 11:52:06.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5383" for this suite. @ 05/08/23 11:52:06.865
• [2.061 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/08/23 11:52:06.872
  May  8 11:52:06.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubelet-test @ 05/08/23 11:52:06.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:06.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:06.884
  May  8 11:52:08.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8944" for this suite. @ 05/08/23 11:52:08.907
• [2.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/08/23 11:52:08.914
  May  8 11:52:08.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubelet-test @ 05/08/23 11:52:08.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:08.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:08.925
  May  8 11:52:10.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8754" for this suite. @ 05/08/23 11:52:10.948
• [2.038 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/08/23 11:52:10.952
  May  8 11:52:10.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/08/23 11:52:10.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:10.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:10.963
  May  8 11:52:12.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/08/23 11:52:12.986
  STEP: Cleaning up the configmap @ 05/08/23 11:52:12.989
  STEP: Cleaning up the pod @ 05/08/23 11:52:12.992
  STEP: Destroying namespace "emptydir-wrapper-4702" for this suite. @ 05/08/23 11:52:13.001
• [2.053 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/08/23 11:52:13.006
  May  8 11:52:13.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 11:52:13.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:13.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:13.019
  STEP: Counting existing ResourceQuota @ 05/08/23 11:52:13.021
  STEP: Creating a ResourceQuota @ 05/08/23 11:52:18.024
  STEP: Ensuring resource quota status is calculated @ 05/08/23 11:52:18.029
  May  8 11:52:20.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9820" for this suite. @ 05/08/23 11:52:20.035
• [7.032 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/08/23 11:52:20.039
  May  8 11:52:20.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 11:52:20.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:20.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:20.051
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/08/23 11:52:20.053
  STEP: Saw pod success @ 05/08/23 11:52:22.064
  May  8 11:52:22.066: INFO: Trying to get logs from node worker-1 pod pod-7789a6b1-1473-4f46-97aa-bdee8f2166a7 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 11:52:22.071
  May  8 11:52:22.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-795" for this suite. @ 05/08/23 11:52:22.081
• [2.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/08/23 11:52:22.089
  May  8 11:52:22.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename containers @ 05/08/23 11:52:22.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:22.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:22.1
  STEP: Creating a pod to test override command @ 05/08/23 11:52:22.102
  STEP: Saw pod success @ 05/08/23 11:52:26.115
  May  8 11:52:26.117: INFO: Trying to get logs from node worker-0 pod client-containers-17a0ac0a-a701-46cf-874a-571e6ba9f9a1 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 11:52:26.122
  May  8 11:52:26.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-340" for this suite. @ 05/08/23 11:52:26.133
• [4.047 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/08/23 11:52:26.137
  May  8 11:52:26.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename svcaccounts @ 05/08/23 11:52:26.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:26.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:26.149
  May  8 11:52:26.153: INFO: Got root ca configmap in namespace "svcaccounts-5137"
  May  8 11:52:26.156: INFO: Deleted root ca configmap in namespace "svcaccounts-5137"
  STEP: waiting for a new root ca configmap created @ 05/08/23 11:52:26.657
  May  8 11:52:26.659: INFO: Recreated root ca configmap in namespace "svcaccounts-5137"
  May  8 11:52:26.663: INFO: Updated root ca configmap in namespace "svcaccounts-5137"
  STEP: waiting for the root ca configmap reconciled @ 05/08/23 11:52:27.163
  May  8 11:52:27.165: INFO: Reconciled root ca configmap in namespace "svcaccounts-5137"
  May  8 11:52:27.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5137" for this suite. @ 05/08/23 11:52:27.168
• [1.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/08/23 11:52:27.174
  May  8 11:52:27.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename dns @ 05/08/23 11:52:27.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:27.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:27.188
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9509.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9509.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/08/23 11:52:27.19
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9509.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9509.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/08/23 11:52:27.19
  STEP: creating a pod to probe /etc/hosts @ 05/08/23 11:52:27.19
  STEP: submitting the pod to kubernetes @ 05/08/23 11:52:27.19
  STEP: retrieving the pod @ 05/08/23 11:52:29.204
  STEP: looking for the results for each expected name from probers @ 05/08/23 11:52:29.206
  May  8 11:52:29.220: INFO: DNS probes using dns-9509/dns-test-7ea5680d-2389-4ad1-a40e-d6d7524fd414 succeeded

  May  8 11:52:29.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 11:52:29.222
  STEP: Destroying namespace "dns-9509" for this suite. @ 05/08/23 11:52:29.232
• [2.065 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/08/23 11:52:29.24
  May  8 11:52:29.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 11:52:29.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:29.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:29.25
  STEP: Creating a pod to test downward api env vars @ 05/08/23 11:52:29.252
  STEP: Saw pod success @ 05/08/23 11:52:33.269
  May  8 11:52:33.271: INFO: Trying to get logs from node worker-0 pod downward-api-3ab47e4d-139e-491d-bf03-5acaa3a6a5ee container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 11:52:33.277
  May  8 11:52:33.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6536" for this suite. @ 05/08/23 11:52:33.289
• [4.052 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/08/23 11:52:33.293
  May  8 11:52:33.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename statefulset @ 05/08/23 11:52:33.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:52:33.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:52:33.305
  STEP: Creating service test in namespace statefulset-437 @ 05/08/23 11:52:33.308
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/08/23 11:52:33.311
  STEP: Creating stateful set ss in namespace statefulset-437 @ 05/08/23 11:52:33.313
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-437 @ 05/08/23 11:52:33.318
  May  8 11:52:33.319: INFO: Found 0 stateful pods, waiting for 1
  May  8 11:52:43.324: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/08/23 11:52:43.324
  May  8 11:52:43.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-437 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 11:52:43.459: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 11:52:43.459: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 11:52:43.459: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 11:52:43.461: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May  8 11:52:53.464: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  8 11:52:53.464: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 11:52:53.474: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998782s
  May  8 11:52:54.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997770122s
  May  8 11:52:55.479: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995157547s
  May  8 11:52:56.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992240829s
  May  8 11:52:57.485: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989592579s
  May  8 11:52:58.487: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98679823s
  May  8 11:52:59.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.984207027s
  May  8 11:53:00.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981471512s
  May  8 11:53:01.495: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978527904s
  May  8 11:53:02.498: INFO: Verifying statefulset ss doesn't scale past 1 for another 975.922555ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-437 @ 05/08/23 11:53:03.499
  May  8 11:53:03.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-437 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 11:53:03.620: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  8 11:53:03.620: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 11:53:03.620: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  8 11:53:03.622: INFO: Found 1 stateful pods, waiting for 3
  May  8 11:53:13.628: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  8 11:53:13.629: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May  8 11:53:13.629: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/08/23 11:53:13.629
  STEP: Scale down will halt with unhealthy stateful pod @ 05/08/23 11:53:13.629
  May  8 11:53:13.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-437 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 11:53:13.744: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 11:53:13.744: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 11:53:13.744: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 11:53:13.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-437 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 11:53:13.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 11:53:13.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 11:53:13.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 11:53:13.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-437 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 11:53:14.035: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 11:53:14.035: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 11:53:14.035: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 11:53:14.035: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 11:53:14.037: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  May  8 11:53:24.043: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  8 11:53:24.043: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May  8 11:53:24.043: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May  8 11:53:24.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998788s
  May  8 11:53:25.055: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997759239s
  May  8 11:53:26.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994457147s
  May  8 11:53:27.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991809027s
  May  8 11:53:28.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989039116s
  May  8 11:53:29.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986394894s
  May  8 11:53:30.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983749417s
  May  8 11:53:31.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.980475438s
  May  8 11:53:32.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977590704s
  May  8 11:53:33.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 974.804843ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-437 @ 05/08/23 11:53:34.078
  May  8 11:53:34.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-437 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 11:53:34.213: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  8 11:53:34.213: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 11:53:34.213: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  8 11:53:34.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-437 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 11:53:34.339: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  8 11:53:34.339: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 11:53:34.339: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  8 11:53:34.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-437 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 11:53:34.461: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  8 11:53:34.461: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 11:53:34.461: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  8 11:53:34.461: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/08/23 11:53:44.471
  May  8 11:53:44.472: INFO: Deleting all statefulset in ns statefulset-437
  May  8 11:53:44.473: INFO: Scaling statefulset ss to 0
  May  8 11:53:44.479: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 11:53:44.480: INFO: Deleting statefulset ss
  May  8 11:53:44.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-437" for this suite. @ 05/08/23 11:53:44.489
• [71.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/08/23 11:53:44.495
  May  8 11:53:44.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename statefulset @ 05/08/23 11:53:44.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:53:44.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:53:44.506
  STEP: Creating service test in namespace statefulset-6062 @ 05/08/23 11:53:44.508
  STEP: Creating a new StatefulSet @ 05/08/23 11:53:44.511
  May  8 11:53:44.518: INFO: Found 0 stateful pods, waiting for 3
  May  8 11:53:54.525: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  8 11:53:54.525: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  8 11:53:54.525: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May  8 11:53:54.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-6062 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 11:53:54.653: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 11:53:54.653: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 11:53:54.653: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/08/23 11:54:04.665
  May  8 11:54:04.681: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/08/23 11:54:04.681
  STEP: Updating Pods in reverse ordinal order @ 05/08/23 11:54:14.692
  May  8 11:54:14.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-6062 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 11:54:14.811: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  8 11:54:14.811: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 11:54:14.811: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 05/08/23 11:54:34.824
  May  8 11:54:34.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-6062 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 11:54:34.946: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 11:54:34.946: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 11:54:34.946: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 11:54:44.973: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 05/08/23 11:54:54.986
  May  8 11:54:54.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-6062 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 11:54:55.102: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  8 11:54:55.102: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 11:54:55.102: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  8 11:55:05.117: INFO: Deleting all statefulset in ns statefulset-6062
  May  8 11:55:05.119: INFO: Scaling statefulset ss2 to 0
  May  8 11:55:15.133: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 11:55:15.135: INFO: Deleting statefulset ss2
  May  8 11:55:15.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6062" for this suite. @ 05/08/23 11:55:15.144
• [90.653 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/08/23 11:55:15.151
  May  8 11:55:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 11:55:15.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:55:15.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:55:15.163
  STEP: Counting existing ResourceQuota @ 05/08/23 11:55:15.165
  STEP: Creating a ResourceQuota @ 05/08/23 11:55:20.167
  STEP: Ensuring resource quota status is calculated @ 05/08/23 11:55:20.171
  STEP: Creating a Service @ 05/08/23 11:55:22.174
  STEP: Creating a NodePort Service @ 05/08/23 11:55:22.187
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/08/23 11:55:22.203
  STEP: Ensuring resource quota status captures service creation @ 05/08/23 11:55:22.218
  STEP: Deleting Services @ 05/08/23 11:55:24.221
  STEP: Ensuring resource quota status released usage @ 05/08/23 11:55:24.244
  May  8 11:55:26.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9501" for this suite. @ 05/08/23 11:55:26.25
• [11.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/08/23 11:55:26.256
  May  8 11:55:26.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 11:55:26.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:55:26.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:55:26.268
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-892 @ 05/08/23 11:55:26.271
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/08/23 11:55:26.279
  STEP: creating service externalsvc in namespace services-892 @ 05/08/23 11:55:26.279
  STEP: creating replication controller externalsvc in namespace services-892 @ 05/08/23 11:55:26.293
  I0508 11:55:26.298944      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-892, replica count: 2
  I0508 11:55:29.350231      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/08/23 11:55:29.352
  May  8 11:55:29.362: INFO: Creating new exec pod
  May  8 11:55:31.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-892 exec execpodtxq76 -- /bin/sh -x -c nslookup clusterip-service.services-892.svc.cluster.local'
  May  8 11:55:31.524: INFO: stderr: "+ nslookup clusterip-service.services-892.svc.cluster.local\n"
  May  8 11:55:31.524: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-892.svc.cluster.local\tcanonical name = externalsvc.services-892.svc.cluster.local.\nName:\texternalsvc.services-892.svc.cluster.local\nAddress: 10.103.12.25\n\n"
  May  8 11:55:31.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-892, will wait for the garbage collector to delete the pods @ 05/08/23 11:55:31.527
  May  8 11:55:31.583: INFO: Deleting ReplicationController externalsvc took: 3.500218ms
  May  8 11:55:31.683: INFO: Terminating ReplicationController externalsvc pods took: 100.770001ms
  May  8 11:55:33.297: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-892" for this suite. @ 05/08/23 11:55:33.304
• [7.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/08/23 11:55:33.311
  May  8 11:55:33.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 11:55:33.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:55:33.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:55:33.321
  STEP: Creating pod test-grpc-c68544f9-7dde-4c45-ac00-c2f40d2d447a in namespace container-probe-7158 @ 05/08/23 11:55:33.323
  May  8 11:55:35.333: INFO: Started pod test-grpc-c68544f9-7dde-4c45-ac00-c2f40d2d447a in namespace container-probe-7158
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/08/23 11:55:35.333
  May  8 11:55:35.334: INFO: Initial restart count of pod test-grpc-c68544f9-7dde-4c45-ac00-c2f40d2d447a is 0
  May  8 11:59:35.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 11:59:35.73
  STEP: Destroying namespace "container-probe-7158" for this suite. @ 05/08/23 11:59:35.739
• [242.432 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/08/23 11:59:35.744
  May  8 11:59:35.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 11:59:35.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 11:59:35.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 11:59:35.757
  STEP: Creating pod busybox-3c400190-6336-49e3-b85e-6dd141e7c667 in namespace container-probe-9260 @ 05/08/23 11:59:35.76
  May  8 11:59:37.770: INFO: Started pod busybox-3c400190-6336-49e3-b85e-6dd141e7c667 in namespace container-probe-9260
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/08/23 11:59:37.77
  May  8 11:59:37.772: INFO: Initial restart count of pod busybox-3c400190-6336-49e3-b85e-6dd141e7c667 is 0
  May  8 12:03:38.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 12:03:38.163
  STEP: Destroying namespace "container-probe-9260" for this suite. @ 05/08/23 12:03:38.169
• [242.432 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/08/23 12:03:38.176
  May  8 12:03:38.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:03:38.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:03:38.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:03:38.188
  STEP: creating an Endpoint @ 05/08/23 12:03:38.192
  STEP: waiting for available Endpoint @ 05/08/23 12:03:38.195
  STEP: listing all Endpoints @ 05/08/23 12:03:38.196
  STEP: updating the Endpoint @ 05/08/23 12:03:38.198
  STEP: fetching the Endpoint @ 05/08/23 12:03:38.202
  STEP: patching the Endpoint @ 05/08/23 12:03:38.204
  STEP: fetching the Endpoint @ 05/08/23 12:03:38.21
  STEP: deleting the Endpoint by Collection @ 05/08/23 12:03:38.212
  STEP: waiting for Endpoint deletion @ 05/08/23 12:03:38.215
  STEP: fetching the Endpoint @ 05/08/23 12:03:38.217
  May  8 12:03:38.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8038" for this suite. @ 05/08/23 12:03:38.22
• [0.047 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/08/23 12:03:38.224
  May  8 12:03:38.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:03:38.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:03:38.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:03:38.235
  STEP: Setting up server cert @ 05/08/23 12:03:38.247
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:03:38.475
  STEP: Deploying the webhook pod @ 05/08/23 12:03:38.481
  STEP: Wait for the deployment to be ready @ 05/08/23 12:03:38.493
  May  8 12:03:38.499: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:03:40.506
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:03:40.515
  May  8 12:03:41.515: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/08/23 12:03:41.518
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/08/23 12:03:41.535
  May  8 12:03:41.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:03:41.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4485" for this suite. @ 05/08/23 12:03:41.577
  STEP: Destroying namespace "webhook-markers-609" for this suite. @ 05/08/23 12:03:41.583
• [3.363 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/08/23 12:03:41.588
  May  8 12:03:41.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 12:03:41.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:03:41.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:03:41.601
  STEP: Counting existing ResourceQuota @ 05/08/23 12:03:58.605
  STEP: Creating a ResourceQuota @ 05/08/23 12:04:03.608
  STEP: Ensuring resource quota status is calculated @ 05/08/23 12:04:03.611
  STEP: Creating a ConfigMap @ 05/08/23 12:04:05.614
  STEP: Ensuring resource quota status captures configMap creation @ 05/08/23 12:04:05.623
  STEP: Deleting a ConfigMap @ 05/08/23 12:04:07.627
  STEP: Ensuring resource quota status released usage @ 05/08/23 12:04:07.63
  May  8 12:04:09.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5869" for this suite. @ 05/08/23 12:04:09.636
• [28.053 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/08/23 12:04:09.642
  May  8 12:04:09.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replicaset @ 05/08/23 12:04:09.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:09.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:09.653
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/08/23 12:04:09.655
  May  8 12:04:09.661: INFO: Pod name sample-pod: Found 0 pods out of 1
  May  8 12:04:14.664: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/08/23 12:04:14.664
  STEP: getting scale subresource @ 05/08/23 12:04:14.665
  STEP: updating a scale subresource @ 05/08/23 12:04:14.666
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/08/23 12:04:14.671
  STEP: Patch a scale subresource @ 05/08/23 12:04:14.672
  May  8 12:04:14.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7584" for this suite. @ 05/08/23 12:04:14.684
• [5.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/08/23 12:04:14.692
  May  8 12:04:14.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:04:14.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:14.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:14.706
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:04:14.708
  STEP: Saw pod success @ 05/08/23 12:04:18.725
  May  8 12:04:18.727: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-5b7e1713-b5c7-4973-9754-2da9a5029a77 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:04:18.74
  May  8 12:04:18.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2100" for this suite. @ 05/08/23 12:04:18.753
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/08/23 12:04:18.759
  May  8 12:04:18.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename svcaccounts @ 05/08/23 12:04:18.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:18.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:18.769
  STEP: creating a ServiceAccount @ 05/08/23 12:04:18.772
  STEP: watching for the ServiceAccount to be added @ 05/08/23 12:04:18.776
  STEP: patching the ServiceAccount @ 05/08/23 12:04:18.778
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/08/23 12:04:18.782
  STEP: deleting the ServiceAccount @ 05/08/23 12:04:18.784
  May  8 12:04:18.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1485" for this suite. @ 05/08/23 12:04:18.793
• [0.038 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/08/23 12:04:18.797
  May  8 12:04:18.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:04:18.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:18.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:18.809
  STEP: Setting up server cert @ 05/08/23 12:04:18.822
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:04:19.108
  STEP: Deploying the webhook pod @ 05/08/23 12:04:19.113
  STEP: Wait for the deployment to be ready @ 05/08/23 12:04:19.123
  May  8 12:04:19.127: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:04:21.134
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:04:21.146
  May  8 12:04:22.146: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/08/23 12:04:22.195
  STEP: Creating a configMap that should be mutated @ 05/08/23 12:04:22.209
  STEP: Deleting the collection of validation webhooks @ 05/08/23 12:04:22.239
  STEP: Creating a configMap that should not be mutated @ 05/08/23 12:04:22.265
  May  8 12:04:22.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-264" for this suite. @ 05/08/23 12:04:22.298
  STEP: Destroying namespace "webhook-markers-2080" for this suite. @ 05/08/23 12:04:22.303
• [3.510 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/08/23 12:04:22.308
  May  8 12:04:22.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename security-context-test @ 05/08/23 12:04:22.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:22.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:22.32
  May  8 12:04:24.336: INFO: Got logs for pod "busybox-privileged-false-55a3db51-9468-44ef-aa91-264cb05084dc": "ip: RTNETLINK answers: Operation not permitted\n"
  May  8 12:04:24.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-885" for this suite. @ 05/08/23 12:04:24.34
• [2.036 seconds]
------------------------------
S
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/08/23 12:04:24.344
  May  8 12:04:24.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:04:24.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:24.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:24.354
  STEP: creating service in namespace services-7724 @ 05/08/23 12:04:24.356
  STEP: creating service affinity-clusterip in namespace services-7724 @ 05/08/23 12:04:24.356
  STEP: creating replication controller affinity-clusterip in namespace services-7724 @ 05/08/23 12:04:24.364
  I0508 12:04:24.370862      23 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-7724, replica count: 3
  I0508 12:04:27.423806      23 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 12:04:27.430: INFO: Creating new exec pod
  May  8 12:04:30.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7724 exec execpod-affinityf7vx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May  8 12:04:30.570: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May  8 12:04:30.570: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:04:30.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7724 exec execpod-affinityf7vx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.43.217 80'
  May  8 12:04:30.688: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.43.217 80\nConnection to 10.99.43.217 80 port [tcp/http] succeeded!\n"
  May  8 12:04:30.688: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:04:30.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7724 exec execpod-affinityf7vx8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.43.217:80/ ; done'
  May  8 12:04:30.863: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.43.217:80/\n"
  May  8 12:04:30.863: INFO: stdout: "\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv\naffinity-clusterip-nptwv"
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Received response from host: affinity-clusterip-nptwv
  May  8 12:04:30.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:04:30.866: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-7724, will wait for the garbage collector to delete the pods @ 05/08/23 12:04:30.874
  May  8 12:04:30.930: INFO: Deleting ReplicationController affinity-clusterip took: 4.006325ms
  May  8 12:04:31.031: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.134829ms
  STEP: Destroying namespace "services-7724" for this suite. @ 05/08/23 12:04:32.646
• [8.306 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/08/23 12:04:32.652
  May  8 12:04:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename var-expansion @ 05/08/23 12:04:32.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:32.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:32.663
  May  8 12:04:34.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:04:34.678: INFO: Deleting pod "var-expansion-7d6711f6-14f5-44f2-afeb-5c851e88766c" in namespace "var-expansion-9178"
  May  8 12:04:34.682: INFO: Wait up to 5m0s for pod "var-expansion-7d6711f6-14f5-44f2-afeb-5c851e88766c" to be fully deleted
  STEP: Destroying namespace "var-expansion-9178" for this suite. @ 05/08/23 12:04:36.687
• [4.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/08/23 12:04:36.697
  May  8 12:04:36.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename containers @ 05/08/23 12:04:36.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:36.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:36.708
  May  8 12:04:38.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2300" for this suite. @ 05/08/23 12:04:38.727
• [2.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/08/23 12:04:38.732
  May  8 12:04:38.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:04:38.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:38.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:38.744
  STEP: Creating projection with secret that has name projected-secret-test-165c21b4-d774-4238-8863-8f2f1e434726 @ 05/08/23 12:04:38.746
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:04:38.749
  STEP: Saw pod success @ 05/08/23 12:04:42.764
  May  8 12:04:42.765: INFO: Trying to get logs from node worker-0 pod pod-projected-secrets-cb62fcdf-1546-4202-a8e1-1ed436f31e54 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:04:42.77
  May  8 12:04:42.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-749" for this suite. @ 05/08/23 12:04:42.783
• [4.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/08/23 12:04:42.788
  May  8 12:04:42.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 12:04:42.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:42.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:42.8
  STEP: Creating resourceQuota "e2e-rq-status-vs7t5" @ 05/08/23 12:04:42.804
  May  8 12:04:42.808: INFO: Resource quota "e2e-rq-status-vs7t5" reports spec: hard cpu limit of 500m
  May  8 12:04:42.808: INFO: Resource quota "e2e-rq-status-vs7t5" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-vs7t5" /status @ 05/08/23 12:04:42.808
  STEP: Confirm /status for "e2e-rq-status-vs7t5" resourceQuota via watch @ 05/08/23 12:04:42.814
  May  8 12:04:42.815: INFO: observed resourceQuota "e2e-rq-status-vs7t5" in namespace "resourcequota-5351" with hard status: v1.ResourceList(nil)
  May  8 12:04:42.815: INFO: Found resourceQuota "e2e-rq-status-vs7t5" in namespace "resourcequota-5351" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May  8 12:04:42.815: INFO: ResourceQuota "e2e-rq-status-vs7t5" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/08/23 12:04:42.817
  May  8 12:04:42.822: INFO: Resource quota "e2e-rq-status-vs7t5" reports spec: hard cpu limit of 1
  May  8 12:04:42.822: INFO: Resource quota "e2e-rq-status-vs7t5" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-vs7t5" /status @ 05/08/23 12:04:42.822
  STEP: Confirm /status for "e2e-rq-status-vs7t5" resourceQuota via watch @ 05/08/23 12:04:42.826
  May  8 12:04:42.827: INFO: observed resourceQuota "e2e-rq-status-vs7t5" in namespace "resourcequota-5351" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May  8 12:04:42.827: INFO: Found resourceQuota "e2e-rq-status-vs7t5" in namespace "resourcequota-5351" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May  8 12:04:42.827: INFO: ResourceQuota "e2e-rq-status-vs7t5" /status was patched
  STEP: Get "e2e-rq-status-vs7t5" /status @ 05/08/23 12:04:42.827
  May  8 12:04:42.830: INFO: Resourcequota "e2e-rq-status-vs7t5" reports status: hard cpu of 1
  May  8 12:04:42.830: INFO: Resourcequota "e2e-rq-status-vs7t5" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-vs7t5" /status before checking Spec is unchanged @ 05/08/23 12:04:42.831
  May  8 12:04:42.835: INFO: Resourcequota "e2e-rq-status-vs7t5" reports status: hard cpu of 2
  May  8 12:04:42.835: INFO: Resourcequota "e2e-rq-status-vs7t5" reports status: hard memory of 2Gi
  May  8 12:04:42.836: INFO: Found resourceQuota "e2e-rq-status-vs7t5" in namespace "resourcequota-5351" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  May  8 12:04:47.848: INFO: ResourceQuota "e2e-rq-status-vs7t5" Spec was unchanged and /status reset
  May  8 12:04:47.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5351" for this suite. @ 05/08/23 12:04:47.85
• [5.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/08/23 12:04:47.857
  May  8 12:04:47.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-pred @ 05/08/23 12:04:47.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:47.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:47.871
  May  8 12:04:47.873: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  8 12:04:47.878: INFO: Waiting for terminating namespaces to be deleted...
  May  8 12:04:47.880: INFO: 
  Logging pods the apiserver thinks is on node worker-0 before test
  May  8 12:04:47.884: INFO: coredns-878bb57ff-8r9bz from kube-system started at 2023-05-08 11:38:09 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.884: INFO: 	Container coredns ready: true, restart count 0
  May  8 12:04:47.884: INFO: konnectivity-agent-j9zk2 from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.884: INFO: 	Container konnectivity-agent ready: true, restart count 0
  May  8 12:04:47.884: INFO: kube-proxy-gx5cm from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.884: INFO: 	Container kube-proxy ready: true, restart count 0
  May  8 12:04:47.885: INFO: kube-router-ckb57 from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.885: INFO: 	Container kube-router ready: true, restart count 0
  May  8 12:04:47.885: INFO: sonobuoy from sonobuoy started at 2023-05-08 11:39:39 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.885: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  8 12:04:47.885: INFO: sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-zf5gq from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:04:47.885: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:04:47.885: INFO: 	Container systemd-logs ready: true, restart count 0
  May  8 12:04:47.885: INFO: 
  Logging pods the apiserver thinks is on node worker-1 before test
  May  8 12:04:47.888: INFO: coredns-878bb57ff-g976b from kube-system started at 2023-05-08 11:38:04 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.888: INFO: 	Container coredns ready: true, restart count 0
  May  8 12:04:47.888: INFO: konnectivity-agent-298xg from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.888: INFO: 	Container konnectivity-agent ready: true, restart count 0
  May  8 12:04:47.888: INFO: kube-proxy-57cqn from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.888: INFO: 	Container kube-proxy ready: true, restart count 0
  May  8 12:04:47.888: INFO: kube-router-6pdq6 from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.888: INFO: 	Container kube-router ready: true, restart count 0
  May  8 12:04:47.888: INFO: metrics-server-7f86dff975-rtj65 from kube-system started at 2023-05-08 11:38:04 +0000 UTC (1 container statuses recorded)
  May  8 12:04:47.888: INFO: 	Container metrics-server ready: true, restart count 0
  May  8 12:04:47.888: INFO: sonobuoy-e2e-job-dd8349e2c331446f from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:04:47.888: INFO: 	Container e2e ready: true, restart count 0
  May  8 12:04:47.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:04:47.889: INFO: sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-5fk2l from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:04:47.889: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:04:47.889: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node worker-0 @ 05/08/23 12:04:47.9
  STEP: verifying the node has the label node worker-1 @ 05/08/23 12:04:47.91
  May  8 12:04:47.918: INFO: Pod coredns-878bb57ff-8r9bz requesting resource cpu=100m on Node worker-0
  May  8 12:04:47.918: INFO: Pod coredns-878bb57ff-g976b requesting resource cpu=100m on Node worker-1
  May  8 12:04:47.918: INFO: Pod konnectivity-agent-298xg requesting resource cpu=0m on Node worker-1
  May  8 12:04:47.918: INFO: Pod konnectivity-agent-j9zk2 requesting resource cpu=0m on Node worker-0
  May  8 12:04:47.918: INFO: Pod kube-proxy-57cqn requesting resource cpu=0m on Node worker-1
  May  8 12:04:47.918: INFO: Pod kube-proxy-gx5cm requesting resource cpu=0m on Node worker-0
  May  8 12:04:47.918: INFO: Pod kube-router-6pdq6 requesting resource cpu=250m on Node worker-1
  May  8 12:04:47.918: INFO: Pod kube-router-ckb57 requesting resource cpu=250m on Node worker-0
  May  8 12:04:47.918: INFO: Pod metrics-server-7f86dff975-rtj65 requesting resource cpu=10m on Node worker-1
  May  8 12:04:47.918: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-0
  May  8 12:04:47.918: INFO: Pod sonobuoy-e2e-job-dd8349e2c331446f requesting resource cpu=0m on Node worker-1
  May  8 12:04:47.918: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-5fk2l requesting resource cpu=0m on Node worker-1
  May  8 12:04:47.918: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-zf5gq requesting resource cpu=0m on Node worker-0
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/08/23 12:04:47.919
  May  8 12:04:47.919: INFO: Creating a pod which consumes cpu=2555m on Node worker-0
  May  8 12:04:47.926: INFO: Creating a pod which consumes cpu=2548m on Node worker-1
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/08/23 12:04:49.94
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7468498a-77c9-49db-b5b4-17d72eeb972c.175d29853ce7eaaa], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8812/filler-pod-7468498a-77c9-49db-b5b4-17d72eeb972c to worker-1] @ 05/08/23 12:04:49.943
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7468498a-77c9-49db-b5b4-17d72eeb972c.175d2985574311cf], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/08/23 12:04:49.943
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7468498a-77c9-49db-b5b4-17d72eeb972c.175d2985584ec1f5], Reason = [Created], Message = [Created container filler-pod-7468498a-77c9-49db-b5b4-17d72eeb972c] @ 05/08/23 12:04:49.943
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7468498a-77c9-49db-b5b4-17d72eeb972c.175d29855cbfe579], Reason = [Started], Message = [Started container filler-pod-7468498a-77c9-49db-b5b4-17d72eeb972c] @ 05/08/23 12:04:49.943
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7b917ba9-279d-4642-b182-e9196f169088.175d29853c91479f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8812/filler-pod-7b917ba9-279d-4642-b182-e9196f169088 to worker-0] @ 05/08/23 12:04:49.943
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7b917ba9-279d-4642-b182-e9196f169088.175d298557614151], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/08/23 12:04:49.943
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7b917ba9-279d-4642-b182-e9196f169088.175d29855865ff8f], Reason = [Created], Message = [Created container filler-pod-7b917ba9-279d-4642-b182-e9196f169088] @ 05/08/23 12:04:49.943
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7b917ba9-279d-4642-b182-e9196f169088.175d29855cf1b216], Reason = [Started], Message = [Started container filler-pod-7b917ba9-279d-4642-b182-e9196f169088] @ 05/08/23 12:04:49.943
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175d2985b4d98f72], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] @ 05/08/23 12:04:50.041
  STEP: removing the label node off the node worker-0 @ 05/08/23 12:04:50.952
  STEP: verifying the node doesn't have the label node @ 05/08/23 12:04:50.96
  STEP: removing the label node off the node worker-1 @ 05/08/23 12:04:50.962
  STEP: verifying the node doesn't have the label node @ 05/08/23 12:04:50.97
  May  8 12:04:50.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8812" for this suite. @ 05/08/23 12:04:50.975
• [3.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/08/23 12:04:50.991
  May  8 12:04:50.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:04:50.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:51.003
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:04:51.006
  STEP: Saw pod success @ 05/08/23 12:04:55.021
  May  8 12:04:55.023: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-738ebf71-ca18-4701-8988-d0504e823788 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:04:55.028
  May  8 12:04:55.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4961" for this suite. @ 05/08/23 12:04:55.038
• [4.051 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/08/23 12:04:55.042
  May  8 12:04:55.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename watch @ 05/08/23 12:04:55.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:04:55.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:04:55.054
  STEP: creating a watch on configmaps with a certain label @ 05/08/23 12:04:55.057
  STEP: creating a new configmap @ 05/08/23 12:04:55.058
  STEP: modifying the configmap once @ 05/08/23 12:04:55.061
  STEP: changing the label value of the configmap @ 05/08/23 12:04:55.065
  STEP: Expecting to observe a delete notification for the watched object @ 05/08/23 12:04:55.07
  May  8 12:04:55.070: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  fd9c347e-4dc5-441a-a75c-5841c9251d96 11921 0 2023-05-08 12:04:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-08 12:04:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:04:55.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  fd9c347e-4dc5-441a-a75c-5841c9251d96 11922 0 2023-05-08 12:04:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-08 12:04:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:04:55.070: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  fd9c347e-4dc5-441a-a75c-5841c9251d96 11923 0 2023-05-08 12:04:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-08 12:04:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/08/23 12:04:55.07
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/08/23 12:04:55.074
  STEP: changing the label value of the configmap back @ 05/08/23 12:05:05.078
  STEP: modifying the configmap a third time @ 05/08/23 12:05:05.084
  STEP: deleting the configmap @ 05/08/23 12:05:05.088
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/08/23 12:05:05.091
  May  8 12:05:05.091: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  fd9c347e-4dc5-441a-a75c-5841c9251d96 11980 0 2023-05-08 12:04:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-08 12:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:05:05.091: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  fd9c347e-4dc5-441a-a75c-5841c9251d96 11981 0 2023-05-08 12:04:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-08 12:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:05:05.092: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  fd9c347e-4dc5-441a-a75c-5841c9251d96 11982 0 2023-05-08 12:04:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-08 12:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:05:05.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7642" for this suite. @ 05/08/23 12:05:05.094
• [10.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/08/23 12:05:05.099
  May  8 12:05:05.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename deployment @ 05/08/23 12:05:05.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:05:05.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:05:05.112
  May  8 12:05:05.119: INFO: Pod name rollover-pod: Found 0 pods out of 1
  May  8 12:05:10.126: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/08/23 12:05:10.126
  May  8 12:05:10.126: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  May  8 12:05:12.130: INFO: Creating deployment "test-rollover-deployment"
  May  8 12:05:12.135: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  May  8 12:05:14.140: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May  8 12:05:14.143: INFO: Ensure that both replica sets have 1 created replica
  May  8 12:05:14.147: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May  8 12:05:14.152: INFO: Updating deployment test-rollover-deployment
  May  8 12:05:14.152: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  May  8 12:05:16.157: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May  8 12:05:16.161: INFO: Make sure deployment "test-rollover-deployment" is complete
  May  8 12:05:16.165: INFO: all replica sets need to contain the pod-template-hash label
  May  8 12:05:16.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:05:18.171: INFO: all replica sets need to contain the pod-template-hash label
  May  8 12:05:18.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:05:20.171: INFO: all replica sets need to contain the pod-template-hash label
  May  8 12:05:20.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:05:22.170: INFO: all replica sets need to contain the pod-template-hash label
  May  8 12:05:22.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:05:24.171: INFO: all replica sets need to contain the pod-template-hash label
  May  8 12:05:24.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 5, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 5, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:05:26.170: INFO: 
  May  8 12:05:26.170: INFO: Ensure that both old replica sets have no replicas
  May  8 12:05:26.176: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2100  dcddbda6-8d80-4fa0-ac9a-ae46dcaba270 12104 2 2023-05-08 12:05:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-08 12:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ad7658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-08 12:05:12 +0000 UTC,LastTransitionTime:2023-05-08 12:05:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-08 12:05:25 +0000 UTC,LastTransitionTime:2023-05-08 12:05:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  8 12:05:26.178: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-2100  539f5430-cea9-4d5d-9b94-96fcac6e6a0f 12094 2 2023-05-08 12:05:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment dcddbda6-8d80-4fa0-ac9a-ae46dcaba270 0xc004a304e7 0xc004a304e8}] [] [{kube-controller-manager Update apps/v1 2023-05-08 12:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcddbda6-8d80-4fa0-ac9a-ae46dcaba270\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a30598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  8 12:05:26.178: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May  8 12:05:26.178: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2100  b63ebdfa-8919-4abd-8c9e-5b8ef1b1b8da 12103 2 2023-05-08 12:05:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment dcddbda6-8d80-4fa0-ac9a-ae46dcaba270 0xc004a303b7 0xc004a303b8}] [] [{e2e.test Update apps/v1 2023-05-08 12:05:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcddbda6-8d80-4fa0-ac9a-ae46dcaba270\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a30478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  8 12:05:26.178: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-2100  aca5bf10-b43a-4133-82e2-d1db5cd71b86 12057 2 2023-05-08 12:05:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment dcddbda6-8d80-4fa0-ac9a-ae46dcaba270 0xc004a30607 0xc004a30608}] [] [{kube-controller-manager Update apps/v1 2023-05-08 12:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcddbda6-8d80-4fa0-ac9a-ae46dcaba270\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:05:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a306b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  8 12:05:26.180: INFO: Pod "test-rollover-deployment-57777854c9-nwsh2" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-nwsh2 test-rollover-deployment-57777854c9- deployment-2100  0e15aa44-a7e3-43d8-90f5-44bcb2482376 12071 0 2023-05-08 12:05:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 539f5430-cea9-4d5d-9b94-96fcac6e6a0f 0xc0046fac57 0xc0046fac58}] [] [{kube-controller-manager Update v1 2023-05-08 12:05:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"539f5430-cea9-4d5d-9b94-96fcac6e6a0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 12:05:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ks4hn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ks4hn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:05:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:05:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:05:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:05:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:10.244.1.193,StartTime:2023-05-08 12:05:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 12:05:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://bcfdbbfe1ab68964584491b47347184abc20ed66135265e9d56efdebaad989ac,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.193,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 12:05:26.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2100" for this suite. @ 05/08/23 12:05:26.183
• [21.087 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/08/23 12:05:26.187
  May  8 12:05:26.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:05:26.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:05:26.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:05:26.2
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-9853 @ 05/08/23 12:05:26.203
  STEP: changing the ExternalName service to type=NodePort @ 05/08/23 12:05:26.206
  STEP: creating replication controller externalname-service in namespace services-9853 @ 05/08/23 12:05:26.223
  I0508 12:05:26.228337      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-9853, replica count: 2
  I0508 12:05:29.279671      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 12:05:29.279: INFO: Creating new exec pod
  May  8 12:05:32.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  8 12:05:32.424: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  8 12:05:32.424: INFO: stdout: ""
  May  8 12:05:33.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  8 12:05:33.548: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  8 12:05:33.548: INFO: stdout: ""
  May  8 12:05:34.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  8 12:05:34.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  8 12:05:34.544: INFO: stdout: ""
  May  8 12:05:35.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  8 12:05:35.541: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  8 12:05:35.541: INFO: stdout: ""
  May  8 12:05:36.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  8 12:05:36.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  8 12:05:36.545: INFO: stdout: ""
  May  8 12:05:37.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  8 12:05:37.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  8 12:05:37.552: INFO: stdout: "externalname-service-kwn9g"
  May  8 12:05:37.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.254.70 80'
  May  8 12:05:37.667: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.254.70 80\nConnection to 10.111.254.70 80 port [tcp/http] succeeded!\n"
  May  8 12:05:37.667: INFO: stdout: "externalname-service-2vqvw"
  May  8 12:05:37.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.39.71 31855'
  May  8 12:05:37.801: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.39.71 31855\nConnection to 10.0.39.71 31855 port [tcp/*] succeeded!\n"
  May  8 12:05:37.801: INFO: stdout: "externalname-service-2vqvw"
  May  8 12:05:37.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.53.117 31855'
  May  8 12:05:37.919: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.53.117 31855\nConnection to 10.0.53.117 31855 port [tcp/*] succeeded!\n"
  May  8 12:05:37.919: INFO: stdout: ""
  May  8 12:05:38.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9853 exec execpodd2h2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.53.117 31855'
  May  8 12:05:39.039: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.53.117 31855\nConnection to 10.0.53.117 31855 port [tcp/*] succeeded!\n"
  May  8 12:05:39.039: INFO: stdout: "externalname-service-2vqvw"
  May  8 12:05:39.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:05:39.042: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-9853" for this suite. @ 05/08/23 12:05:39.056
• [12.874 seconds]
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/08/23 12:05:39.061
  May  8 12:05:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:05:39.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:05:39.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:05:39.072
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-9951 @ 05/08/23 12:05:39.075
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/08/23 12:05:39.085
  STEP: creating service externalsvc in namespace services-9951 @ 05/08/23 12:05:39.085
  STEP: creating replication controller externalsvc in namespace services-9951 @ 05/08/23 12:05:39.098
  I0508 12:05:39.104286      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9951, replica count: 2
  I0508 12:05:42.155046      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/08/23 12:05:42.157
  May  8 12:05:42.169: INFO: Creating new exec pod
  May  8 12:05:44.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-9951 exec execpodxtdjq -- /bin/sh -x -c nslookup nodeport-service.services-9951.svc.cluster.local'
  May  8 12:05:44.341: INFO: stderr: "+ nslookup nodeport-service.services-9951.svc.cluster.local\n"
  May  8 12:05:44.341: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9951.svc.cluster.local\tcanonical name = externalsvc.services-9951.svc.cluster.local.\nName:\texternalsvc.services-9951.svc.cluster.local\nAddress: 10.100.214.171\n\n"
  May  8 12:05:44.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9951, will wait for the garbage collector to delete the pods @ 05/08/23 12:05:44.343
  May  8 12:05:44.400: INFO: Deleting ReplicationController externalsvc took: 4.004705ms
  May  8 12:05:44.501: INFO: Terminating ReplicationController externalsvc pods took: 101.070172ms
  May  8 12:05:46.515: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-9951" for this suite. @ 05/08/23 12:05:46.521
• [7.464 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/08/23 12:05:46.528
  May  8 12:05:46.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename statefulset @ 05/08/23 12:05:46.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:05:46.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:05:46.54
  STEP: Creating service test in namespace statefulset-3385 @ 05/08/23 12:05:46.543
  STEP: Creating stateful set ss in namespace statefulset-3385 @ 05/08/23 12:05:46.546
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3385 @ 05/08/23 12:05:46.551
  May  8 12:05:46.553: INFO: Found 0 stateful pods, waiting for 1
  May  8 12:05:56.556: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/08/23 12:05:56.556
  May  8 12:05:56.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-3385 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 12:05:56.679: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 12:05:56.679: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 12:05:56.679: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 12:05:56.682: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May  8 12:06:06.686: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  8 12:06:06.686: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 12:06:06.696: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
  May  8 12:06:06.696: INFO: ss-0  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:05:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:05:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:05:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:05:46 +0000 UTC  }]
  May  8 12:06:06.696: INFO: 
  May  8 12:06:06.696: INFO: StatefulSet ss has not reached scale 3, at 1
  May  8 12:06:07.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99787591s
  May  8 12:06:08.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994916522s
  May  8 12:06:09.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.99181796s
  May  8 12:06:10.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988490582s
  May  8 12:06:11.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984577085s
  May  8 12:06:12.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981548371s
  May  8 12:06:13.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978629467s
  May  8 12:06:14.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975486529s
  May  8 12:06:15.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.614346ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3385 @ 05/08/23 12:06:16.726
  May  8 12:06:16.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-3385 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 12:06:16.845: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  8 12:06:16.845: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 12:06:16.845: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  8 12:06:16.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-3385 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 12:06:16.975: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May  8 12:06:16.975: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 12:06:16.975: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  8 12:06:16.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-3385 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  8 12:06:17.101: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May  8 12:06:17.101: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  8 12:06:17.101: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  8 12:06:17.103: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  8 12:06:17.103: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May  8 12:06:17.103: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/08/23 12:06:17.103
  May  8 12:06:17.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-3385 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 12:06:17.237: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 12:06:17.237: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 12:06:17.237: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 12:06:17.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-3385 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 12:06:17.358: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 12:06:17.358: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 12:06:17.358: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 12:06:17.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=statefulset-3385 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  8 12:06:17.479: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  8 12:06:17.479: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  8 12:06:17.479: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  8 12:06:17.479: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 12:06:17.481: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May  8 12:06:27.487: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  8 12:06:27.487: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May  8 12:06:27.487: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May  8 12:06:27.495: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
  May  8 12:06:27.495: INFO: ss-0  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:05:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:05:46 +0000 UTC  }]
  May  8 12:06:27.495: INFO: ss-1  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:06 +0000 UTC  }]
  May  8 12:06:27.495: INFO: ss-2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:06 +0000 UTC  }]
  May  8 12:06:27.495: INFO: 
  May  8 12:06:27.495: INFO: StatefulSet ss has not reached scale 0, at 3
  May  8 12:06:28.498: INFO: POD   NODE      PHASE      GRACE  CONDITIONS
  May  8 12:06:28.498: INFO: ss-0  worker-0  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:05:46 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:17 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:17 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:05:46 +0000 UTC  }]
  May  8 12:06:28.498: INFO: ss-1  worker-1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:06 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:18 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:18 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:06 +0000 UTC  }]
  May  8 12:06:28.498: INFO: ss-2  worker-0  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:06 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:18 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:18 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:06:06 +0000 UTC  }]
  May  8 12:06:28.498: INFO: 
  May  8 12:06:28.498: INFO: StatefulSet ss has not reached scale 0, at 3
  May  8 12:06:29.501: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993557313s
  May  8 12:06:30.504: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.990063605s
  May  8 12:06:31.507: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.987627224s
  May  8 12:06:32.510: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.985063802s
  May  8 12:06:33.512: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.982279552s
  May  8 12:06:34.514: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.979969683s
  May  8 12:06:35.517: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.977634235s
  May  8 12:06:36.519: INFO: Verifying statefulset ss doesn't scale past 0 for another 974.950547ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3385 @ 05/08/23 12:06:37.52
  May  8 12:06:37.522: INFO: Scaling statefulset ss to 0
  May  8 12:06:37.529: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 12:06:37.530: INFO: Deleting all statefulset in ns statefulset-3385
  May  8 12:06:37.532: INFO: Scaling statefulset ss to 0
  May  8 12:06:37.538: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 12:06:37.539: INFO: Deleting statefulset ss
  May  8 12:06:37.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3385" for this suite. @ 05/08/23 12:06:37.549
• [51.025 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/08/23 12:06:37.553
  May  8 12:06:37.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename field-validation @ 05/08/23 12:06:37.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:06:37.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:06:37.564
  May  8 12:06:37.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  W0508 12:06:37.567261      23 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc0014dc280 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0508 12:06:40.102266      23 warnings.go:70] unknown field "alpha"
  W0508 12:06:40.102292      23 warnings.go:70] unknown field "beta"
  W0508 12:06:40.102298      23 warnings.go:70] unknown field "delta"
  W0508 12:06:40.102303      23 warnings.go:70] unknown field "epsilon"
  W0508 12:06:40.102309      23 warnings.go:70] unknown field "gamma"
  May  8 12:06:40.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9990" for this suite. @ 05/08/23 12:06:40.122
• [2.573 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/08/23 12:06:40.126
  May  8 12:06:40.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:06:40.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:06:40.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:06:40.136
  STEP: Creating a pod to test downward api env vars @ 05/08/23 12:06:40.139
  STEP: Saw pod success @ 05/08/23 12:06:44.154
  May  8 12:06:44.155: INFO: Trying to get logs from node worker-0 pod downward-api-8973f4aa-5539-41fe-a18b-f2e07bd33884 container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 12:06:44.169
  May  8 12:06:44.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8789" for this suite. @ 05/08/23 12:06:44.18
• [4.058 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/08/23 12:06:44.184
  May  8 12:06:44.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replicaset @ 05/08/23 12:06:44.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:06:44.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:06:44.196
  STEP: Create a Replicaset @ 05/08/23 12:06:44.2
  STEP: Verify that the required pods have come up. @ 05/08/23 12:06:44.203
  May  8 12:06:44.205: INFO: Pod name sample-pod: Found 0 pods out of 1
  May  8 12:06:49.208: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/08/23 12:06:49.209
  STEP: Getting /status @ 05/08/23 12:06:49.209
  May  8 12:06:49.212: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/08/23 12:06:49.212
  May  8 12:06:49.218: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/08/23 12:06:49.218
  May  8 12:06:49.219: INFO: Observed &ReplicaSet event: ADDED
  May  8 12:06:49.219: INFO: Observed &ReplicaSet event: MODIFIED
  May  8 12:06:49.220: INFO: Observed &ReplicaSet event: MODIFIED
  May  8 12:06:49.220: INFO: Observed &ReplicaSet event: MODIFIED
  May  8 12:06:49.220: INFO: Found replicaset test-rs in namespace replicaset-4811 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  8 12:06:49.220: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/08/23 12:06:49.22
  May  8 12:06:49.220: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  8 12:06:49.226: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/08/23 12:06:49.226
  May  8 12:06:49.227: INFO: Observed &ReplicaSet event: ADDED
  May  8 12:06:49.228: INFO: Observed &ReplicaSet event: MODIFIED
  May  8 12:06:49.228: INFO: Observed &ReplicaSet event: MODIFIED
  May  8 12:06:49.228: INFO: Observed &ReplicaSet event: MODIFIED
  May  8 12:06:49.228: INFO: Observed replicaset test-rs in namespace replicaset-4811 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  8 12:06:49.228: INFO: Observed &ReplicaSet event: MODIFIED
  May  8 12:06:49.228: INFO: Found replicaset test-rs in namespace replicaset-4811 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May  8 12:06:49.228: INFO: Replicaset test-rs has a patched status
  May  8 12:06:49.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4811" for this suite. @ 05/08/23 12:06:49.231
• [5.050 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/08/23 12:06:49.236
  May  8 12:06:49.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename var-expansion @ 05/08/23 12:06:49.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:06:49.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:06:49.249
  STEP: creating the pod with failed condition @ 05/08/23 12:06:49.251
  STEP: updating the pod @ 05/08/23 12:08:49.259
  May  8 12:08:49.768: INFO: Successfully updated pod "var-expansion-e6767c40-3e4f-467c-980c-c3a96bca6b49"
  STEP: waiting for pod running @ 05/08/23 12:08:49.768
  STEP: deleting the pod gracefully @ 05/08/23 12:08:51.773
  May  8 12:08:51.773: INFO: Deleting pod "var-expansion-e6767c40-3e4f-467c-980c-c3a96bca6b49" in namespace "var-expansion-6122"
  May  8 12:08:51.778: INFO: Wait up to 5m0s for pod "var-expansion-e6767c40-3e4f-467c-980c-c3a96bca6b49" to be fully deleted
  May  8 12:09:23.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6122" for this suite. @ 05/08/23 12:09:23.834
• [154.601 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/08/23 12:09:23.839
  May  8 12:09:23.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 12:09:23.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:09:23.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:09:23.85
  STEP: Creating pod liveness-15ae68a9-69de-4e5c-af75-4faf8b4b18f7 in namespace container-probe-9222 @ 05/08/23 12:09:23.852
  May  8 12:09:25.861: INFO: Started pod liveness-15ae68a9-69de-4e5c-af75-4faf8b4b18f7 in namespace container-probe-9222
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/08/23 12:09:25.861
  May  8 12:09:25.863: INFO: Initial restart count of pod liveness-15ae68a9-69de-4e5c-af75-4faf8b4b18f7 is 0
  May  8 12:13:26.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 12:13:26.241
  STEP: Destroying namespace "container-probe-9222" for this suite. @ 05/08/23 12:13:26.248
• [242.413 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/08/23 12:13:26.253
  May  8 12:13:26.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/08/23 12:13:26.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:13:26.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:13:26.267
  STEP: creating @ 05/08/23 12:13:26.269
  STEP: getting @ 05/08/23 12:13:26.28
  STEP: listing @ 05/08/23 12:13:26.283
  STEP: deleting @ 05/08/23 12:13:26.285
  May  8 12:13:26.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9404" for this suite. @ 05/08/23 12:13:26.296
• [0.046 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/08/23 12:13:26.3
  May  8 12:13:26.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 12:13:26.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:13:26.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:13:26.31
  May  8 12:13:26.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: creating the pod @ 05/08/23 12:13:26.313
  STEP: submitting the pod to kubernetes @ 05/08/23 12:13:26.313
  May  8 12:13:28.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4178" for this suite. @ 05/08/23 12:13:28.412
• [2.116 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/08/23 12:13:28.417
  May  8 12:13:28.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/08/23 12:13:28.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:13:28.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:13:28.428
  May  8 12:13:28.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:13:28.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2111" for this suite. @ 05/08/23 12:13:28.961
• [0.547 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/08/23 12:13:28.966
  May  8 12:13:28.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:13:28.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:13:28.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:13:28.976
  STEP: Creating configMap with name projected-configmap-test-volume-map-b0a91549-fe6d-4371-9d79-41ea86cb11e5 @ 05/08/23 12:13:28.979
  STEP: Creating a pod to test consume configMaps @ 05/08/23 12:13:28.982
  STEP: Saw pod success @ 05/08/23 12:13:32.996
  May  8 12:13:32.998: INFO: Trying to get logs from node worker-0 pod pod-projected-configmaps-884b38a6-bd96-47f8-814a-e93a8ec3a021 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 12:13:33.01
  May  8 12:13:33.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9367" for this suite. @ 05/08/23 12:13:33.022
• [4.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/08/23 12:13:33.028
  May  8 12:13:33.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 12:13:33.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:13:33.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:13:33.041
  STEP: Creating pod busybox-779967a8-04b3-4339-94f2-36da8f3761a2 in namespace container-probe-6232 @ 05/08/23 12:13:33.043
  May  8 12:13:35.053: INFO: Started pod busybox-779967a8-04b3-4339-94f2-36da8f3761a2 in namespace container-probe-6232
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/08/23 12:13:35.053
  May  8 12:13:35.055: INFO: Initial restart count of pod busybox-779967a8-04b3-4339-94f2-36da8f3761a2 is 0
  May  8 12:14:25.137: INFO: Restart count of pod container-probe-6232/busybox-779967a8-04b3-4339-94f2-36da8f3761a2 is now 1 (50.081981863s elapsed)
  May  8 12:14:25.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 12:14:25.139
  STEP: Destroying namespace "container-probe-6232" for this suite. @ 05/08/23 12:14:25.146
• [52.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/08/23 12:14:25.152
  May  8 12:14:25.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:14:25.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:14:25.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:14:25.163
  STEP: Creating configMap with name configmap-test-upd-d7ca1e45-e497-421d-b64f-dcc3299432d0 @ 05/08/23 12:14:25.167
  STEP: Creating the pod @ 05/08/23 12:14:25.17
  STEP: Waiting for pod with text data @ 05/08/23 12:14:27.18
  STEP: Waiting for pod with binary data @ 05/08/23 12:14:27.185
  May  8 12:14:27.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3312" for this suite. @ 05/08/23 12:14:27.192
• [2.044 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/08/23 12:14:27.196
  May  8 12:14:27.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 12:14:27.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:14:27.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:14:27.208
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/08/23 12:14:27.21
  STEP: Saw pod success @ 05/08/23 12:14:31.222
  May  8 12:14:31.224: INFO: Trying to get logs from node worker-1 pod pod-d6bf8987-0beb-4fa4-b93c-2073c1a4b3b4 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 12:14:31.238
  May  8 12:14:31.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5099" for this suite. @ 05/08/23 12:14:31.25
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/08/23 12:14:31.255
  May  8 12:14:31.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename daemonsets @ 05/08/23 12:14:31.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:14:31.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:14:31.265
  May  8 12:14:31.284: INFO: Create a RollingUpdate DaemonSet
  May  8 12:14:31.287: INFO: Check that daemon pods launch on every node of the cluster
  May  8 12:14:31.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 12:14:31.293: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 12:14:32.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 12:14:32.298: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 12:14:33.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 12:14:33.299: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  May  8 12:14:33.299: INFO: Update the DaemonSet to trigger a rollout
  May  8 12:14:33.304: INFO: Updating DaemonSet daemon-set
  May  8 12:14:35.314: INFO: Roll back the DaemonSet before rollout is complete
  May  8 12:14:35.319: INFO: Updating DaemonSet daemon-set
  May  8 12:14:35.319: INFO: Make sure DaemonSet rollback is complete
  May  8 12:14:35.321: INFO: Wrong image for pod: daemon-set-qzxh8. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May  8 12:14:35.321: INFO: Pod daemon-set-qzxh8 is not available
  May  8 12:14:38.326: INFO: Pod daemon-set-9ngmg is not available
  STEP: Deleting DaemonSet "daemon-set" @ 05/08/23 12:14:38.332
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9424, will wait for the garbage collector to delete the pods @ 05/08/23 12:14:38.332
  May  8 12:14:38.387: INFO: Deleting DaemonSet.extensions daemon-set took: 3.531077ms
  May  8 12:14:38.488: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.886581ms
  May  8 12:14:40.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 12:14:40.491: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  8 12:14:40.493: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14006"},"items":null}

  May  8 12:14:40.495: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14006"},"items":null}

  May  8 12:14:40.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9424" for this suite. @ 05/08/23 12:14:40.503
• [9.251 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/08/23 12:14:40.507
  May  8 12:14:40.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 12:14:40.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:14:40.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:14:40.519
  STEP: creating a Pod with a static label @ 05/08/23 12:14:40.524
  STEP: watching for Pod to be ready @ 05/08/23 12:14:40.53
  May  8 12:14:40.531: INFO: observed Pod pod-test in namespace pods-8046 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May  8 12:14:40.533: INFO: observed Pod pod-test in namespace pods-8046 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:40 +0000 UTC  }]
  May  8 12:14:40.542: INFO: observed Pod pod-test in namespace pods-8046 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:40 +0000 UTC  }]
  May  8 12:14:41.521: INFO: Found Pod pod-test in namespace pods-8046 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-08 12:14:40 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/08/23 12:14:41.524
  STEP: getting the Pod and ensuring that it's patched @ 05/08/23 12:14:41.531
  STEP: replacing the Pod's status Ready condition to False @ 05/08/23 12:14:41.533
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/08/23 12:14:41.54
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/08/23 12:14:41.54
  STEP: watching for the Pod to be deleted @ 05/08/23 12:14:41.545
  May  8 12:14:41.547: INFO: observed event type MODIFIED
  May  8 12:14:43.529: INFO: observed event type MODIFIED
  May  8 12:14:43.637: INFO: observed event type MODIFIED
  May  8 12:14:44.532: INFO: observed event type MODIFIED
  May  8 12:14:44.537: INFO: observed event type MODIFIED
  May  8 12:14:44.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8046" for this suite. @ 05/08/23 12:14:44.543
• [4.042 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/08/23 12:14:44.549
  May  8 12:14:44.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replicaset @ 05/08/23 12:14:44.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:14:44.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:14:44.559
  STEP: Create a ReplicaSet @ 05/08/23 12:14:44.562
  STEP: Verify that the required pods have come up @ 05/08/23 12:14:44.567
  May  8 12:14:44.569: INFO: Pod name sample-pod: Found 0 pods out of 3
  May  8 12:14:49.575: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/08/23 12:14:49.575
  May  8 12:14:49.577: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/08/23 12:14:49.577
  STEP: DeleteCollection of the ReplicaSets @ 05/08/23 12:14:49.579
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/08/23 12:14:49.584
  May  8 12:14:49.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7679" for this suite. @ 05/08/23 12:14:49.59
• [5.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/08/23 12:14:49.602
  May  8 12:14:49.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-preemption @ 05/08/23 12:14:49.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:14:49.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:14:49.615
  May  8 12:14:49.629: INFO: Waiting up to 1m0s for all nodes to be ready
  May  8 12:15:49.644: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/08/23 12:15:49.646
  May  8 12:15:49.659: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May  8 12:15:49.665: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May  8 12:15:49.679: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May  8 12:15:49.684: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/08/23 12:15:49.684
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/08/23 12:15:51.696
  May  8 12:15:55.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-4394" for this suite. @ 05/08/23 12:15:55.736
• [66.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/08/23 12:15:55.741
  May  8 12:15:55.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 12:15:55.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:15:55.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:15:55.754
  STEP: creating Agnhost RC @ 05/08/23 12:15:55.756
  May  8 12:15:55.756: INFO: namespace kubectl-5628
  May  8 12:15:55.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5628 create -f -'
  May  8 12:15:56.457: INFO: stderr: ""
  May  8 12:15:56.457: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/08/23 12:15:56.457
  May  8 12:15:57.459: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 12:15:57.459: INFO: Found 0 / 1
  May  8 12:15:58.460: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 12:15:58.460: INFO: Found 1 / 1
  May  8 12:15:58.460: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May  8 12:15:58.462: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 12:15:58.462: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  8 12:15:58.462: INFO: wait on agnhost-primary startup in kubectl-5628 
  May  8 12:15:58.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5628 logs agnhost-primary-kdsz2 agnhost-primary'
  May  8 12:15:58.534: INFO: stderr: ""
  May  8 12:15:58.534: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/08/23 12:15:58.534
  May  8 12:15:58.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5628 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May  8 12:15:58.607: INFO: stderr: ""
  May  8 12:15:58.608: INFO: stdout: "service/rm2 exposed\n"
  May  8 12:15:58.611: INFO: Service rm2 in namespace kubectl-5628 found.
  STEP: exposing service @ 05/08/23 12:16:00.616
  May  8 12:16:00.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5628 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May  8 12:16:00.686: INFO: stderr: ""
  May  8 12:16:00.686: INFO: stdout: "service/rm3 exposed\n"
  May  8 12:16:00.688: INFO: Service rm3 in namespace kubectl-5628 found.
  May  8 12:16:02.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5628" for this suite. @ 05/08/23 12:16:02.695
• [6.958 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/08/23 12:16:02.7
  May  8 12:16:02.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename lease-test @ 05/08/23 12:16:02.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:16:02.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:16:02.713
  May  8 12:16:02.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-4957" for this suite. @ 05/08/23 12:16:02.75
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/08/23 12:16:02.756
  May  8 12:16:02.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:16:02.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:16:02.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:16:02.766
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:16:02.768
  STEP: Saw pod success @ 05/08/23 12:16:06.782
  May  8 12:16:06.783: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-deb30c6f-7b6e-4522-b46e-7d5202ee3709 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:16:06.788
  May  8 12:16:06.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2708" for this suite. @ 05/08/23 12:16:06.799
• [4.047 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/08/23 12:16:06.803
  May  8 12:16:06.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename job @ 05/08/23 12:16:06.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:16:06.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:16:06.814
  STEP: Creating a job @ 05/08/23 12:16:06.816
  STEP: Ensuring job reaches completions @ 05/08/23 12:16:06.82
  May  8 12:16:14.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7792" for this suite. @ 05/08/23 12:16:14.826
• [8.026 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/08/23 12:16:14.83
  May  8 12:16:14.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-pred @ 05/08/23 12:16:14.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:16:14.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:16:14.84
  May  8 12:16:14.843: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  8 12:16:14.847: INFO: Waiting for terminating namespaces to be deleted...
  May  8 12:16:14.848: INFO: 
  Logging pods the apiserver thinks is on node worker-0 before test
  May  8 12:16:14.852: INFO: fail-once-local-5g8r2 from job-7792 started at 2023-05-08 12:16:10 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.852: INFO: 	Container c ready: false, restart count 1
  May  8 12:16:14.852: INFO: fail-once-local-d5hp8 from job-7792 started at 2023-05-08 12:16:10 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.852: INFO: 	Container c ready: false, restart count 1
  May  8 12:16:14.852: INFO: fail-once-local-sbv45 from job-7792 started at 2023-05-08 12:16:06 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.852: INFO: 	Container c ready: false, restart count 1
  May  8 12:16:14.852: INFO: coredns-878bb57ff-8r9bz from kube-system started at 2023-05-08 11:38:09 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.852: INFO: 	Container coredns ready: true, restart count 0
  May  8 12:16:14.853: INFO: konnectivity-agent-j9zk2 from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.853: INFO: 	Container konnectivity-agent ready: true, restart count 0
  May  8 12:16:14.853: INFO: kube-proxy-gx5cm from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.853: INFO: 	Container kube-proxy ready: true, restart count 0
  May  8 12:16:14.853: INFO: kube-router-ckb57 from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.853: INFO: 	Container kube-router ready: true, restart count 0
  May  8 12:16:14.853: INFO: sonobuoy from sonobuoy started at 2023-05-08 11:39:39 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.853: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  8 12:16:14.853: INFO: sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-zf5gq from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:16:14.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:16:14.853: INFO: 	Container systemd-logs ready: true, restart count 0
  May  8 12:16:14.853: INFO: 
  Logging pods the apiserver thinks is on node worker-1 before test
  May  8 12:16:14.857: INFO: fail-once-local-hxsz6 from job-7792 started at 2023-05-08 12:16:06 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.857: INFO: 	Container c ready: false, restart count 1
  May  8 12:16:14.857: INFO: coredns-878bb57ff-g976b from kube-system started at 2023-05-08 11:38:04 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.857: INFO: 	Container coredns ready: true, restart count 0
  May  8 12:16:14.857: INFO: konnectivity-agent-298xg from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.857: INFO: 	Container konnectivity-agent ready: true, restart count 0
  May  8 12:16:14.857: INFO: kube-proxy-57cqn from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.857: INFO: 	Container kube-proxy ready: true, restart count 0
  May  8 12:16:14.857: INFO: kube-router-6pdq6 from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.857: INFO: 	Container kube-router ready: true, restart count 0
  May  8 12:16:14.857: INFO: metrics-server-7f86dff975-rtj65 from kube-system started at 2023-05-08 11:38:04 +0000 UTC (1 container statuses recorded)
  May  8 12:16:14.857: INFO: 	Container metrics-server ready: true, restart count 0
  May  8 12:16:14.857: INFO: sonobuoy-e2e-job-dd8349e2c331446f from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:16:14.857: INFO: 	Container e2e ready: true, restart count 0
  May  8 12:16:14.857: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:16:14.857: INFO: sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-5fk2l from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:16:14.857: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:16:14.857: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/08/23 12:16:14.857
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/08/23 12:16:16.87
  STEP: Trying to apply a random label on the found node. @ 05/08/23 12:16:16.881
  STEP: verifying the node has the label kubernetes.io/e2e-2a8701a7-dd66-4bdd-a93b-9130fde607b8 95 @ 05/08/23 12:16:16.887
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/08/23 12:16:16.889
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.39.71 on the node which pod4 resides and expect not scheduled @ 05/08/23 12:16:18.898
  STEP: removing the label kubernetes.io/e2e-2a8701a7-dd66-4bdd-a93b-9130fde607b8 off the node worker-0 @ 05/08/23 12:21:18.906
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-2a8701a7-dd66-4bdd-a93b-9130fde607b8 @ 05/08/23 12:21:18.914
  May  8 12:21:18.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5529" for this suite. @ 05/08/23 12:21:18.919
• [304.093 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/08/23 12:21:18.923
  May  8 12:21:18.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename dns @ 05/08/23 12:21:18.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:21:18.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:21:18.936
  STEP: Creating a test headless service @ 05/08/23 12:21:18.939
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8030.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8030.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8030.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8030.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8030.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8030.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8030.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8030.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8030.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 129.247.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.247.129_udp@PTR;check="$$(dig +tcp +noall +answer +search 129.247.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.247.129_tcp@PTR;sleep 1; done
   @ 05/08/23 12:21:18.951
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8030.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8030.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8030.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8030.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8030.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8030.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8030.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8030.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8030.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8030.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 129.247.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.247.129_udp@PTR;check="$$(dig +tcp +noall +answer +search 129.247.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.247.129_tcp@PTR;sleep 1; done
   @ 05/08/23 12:21:18.951
  STEP: creating a pod to probe DNS @ 05/08/23 12:21:18.951
  STEP: submitting the pod to kubernetes @ 05/08/23 12:21:18.951
  STEP: retrieving the pod @ 05/08/23 12:21:20.965
  STEP: looking for the results for each expected name from probers @ 05/08/23 12:21:20.967
  May  8 12:21:20.972: INFO: Unable to read wheezy_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:20.975: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:20.977: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:20.980: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:20.993: INFO: Unable to read jessie_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:20.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:20.998: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:21.001: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:21.010: INFO: Lookups using dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618 failed for: [wheezy_udp@dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_udp@dns-test-service.dns-8030.svc.cluster.local jessie_tcp@dns-test-service.dns-8030.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local]

  May  8 12:21:26.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:26.017: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:26.020: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:26.022: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:26.035: INFO: Unable to read jessie_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:26.038: INFO: Unable to read jessie_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:26.040: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:26.043: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:26.053: INFO: Lookups using dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618 failed for: [wheezy_udp@dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_udp@dns-test-service.dns-8030.svc.cluster.local jessie_tcp@dns-test-service.dns-8030.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local]

  May  8 12:21:31.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:31.017: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:31.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:31.022: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:31.035: INFO: Unable to read jessie_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:31.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:31.040: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:31.042: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:31.052: INFO: Lookups using dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618 failed for: [wheezy_udp@dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_udp@dns-test-service.dns-8030.svc.cluster.local jessie_tcp@dns-test-service.dns-8030.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local]

  May  8 12:21:36.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:36.017: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:36.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:36.022: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:36.034: INFO: Unable to read jessie_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:36.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:36.039: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:36.042: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:36.052: INFO: Lookups using dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618 failed for: [wheezy_udp@dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_udp@dns-test-service.dns-8030.svc.cluster.local jessie_tcp@dns-test-service.dns-8030.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local]

  May  8 12:21:41.015: INFO: Unable to read wheezy_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:41.018: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:41.021: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:41.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:41.036: INFO: Unable to read jessie_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:41.038: INFO: Unable to read jessie_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:41.041: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:41.043: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:41.053: INFO: Lookups using dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618 failed for: [wheezy_udp@dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_udp@dns-test-service.dns-8030.svc.cluster.local jessie_tcp@dns-test-service.dns-8030.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local]

  May  8 12:21:46.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:46.017: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:46.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:46.022: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:46.036: INFO: Unable to read jessie_udp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:46.038: INFO: Unable to read jessie_tcp@dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:46.041: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:46.044: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local from pod dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618: the server could not find the requested resource (get pods dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618)
  May  8 12:21:46.054: INFO: Lookups using dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618 failed for: [wheezy_udp@dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@dns-test-service.dns-8030.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_udp@dns-test-service.dns-8030.svc.cluster.local jessie_tcp@dns-test-service.dns-8030.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8030.svc.cluster.local]

  May  8 12:21:51.052: INFO: DNS probes using dns-8030/dns-test-d6da45b2-2a3d-4334-a636-6d598daa3618 succeeded

  May  8 12:21:51.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 12:21:51.054
  STEP: deleting the test service @ 05/08/23 12:21:51.066
  STEP: deleting the test headless service @ 05/08/23 12:21:51.084
  STEP: Destroying namespace "dns-8030" for this suite. @ 05/08/23 12:21:51.09
• [32.171 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/08/23 12:21:51.095
  May  8 12:21:51.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:21:51.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:21:51.108
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:21:51.112
  STEP: Creating configMap with name projected-configmap-test-volume-c11a51b2-89a6-4519-ba49-8f4f887dc005 @ 05/08/23 12:21:51.114
  STEP: Creating a pod to test consume configMaps @ 05/08/23 12:21:51.117
  STEP: Saw pod success @ 05/08/23 12:21:55.132
  May  8 12:21:55.134: INFO: Trying to get logs from node worker-0 pod pod-projected-configmaps-fc8267d7-7f2b-4b52-babb-05986d300b50 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 12:21:55.148
  May  8 12:21:55.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-507" for this suite. @ 05/08/23 12:21:55.16
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/08/23 12:21:55.164
  May  8 12:21:55.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename watch @ 05/08/23 12:21:55.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:21:55.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:21:55.175
  STEP: creating a new configmap @ 05/08/23 12:21:55.177
  STEP: modifying the configmap once @ 05/08/23 12:21:55.18
  STEP: modifying the configmap a second time @ 05/08/23 12:21:55.185
  STEP: deleting the configmap @ 05/08/23 12:21:55.189
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/08/23 12:21:55.192
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/08/23 12:21:55.193
  May  8 12:21:55.193: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2203  0d157acb-ab56-437c-b58b-59a38efdbc4c 15441 0 2023-05-08 12:21:55 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-08 12:21:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:21:55.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2203  0d157acb-ab56-437c-b58b-59a38efdbc4c 15442 0 2023-05-08 12:21:55 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-08 12:21:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:21:55.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2203" for this suite. @ 05/08/23 12:21:55.196
• [0.037 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/08/23 12:21:55.202
  May  8 12:21:55.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:21:55.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:21:55.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:21:55.212
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:21:55.214
  STEP: Saw pod success @ 05/08/23 12:21:59.229
  May  8 12:21:59.231: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-2455f58f-a515-40b5-992f-d2deae06c1f7 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:21:59.236
  May  8 12:21:59.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-152" for this suite. @ 05/08/23 12:21:59.248
• [4.050 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/08/23 12:21:59.252
  May  8 12:21:59.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-pred @ 05/08/23 12:21:59.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:21:59.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:21:59.263
  May  8 12:21:59.266: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  8 12:21:59.270: INFO: Waiting for terminating namespaces to be deleted...
  May  8 12:21:59.272: INFO: 
  Logging pods the apiserver thinks is on node worker-0 before test
  May  8 12:21:59.276: INFO: coredns-878bb57ff-8r9bz from kube-system started at 2023-05-08 11:38:09 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.276: INFO: 	Container coredns ready: true, restart count 0
  May  8 12:21:59.276: INFO: konnectivity-agent-j9zk2 from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.276: INFO: 	Container konnectivity-agent ready: true, restart count 0
  May  8 12:21:59.276: INFO: kube-proxy-gx5cm from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.276: INFO: 	Container kube-proxy ready: true, restart count 0
  May  8 12:21:59.276: INFO: kube-router-ckb57 from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.276: INFO: 	Container kube-router ready: true, restart count 0
  May  8 12:21:59.276: INFO: sonobuoy from sonobuoy started at 2023-05-08 11:39:39 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.276: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  8 12:21:59.276: INFO: sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-zf5gq from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:21:59.276: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:21:59.276: INFO: 	Container systemd-logs ready: true, restart count 0
  May  8 12:21:59.276: INFO: 
  Logging pods the apiserver thinks is on node worker-1 before test
  May  8 12:21:59.279: INFO: coredns-878bb57ff-g976b from kube-system started at 2023-05-08 11:38:04 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.279: INFO: 	Container coredns ready: true, restart count 0
  May  8 12:21:59.279: INFO: konnectivity-agent-298xg from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.279: INFO: 	Container konnectivity-agent ready: true, restart count 0
  May  8 12:21:59.279: INFO: kube-proxy-57cqn from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.279: INFO: 	Container kube-proxy ready: true, restart count 0
  May  8 12:21:59.279: INFO: kube-router-6pdq6 from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.279: INFO: 	Container kube-router ready: true, restart count 0
  May  8 12:21:59.279: INFO: metrics-server-7f86dff975-rtj65 from kube-system started at 2023-05-08 11:38:04 +0000 UTC (1 container statuses recorded)
  May  8 12:21:59.279: INFO: 	Container metrics-server ready: true, restart count 0
  May  8 12:21:59.279: INFO: sonobuoy-e2e-job-dd8349e2c331446f from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:21:59.279: INFO: 	Container e2e ready: true, restart count 0
  May  8 12:21:59.279: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:21:59.279: INFO: sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-5fk2l from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:21:59.279: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:21:59.279: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/08/23 12:21:59.28
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175d2a755e82ca02], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] @ 05/08/23 12:21:59.294
  May  8 12:22:00.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5039" for this suite. @ 05/08/23 12:22:00.298
• [1.050 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/08/23 12:22:00.306
  May  8 12:22:00.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 12:22:00.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:22:00.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:22:00.317
  STEP: Creating a ResourceQuota with terminating scope @ 05/08/23 12:22:00.32
  STEP: Ensuring ResourceQuota status is calculated @ 05/08/23 12:22:00.323
  STEP: Creating a ResourceQuota with not terminating scope @ 05/08/23 12:22:02.326
  STEP: Ensuring ResourceQuota status is calculated @ 05/08/23 12:22:02.331
  STEP: Creating a long running pod @ 05/08/23 12:22:04.333
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/08/23 12:22:04.345
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/08/23 12:22:06.347
  STEP: Deleting the pod @ 05/08/23 12:22:08.35
  STEP: Ensuring resource quota status released the pod usage @ 05/08/23 12:22:08.358
  STEP: Creating a terminating pod @ 05/08/23 12:22:10.361
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/08/23 12:22:10.369
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/08/23 12:22:12.373
  STEP: Deleting the pod @ 05/08/23 12:22:14.375
  STEP: Ensuring resource quota status released the pod usage @ 05/08/23 12:22:14.381
  May  8 12:22:16.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-900" for this suite. @ 05/08/23 12:22:16.386
• [16.084 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/08/23 12:22:16.391
  May  8 12:22:16.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 12:22:16.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:22:16.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:22:16.404
  STEP: set up a multi version CRD @ 05/08/23 12:22:16.406
  May  8 12:22:16.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: rename a version @ 05/08/23 12:22:19.779
  STEP: check the new version name is served @ 05/08/23 12:22:19.791
  STEP: check the old version name is removed @ 05/08/23 12:22:20.532
  STEP: check the other version is not changed @ 05/08/23 12:22:21.222
  May  8 12:22:23.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2240" for this suite. @ 05/08/23 12:22:23.885
• [7.498 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/08/23 12:22:23.89
  May  8 12:22:23.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:22:23.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:22:23.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:22:23.9
  STEP: Creating configMap with name configmap-test-volume-map-d1141884-1993-4a9a-a2d8-5ef480eae979 @ 05/08/23 12:22:23.902
  STEP: Creating a pod to test consume configMaps @ 05/08/23 12:22:23.905
  STEP: Saw pod success @ 05/08/23 12:22:27.919
  May  8 12:22:27.921: INFO: Trying to get logs from node worker-0 pod pod-configmaps-c2939884-0764-475e-9447-48d6173ff7a9 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 12:22:27.926
  May  8 12:22:27.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6097" for this suite. @ 05/08/23 12:22:27.937
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/08/23 12:22:27.943
  May  8 12:22:27.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename field-validation @ 05/08/23 12:22:27.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:22:27.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:22:27.954
  May  8 12:22:27.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:22:30.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4194" for this suite. @ 05/08/23 12:22:30.513
• [2.573 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/08/23 12:22:30.517
  May  8 12:22:30.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:22:30.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:22:30.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:22:30.528
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:22:30.531
  STEP: Saw pod success @ 05/08/23 12:22:34.545
  May  8 12:22:34.547: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-f84d8b66-a373-4966-8a96-ac31816dc1fc container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:22:34.551
  May  8 12:22:34.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9777" for this suite. @ 05/08/23 12:22:34.564
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/08/23 12:22:34.569
  May  8 12:22:34.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/08/23 12:22:34.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:22:34.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:22:34.58
  STEP: create the container to handle the HTTPGet hook request. @ 05/08/23 12:22:34.584
  STEP: create the pod with lifecycle hook @ 05/08/23 12:22:36.598
  STEP: check poststart hook @ 05/08/23 12:22:38.61
  STEP: delete the pod with lifecycle hook @ 05/08/23 12:22:38.615
  May  8 12:22:40.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5456" for this suite. @ 05/08/23 12:22:40.627
• [6.062 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/08/23 12:22:40.632
  May  8 12:22:40.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename statefulset @ 05/08/23 12:22:40.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:22:40.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:22:40.645
  STEP: Creating service test in namespace statefulset-7227 @ 05/08/23 12:22:40.648
  STEP: Creating a new StatefulSet @ 05/08/23 12:22:40.651
  May  8 12:22:40.659: INFO: Found 0 stateful pods, waiting for 3
  May  8 12:22:50.664: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  8 12:22:50.664: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  8 12:22:50.664: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/08/23 12:22:50.669
  May  8 12:22:50.686: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/08/23 12:22:50.686
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/08/23 12:23:00.7
  STEP: Performing a canary update @ 05/08/23 12:23:00.7
  May  8 12:23:00.717: INFO: Updating stateful set ss2
  May  8 12:23:00.721: INFO: Waiting for Pod statefulset-7227/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/08/23 12:23:10.727
  May  8 12:23:10.753: INFO: Found 2 stateful pods, waiting for 3
  May  8 12:23:20.758: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  8 12:23:20.758: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  8 12:23:20.758: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/08/23 12:23:20.761
  May  8 12:23:20.779: INFO: Updating stateful set ss2
  May  8 12:23:20.784: INFO: Waiting for Pod statefulset-7227/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May  8 12:23:30.809: INFO: Updating stateful set ss2
  May  8 12:23:30.813: INFO: Waiting for StatefulSet statefulset-7227/ss2 to complete update
  May  8 12:23:30.813: INFO: Waiting for Pod statefulset-7227/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May  8 12:23:40.819: INFO: Deleting all statefulset in ns statefulset-7227
  May  8 12:23:40.821: INFO: Scaling statefulset ss2 to 0
  May  8 12:23:50.833: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 12:23:50.835: INFO: Deleting statefulset ss2
  May  8 12:23:50.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7227" for this suite. @ 05/08/23 12:23:50.844
• [70.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/08/23 12:23:50.851
  May  8 12:23:50.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:23:50.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:23:50.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:23:50.862
  STEP: Creating configMap with name projected-configmap-test-volume-map-301ca67f-24c7-486a-b802-fc1de9969bbc @ 05/08/23 12:23:50.865
  STEP: Creating a pod to test consume configMaps @ 05/08/23 12:23:50.869
  STEP: Saw pod success @ 05/08/23 12:23:52.879
  May  8 12:23:52.881: INFO: Trying to get logs from node worker-0 pod pod-projected-configmaps-82b47d49-ed52-4c9a-8a5b-147d8f9b4e71 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 12:23:52.886
  May  8 12:23:52.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8936" for this suite. @ 05/08/23 12:23:52.897
• [2.049 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/08/23 12:23:52.901
  May  8 12:23:52.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 12:23:52.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:23:52.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:23:52.912
  STEP: starting the proxy server @ 05/08/23 12:23:52.915
  May  8 12:23:52.915: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-8206 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/08/23 12:23:52.96
  May  8 12:23:52.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8206" for this suite. @ 05/08/23 12:23:52.971
• [0.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/08/23 12:23:52.976
  May  8 12:23:52.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 12:23:52.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:23:52.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:23:52.989
  STEP: Counting existing ResourceQuota @ 05/08/23 12:23:52.992
  STEP: Creating a ResourceQuota @ 05/08/23 12:23:57.995
  STEP: Ensuring resource quota status is calculated @ 05/08/23 12:23:57.999
  STEP: Creating a ReplicaSet @ 05/08/23 12:24:00.003
  STEP: Ensuring resource quota status captures replicaset creation @ 05/08/23 12:24:00.012
  STEP: Deleting a ReplicaSet @ 05/08/23 12:24:02.015
  STEP: Ensuring resource quota status released usage @ 05/08/23 12:24:02.019
  May  8 12:24:04.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-566" for this suite. @ 05/08/23 12:24:04.025
• [11.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/08/23 12:24:04.035
  May  8 12:24:04.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/08/23 12:24:04.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:24:04.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:24:04.046
  STEP: creating @ 05/08/23 12:24:04.049
  STEP: getting @ 05/08/23 12:24:04.06
  STEP: listing in namespace @ 05/08/23 12:24:04.063
  STEP: patching @ 05/08/23 12:24:04.065
  STEP: deleting @ 05/08/23 12:24:04.072
  May  8 12:24:04.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1771" for this suite. @ 05/08/23 12:24:04.082
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/08/23 12:24:04.088
  May  8 12:24:04.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 12:24:04.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:24:04.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:24:04.098
  STEP: Creating pod test-grpc-e42a6e72-eba5-4ab8-b641-47b21f947ee1 in namespace container-probe-5385 @ 05/08/23 12:24:04.101
  May  8 12:24:06.110: INFO: Started pod test-grpc-e42a6e72-eba5-4ab8-b641-47b21f947ee1 in namespace container-probe-5385
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/08/23 12:24:06.11
  May  8 12:24:06.112: INFO: Initial restart count of pod test-grpc-e42a6e72-eba5-4ab8-b641-47b21f947ee1 is 0
  May  8 12:25:10.221: INFO: Restart count of pod container-probe-5385/test-grpc-e42a6e72-eba5-4ab8-b641-47b21f947ee1 is now 1 (1m4.10905513s elapsed)
  May  8 12:25:10.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 12:25:10.223
  STEP: Destroying namespace "container-probe-5385" for this suite. @ 05/08/23 12:25:10.231
• [66.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/08/23 12:25:10.238
  May  8 12:25:10.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-pred @ 05/08/23 12:25:10.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:10.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:10.25
  May  8 12:25:10.252: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  8 12:25:10.256: INFO: Waiting for terminating namespaces to be deleted...
  May  8 12:25:10.258: INFO: 
  Logging pods the apiserver thinks is on node worker-0 before test
  May  8 12:25:10.262: INFO: coredns-878bb57ff-8r9bz from kube-system started at 2023-05-08 11:38:09 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.262: INFO: 	Container coredns ready: true, restart count 0
  May  8 12:25:10.262: INFO: konnectivity-agent-j9zk2 from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.262: INFO: 	Container konnectivity-agent ready: true, restart count 0
  May  8 12:25:10.262: INFO: kube-proxy-gx5cm from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.262: INFO: 	Container kube-proxy ready: true, restart count 0
  May  8 12:25:10.262: INFO: kube-router-ckb57 from kube-system started at 2023-05-08 11:37:57 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.262: INFO: 	Container kube-router ready: true, restart count 0
  May  8 12:25:10.262: INFO: sonobuoy from sonobuoy started at 2023-05-08 11:39:39 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.262: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  8 12:25:10.262: INFO: sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-zf5gq from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:25:10.262: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:25:10.262: INFO: 	Container systemd-logs ready: true, restart count 0
  May  8 12:25:10.262: INFO: 
  Logging pods the apiserver thinks is on node worker-1 before test
  May  8 12:25:10.266: INFO: coredns-878bb57ff-g976b from kube-system started at 2023-05-08 11:38:04 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.266: INFO: 	Container coredns ready: true, restart count 0
  May  8 12:25:10.266: INFO: konnectivity-agent-298xg from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.266: INFO: 	Container konnectivity-agent ready: true, restart count 0
  May  8 12:25:10.266: INFO: kube-proxy-57cqn from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.266: INFO: 	Container kube-proxy ready: true, restart count 0
  May  8 12:25:10.266: INFO: kube-router-6pdq6 from kube-system started at 2023-05-08 11:37:52 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.266: INFO: 	Container kube-router ready: true, restart count 0
  May  8 12:25:10.266: INFO: metrics-server-7f86dff975-rtj65 from kube-system started at 2023-05-08 11:38:04 +0000 UTC (1 container statuses recorded)
  May  8 12:25:10.266: INFO: 	Container metrics-server ready: true, restart count 0
  May  8 12:25:10.266: INFO: sonobuoy-e2e-job-dd8349e2c331446f from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:25:10.266: INFO: 	Container e2e ready: true, restart count 0
  May  8 12:25:10.266: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:25:10.266: INFO: sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-5fk2l from sonobuoy started at 2023-05-08 11:39:42 +0000 UTC (2 container statuses recorded)
  May  8 12:25:10.266: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  8 12:25:10.266: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/08/23 12:25:10.266
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/08/23 12:25:12.279
  STEP: Trying to apply a random label on the found node. @ 05/08/23 12:25:12.286
  STEP: verifying the node has the label kubernetes.io/e2e-287a270a-6d5b-4ab8-bd74-d0e54524c429 42 @ 05/08/23 12:25:12.293
  STEP: Trying to relaunch the pod, now with labels. @ 05/08/23 12:25:12.295
  STEP: removing the label kubernetes.io/e2e-287a270a-6d5b-4ab8-bd74-d0e54524c429 off the node worker-0 @ 05/08/23 12:25:14.307
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-287a270a-6d5b-4ab8-bd74-d0e54524c429 @ 05/08/23 12:25:14.316
  May  8 12:25:14.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8049" for this suite. @ 05/08/23 12:25:14.32
• [4.086 seconds]
------------------------------
SS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/08/23 12:25:14.324
  May  8 12:25:14.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename podtemplate @ 05/08/23 12:25:14.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:14.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:14.337
  STEP: Create a pod template @ 05/08/23 12:25:14.339
  STEP: Replace a pod template @ 05/08/23 12:25:14.343
  May  8 12:25:14.349: INFO: Found updated podtemplate annotation: "true"

  May  8 12:25:14.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4365" for this suite. @ 05/08/23 12:25:14.352
• [0.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/08/23 12:25:14.357
  May  8 12:25:14.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:25:14.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:14.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:14.368
  STEP: Creating the pod @ 05/08/23 12:25:14.37
  May  8 12:25:16.895: INFO: Successfully updated pod "labelsupdateae2a4beb-06af-43e9-a6e0-cc0ad54b3de7"
  May  8 12:25:20.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-245" for this suite. @ 05/08/23 12:25:20.915
• [6.564 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/08/23 12:25:20.921
  May  8 12:25:20.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename disruption @ 05/08/23 12:25:20.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:20.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:20.933
  STEP: creating the pdb @ 05/08/23 12:25:20.935
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:25:20.939
  STEP: updating the pdb @ 05/08/23 12:25:22.944
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:25:22.949
  STEP: patching the pdb @ 05/08/23 12:25:24.953
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:25:24.961
  STEP: Waiting for the pdb to be deleted @ 05/08/23 12:25:26.971
  May  8 12:25:26.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7811" for this suite. @ 05/08/23 12:25:26.975
• [6.057 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/08/23 12:25:26.978
  May  8 12:25:26.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:25:26.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:26.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:26.99
  STEP: creating service in namespace services-6897 @ 05/08/23 12:25:26.993
  STEP: creating service affinity-nodeport-transition in namespace services-6897 @ 05/08/23 12:25:26.993
  STEP: creating replication controller affinity-nodeport-transition in namespace services-6897 @ 05/08/23 12:25:27.003
  I0508 12:25:27.009667      23 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-6897, replica count: 3
  I0508 12:25:30.061738      23 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 12:25:30.068: INFO: Creating new exec pod
  May  8 12:25:33.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6897 exec execpod-affinityz9zc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  May  8 12:25:33.214: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May  8 12:25:33.214: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:25:33.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6897 exec execpod-affinityz9zc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.190.245 80'
  May  8 12:25:33.330: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.190.245 80\nConnection to 10.100.190.245 80 port [tcp/http] succeeded!\n"
  May  8 12:25:33.330: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:25:33.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6897 exec execpod-affinityz9zc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.39.71 31148'
  May  8 12:25:33.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.39.71 31148\nConnection to 10.0.39.71 31148 port [tcp/*] succeeded!\n"
  May  8 12:25:33.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:25:33.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6897 exec execpod-affinityz9zc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.53.117 31148'
  May  8 12:25:33.581: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.53.117 31148\nConnection to 10.0.53.117 31148 port [tcp/*] succeeded!\n"
  May  8 12:25:33.581: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:25:33.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6897 exec execpod-affinityz9zc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.39.71:31148/ ; done'
  May  8 12:25:33.769: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n"
  May  8 12:25:33.769: INFO: stdout: "\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-gflmv\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-spthq\naffinity-nodeport-transition-gflmv\naffinity-nodeport-transition-gflmv\naffinity-nodeport-transition-spthq\naffinity-nodeport-transition-spthq\naffinity-nodeport-transition-gflmv\naffinity-nodeport-transition-gflmv\naffinity-nodeport-transition-gflmv\naffinity-nodeport-transition-spthq\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-spthq\naffinity-nodeport-transition-gflmv"
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-gflmv
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-spthq
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-gflmv
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-gflmv
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-spthq
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-spthq
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-gflmv
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-gflmv
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-gflmv
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-spthq
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-spthq
  May  8 12:25:33.769: INFO: Received response from host: affinity-nodeport-transition-gflmv
  May  8 12:25:33.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6897 exec execpod-affinityz9zc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.39.71:31148/ ; done'
  May  8 12:25:33.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:31148/\n"
  May  8 12:25:33.971: INFO: stdout: "\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc\naffinity-nodeport-transition-js4rc"
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Received response from host: affinity-nodeport-transition-js4rc
  May  8 12:25:33.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:25:33.974: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6897, will wait for the garbage collector to delete the pods @ 05/08/23 12:25:33.981
  May  8 12:25:34.038: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.041093ms
  May  8 12:25:34.139: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.03625ms
  STEP: Destroying namespace "services-6897" for this suite. @ 05/08/23 12:25:36.059
• [9.084 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/08/23 12:25:36.063
  May  8 12:25:36.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename init-container @ 05/08/23 12:25:36.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:36.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:36.074
  STEP: creating the pod @ 05/08/23 12:25:36.076
  May  8 12:25:36.076: INFO: PodSpec: initContainers in spec.initContainers
  May  8 12:25:39.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6609" for this suite. @ 05/08/23 12:25:39.046
• [2.987 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/08/23 12:25:39.05
  May  8 12:25:39.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:25:39.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:39.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:39.061
  STEP: Setting up server cert @ 05/08/23 12:25:39.076
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:25:39.959
  STEP: Deploying the webhook pod @ 05/08/23 12:25:39.964
  STEP: Wait for the deployment to be ready @ 05/08/23 12:25:39.974
  May  8 12:25:39.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:25:41.985
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:25:41.995
  May  8 12:25:42.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/08/23 12:25:42.998
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/08/23 12:25:43.014
  STEP: Creating a dummy validating-webhook-configuration object @ 05/08/23 12:25:43.029
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/08/23 12:25:43.037
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/08/23 12:25:43.04
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/08/23 12:25:43.046
  May  8 12:25:43.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8904" for this suite. @ 05/08/23 12:25:43.089
  STEP: Destroying namespace "webhook-markers-1911" for this suite. @ 05/08/23 12:25:43.092
• [4.045 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/08/23 12:25:43.096
  May  8 12:25:43.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename limitrange @ 05/08/23 12:25:43.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:43.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:43.107
  STEP: Creating a LimitRange @ 05/08/23 12:25:43.11
  STEP: Setting up watch @ 05/08/23 12:25:43.11
  STEP: Submitting a LimitRange @ 05/08/23 12:25:43.212
  STEP: Verifying LimitRange creation was observed @ 05/08/23 12:25:43.216
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/08/23 12:25:43.216
  May  8 12:25:43.218: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May  8 12:25:43.218: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/08/23 12:25:43.218
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/08/23 12:25:43.221
  May  8 12:25:43.224: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May  8 12:25:43.224: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/08/23 12:25:43.224
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/08/23 12:25:43.23
  May  8 12:25:43.232: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May  8 12:25:43.232: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/08/23 12:25:43.232
  STEP: Failing to create a Pod with more than max resources @ 05/08/23 12:25:43.234
  STEP: Updating a LimitRange @ 05/08/23 12:25:43.236
  STEP: Verifying LimitRange updating is effective @ 05/08/23 12:25:43.24
  STEP: Creating a Pod with less than former min resources @ 05/08/23 12:25:45.244
  STEP: Failing to create a Pod with more than max resources @ 05/08/23 12:25:45.248
  STEP: Deleting a LimitRange @ 05/08/23 12:25:45.249
  STEP: Verifying the LimitRange was deleted @ 05/08/23 12:25:45.255
  May  8 12:25:50.261: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/08/23 12:25:50.261
  May  8 12:25:50.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6833" for this suite. @ 05/08/23 12:25:50.269
• [7.179 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/08/23 12:25:50.277
  May  8 12:25:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename namespaces @ 05/08/23 12:25:50.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:50.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:50.299
  STEP: Updating Namespace "namespaces-1349" @ 05/08/23 12:25:50.302
  May  8 12:25:50.307: INFO: Namespace "namespaces-1349" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"928fc166-9554-4a46-90b9-1821fb86d69c", "kubernetes.io/metadata.name":"namespaces-1349", "namespaces-1349":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May  8 12:25:50.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1349" for this suite. @ 05/08/23 12:25:50.309
• [0.036 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/08/23 12:25:50.313
  May  8 12:25:50.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename daemonsets @ 05/08/23 12:25:50.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:50.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:50.324
  STEP: Creating simple DaemonSet "daemon-set" @ 05/08/23 12:25:50.337
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/08/23 12:25:50.342
  May  8 12:25:50.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 12:25:50.346: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 12:25:51.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 12:25:51.351: INFO: Node worker-1 is running 0 daemon pod, expected 1
  May  8 12:25:52.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 12:25:52.351: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 05/08/23 12:25:52.353
  May  8 12:25:52.355: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/08/23 12:25:52.355
  May  8 12:25:52.362: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/08/23 12:25:52.362
  May  8 12:25:52.364: INFO: Observed &DaemonSet event: ADDED
  May  8 12:25:52.364: INFO: Observed &DaemonSet event: MODIFIED
  May  8 12:25:52.364: INFO: Observed &DaemonSet event: MODIFIED
  May  8 12:25:52.365: INFO: Observed &DaemonSet event: MODIFIED
  May  8 12:25:52.365: INFO: Found daemon set daemon-set in namespace daemonsets-9890 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  8 12:25:52.365: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/08/23 12:25:52.365
  STEP: watching for the daemon set status to be patched @ 05/08/23 12:25:52.369
  May  8 12:25:52.370: INFO: Observed &DaemonSet event: ADDED
  May  8 12:25:52.370: INFO: Observed &DaemonSet event: MODIFIED
  May  8 12:25:52.371: INFO: Observed &DaemonSet event: MODIFIED
  May  8 12:25:52.371: INFO: Observed &DaemonSet event: MODIFIED
  May  8 12:25:52.371: INFO: Observed daemon set daemon-set in namespace daemonsets-9890 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  8 12:25:52.371: INFO: Observed &DaemonSet event: MODIFIED
  May  8 12:25:52.371: INFO: Found daemon set daemon-set in namespace daemonsets-9890 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May  8 12:25:52.371: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/08/23 12:25:52.375
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9890, will wait for the garbage collector to delete the pods @ 05/08/23 12:25:52.375
  May  8 12:25:52.431: INFO: Deleting DaemonSet.extensions daemon-set took: 3.982916ms
  May  8 12:25:52.531: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.15899ms
  May  8 12:25:53.834: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 12:25:53.834: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  8 12:25:53.836: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16943"},"items":null}

  May  8 12:25:53.838: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16943"},"items":null}

  May  8 12:25:53.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9890" for this suite. @ 05/08/23 12:25:53.846
• [3.537 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/08/23 12:25:53.851
  May  8 12:25:53.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename tables @ 05/08/23 12:25:53.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:53.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:53.862
  May  8 12:25:53.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-1388" for this suite. @ 05/08/23 12:25:53.868
• [0.021 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/08/23 12:25:53.873
  May  8 12:25:53.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename statefulset @ 05/08/23 12:25:53.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:25:53.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:25:53.882
  STEP: Creating service test in namespace statefulset-5896 @ 05/08/23 12:25:53.885
  STEP: Creating statefulset ss in namespace statefulset-5896 @ 05/08/23 12:25:53.888
  May  8 12:25:53.894: INFO: Found 0 stateful pods, waiting for 1
  May  8 12:26:03.900: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/08/23 12:26:03.904
  STEP: updating a scale subresource @ 05/08/23 12:26:03.906
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/08/23 12:26:03.911
  STEP: Patch a scale subresource @ 05/08/23 12:26:03.912
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/08/23 12:26:03.918
  May  8 12:26:03.922: INFO: Deleting all statefulset in ns statefulset-5896
  May  8 12:26:03.924: INFO: Scaling statefulset ss to 0
  May  8 12:26:13.958: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 12:26:13.960: INFO: Deleting statefulset ss
  May  8 12:26:13.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5896" for this suite. @ 05/08/23 12:26:13.97
• [20.100 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/08/23 12:26:13.974
  May  8 12:26:13.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:26:13.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:26:13.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:26:13.987
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:26:13.989
  STEP: Saw pod success @ 05/08/23 12:26:18.003
  May  8 12:26:18.005: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-c2c24930-085d-459b-a931-fdf67286f2a8 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:26:18.01
  May  8 12:26:18.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6288" for this suite. @ 05/08/23 12:26:18.022
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/08/23 12:26:18.029
  May  8 12:26:18.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:26:18.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:26:18.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:26:18.039
  STEP: Setting up server cert @ 05/08/23 12:26:18.052
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:26:18.676
  STEP: Deploying the webhook pod @ 05/08/23 12:26:18.682
  STEP: Wait for the deployment to be ready @ 05/08/23 12:26:18.69
  May  8 12:26:18.693: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/08/23 12:26:20.7
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:26:20.709
  May  8 12:26:21.709: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  8 12:26:21.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4953-crds.webhook.example.com via the AdmissionRegistration API @ 05/08/23 12:26:22.22
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/08/23 12:26:22.236
  May  8 12:26:24.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9569" for this suite. @ 05/08/23 12:26:24.822
  STEP: Destroying namespace "webhook-markers-4313" for this suite. @ 05/08/23 12:26:24.827
• [6.802 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/08/23 12:26:24.834
  May  8 12:26:24.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename subpath @ 05/08/23 12:26:24.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:26:24.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:26:24.845
  STEP: Setting up data @ 05/08/23 12:26:24.847
  STEP: Creating pod pod-subpath-test-downwardapi-4vsb @ 05/08/23 12:26:24.853
  STEP: Creating a pod to test atomic-volume-subpath @ 05/08/23 12:26:24.853
  STEP: Saw pod success @ 05/08/23 12:26:48.898
  May  8 12:26:48.900: INFO: Trying to get logs from node worker-0 pod pod-subpath-test-downwardapi-4vsb container test-container-subpath-downwardapi-4vsb: <nil>
  STEP: delete the pod @ 05/08/23 12:26:48.906
  STEP: Deleting pod pod-subpath-test-downwardapi-4vsb @ 05/08/23 12:26:48.915
  May  8 12:26:48.915: INFO: Deleting pod "pod-subpath-test-downwardapi-4vsb" in namespace "subpath-5830"
  May  8 12:26:48.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5830" for this suite. @ 05/08/23 12:26:48.919
• [24.090 seconds]
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/08/23 12:26:48.924
  May  8 12:26:48.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename containers @ 05/08/23 12:26:48.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:26:48.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:26:48.936
  STEP: Creating a pod to test override all @ 05/08/23 12:26:48.938
  STEP: Saw pod success @ 05/08/23 12:26:52.953
  May  8 12:26:52.954: INFO: Trying to get logs from node worker-0 pod client-containers-59cbab1d-01a2-4557-a000-e21845767dda container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 12:26:52.959
  May  8 12:26:52.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8395" for this suite. @ 05/08/23 12:26:52.968
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/08/23 12:26:52.975
  May  8 12:26:52.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:26:52.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:26:52.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:26:52.985
  STEP: Setting up server cert @ 05/08/23 12:26:53
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:26:53.27
  STEP: Deploying the webhook pod @ 05/08/23 12:26:53.275
  STEP: Wait for the deployment to be ready @ 05/08/23 12:26:53.284
  May  8 12:26:53.290: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:26:55.298
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:26:55.308
  May  8 12:26:56.309: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  8 12:26:56.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8703-crds.webhook.example.com via the AdmissionRegistration API @ 05/08/23 12:26:56.82
  STEP: Creating a custom resource while v1 is storage version @ 05/08/23 12:26:56.836
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/08/23 12:26:58.873
  STEP: Patching the custom resource while v2 is storage version @ 05/08/23 12:26:58.887
  May  8 12:26:58.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7387" for this suite. @ 05/08/23 12:26:59.456
  STEP: Destroying namespace "webhook-markers-705" for this suite. @ 05/08/23 12:26:59.46
• [6.489 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/08/23 12:26:59.464
  May  8 12:26:59.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:26:59.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:26:59.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:26:59.475
  STEP: Setting up server cert @ 05/08/23 12:26:59.488
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:26:59.974
  STEP: Deploying the webhook pod @ 05/08/23 12:26:59.977
  STEP: Wait for the deployment to be ready @ 05/08/23 12:26:59.986
  May  8 12:26:59.991: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:27:01.998
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:27:02.007
  May  8 12:27:03.007: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/08/23 12:27:03.01
  STEP: create a pod that should be updated by the webhook @ 05/08/23 12:27:03.025
  May  8 12:27:03.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-341" for this suite. @ 05/08/23 12:27:03.076
  STEP: Destroying namespace "webhook-markers-1917" for this suite. @ 05/08/23 12:27:03.08
• [3.620 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/08/23 12:27:03.085
  May  8 12:27:03.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-runtime @ 05/08/23 12:27:03.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:27:03.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:27:03.097
  STEP: create the container @ 05/08/23 12:27:03.099
  W0508 12:27:03.105760      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/08/23 12:27:03.105
  STEP: get the container status @ 05/08/23 12:27:06.116
  STEP: the container should be terminated @ 05/08/23 12:27:06.117
  STEP: the termination message should be set @ 05/08/23 12:27:06.118
  May  8 12:27:06.118: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/08/23 12:27:06.118
  May  8 12:27:06.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2649" for this suite. @ 05/08/23 12:27:06.13
• [3.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/08/23 12:27:06.137
  May  8 12:27:06.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pod-network-test @ 05/08/23 12:27:06.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:27:06.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:27:06.149
  STEP: Performing setup for networking test in namespace pod-network-test-2598 @ 05/08/23 12:27:06.151
  STEP: creating a selector @ 05/08/23 12:27:06.151
  STEP: Creating the service pods in kubernetes @ 05/08/23 12:27:06.152
  May  8 12:27:06.152: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/08/23 12:27:18.194
  May  8 12:27:20.212: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May  8 12:27:20.212: INFO: Going to poll 10.244.1.250 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May  8 12:27:20.214: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.250 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2598 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:27:20.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:27:20.214: INFO: ExecWithOptions: Clientset creation
  May  8 12:27:20.214: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2598/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.250+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  8 12:27:21.279: INFO: Found all 1 expected endpoints: [netserver-0]
  May  8 12:27:21.279: INFO: Going to poll 10.244.0.167 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May  8 12:27:21.282: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.167 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2598 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:27:21.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:27:21.282: INFO: ExecWithOptions: Clientset creation
  May  8 12:27:21.282: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2598/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.167+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  8 12:27:22.343: INFO: Found all 1 expected endpoints: [netserver-1]
  May  8 12:27:22.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2598" for this suite. @ 05/08/23 12:27:22.346
• [16.214 seconds]
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/08/23 12:27:22.352
  May  8 12:27:22.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replication-controller @ 05/08/23 12:27:22.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:27:22.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:27:22.363
  STEP: Creating ReplicationController "e2e-rc-66vgk" @ 05/08/23 12:27:22.365
  May  8 12:27:22.370: INFO: Get Replication Controller "e2e-rc-66vgk" to confirm replicas
  May  8 12:27:23.372: INFO: Get Replication Controller "e2e-rc-66vgk" to confirm replicas
  May  8 12:27:23.374: INFO: Found 1 replicas for "e2e-rc-66vgk" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-66vgk" @ 05/08/23 12:27:23.374
  STEP: Updating a scale subresource @ 05/08/23 12:27:23.376
  STEP: Verifying replicas where modified for replication controller "e2e-rc-66vgk" @ 05/08/23 12:27:23.381
  May  8 12:27:23.381: INFO: Get Replication Controller "e2e-rc-66vgk" to confirm replicas
  May  8 12:27:24.383: INFO: Get Replication Controller "e2e-rc-66vgk" to confirm replicas
  May  8 12:27:24.386: INFO: Found 2 replicas for "e2e-rc-66vgk" replication controller
  May  8 12:27:24.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9455" for this suite. @ 05/08/23 12:27:24.388
• [2.040 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/08/23 12:27:24.392
  May  8 12:27:24.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 12:27:24.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:27:24.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:27:24.403
  May  8 12:27:24.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/08/23 12:27:25.718
  May  8 12:27:25.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-116 --namespace=crd-publish-openapi-116 create -f -'
  May  8 12:27:26.283: INFO: stderr: ""
  May  8 12:27:26.283: INFO: stdout: "e2e-test-crd-publish-openapi-4758-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May  8 12:27:26.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-116 --namespace=crd-publish-openapi-116 delete e2e-test-crd-publish-openapi-4758-crds test-cr'
  May  8 12:27:26.346: INFO: stderr: ""
  May  8 12:27:26.346: INFO: stdout: "e2e-test-crd-publish-openapi-4758-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May  8 12:27:26.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-116 --namespace=crd-publish-openapi-116 apply -f -'
  May  8 12:27:26.541: INFO: stderr: ""
  May  8 12:27:26.541: INFO: stdout: "e2e-test-crd-publish-openapi-4758-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May  8 12:27:26.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-116 --namespace=crd-publish-openapi-116 delete e2e-test-crd-publish-openapi-4758-crds test-cr'
  May  8 12:27:26.603: INFO: stderr: ""
  May  8 12:27:26.603: INFO: stdout: "e2e-test-crd-publish-openapi-4758-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/08/23 12:27:26.603
  May  8 12:27:26.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-116 explain e2e-test-crd-publish-openapi-4758-crds'
  May  8 12:27:26.793: INFO: stderr: ""
  May  8 12:27:26.793: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-4758-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May  8 12:27:28.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-116" for this suite. @ 05/08/23 12:27:28.089
• [3.703 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/08/23 12:27:28.099
  May  8 12:27:28.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/08/23 12:27:28.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:27:28.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:27:28.109
  May  8 12:27:28.111: INFO: Waiting up to 1m0s for all nodes to be ready
  May  8 12:28:28.124: INFO: Waiting for terminating namespaces to be deleted...
  May  8 12:28:28.126: INFO: Starting informer...
  STEP: Starting pods... @ 05/08/23 12:28:28.126
  May  8 12:28:28.338: INFO: Pod1 is running on worker-0. Tainting Node
  May  8 12:28:30.552: INFO: Pod2 is running on worker-0. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/08/23 12:28:30.552
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/08/23 12:28:30.561
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/08/23 12:28:30.564
  May  8 12:28:36.364: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  May  8 12:28:56.401: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May  8 12:28:56.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/08/23 12:28:56.412
  STEP: Destroying namespace "taint-multiple-pods-7970" for this suite. @ 05/08/23 12:28:56.414
• [88.319 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/08/23 12:28:56.419
  May  8 12:28:56.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:28:56.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:28:56.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:28:56.429
  STEP: creating service endpoint-test2 in namespace services-6874 @ 05/08/23 12:28:56.431
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6874 to expose endpoints map[] @ 05/08/23 12:28:56.441
  May  8 12:28:56.448: INFO: successfully validated that service endpoint-test2 in namespace services-6874 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6874 @ 05/08/23 12:28:56.448
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6874 to expose endpoints map[pod1:[80]] @ 05/08/23 12:28:58.459
  May  8 12:28:58.465: INFO: successfully validated that service endpoint-test2 in namespace services-6874 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/08/23 12:28:58.465
  May  8 12:28:58.465: INFO: Creating new exec pod
  May  8 12:29:01.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  8 12:29:03.595: INFO: rc: 1
  May  8 12:29:03.595: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + + echo hostName
  nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  May  8 12:29:04.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  8 12:29:06.722: INFO: rc: 1
  May  8 12:29:06.722: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  May  8 12:29:07.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  8 12:29:09.723: INFO: rc: 1
  May  8 12:29:09.723: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  May  8 12:29:10.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  8 12:29:12.734: INFO: rc: 1
  May  8 12:29:12.734: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  May  8 12:29:13.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  8 12:29:15.714: INFO: rc: 1
  May  8 12:29:15.714: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  May  8 12:29:16.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  8 12:29:16.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  8 12:29:16.713: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:29:16.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.11.235 80'
  May  8 12:29:16.832: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.11.235 80\nConnection to 10.108.11.235 80 port [tcp/http] succeeded!\n"
  May  8 12:29:16.832: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-6874 @ 05/08/23 12:29:16.832
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6874 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/08/23 12:29:18.844
  May  8 12:29:18.851: INFO: successfully validated that service endpoint-test2 in namespace services-6874 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/08/23 12:29:18.851
  May  8 12:29:19.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  8 12:29:19.988: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  8 12:29:19.988: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:29:19.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.11.235 80'
  May  8 12:29:20.108: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.11.235 80\nConnection to 10.108.11.235 80 port [tcp/http] succeeded!\n"
  May  8 12:29:20.108: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6874 @ 05/08/23 12:29:20.108
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6874 to expose endpoints map[pod2:[80]] @ 05/08/23 12:29:20.117
  May  8 12:29:20.128: INFO: successfully validated that service endpoint-test2 in namespace services-6874 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/08/23 12:29:20.128
  May  8 12:29:21.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  8 12:29:21.244: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  8 12:29:21.244: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:29:21.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6874 exec execpodd244j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.11.235 80'
  May  8 12:29:21.368: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.11.235 80\nConnection to 10.108.11.235 80 port [tcp/http] succeeded!\n"
  May  8 12:29:21.368: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-6874 @ 05/08/23 12:29:21.368
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6874 to expose endpoints map[] @ 05/08/23 12:29:21.378
  May  8 12:29:22.387: INFO: successfully validated that service endpoint-test2 in namespace services-6874 exposes endpoints map[]
  May  8 12:29:22.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6874" for this suite. @ 05/08/23 12:29:22.401
• [25.986 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/08/23 12:29:22.407
  May  8 12:29:22.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 12:29:22.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:29:22.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:29:22.419
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/08/23 12:29:22.422
  STEP: Saw pod success @ 05/08/23 12:29:26.436
  May  8 12:29:26.438: INFO: Trying to get logs from node worker-0 pod pod-4f48d5ff-9ebf-406a-ad53-175c01eb982b container test-container: <nil>
  STEP: delete the pod @ 05/08/23 12:29:26.456
  May  8 12:29:26.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4812" for this suite. @ 05/08/23 12:29:26.468
• [4.065 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/08/23 12:29:26.473
  May  8 12:29:26.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename job @ 05/08/23 12:29:26.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:29:26.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:29:26.485
  STEP: Creating a suspended job @ 05/08/23 12:29:26.489
  STEP: Patching the Job @ 05/08/23 12:29:26.494
  STEP: Watching for Job to be patched @ 05/08/23 12:29:26.507
  May  8 12:29:26.508: INFO: Event ADDED observed for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k] and annotations: map[batch.kubernetes.io/job-tracking:]
  May  8 12:29:26.508: INFO: Event MODIFIED observed for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k] and annotations: map[batch.kubernetes.io/job-tracking:]
  May  8 12:29:26.508: INFO: Event MODIFIED found for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/08/23 12:29:26.508
  STEP: Watching for Job to be updated @ 05/08/23 12:29:26.515
  May  8 12:29:26.516: INFO: Event MODIFIED found for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  8 12:29:26.516: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/08/23 12:29:26.516
  May  8 12:29:26.518: INFO: Job: e2e-lfr4k as labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched]
  STEP: Waiting for job to complete @ 05/08/23 12:29:26.518
  STEP: Delete a job collection with a labelselector @ 05/08/23 12:29:34.523
  STEP: Watching for Job to be deleted @ 05/08/23 12:29:34.527
  May  8 12:29:34.528: INFO: Event MODIFIED observed for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  8 12:29:34.529: INFO: Event MODIFIED observed for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  8 12:29:34.529: INFO: Event MODIFIED observed for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  8 12:29:34.529: INFO: Event MODIFIED observed for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  8 12:29:34.529: INFO: Event MODIFIED observed for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  8 12:29:34.529: INFO: Event DELETED found for Job e2e-lfr4k in namespace job-5445 with labels: map[e2e-job-label:e2e-lfr4k e2e-lfr4k:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/08/23 12:29:34.529
  May  8 12:29:34.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5445" for this suite. @ 05/08/23 12:29:34.534
• [8.069 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/08/23 12:29:34.542
  May  8 12:29:34.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:29:34.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:29:34.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:29:34.553
  STEP: Creating configMap with name cm-test-opt-del-bd75655c-178e-40bb-89fd-a0f7de5d15a0 @ 05/08/23 12:29:34.557
  STEP: Creating configMap with name cm-test-opt-upd-bb0b0ac4-7cdf-413d-baf9-48b7fd568f97 @ 05/08/23 12:29:34.562
  STEP: Creating the pod @ 05/08/23 12:29:34.565
  STEP: Deleting configmap cm-test-opt-del-bd75655c-178e-40bb-89fd-a0f7de5d15a0 @ 05/08/23 12:29:36.592
  STEP: Updating configmap cm-test-opt-upd-bb0b0ac4-7cdf-413d-baf9-48b7fd568f97 @ 05/08/23 12:29:36.595
  STEP: Creating configMap with name cm-test-opt-create-a6e0bb37-a10c-48b7-be99-7250dd5903f8 @ 05/08/23 12:29:36.6
  STEP: waiting to observe update in volume @ 05/08/23 12:29:36.603
  May  8 12:31:06.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3420" for this suite. @ 05/08/23 12:31:06.96
• [92.423 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/08/23 12:31:06.966
  May  8 12:31:06.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename cronjob @ 05/08/23 12:31:06.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:31:06.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:31:06.977
  STEP: Creating a ReplaceConcurrent cronjob @ 05/08/23 12:31:06.979
  STEP: Ensuring a job is scheduled @ 05/08/23 12:31:06.984
  STEP: Ensuring exactly one is scheduled @ 05/08/23 12:32:00.987
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/08/23 12:32:00.989
  STEP: Ensuring the job is replaced with a new one @ 05/08/23 12:32:00.99
  STEP: Removing cronjob @ 05/08/23 12:33:00.994
  May  8 12:33:00.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5435" for this suite. @ 05/08/23 12:33:01
• [114.038 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/08/23 12:33:01.005
  May  8 12:33:01.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:33:01.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:33:01.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:33:01.02
  STEP: creating a ConfigMap @ 05/08/23 12:33:01.023
  STEP: fetching the ConfigMap @ 05/08/23 12:33:01.025
  STEP: patching the ConfigMap @ 05/08/23 12:33:01.027
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/08/23 12:33:01.033
  STEP: deleting the ConfigMap by collection with a label selector @ 05/08/23 12:33:01.035
  STEP: listing all ConfigMaps in test namespace @ 05/08/23 12:33:01.039
  May  8 12:33:01.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5527" for this suite. @ 05/08/23 12:33:01.043
• [0.041 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/08/23 12:33:01.046
  May  8 12:33:01.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename subpath @ 05/08/23 12:33:01.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:33:01.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:33:01.057
  STEP: Setting up data @ 05/08/23 12:33:01.059
  STEP: Creating pod pod-subpath-test-projected-8cbk @ 05/08/23 12:33:01.065
  STEP: Creating a pod to test atomic-volume-subpath @ 05/08/23 12:33:01.065
  STEP: Saw pod success @ 05/08/23 12:33:25.107
  May  8 12:33:25.109: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-projected-8cbk container test-container-subpath-projected-8cbk: <nil>
  STEP: delete the pod @ 05/08/23 12:33:25.123
  STEP: Deleting pod pod-subpath-test-projected-8cbk @ 05/08/23 12:33:25.134
  May  8 12:33:25.134: INFO: Deleting pod "pod-subpath-test-projected-8cbk" in namespace "subpath-640"
  May  8 12:33:25.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-640" for this suite. @ 05/08/23 12:33:25.138
• [24.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/08/23 12:33:25.143
  May  8 12:33:25.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 12:33:25.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:33:25.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:33:25.156
  May  8 12:33:47.204: INFO: Container started at 2023-05-08 12:33:25 +0000 UTC, pod became ready at 2023-05-08 12:33:45 +0000 UTC
  May  8 12:33:47.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3420" for this suite. @ 05/08/23 12:33:47.206
• [22.067 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/08/23 12:33:47.211
  May  8 12:33:47.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-runtime @ 05/08/23 12:33:47.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:33:47.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:33:47.223
  STEP: create the container @ 05/08/23 12:33:47.226
  W0508 12:33:47.231591      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/08/23 12:33:47.231
  STEP: get the container status @ 05/08/23 12:33:49.239
  STEP: the container should be terminated @ 05/08/23 12:33:49.24
  STEP: the termination message should be set @ 05/08/23 12:33:49.24
  May  8 12:33:49.240: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/08/23 12:33:49.24
  May  8 12:33:49.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1083" for this suite. @ 05/08/23 12:33:49.249
• [2.043 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/08/23 12:33:49.254
  May  8 12:33:49.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:33:49.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:33:49.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:33:49.265
  STEP: Creating configMap with name configmap-test-volume-953f7382-5e50-4276-95d0-b61a47688d9a @ 05/08/23 12:33:49.267
  STEP: Creating a pod to test consume configMaps @ 05/08/23 12:33:49.27
  STEP: Saw pod success @ 05/08/23 12:33:51.281
  May  8 12:33:51.283: INFO: Trying to get logs from node worker-0 pod pod-configmaps-29d7e8ec-c041-4246-a0c2-59f6fc4f3dd9 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 12:33:51.296
  May  8 12:33:51.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9338" for this suite. @ 05/08/23 12:33:51.305
• [2.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/08/23 12:33:51.315
  May  8 12:33:51.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename runtimeclass @ 05/08/23 12:33:51.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:33:51.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:33:51.325
  STEP: getting /apis @ 05/08/23 12:33:51.327
  STEP: getting /apis/node.k8s.io @ 05/08/23 12:33:51.331
  STEP: getting /apis/node.k8s.io/v1 @ 05/08/23 12:33:51.331
  STEP: creating @ 05/08/23 12:33:51.332
  STEP: watching @ 05/08/23 12:33:51.344
  May  8 12:33:51.344: INFO: starting watch
  STEP: getting @ 05/08/23 12:33:51.348
  STEP: listing @ 05/08/23 12:33:51.349
  STEP: patching @ 05/08/23 12:33:51.351
  STEP: updating @ 05/08/23 12:33:51.354
  May  8 12:33:51.357: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/08/23 12:33:51.358
  STEP: deleting a collection @ 05/08/23 12:33:51.364
  May  8 12:33:51.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1859" for this suite. @ 05/08/23 12:33:51.374
• [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/08/23 12:33:51.381
  May  8 12:33:51.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename statefulset @ 05/08/23 12:33:51.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:33:51.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:33:51.392
  STEP: Creating service test in namespace statefulset-8216 @ 05/08/23 12:33:51.394
  STEP: Looking for a node to schedule stateful set and pod @ 05/08/23 12:33:51.399
  STEP: Creating pod with conflicting port in namespace statefulset-8216 @ 05/08/23 12:33:51.402
  STEP: Waiting until pod test-pod will start running in namespace statefulset-8216 @ 05/08/23 12:33:51.406
  STEP: Creating statefulset with conflicting port in namespace statefulset-8216 @ 05/08/23 12:33:53.412
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8216 @ 05/08/23 12:33:53.415
  May  8 12:33:53.425: INFO: Observed stateful pod in namespace: statefulset-8216, name: ss-0, uid: 3effc029-bde4-4464-bc0e-81f71ebe96e1, status phase: Pending. Waiting for statefulset controller to delete.
  May  8 12:33:53.434: INFO: Observed stateful pod in namespace: statefulset-8216, name: ss-0, uid: 3effc029-bde4-4464-bc0e-81f71ebe96e1, status phase: Failed. Waiting for statefulset controller to delete.
  May  8 12:33:53.441: INFO: Observed stateful pod in namespace: statefulset-8216, name: ss-0, uid: 3effc029-bde4-4464-bc0e-81f71ebe96e1, status phase: Failed. Waiting for statefulset controller to delete.
  May  8 12:33:53.443: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8216
  STEP: Removing pod with conflicting port in namespace statefulset-8216 @ 05/08/23 12:33:53.443
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8216 and will be in running state @ 05/08/23 12:33:53.452
  May  8 12:33:55.460: INFO: Deleting all statefulset in ns statefulset-8216
  May  8 12:33:55.462: INFO: Scaling statefulset ss to 0
  May  8 12:34:05.475: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 12:34:05.477: INFO: Deleting statefulset ss
  May  8 12:34:05.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8216" for this suite. @ 05/08/23 12:34:05.486
• [14.108 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/08/23 12:34:05.49
  May  8 12:34:05.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 12:34:05.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:05.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:05.503
  STEP: Creating secret with name secret-test-map-cebf92ab-29b9-4a8a-bc00-07cde04ac773 @ 05/08/23 12:34:05.505
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:34:05.508
  STEP: Saw pod success @ 05/08/23 12:34:09.523
  May  8 12:34:09.524: INFO: Trying to get logs from node worker-0 pod pod-secrets-c53c2199-32f7-4208-842b-88581b655544 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:34:09.53
  May  8 12:34:09.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-204" for this suite. @ 05/08/23 12:34:09.54
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/08/23 12:34:09.546
  May  8 12:34:09.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename disruption @ 05/08/23 12:34:09.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:09.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:09.557
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:34:09.562
  STEP: Updating PodDisruptionBudget status @ 05/08/23 12:34:11.567
  STEP: Waiting for all pods to be running @ 05/08/23 12:34:11.572
  May  8 12:34:11.575: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 05/08/23 12:34:13.577
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:34:13.584
  STEP: Patching PodDisruptionBudget status @ 05/08/23 12:34:13.59
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:34:13.595
  May  8 12:34:13.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2048" for this suite. @ 05/08/23 12:34:13.6
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/08/23 12:34:13.604
  May  8 12:34:13.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename conformance-tests @ 05/08/23 12:34:13.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:13.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:13.616
  STEP: Getting node addresses @ 05/08/23 12:34:13.618
  May  8 12:34:13.618: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May  8 12:34:13.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-894" for this suite. @ 05/08/23 12:34:13.624
• [0.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/08/23 12:34:13.63
  May  8 12:34:13.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 12:34:13.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:13.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:13.641
  STEP: set up a multi version CRD @ 05/08/23 12:34:13.643
  May  8 12:34:13.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: mark a version not serverd @ 05/08/23 12:34:17.061
  STEP: check the unserved version gets removed @ 05/08/23 12:34:17.075
  STEP: check the other version is not changed @ 05/08/23 12:34:18.866
  May  8 12:34:21.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4548" for this suite. @ 05/08/23 12:34:21.581
• [7.958 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/08/23 12:34:21.588
  May  8 12:34:21.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 12:34:21.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:21.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:21.601
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/08/23 12:34:21.603
  STEP: Saw pod success @ 05/08/23 12:34:25.617
  May  8 12:34:25.619: INFO: Trying to get logs from node worker-0 pod pod-41f3128d-b351-4f45-a366-5e94df259215 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 12:34:25.624
  May  8 12:34:25.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8210" for this suite. @ 05/08/23 12:34:25.636
• [4.051 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/08/23 12:34:25.64
  May  8 12:34:25.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 12:34:25.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:25.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:25.651
  STEP: Create a pod @ 05/08/23 12:34:25.653
  STEP: patching /status @ 05/08/23 12:34:27.663
  May  8 12:34:27.669: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May  8 12:34:27.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7231" for this suite. @ 05/08/23 12:34:27.672
• [2.035 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/08/23 12:34:27.675
  May  8 12:34:27.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 12:34:27.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:27.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:27.686
  STEP: Creating Pod @ 05/08/23 12:34:27.688
  STEP: Reading file content from the nginx-container @ 05/08/23 12:34:29.699
  May  8 12:34:29.699: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6213 PodName:pod-sharedvolume-cd704a70-0cfe-45d2-9c5f-35978e0460e9 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:34:29.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:34:29.699: INFO: ExecWithOptions: Clientset creation
  May  8 12:34:29.699: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-6213/pods/pod-sharedvolume-cd704a70-0cfe-45d2-9c5f-35978e0460e9/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May  8 12:34:29.771: INFO: Exec stderr: ""
  May  8 12:34:29.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6213" for this suite. @ 05/08/23 12:34:29.773
• [2.101 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/08/23 12:34:29.778
  May  8 12:34:29.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pod-network-test @ 05/08/23 12:34:29.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:29.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:29.788
  STEP: Performing setup for networking test in namespace pod-network-test-5576 @ 05/08/23 12:34:29.791
  STEP: creating a selector @ 05/08/23 12:34:29.791
  STEP: Creating the service pods in kubernetes @ 05/08/23 12:34:29.791
  May  8 12:34:29.791: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/08/23 12:34:41.833
  May  8 12:34:43.844: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May  8 12:34:43.844: INFO: Breadth first check of 10.244.1.21 on host 10.0.39.71...
  May  8 12:34:43.846: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.22:9080/dial?request=hostname&protocol=udp&host=10.244.1.21&port=8081&tries=1'] Namespace:pod-network-test-5576 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:34:43.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:34:43.847: INFO: ExecWithOptions: Clientset creation
  May  8 12:34:43.847: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5576/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.22%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.21%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  8 12:34:43.912: INFO: Waiting for responses: map[]
  May  8 12:34:43.912: INFO: reached 10.244.1.21 after 0/1 tries
  May  8 12:34:43.912: INFO: Breadth first check of 10.244.0.175 on host 10.0.53.117...
  May  8 12:34:43.914: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.22:9080/dial?request=hostname&protocol=udp&host=10.244.0.175&port=8081&tries=1'] Namespace:pod-network-test-5576 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:34:43.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:34:43.915: INFO: ExecWithOptions: Clientset creation
  May  8 12:34:43.915: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5576/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.22%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.175%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  8 12:34:43.983: INFO: Waiting for responses: map[]
  May  8 12:34:43.983: INFO: reached 10.244.0.175 after 0/1 tries
  May  8 12:34:43.983: INFO: Going to retry 0 out of 2 pods....
  May  8 12:34:43.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5576" for this suite. @ 05/08/23 12:34:43.985
• [14.213 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/08/23 12:34:43.991
  May  8 12:34:43.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:34:43.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:44.002
  STEP: creating service in namespace services-6412 @ 05/08/23 12:34:44.005
  STEP: creating service affinity-nodeport in namespace services-6412 @ 05/08/23 12:34:44.005
  STEP: creating replication controller affinity-nodeport in namespace services-6412 @ 05/08/23 12:34:44.016
  I0508 12:34:44.025590      23 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-6412, replica count: 3
  I0508 12:34:47.077350      23 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 12:34:47.083: INFO: Creating new exec pod
  May  8 12:34:50.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6412 exec execpod-affinity4tskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May  8 12:34:50.221: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May  8 12:34:50.221: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:34:50.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6412 exec execpod-affinity4tskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.147.241 80'
  May  8 12:34:50.355: INFO: stderr: "+ nc -v -t -w 2 10.100.147.241 80\n+ echo hostName\nConnection to 10.100.147.241 80 port [tcp/http] succeeded!\n"
  May  8 12:34:50.355: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:34:50.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6412 exec execpod-affinity4tskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.39.71 30100'
  May  8 12:34:50.481: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.39.71 30100\nConnection to 10.0.39.71 30100 port [tcp/*] succeeded!\n"
  May  8 12:34:50.481: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:34:50.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6412 exec execpod-affinity4tskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.53.117 30100'
  May  8 12:34:50.597: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.53.117 30100\nConnection to 10.0.53.117 30100 port [tcp/*] succeeded!\n"
  May  8 12:34:50.597: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:34:50.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-6412 exec execpod-affinity4tskk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.39.71:30100/ ; done'
  May  8 12:34:50.778: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.39.71:30100/\n"
  May  8 12:34:50.778: INFO: stdout: "\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k\naffinity-nodeport-kxf6k"
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Received response from host: affinity-nodeport-kxf6k
  May  8 12:34:50.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:34:50.781: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-6412, will wait for the garbage collector to delete the pods @ 05/08/23 12:34:50.79
  May  8 12:34:50.847: INFO: Deleting ReplicationController affinity-nodeport took: 3.856107ms
  May  8 12:34:50.947: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.172424ms
  STEP: Destroying namespace "services-6412" for this suite. @ 05/08/23 12:34:52.967
• [8.979 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/08/23 12:34:52.972
  May  8 12:34:52.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/08/23 12:34:52.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:52.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:52.984
  May  8 12:34:52.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:34:54.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8048" for this suite. @ 05/08/23 12:34:54.005
• [1.038 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/08/23 12:34:54.01
  May  8 12:34:54.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:34:54.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:54.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:54.023
  STEP: Creating projection with secret that has name projected-secret-test-map-633366f2-0035-4a76-81cc-93765d86683a @ 05/08/23 12:34:54.026
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:34:54.029
  STEP: Saw pod success @ 05/08/23 12:34:58.045
  May  8 12:34:58.047: INFO: Trying to get logs from node worker-0 pod pod-projected-secrets-b54513f4-aebe-4f2d-8946-b0a103f77a87 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:34:58.056
  May  8 12:34:58.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2590" for this suite. @ 05/08/23 12:34:58.068
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/08/23 12:34:58.072
  May  8 12:34:58.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename podtemplate @ 05/08/23 12:34:58.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:58.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:58.085
  STEP: Create set of pod templates @ 05/08/23 12:34:58.088
  May  8 12:34:58.093: INFO: created test-podtemplate-1
  May  8 12:34:58.096: INFO: created test-podtemplate-2
  May  8 12:34:58.100: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/08/23 12:34:58.1
  STEP: delete collection of pod templates @ 05/08/23 12:34:58.102
  May  8 12:34:58.102: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/08/23 12:34:58.111
  May  8 12:34:58.111: INFO: requesting list of pod templates to confirm quantity
  May  8 12:34:58.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4642" for this suite. @ 05/08/23 12:34:58.115
• [0.048 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/08/23 12:34:58.121
  May  8 12:34:58.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename field-validation @ 05/08/23 12:34:58.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:58.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:58.132
  STEP: apply creating a deployment @ 05/08/23 12:34:58.134
  May  8 12:34:58.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8814" for this suite. @ 05/08/23 12:34:58.147
• [0.031 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/08/23 12:34:58.153
  May  8 12:34:58.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:34:58.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:34:58.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:34:58.166
  STEP: Creating projection with secret that has name projected-secret-test-map-dc0cc284-7d0f-4367-bfb4-07082924da29 @ 05/08/23 12:34:58.169
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:34:58.171
  STEP: Saw pod success @ 05/08/23 12:35:02.186
  May  8 12:35:02.188: INFO: Trying to get logs from node worker-0 pod pod-projected-secrets-3716b588-3d49-40c0-b5a2-65f098dfe422 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:35:02.193
  May  8 12:35:02.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-991" for this suite. @ 05/08/23 12:35:02.203
• [4.053 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/08/23 12:35:02.207
  May  8 12:35:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename disruption @ 05/08/23 12:35:02.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:02.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:02.22
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:35:02.225
  STEP: Waiting for all pods to be running @ 05/08/23 12:35:04.244
  May  8 12:35:04.247: INFO: running pods: 0 < 3
  May  8 12:35:06.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8343" for this suite. @ 05/08/23 12:35:06.254
• [4.051 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/08/23 12:35:06.258
  May  8 12:35:06.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename disruption @ 05/08/23 12:35:06.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:06.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:06.27
  STEP: Creating a kubernetes client @ 05/08/23 12:35:06.272
  May  8 12:35:06.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename disruption-2 @ 05/08/23 12:35:06.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:06.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:06.282
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:35:06.287
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:35:08.296
  STEP: Waiting for the pdb to be processed @ 05/08/23 12:35:10.305
  STEP: listing a collection of PDBs across all namespaces @ 05/08/23 12:35:12.31
  STEP: listing a collection of PDBs in namespace disruption-715 @ 05/08/23 12:35:12.313
  STEP: deleting a collection of PDBs @ 05/08/23 12:35:12.315
  STEP: Waiting for the PDB collection to be deleted @ 05/08/23 12:35:12.321
  May  8 12:35:12.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:35:12.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-8533" for this suite. @ 05/08/23 12:35:12.327
  STEP: Destroying namespace "disruption-715" for this suite. @ 05/08/23 12:35:12.332
• [6.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/08/23 12:35:12.338
  May  8 12:35:12.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename discovery @ 05/08/23 12:35:12.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:12.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:12.349
  STEP: Setting up server cert @ 05/08/23 12:35:12.352
  May  8 12:35:12.884: INFO: Checking APIGroup: apiregistration.k8s.io
  May  8 12:35:12.885: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May  8 12:35:12.885: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May  8 12:35:12.885: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May  8 12:35:12.885: INFO: Checking APIGroup: apps
  May  8 12:35:12.886: INFO: PreferredVersion.GroupVersion: apps/v1
  May  8 12:35:12.886: INFO: Versions found [{apps/v1 v1}]
  May  8 12:35:12.886: INFO: apps/v1 matches apps/v1
  May  8 12:35:12.886: INFO: Checking APIGroup: events.k8s.io
  May  8 12:35:12.887: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May  8 12:35:12.887: INFO: Versions found [{events.k8s.io/v1 v1}]
  May  8 12:35:12.887: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May  8 12:35:12.887: INFO: Checking APIGroup: authentication.k8s.io
  May  8 12:35:12.888: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May  8 12:35:12.888: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May  8 12:35:12.888: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May  8 12:35:12.888: INFO: Checking APIGroup: authorization.k8s.io
  May  8 12:35:12.888: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May  8 12:35:12.888: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May  8 12:35:12.888: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May  8 12:35:12.888: INFO: Checking APIGroup: autoscaling
  May  8 12:35:12.889: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May  8 12:35:12.889: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May  8 12:35:12.889: INFO: autoscaling/v2 matches autoscaling/v2
  May  8 12:35:12.889: INFO: Checking APIGroup: batch
  May  8 12:35:12.890: INFO: PreferredVersion.GroupVersion: batch/v1
  May  8 12:35:12.890: INFO: Versions found [{batch/v1 v1}]
  May  8 12:35:12.890: INFO: batch/v1 matches batch/v1
  May  8 12:35:12.890: INFO: Checking APIGroup: certificates.k8s.io
  May  8 12:35:12.891: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May  8 12:35:12.891: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May  8 12:35:12.891: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May  8 12:35:12.891: INFO: Checking APIGroup: networking.k8s.io
  May  8 12:35:12.892: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May  8 12:35:12.892: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May  8 12:35:12.892: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May  8 12:35:12.892: INFO: Checking APIGroup: policy
  May  8 12:35:12.894: INFO: PreferredVersion.GroupVersion: policy/v1
  May  8 12:35:12.894: INFO: Versions found [{policy/v1 v1}]
  May  8 12:35:12.894: INFO: policy/v1 matches policy/v1
  May  8 12:35:12.894: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May  8 12:35:12.895: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May  8 12:35:12.895: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May  8 12:35:12.895: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May  8 12:35:12.895: INFO: Checking APIGroup: storage.k8s.io
  May  8 12:35:12.896: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May  8 12:35:12.896: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May  8 12:35:12.896: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May  8 12:35:12.896: INFO: Checking APIGroup: admissionregistration.k8s.io
  May  8 12:35:12.896: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May  8 12:35:12.896: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May  8 12:35:12.896: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May  8 12:35:12.896: INFO: Checking APIGroup: apiextensions.k8s.io
  May  8 12:35:12.897: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May  8 12:35:12.897: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May  8 12:35:12.897: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May  8 12:35:12.897: INFO: Checking APIGroup: scheduling.k8s.io
  May  8 12:35:12.898: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May  8 12:35:12.898: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May  8 12:35:12.898: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May  8 12:35:12.898: INFO: Checking APIGroup: coordination.k8s.io
  May  8 12:35:12.899: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May  8 12:35:12.899: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May  8 12:35:12.899: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May  8 12:35:12.899: INFO: Checking APIGroup: node.k8s.io
  May  8 12:35:12.900: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May  8 12:35:12.900: INFO: Versions found [{node.k8s.io/v1 v1}]
  May  8 12:35:12.900: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May  8 12:35:12.900: INFO: Checking APIGroup: discovery.k8s.io
  May  8 12:35:12.901: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May  8 12:35:12.901: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May  8 12:35:12.901: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May  8 12:35:12.901: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May  8 12:35:12.902: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May  8 12:35:12.902: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May  8 12:35:12.902: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May  8 12:35:12.902: INFO: Checking APIGroup: helm.k0sproject.io
  May  8 12:35:12.902: INFO: PreferredVersion.GroupVersion: helm.k0sproject.io/v1beta1
  May  8 12:35:12.902: INFO: Versions found [{helm.k0sproject.io/v1beta1 v1beta1}]
  May  8 12:35:12.902: INFO: helm.k0sproject.io/v1beta1 matches helm.k0sproject.io/v1beta1
  May  8 12:35:12.902: INFO: Checking APIGroup: autopilot.k0sproject.io
  May  8 12:35:12.904: INFO: PreferredVersion.GroupVersion: autopilot.k0sproject.io/v1beta2
  May  8 12:35:12.904: INFO: Versions found [{autopilot.k0sproject.io/v1beta2 v1beta2}]
  May  8 12:35:12.904: INFO: autopilot.k0sproject.io/v1beta2 matches autopilot.k0sproject.io/v1beta2
  May  8 12:35:12.904: INFO: Checking APIGroup: metrics.k8s.io
  May  8 12:35:12.904: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  May  8 12:35:12.904: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  May  8 12:35:12.904: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  May  8 12:35:12.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-3931" for this suite. @ 05/08/23 12:35:12.907
• [0.572 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/08/23 12:35:12.912
  May  8 12:35:12.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 12:35:12.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:12.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:12.924
  STEP: Saw pod success @ 05/08/23 12:35:18.961
  May  8 12:35:18.963: INFO: Trying to get logs from node worker-0 pod client-envvars-7e48efbc-4469-4233-953a-5ad8081d2d2f container env3cont: <nil>
  STEP: delete the pod @ 05/08/23 12:35:18.969
  May  8 12:35:18.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-967" for this suite. @ 05/08/23 12:35:18.982
• [6.073 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/08/23 12:35:18.986
  May  8 12:35:18.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 12:35:18.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:18.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:18.999
  STEP: Creating pod liveness-d5bae0f2-ed46-4781-a7b1-059c5eb2a4ad in namespace container-probe-561 @ 05/08/23 12:35:19.001
  May  8 12:35:21.011: INFO: Started pod liveness-d5bae0f2-ed46-4781-a7b1-059c5eb2a4ad in namespace container-probe-561
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/08/23 12:35:21.012
  May  8 12:35:21.013: INFO: Initial restart count of pod liveness-d5bae0f2-ed46-4781-a7b1-059c5eb2a4ad is 0
  May  8 12:35:43.050: INFO: Restart count of pod container-probe-561/liveness-d5bae0f2-ed46-4781-a7b1-059c5eb2a4ad is now 1 (22.036582342s elapsed)
  May  8 12:35:43.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 12:35:43.052
  STEP: Destroying namespace "container-probe-561" for this suite. @ 05/08/23 12:35:43.061
• [24.081 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/08/23 12:35:43.067
  May  8 12:35:43.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 12:35:43.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:43.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:43.079
  STEP: Creating a ResourceQuota @ 05/08/23 12:35:43.081
  STEP: Getting a ResourceQuota @ 05/08/23 12:35:43.085
  STEP: Updating a ResourceQuota @ 05/08/23 12:35:43.087
  STEP: Verifying a ResourceQuota was modified @ 05/08/23 12:35:43.092
  STEP: Deleting a ResourceQuota @ 05/08/23 12:35:43.094
  STEP: Verifying the deleted ResourceQuota @ 05/08/23 12:35:43.097
  May  8 12:35:43.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2825" for this suite. @ 05/08/23 12:35:43.1
• [0.038 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/08/23 12:35:43.106
  May  8 12:35:43.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 12:35:43.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:43.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:43.115
  May  8 12:35:43.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6435" for this suite. @ 05/08/23 12:35:43.144
• [0.042 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/08/23 12:35:43.148
  May  8 12:35:43.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:35:43.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:43.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:43.158
  STEP: Setting up server cert @ 05/08/23 12:35:43.171
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:35:43.823
  STEP: Deploying the webhook pod @ 05/08/23 12:35:43.828
  STEP: Wait for the deployment to be ready @ 05/08/23 12:35:43.836
  May  8 12:35:43.842: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:35:45.849
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:35:45.857
  May  8 12:35:46.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  8 12:35:46.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5612-crds.webhook.example.com via the AdmissionRegistration API @ 05/08/23 12:35:47.367
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/08/23 12:35:47.384
  May  8 12:35:49.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6749" for this suite. @ 05/08/23 12:35:49.953
  STEP: Destroying namespace "webhook-markers-5988" for this suite. @ 05/08/23 12:35:49.958
• [6.814 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/08/23 12:35:49.962
  May  8 12:35:49.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 12:35:49.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:49.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:49.973
  STEP: Creating secret with name secret-test-0b5a4a29-8714-41be-ac35-6cd82fc742aa @ 05/08/23 12:35:49.975
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:35:49.978
  STEP: Saw pod success @ 05/08/23 12:35:53.993
  May  8 12:35:53.995: INFO: Trying to get logs from node worker-0 pod pod-secrets-be9c5574-c8e9-48b6-ae8b-bcf0108da7b7 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:35:54
  May  8 12:35:54.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-154" for this suite. @ 05/08/23 12:35:54.012
• [4.053 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/08/23 12:35:54.016
  May  8 12:35:54.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:35:54.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:54.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:54.036
  STEP: Creating projection with secret that has name projected-secret-test-09a1458b-c615-4eb0-8349-39e6a096c076 @ 05/08/23 12:35:54.038
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:35:54.042
  STEP: Saw pod success @ 05/08/23 12:35:58.056
  May  8 12:35:58.058: INFO: Trying to get logs from node worker-0 pod pod-projected-secrets-4f5eaf34-7921-41fd-a0db-ddb2c44a3ea8 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:35:58.062
  May  8 12:35:58.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4738" for this suite. @ 05/08/23 12:35:58.074
• [4.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/08/23 12:35:58.088
  May  8 12:35:58.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename server-version @ 05/08/23 12:35:58.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:58.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:58.1
  STEP: Request ServerVersion @ 05/08/23 12:35:58.103
  STEP: Confirm major version @ 05/08/23 12:35:58.104
  May  8 12:35:58.104: INFO: Major version: 1
  STEP: Confirm minor version @ 05/08/23 12:35:58.104
  May  8 12:35:58.104: INFO: cleanMinorVersion: 27
  May  8 12:35:58.104: INFO: Minor version: 27
  May  8 12:35:58.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-5836" for this suite. @ 05/08/23 12:35:58.108
• [0.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/08/23 12:35:58.115
  May  8 12:35:58.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename events @ 05/08/23 12:35:58.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:58.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:58.125
  STEP: Create set of events @ 05/08/23 12:35:58.127
  STEP: get a list of Events with a label in the current namespace @ 05/08/23 12:35:58.135
  STEP: delete a list of events @ 05/08/23 12:35:58.137
  May  8 12:35:58.137: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/08/23 12:35:58.147
  May  8 12:35:58.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-431" for this suite. @ 05/08/23 12:35:58.151
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/08/23 12:35:58.156
  May  8 12:35:58.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 12:35:58.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:35:58.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:35:58.166
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/08/23 12:35:58.168
  May  8 12:35:58.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:35:59.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:36:05.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5564" for this suite. @ 05/08/23 12:36:05.138
• [6.986 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/08/23 12:36:05.143
  May  8 12:36:05.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:36:05.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:36:05.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:36:05.155
  May  8 12:36:05.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5315" for this suite. @ 05/08/23 12:36:05.182
• [0.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/08/23 12:36:05.189
  May  8 12:36:05.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename namespaces @ 05/08/23 12:36:05.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:36:05.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:36:05.199
  STEP: creating a Namespace @ 05/08/23 12:36:05.201
  STEP: patching the Namespace @ 05/08/23 12:36:05.21
  STEP: get the Namespace and ensuring it has the label @ 05/08/23 12:36:05.214
  May  8 12:36:05.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4093" for this suite. @ 05/08/23 12:36:05.217
  STEP: Destroying namespace "nspatchtest-2d8aa465-0fab-4bf8-b564-25b06b31accd-4979" for this suite. @ 05/08/23 12:36:05.222
• [0.037 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/08/23 12:36:05.227
  May  8 12:36:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:36:05.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:36:05.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:36:05.238
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:36:05.24
  STEP: Saw pod success @ 05/08/23 12:36:09.253
  May  8 12:36:09.255: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-24b976e5-cabf-43c3-a737-5a0e0edec49c container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:36:09.26
  May  8 12:36:09.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6719" for this suite. @ 05/08/23 12:36:09.273
• [4.050 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/08/23 12:36:09.277
  May  8 12:36:09.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename init-container @ 05/08/23 12:36:09.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:36:09.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:36:09.29
  STEP: creating the pod @ 05/08/23 12:36:09.293
  May  8 12:36:09.293: INFO: PodSpec: initContainers in spec.initContainers
  May  8 12:36:57.488: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-233b8c79-8235-4806-bb1c-af8881437b1e", GenerateName:"", Namespace:"init-container-5997", SelfLink:"", UID:"8cd64b61-4746-4d5a-bf42-6a3ea8d76b5d", ResourceVersion:"20408", Generation:0, CreationTimestamp:time.Date(2023, time.May, 8, 12, 36, 9, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"293171081"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 8, 12, 36, 9, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004860510), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 8, 12, 36, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004860570), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-6jxrd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003dd9f00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6jxrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6jxrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6jxrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006c0e4a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0002c1960), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006c0e530)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006c0e560)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006c0e568), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006c0e56c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00151ede0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 8, 12, 36, 9, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 8, 12, 36, 9, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 8, 12, 36, 9, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 8, 12, 36, 9, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.39.71", PodIP:"10.244.1.36", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.36"}}, StartTime:time.Date(2023, time.May, 8, 12, 36, 9, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002c1a40)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002c1ab0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://a032d92c20fdcf55f73a4238fcf9eacc6585896e0a9f1046761471318ee3db6a", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003dd9f80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003dd9f60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc006c0e5ef), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May  8 12:36:57.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5997" for this suite. @ 05/08/23 12:36:57.492
• [48.217 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/08/23 12:36:57.496
  May  8 12:36:57.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename events @ 05/08/23 12:36:57.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:36:57.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:36:57.509
  STEP: creating a test event @ 05/08/23 12:36:57.511
  STEP: listing events in all namespaces @ 05/08/23 12:36:57.516
  STEP: listing events in test namespace @ 05/08/23 12:36:57.52
  STEP: listing events with field selection filtering on source @ 05/08/23 12:36:57.521
  STEP: listing events with field selection filtering on reportingController @ 05/08/23 12:36:57.523
  STEP: getting the test event @ 05/08/23 12:36:57.524
  STEP: patching the test event @ 05/08/23 12:36:57.526
  STEP: getting the test event @ 05/08/23 12:36:57.53
  STEP: updating the test event @ 05/08/23 12:36:57.532
  STEP: getting the test event @ 05/08/23 12:36:57.536
  STEP: deleting the test event @ 05/08/23 12:36:57.537
  STEP: listing events in all namespaces @ 05/08/23 12:36:57.541
  STEP: listing events in test namespace @ 05/08/23 12:36:57.545
  May  8 12:36:57.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6014" for this suite. @ 05/08/23 12:36:57.549
• [0.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/08/23 12:36:57.558
  May  8 12:36:57.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 12:36:57.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:36:57.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:36:57.57
  STEP: Create set of pods @ 05/08/23 12:36:57.572
  May  8 12:36:57.578: INFO: created test-pod-1
  May  8 12:36:57.582: INFO: created test-pod-2
  May  8 12:36:57.587: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/08/23 12:36:57.587
  STEP: waiting for all pods to be deleted @ 05/08/23 12:36:59.615
  May  8 12:36:59.617: INFO: Pod quantity 3 is different from expected quantity 0
  May  8 12:37:00.620: INFO: Pod quantity 2 is different from expected quantity 0
  May  8 12:37:01.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9668" for this suite. @ 05/08/23 12:37:01.622
• [4.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/08/23 12:37:01.628
  May  8 12:37:01.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:37:01.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:37:01.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:37:01.641
  STEP: Creating projection with secret that has name projected-secret-test-612d7d86-0f38-44e6-b724-d70fc8d5bd05 @ 05/08/23 12:37:01.644
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:37:01.647
  STEP: Saw pod success @ 05/08/23 12:37:05.662
  May  8 12:37:05.664: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-1c09d833-0a64-4c40-b610-88f865e1a06e container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:37:05.676
  May  8 12:37:05.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3908" for this suite. @ 05/08/23 12:37:05.689
• [4.066 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/08/23 12:37:05.695
  May  8 12:37:05.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:37:05.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:37:05.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:37:05.706
  STEP: Creating configMap with name configmap-test-upd-8d7fc35d-b3b9-4e5c-a92b-291aad14874c @ 05/08/23 12:37:05.71
  STEP: Creating the pod @ 05/08/23 12:37:05.713
  STEP: Updating configmap configmap-test-upd-8d7fc35d-b3b9-4e5c-a92b-291aad14874c @ 05/08/23 12:37:07.734
  STEP: waiting to observe update in volume @ 05/08/23 12:37:07.736
  May  8 12:38:24.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3666" for this suite. @ 05/08/23 12:38:24.019
• [78.327 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/08/23 12:38:24.024
  May  8 12:38:24.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename var-expansion @ 05/08/23 12:38:24.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:38:24.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:38:24.035
  STEP: Creating a pod to test substitution in container's command @ 05/08/23 12:38:24.037
  STEP: Saw pod success @ 05/08/23 12:38:26.047
  May  8 12:38:26.049: INFO: Trying to get logs from node worker-0 pod var-expansion-83ff3fe3-93a5-45a5-b1f0-99a042c90051 container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 12:38:26.054
  May  8 12:38:26.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2209" for this suite. @ 05/08/23 12:38:26.066
• [2.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/08/23 12:38:26.073
  May  8 12:38:26.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:38:26.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:38:26.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:38:26.085
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:38:26.087
  STEP: Saw pod success @ 05/08/23 12:38:30.102
  May  8 12:38:30.104: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-66938b98-c1fd-4d84-878c-c87248a5f120 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:38:30.11
  May  8 12:38:30.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-711" for this suite. @ 05/08/23 12:38:30.123
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/08/23 12:38:30.128
  May  8 12:38:30.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename svcaccounts @ 05/08/23 12:38:30.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:38:30.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:38:30.139
  May  8 12:38:30.151: INFO: created pod pod-service-account-defaultsa
  May  8 12:38:30.151: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May  8 12:38:30.155: INFO: created pod pod-service-account-mountsa
  May  8 12:38:30.155: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May  8 12:38:30.159: INFO: created pod pod-service-account-nomountsa
  May  8 12:38:30.159: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May  8 12:38:30.166: INFO: created pod pod-service-account-defaultsa-mountspec
  May  8 12:38:30.166: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May  8 12:38:30.173: INFO: created pod pod-service-account-mountsa-mountspec
  May  8 12:38:30.173: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May  8 12:38:30.178: INFO: created pod pod-service-account-nomountsa-mountspec
  May  8 12:38:30.178: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May  8 12:38:30.186: INFO: created pod pod-service-account-defaultsa-nomountspec
  May  8 12:38:30.186: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May  8 12:38:30.194: INFO: created pod pod-service-account-mountsa-nomountspec
  May  8 12:38:30.194: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May  8 12:38:30.199: INFO: created pod pod-service-account-nomountsa-nomountspec
  May  8 12:38:30.200: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May  8 12:38:30.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3322" for this suite. @ 05/08/23 12:38:30.208
• [0.084 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/08/23 12:38:30.213
  May  8 12:38:30.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename gc @ 05/08/23 12:38:30.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:38:30.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:38:30.229
  STEP: create the rc1 @ 05/08/23 12:38:30.235
  STEP: create the rc2 @ 05/08/23 12:38:30.241
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/08/23 12:38:36.249
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/08/23 12:38:36.612
  STEP: wait for the rc to be deleted @ 05/08/23 12:38:36.621
  May  8 12:38:41.632: INFO: 74 pods remaining
  May  8 12:38:41.632: INFO: 74 pods has nil DeletionTimestamp
  May  8 12:38:41.632: INFO: 
  STEP: Gathering metrics @ 05/08/23 12:38:46.629
  W0508 12:38:46.633115      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  8 12:38:46.633: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  8 12:38:46.633: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g4sp" in namespace "gc-2862"
  May  8 12:38:46.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rnkk" in namespace "gc-2862"
  May  8 12:38:46.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b6pg" in namespace "gc-2862"
  May  8 12:38:46.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h7df" in namespace "gc-2862"
  May  8 12:38:46.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n7x5" in namespace "gc-2862"
  May  8 12:38:46.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-6d5m8" in namespace "gc-2862"
  May  8 12:38:46.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fvjg" in namespace "gc-2862"
  May  8 12:38:46.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gl6r" in namespace "gc-2862"
  May  8 12:38:46.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nrjv" in namespace "gc-2862"
  May  8 12:38:46.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p8z5" in namespace "gc-2862"
  May  8 12:38:46.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-72m6p" in namespace "gc-2862"
  May  8 12:38:46.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bgt5" in namespace "gc-2862"
  May  8 12:38:46.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mgpl" in namespace "gc-2862"
  May  8 12:38:46.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-84m6z" in namespace "gc-2862"
  May  8 12:38:46.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bbk5" in namespace "gc-2862"
  May  8 12:38:46.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-8brq6" in namespace "gc-2862"
  May  8 12:38:46.758: INFO: Deleting pod "simpletest-rc-to-be-deleted-8j78j" in namespace "gc-2862"
  May  8 12:38:46.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jbxg" in namespace "gc-2862"
  May  8 12:38:46.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l6gv" in namespace "gc-2862"
  May  8 12:38:46.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wg9c" in namespace "gc-2862"
  May  8 12:38:46.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-94kj6" in namespace "gc-2862"
  May  8 12:38:46.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-98qr9" in namespace "gc-2862"
  May  8 12:38:46.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lxrk" in namespace "gc-2862"
  May  8 12:38:46.824: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vxsm" in namespace "gc-2862"
  May  8 12:38:46.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbzx6" in namespace "gc-2862"
  May  8 12:38:46.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcqlj" in namespace "gc-2862"
  May  8 12:38:46.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhmcl" in namespace "gc-2862"
  May  8 12:38:46.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmzqh" in namespace "gc-2862"
  May  8 12:38:46.892: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv4dq" in namespace "gc-2862"
  May  8 12:38:46.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9zkn" in namespace "gc-2862"
  May  8 12:38:46.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckm6p" in namespace "gc-2862"
  May  8 12:38:46.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmp4q" in namespace "gc-2862"
  May  8 12:38:46.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvmxv" in namespace "gc-2862"
  May  8 12:38:46.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5wrc" in namespace "gc-2862"
  May  8 12:38:46.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-d68kj" in namespace "gc-2862"
  May  8 12:38:46.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhsmt" in namespace "gc-2862"
  May  8 12:38:46.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnc8g" in namespace "gc-2862"
  May  8 12:38:46.979: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv5t5" in namespace "gc-2862"
  May  8 12:38:46.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhkfz" in namespace "gc-2862"
  May  8 12:38:46.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnfgs" in namespace "gc-2862"
  May  8 12:38:47.007: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp879" in namespace "gc-2862"
  May  8 12:38:47.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5r84" in namespace "gc-2862"
  May  8 12:38:47.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6mzp" in namespace "gc-2862"
  May  8 12:38:47.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-hl4kn" in namespace "gc-2862"
  May  8 12:38:47.043: INFO: Deleting pod "simpletest-rc-to-be-deleted-httzt" in namespace "gc-2862"
  May  8 12:38:47.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-htwq8" in namespace "gc-2862"
  May  8 12:38:47.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-hznhf" in namespace "gc-2862"
  May  8 12:38:47.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjmqt" in namespace "gc-2862"
  May  8 12:38:47.077: INFO: Deleting pod "simpletest-rc-to-be-deleted-jl2rb" in namespace "gc-2862"
  May  8 12:38:47.088: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqptq" in namespace "gc-2862"
  May  8 12:38:47.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2862" for this suite. @ 05/08/23 12:38:47.103
• [16.898 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/08/23 12:38:47.117
  May  8 12:38:47.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:38:47.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:38:47.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:38:47.16
  STEP: Setting up server cert @ 05/08/23 12:38:47.181
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:38:47.574
  STEP: Deploying the webhook pod @ 05/08/23 12:38:47.579
  STEP: Wait for the deployment to be ready @ 05/08/23 12:38:47.587
  May  8 12:38:47.593: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:38:49.6
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:38:49.609
  May  8 12:38:50.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/08/23 12:38:50.611
  STEP: create a pod @ 05/08/23 12:38:50.633
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/08/23 12:38:52.642
  May  8 12:38:52.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=webhook-2685 attach --namespace=webhook-2685 to-be-attached-pod -i -c=container1'
  May  8 12:38:52.717: INFO: rc: 1
  May  8 12:38:52.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2685" for this suite. @ 05/08/23 12:38:52.752
  STEP: Destroying namespace "webhook-markers-7064" for this suite. @ 05/08/23 12:38:52.759
• [5.646 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/08/23 12:38:52.765
  May  8 12:38:52.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:38:52.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:38:52.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:38:52.779
  STEP: Setting up server cert @ 05/08/23 12:38:52.792
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:38:53.197
  STEP: Deploying the webhook pod @ 05/08/23 12:38:53.201
  STEP: Wait for the deployment to be ready @ 05/08/23 12:38:53.211
  May  8 12:38:53.219: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:38:55.226
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:38:55.236
  May  8 12:38:56.236: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/08/23 12:38:56.238
  STEP: create a namespace for the webhook @ 05/08/23 12:38:56.254
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/08/23 12:38:56.263
  May  8 12:38:56.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6482" for this suite. @ 05/08/23 12:38:56.315
  STEP: Destroying namespace "webhook-markers-3700" for this suite. @ 05/08/23 12:38:56.318
  STEP: Destroying namespace "fail-closed-namespace-8978" for this suite. @ 05/08/23 12:38:56.323
• [3.562 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/08/23 12:38:56.327
  May  8 12:38:56.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename dns @ 05/08/23 12:38:56.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:38:56.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:38:56.339
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/08/23 12:38:56.341
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/08/23 12:38:56.341
  STEP: creating a pod to probe DNS @ 05/08/23 12:38:56.341
  STEP: submitting the pod to kubernetes @ 05/08/23 12:38:56.341
  STEP: retrieving the pod @ 05/08/23 12:38:58.352
  STEP: looking for the results for each expected name from probers @ 05/08/23 12:38:58.354
  May  8 12:38:58.367: INFO: DNS probes using dns-4266/dns-test-e73a74f1-b329-4583-89c3-9506a9046d72 succeeded

  May  8 12:38:58.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 12:38:58.37
  STEP: Destroying namespace "dns-4266" for this suite. @ 05/08/23 12:38:58.378
• [2.055 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/08/23 12:38:58.382
  May  8 12:38:58.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 12:38:58.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:38:58.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:38:58.394
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/08/23 12:38:58.396
  STEP: Saw pod success @ 05/08/23 12:39:00.406
  May  8 12:39:00.408: INFO: Trying to get logs from node worker-1 pod pod-90065933-e65d-40f8-a34d-0f6e652f16af container test-container: <nil>
  STEP: delete the pod @ 05/08/23 12:39:00.413
  May  8 12:39:00.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1104" for this suite. @ 05/08/23 12:39:00.426
• [2.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/08/23 12:39:00.432
  May  8 12:39:00.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename deployment @ 05/08/23 12:39:00.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:39:00.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:39:00.442
  STEP: creating a Deployment @ 05/08/23 12:39:00.447
  May  8 12:39:00.447: INFO: Creating simple deployment test-deployment-h8r6f
  May  8 12:39:00.456: INFO: deployment "test-deployment-h8r6f" doesn't have the required revision set
  STEP: Getting /status @ 05/08/23 12:39:02.464
  May  8 12:39:02.466: INFO: Deployment test-deployment-h8r6f has Conditions: [{Available True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-h8r6f-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/08/23 12:39:02.466
  May  8 12:39:02.473: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 39, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 39, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 39, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 39, 0, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-h8r6f-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/08/23 12:39:02.473
  May  8 12:39:02.475: INFO: Observed &Deployment event: ADDED
  May  8 12:39:02.475: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-h8r6f-5994cf9475"}
  May  8 12:39:02.475: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.475: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-h8r6f-5994cf9475"}
  May  8 12:39:02.475: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  8 12:39:02.475: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.475: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  8 12:39:02.475: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-h8r6f-5994cf9475" is progressing.}
  May  8 12:39:02.475: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.475: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  8 12:39:02.475: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-h8r6f-5994cf9475" has successfully progressed.}
  May  8 12:39:02.476: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.476: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  8 12:39:02.476: INFO: Observed Deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-h8r6f-5994cf9475" has successfully progressed.}
  May  8 12:39:02.476: INFO: Found Deployment test-deployment-h8r6f in namespace deployment-5640 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  8 12:39:02.476: INFO: Deployment test-deployment-h8r6f has an updated status
  STEP: patching the Statefulset Status @ 05/08/23 12:39:02.476
  May  8 12:39:02.476: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  8 12:39:02.480: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/08/23 12:39:02.48
  May  8 12:39:02.481: INFO: Observed &Deployment event: ADDED
  May  8 12:39:02.481: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-h8r6f-5994cf9475"}
  May  8 12:39:02.482: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-h8r6f-5994cf9475"}
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  8 12:39:02.482: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:00 +0000 UTC 2023-05-08 12:39:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-h8r6f-5994cf9475" is progressing.}
  May  8 12:39:02.482: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-h8r6f-5994cf9475" has successfully progressed.}
  May  8 12:39:02.482: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-08 12:39:01 +0000 UTC 2023-05-08 12:39:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-h8r6f-5994cf9475" has successfully progressed.}
  May  8 12:39:02.482: INFO: Observed deployment test-deployment-h8r6f in namespace deployment-5640 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  8 12:39:02.482: INFO: Observed &Deployment event: MODIFIED
  May  8 12:39:02.482: INFO: Found deployment test-deployment-h8r6f in namespace deployment-5640 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May  8 12:39:02.482: INFO: Deployment test-deployment-h8r6f has a patched status
  May  8 12:39:02.486: INFO: Deployment "test-deployment-h8r6f":
  &Deployment{ObjectMeta:{test-deployment-h8r6f  deployment-5640  3884f437-1af0-4b2b-ae8c-ac8af5d0a48d 22832 1 2023-05-08 12:39:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-08 12:39:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-08 12:39:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-08 12:39:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bf4578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-h8r6f-5994cf9475",LastUpdateTime:2023-05-08 12:39:02 +0000 UTC,LastTransitionTime:2023-05-08 12:39:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  8 12:39:02.488: INFO: New ReplicaSet "test-deployment-h8r6f-5994cf9475" of Deployment "test-deployment-h8r6f":
  &ReplicaSet{ObjectMeta:{test-deployment-h8r6f-5994cf9475  deployment-5640  49a37898-dd21-4f3f-995d-8eec89a36a0d 22811 1 2023-05-08 12:39:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-h8r6f 3884f437-1af0-4b2b-ae8c-ac8af5d0a48d 0xc0043ca4b0 0xc0043ca4b1}] [] [{kube-controller-manager Update apps/v1 2023-05-08 12:39:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3884f437-1af0-4b2b-ae8c-ac8af5d0a48d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:39:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043ca558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  8 12:39:02.490: INFO: Pod "test-deployment-h8r6f-5994cf9475-cpkdf" is available:
  &Pod{ObjectMeta:{test-deployment-h8r6f-5994cf9475-cpkdf test-deployment-h8r6f-5994cf9475- deployment-5640  9e560582-1425-451d-a0bb-3adbbbaf93c5 22810 0 2023-05-08 12:39:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-h8r6f-5994cf9475 49a37898-dd21-4f3f-995d-8eec89a36a0d 0xc004bf4970 0xc004bf4971}] [] [{kube-controller-manager Update v1 2023-05-08 12:39:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49a37898-dd21-4f3f-995d-8eec89a36a0d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 12:39:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gvgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gvgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:39:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:39:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:39:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:39:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.53.117,PodIP:10.244.0.241,StartTime:2023-05-08 12:39:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 12:39:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://049783eba39a1275dc72a09a71e0b7eecdd1795f2f2e564c82d077a653b367bc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.241,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 12:39:02.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5640" for this suite. @ 05/08/23 12:39:02.492
• [2.064 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/08/23 12:39:02.497
  May  8 12:39:02.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 12:39:02.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:39:02.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:39:02.508
  STEP: Creating pod test-webserver-51c2b0ae-ecbf-4d3a-8ba8-fd3a397773e5 in namespace container-probe-245 @ 05/08/23 12:39:02.51
  May  8 12:39:04.520: INFO: Started pod test-webserver-51c2b0ae-ecbf-4d3a-8ba8-fd3a397773e5 in namespace container-probe-245
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/08/23 12:39:04.52
  May  8 12:39:04.522: INFO: Initial restart count of pod test-webserver-51c2b0ae-ecbf-4d3a-8ba8-fd3a397773e5 is 0
  May  8 12:43:04.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 12:43:04.909
  STEP: Destroying namespace "container-probe-245" for this suite. @ 05/08/23 12:43:04.918
• [242.426 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/08/23 12:43:04.924
  May  8 12:43:04.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 12:43:04.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:43:04.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:43:04.935
  STEP: Creating secret with name secret-test-dffcb95c-8832-4f49-9d4f-6bc05372ea68 @ 05/08/23 12:43:04.938
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:43:04.941
  STEP: Saw pod success @ 05/08/23 12:43:08.954
  May  8 12:43:08.956: INFO: Trying to get logs from node worker-0 pod pod-secrets-2a915e76-794c-4408-a1e8-7159ce36ca35 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:43:08.975
  May  8 12:43:08.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5374" for this suite. @ 05/08/23 12:43:08.985
• [4.065 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/08/23 12:43:08.989
  May  8 12:43:08.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 12:43:08.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:43:08.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:43:09
  STEP: Starting the proxy @ 05/08/23 12:43:09.002
  May  8 12:43:09.002: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-3401 proxy --unix-socket=/tmp/kubectl-proxy-unix1788326461/test'
  STEP: retrieving proxy /api/ output @ 05/08/23 12:43:09.049
  May  8 12:43:09.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3401" for this suite. @ 05/08/23 12:43:09.052
• [0.066 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/08/23 12:43:09.056
  May  8 12:43:09.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:43:09.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:43:09.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:43:09.068
  STEP: Creating secret with name projected-secret-test-384742a8-65c3-46be-96f7-40630d98bc60 @ 05/08/23 12:43:09.072
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:43:09.075
  STEP: Saw pod success @ 05/08/23 12:43:13.088
  May  8 12:43:13.090: INFO: Trying to get logs from node worker-0 pod pod-projected-secrets-d990e703-c9cc-409f-81f5-13e0eadb8471 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:43:13.095
  May  8 12:43:13.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4440" for this suite. @ 05/08/23 12:43:13.107
• [4.055 seconds]
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/08/23 12:43:13.111
  May  8 12:43:13.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename events @ 05/08/23 12:43:13.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:43:13.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:43:13.123
  STEP: Create set of events @ 05/08/23 12:43:13.125
  May  8 12:43:13.128: INFO: created test-event-1
  May  8 12:43:13.131: INFO: created test-event-2
  May  8 12:43:13.135: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/08/23 12:43:13.135
  STEP: delete collection of events @ 05/08/23 12:43:13.136
  May  8 12:43:13.136: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/08/23 12:43:13.146
  May  8 12:43:13.146: INFO: requesting list of events to confirm quantity
  May  8 12:43:13.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-741" for this suite. @ 05/08/23 12:43:13.15
• [0.042 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/08/23 12:43:13.154
  May  8 12:43:13.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:43:13.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:43:13.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:43:13.163
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:43:13.165
  STEP: Saw pod success @ 05/08/23 12:43:17.179
  May  8 12:43:17.180: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-2911a30b-772e-4ddc-872a-6ced0ad1b54c container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:43:17.185
  May  8 12:43:17.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8720" for this suite. @ 05/08/23 12:43:17.197
• [4.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/08/23 12:43:17.203
  May  8 12:43:17.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 12:43:17.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:43:17.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:43:17.214
  STEP: Creating secret with name s-test-opt-del-948c8f91-c39e-454e-b5d6-87248f95a42f @ 05/08/23 12:43:17.218
  STEP: Creating secret with name s-test-opt-upd-5a5817fe-c8b6-41bb-9e5f-f5a6b2ff97c7 @ 05/08/23 12:43:17.223
  STEP: Creating the pod @ 05/08/23 12:43:17.225
  STEP: Deleting secret s-test-opt-del-948c8f91-c39e-454e-b5d6-87248f95a42f @ 05/08/23 12:43:19.251
  STEP: Updating secret s-test-opt-upd-5a5817fe-c8b6-41bb-9e5f-f5a6b2ff97c7 @ 05/08/23 12:43:19.254
  STEP: Creating secret with name s-test-opt-create-8d07f982-1302-4e24-a8de-4fcd5cbd4a10 @ 05/08/23 12:43:19.256
  STEP: waiting to observe update in volume @ 05/08/23 12:43:19.259
  May  8 12:44:23.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9717" for this suite. @ 05/08/23 12:44:23.507
• [66.308 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/08/23 12:44:23.518
  May  8 12:44:23.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 12:44:23.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:44:23.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:44:23.53
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/08/23 12:44:23.533
  STEP: Saw pod success @ 05/08/23 12:44:27.546
  May  8 12:44:27.548: INFO: Trying to get logs from node worker-1 pod pod-55f252b7-883a-430e-800a-746e9e8f2515 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 12:44:27.562
  May  8 12:44:27.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9" for this suite. @ 05/08/23 12:44:27.573
• [4.060 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/08/23 12:44:27.579
  May  8 12:44:27.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:44:27.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:44:27.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:44:27.589
  STEP: Creating the pod @ 05/08/23 12:44:27.592
  May  8 12:44:30.119: INFO: Successfully updated pod "labelsupdate89ca59dc-2b96-4dd2-8526-c72154cff83f"
  May  8 12:44:34.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-745" for this suite. @ 05/08/23 12:44:34.14
• [6.565 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/08/23 12:44:34.147
  May  8 12:44:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 12:44:34.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:44:34.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:44:34.16
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/08/23 12:44:34.162
  May  8 12:44:34.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-3071 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May  8 12:44:34.228: INFO: stderr: ""
  May  8 12:44:34.228: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/08/23 12:44:34.228
  May  8 12:44:34.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-3071 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May  8 12:44:34.296: INFO: stderr: ""
  May  8 12:44:34.296: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/08/23 12:44:34.296
  May  8 12:44:34.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-3071 delete pods e2e-test-httpd-pod'
  May  8 12:44:36.732: INFO: stderr: ""
  May  8 12:44:36.732: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  8 12:44:36.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3071" for this suite. @ 05/08/23 12:44:36.734
• [2.591 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/08/23 12:44:36.739
  May  8 12:44:36.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:44:36.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:44:36.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:44:36.751
  STEP: Setting up server cert @ 05/08/23 12:44:36.764
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:44:37.429
  STEP: Deploying the webhook pod @ 05/08/23 12:44:37.436
  STEP: Wait for the deployment to be ready @ 05/08/23 12:44:37.445
  May  8 12:44:37.450: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/08/23 12:44:39.458
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:44:39.466
  May  8 12:44:40.467: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/08/23 12:44:40.469
  STEP: create a pod that should be denied by the webhook @ 05/08/23 12:44:40.487
  STEP: create a pod that causes the webhook to hang @ 05/08/23 12:44:40.499
  STEP: create a configmap that should be denied by the webhook @ 05/08/23 12:44:50.504
  STEP: create a configmap that should be admitted by the webhook @ 05/08/23 12:44:50.515
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/08/23 12:44:50.525
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/08/23 12:44:50.53
  STEP: create a namespace that bypass the webhook @ 05/08/23 12:44:50.533
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/08/23 12:44:50.543
  May  8 12:44:50.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4030" for this suite. @ 05/08/23 12:44:50.586
  STEP: Destroying namespace "webhook-markers-2036" for this suite. @ 05/08/23 12:44:50.597
  STEP: Destroying namespace "exempted-namespace-5785" for this suite. @ 05/08/23 12:44:50.601
• [13.866 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/08/23 12:44:50.605
  May  8 12:44:50.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 12:44:50.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:44:50.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:44:50.617
  STEP: Setting up server cert @ 05/08/23 12:44:50.63
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 12:44:51.115
  STEP: Deploying the webhook pod @ 05/08/23 12:44:51.119
  STEP: Wait for the deployment to be ready @ 05/08/23 12:44:51.132
  May  8 12:44:51.136: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:44:53.143
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:44:53.153
  May  8 12:44:54.153: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/08/23 12:44:54.2
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/08/23 12:44:54.238
  STEP: Deleting the collection of validation webhooks @ 05/08/23 12:44:54.325
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/08/23 12:44:54.351
  May  8 12:44:54.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-251" for this suite. @ 05/08/23 12:44:54.382
  STEP: Destroying namespace "webhook-markers-4970" for this suite. @ 05/08/23 12:44:54.388
• [3.787 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/08/23 12:44:54.397
  May  8 12:44:54.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 12:44:54.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:44:54.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:44:54.408
  STEP: creating secret secrets-9585/secret-test-e7149c03-bd64-4a35-9c36-4300915df18d @ 05/08/23 12:44:54.41
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:44:54.413
  STEP: Saw pod success @ 05/08/23 12:44:58.429
  May  8 12:44:58.431: INFO: Trying to get logs from node worker-0 pod pod-configmaps-44bbdd3b-d554-46ea-9fdd-d3cacdb66792 container env-test: <nil>
  STEP: delete the pod @ 05/08/23 12:44:58.437
  May  8 12:44:58.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9585" for this suite. @ 05/08/23 12:44:58.447
• [4.054 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/08/23 12:44:58.451
  May  8 12:44:58.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 12:44:58.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:44:58.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:44:58.462
  STEP: Creating a ResourceQuota with best effort scope @ 05/08/23 12:44:58.464
  STEP: Ensuring ResourceQuota status is calculated @ 05/08/23 12:44:58.467
  STEP: Creating a ResourceQuota with not best effort scope @ 05/08/23 12:45:00.471
  STEP: Ensuring ResourceQuota status is calculated @ 05/08/23 12:45:00.474
  STEP: Creating a best-effort pod @ 05/08/23 12:45:02.476
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/08/23 12:45:02.485
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/08/23 12:45:04.489
  STEP: Deleting the pod @ 05/08/23 12:45:06.492
  STEP: Ensuring resource quota status released the pod usage @ 05/08/23 12:45:06.499
  STEP: Creating a not best-effort pod @ 05/08/23 12:45:08.502
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/08/23 12:45:08.51
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/08/23 12:45:10.512
  STEP: Deleting the pod @ 05/08/23 12:45:12.515
  STEP: Ensuring resource quota status released the pod usage @ 05/08/23 12:45:12.522
  May  8 12:45:14.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9179" for this suite. @ 05/08/23 12:45:14.527
• [16.079 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/08/23 12:45:14.531
  May  8 12:45:14.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 12:45:14.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:45:14.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:45:14.542
  STEP: Creating secret with name secret-test-0189a3d5-ed53-43d8-9028-17191c9ed637 @ 05/08/23 12:45:14.544
  STEP: Creating a pod to test consume secrets @ 05/08/23 12:45:14.547
  STEP: Saw pod success @ 05/08/23 12:45:18.563
  May  8 12:45:18.564: INFO: Trying to get logs from node worker-0 pod pod-secrets-62a5ea43-97df-45ff-bcb4-7298b404644f container secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 12:45:18.57
  May  8 12:45:18.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8740" for this suite. @ 05/08/23 12:45:18.582
• [4.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/08/23 12:45:18.591
  May  8 12:45:18.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 12:45:18.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:45:18.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:45:18.603
  STEP: creating the pod @ 05/08/23 12:45:18.605
  STEP: submitting the pod to kubernetes @ 05/08/23 12:45:18.605
  W0508 12:45:18.611680      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 05/08/23 12:45:20.618
  STEP: updating the pod @ 05/08/23 12:45:20.62
  May  8 12:45:21.127: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ea299283-42fd-4901-9c17-87b0b6d2869a"
  May  8 12:45:25.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6930" for this suite. @ 05/08/23 12:45:25.137
• [6.549 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/08/23 12:45:25.142
  May  8 12:45:25.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename var-expansion @ 05/08/23 12:45:25.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:45:25.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:45:25.153
  May  8 12:45:27.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:45:27.167: INFO: Deleting pod "var-expansion-1a7218e7-2d89-4a55-930a-e22543aed0a7" in namespace "var-expansion-7058"
  May  8 12:45:27.171: INFO: Wait up to 5m0s for pod "var-expansion-1a7218e7-2d89-4a55-930a-e22543aed0a7" to be fully deleted
  STEP: Destroying namespace "var-expansion-7058" for this suite. @ 05/08/23 12:45:29.177
• [4.038 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/08/23 12:45:29.181
  May  8 12:45:29.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 12:45:29.182
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:45:29.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:45:29.192
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/08/23 12:45:29.194
  STEP: Saw pod success @ 05/08/23 12:45:31.204
  May  8 12:45:31.205: INFO: Trying to get logs from node worker-0 pod pod-f39f632e-f2c4-454b-9489-102594630ee3 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 12:45:31.21
  May  8 12:45:31.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4603" for this suite. @ 05/08/23 12:45:31.221
• [2.046 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/08/23 12:45:31.228
  May  8 12:45:31.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:45:31.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:45:31.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:45:31.238
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:45:31.241
  STEP: Saw pod success @ 05/08/23 12:45:33.253
  May  8 12:45:33.255: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-d309c5a6-f82a-4f7c-9bfe-021693396dc6 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:45:33.26
  May  8 12:45:33.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9215" for this suite. @ 05/08/23 12:45:33.273
• [2.049 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/08/23 12:45:33.278
  May  8 12:45:33.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 12:45:33.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:45:33.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:45:33.29
  STEP: Creating projection with secret that has name secret-emptykey-test-40db4611-3ec5-4705-a507-e5bb94aa59b4 @ 05/08/23 12:45:33.293
  May  8 12:45:33.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8840" for this suite. @ 05/08/23 12:45:33.296
• [0.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/08/23 12:45:33.303
  May  8 12:45:33.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 12:45:33.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:45:33.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:45:33.316
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/08/23 12:45:33.318
  STEP: Saw pod success @ 05/08/23 12:45:37.334
  May  8 12:45:37.336: INFO: Trying to get logs from node worker-0 pod pod-5371434d-1d8b-421a-98e2-f1d9da4fd2e7 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 12:45:37.34
  May  8 12:45:37.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1370" for this suite. @ 05/08/23 12:45:37.351
• [4.051 seconds]
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/08/23 12:45:37.354
  May  8 12:45:37.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename var-expansion @ 05/08/23 12:45:37.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:45:37.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:45:37.367
  STEP: creating the pod @ 05/08/23 12:45:37.37
  STEP: waiting for pod running @ 05/08/23 12:45:37.376
  STEP: creating a file in subpath @ 05/08/23 12:45:39.382
  May  8 12:45:39.384: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3319 PodName:var-expansion-a410db8d-6ace-48e0-b3c2-847c4bdc8af0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:45:39.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:45:39.384: INFO: ExecWithOptions: Clientset creation
  May  8 12:45:39.384: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3319/pods/var-expansion-a410db8d-6ace-48e0-b3c2-847c4bdc8af0/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/08/23 12:45:39.451
  May  8 12:45:39.453: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3319 PodName:var-expansion-a410db8d-6ace-48e0-b3c2-847c4bdc8af0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:45:39.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:45:39.454: INFO: ExecWithOptions: Clientset creation
  May  8 12:45:39.454: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3319/pods/var-expansion-a410db8d-6ace-48e0-b3c2-847c4bdc8af0/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/08/23 12:45:39.51
  May  8 12:45:40.019: INFO: Successfully updated pod "var-expansion-a410db8d-6ace-48e0-b3c2-847c4bdc8af0"
  STEP: waiting for annotated pod running @ 05/08/23 12:45:40.019
  STEP: deleting the pod gracefully @ 05/08/23 12:45:40.021
  May  8 12:45:40.021: INFO: Deleting pod "var-expansion-a410db8d-6ace-48e0-b3c2-847c4bdc8af0" in namespace "var-expansion-3319"
  May  8 12:45:40.024: INFO: Wait up to 5m0s for pod "var-expansion-a410db8d-6ace-48e0-b3c2-847c4bdc8af0" to be fully deleted
  May  8 12:46:14.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3319" for this suite. @ 05/08/23 12:46:14.084
• [36.733 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/08/23 12:46:14.089
  May  8 12:46:14.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename gc @ 05/08/23 12:46:14.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:46:14.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:46:14.1
  STEP: create the rc @ 05/08/23 12:46:14.102
  W0508 12:46:14.105772      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/08/23 12:46:19.109
  STEP: wait for all pods to be garbage collected @ 05/08/23 12:46:19.113
  STEP: Gathering metrics @ 05/08/23 12:46:24.117
  W0508 12:46:24.120787      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  8 12:46:24.120: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  8 12:46:24.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3666" for this suite. @ 05/08/23 12:46:24.123
• [10.037 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/08/23 12:46:24.126
  May  8 12:46:24.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 12:46:24.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:46:24.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:46:24.139
  May  8 12:46:24.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/08/23 12:46:25.408
  May  8 12:46:25.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-868 --namespace=crd-publish-openapi-868 create -f -'
  May  8 12:46:26.043: INFO: stderr: ""
  May  8 12:46:26.043: INFO: stdout: "e2e-test-crd-publish-openapi-3470-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May  8 12:46:26.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-868 --namespace=crd-publish-openapi-868 delete e2e-test-crd-publish-openapi-3470-crds test-cr'
  May  8 12:46:26.107: INFO: stderr: ""
  May  8 12:46:26.107: INFO: stdout: "e2e-test-crd-publish-openapi-3470-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May  8 12:46:26.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-868 --namespace=crd-publish-openapi-868 apply -f -'
  May  8 12:46:26.323: INFO: stderr: ""
  May  8 12:46:26.323: INFO: stdout: "e2e-test-crd-publish-openapi-3470-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May  8 12:46:26.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-868 --namespace=crd-publish-openapi-868 delete e2e-test-crd-publish-openapi-3470-crds test-cr'
  May  8 12:46:26.385: INFO: stderr: ""
  May  8 12:46:26.385: INFO: stdout: "e2e-test-crd-publish-openapi-3470-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/08/23 12:46:26.385
  May  8 12:46:26.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-868 explain e2e-test-crd-publish-openapi-3470-crds'
  May  8 12:46:26.580: INFO: stderr: ""
  May  8 12:46:26.580: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-3470-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  May  8 12:46:27.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-868" for this suite. @ 05/08/23 12:46:27.843
• [3.721 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/08/23 12:46:27.85
  May  8 12:46:27.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename aggregator @ 05/08/23 12:46:27.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:46:27.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:46:27.86
  May  8 12:46:27.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Registering the sample API server. @ 05/08/23 12:46:27.863
  May  8 12:46:28.621: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May  8 12:46:28.641: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  May  8 12:46:30.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:32.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:34.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:36.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:38.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:40.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:42.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:44.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:46.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:48.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:50.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  8 12:46:52.789: INFO: Waited 109.1411ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/08/23 12:46:52.832
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/08/23 12:46:52.834
  STEP: List APIServices @ 05/08/23 12:46:52.838
  May  8 12:46:52.843: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/08/23 12:46:52.843
  May  8 12:46:52.855: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/08/23 12:46:52.855
  May  8 12:46:52.862: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 52, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/08/23 12:46:52.862
  May  8 12:46:52.864: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-08 12:46:52 +0000 UTC Passed all checks passed}
  May  8 12:46:52.864: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  8 12:46:52.864: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/08/23 12:46:52.864
  May  8 12:46:52.872: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1861697912" @ 05/08/23 12:46:52.872
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/08/23 12:46:52.896
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/08/23 12:46:52.903
  STEP: Patch APIService Status @ 05/08/23 12:46:52.905
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/08/23 12:46:52.91
  May  8 12:46:52.913: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-08 12:46:52 +0000 UTC Passed all checks passed}
  May  8 12:46:52.913: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  8 12:46:52.913: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May  8 12:46:52.913: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/08/23 12:46:52.913
  STEP: Confirm that the generated APIService has been deleted @ 05/08/23 12:46:52.916
  May  8 12:46:52.916: INFO: Requesting list of APIServices to confirm quantity
  May  8 12:46:52.919: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May  8 12:46:52.919: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May  8 12:46:52.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-9392" for this suite. @ 05/08/23 12:46:52.985
• [25.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/08/23 12:46:52.991
  May  8 12:46:52.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename deployment @ 05/08/23 12:46:52.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:46:53.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:46:53.003
  May  8 12:46:53.005: INFO: Creating simple deployment test-new-deployment
  May  8 12:46:53.013: INFO: deployment "test-new-deployment" doesn't have the required revision set
  May  8 12:46:55.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 12, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 12, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-67bd4bf6dc\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: getting scale subresource @ 05/08/23 12:46:57.025
  STEP: updating a scale subresource @ 05/08/23 12:46:57.026
  STEP: verifying the deployment Spec.Replicas was modified @ 05/08/23 12:46:57.03
  STEP: Patch a scale subresource @ 05/08/23 12:46:57.032
  May  8 12:46:57.044: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-1404  1d5ce680-8062-4e34-a293-e2b3a45630cc 24658 3 2023-05-08 12:46:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-08 12:46:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:46:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d00478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-08 12:46:56 +0000 UTC,LastTransitionTime:2023-05-08 12:46:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-08 12:46:56 +0000 UTC,LastTransitionTime:2023-05-08 12:46:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  8 12:46:57.049: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-1404  5d6366a9-417c-43e0-860b-4050b7cdfbd2 24663 2 2023-05-08 12:46:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 1d5ce680-8062-4e34-a293-e2b3a45630cc 0xc004d008d7 0xc004d008d8}] [] [{kube-controller-manager Update apps/v1 2023-05-08 12:46:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d5ce680-8062-4e34-a293-e2b3a45630cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:46:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d00968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  8 12:46:57.051: INFO: Pod "test-new-deployment-67bd4bf6dc-k4nx5" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-k4nx5 test-new-deployment-67bd4bf6dc- deployment-1404  db88bef6-9831-447f-87a3-33e55affc624 24652 0 2023-05-08 12:46:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 5d6366a9-417c-43e0-860b-4050b7cdfbd2 0xc005f21c27 0xc005f21c28}] [] [{kube-controller-manager Update v1 2023-05-08 12:46:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d6366a9-417c-43e0-860b-4050b7cdfbd2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 12:46:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rt9qf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rt9qf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:46:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:46:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:46:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:46:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:10.244.1.108,StartTime:2023-05-08 12:46:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 12:46:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7fce35cda49d95b5ea2dd99515a2639590efe710908cfb57360d88c5c64a82c3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.108,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 12:46:57.051: INFO: Pod "test-new-deployment-67bd4bf6dc-lws2f" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-lws2f test-new-deployment-67bd4bf6dc- deployment-1404  6920a499-6503-4569-b28b-3368fbf2376e 24661 0 2023-05-08 12:46:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 5d6366a9-417c-43e0-860b-4050b7cdfbd2 0xc005f21e10 0xc005f21e11}] [] [{kube-controller-manager Update v1 2023-05-08 12:46:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d6366a9-417c-43e0-860b-4050b7cdfbd2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dwn6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dwn6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:46:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 12:46:57.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1404" for this suite. @ 05/08/23 12:46:57.055
• [4.068 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/08/23 12:46:57.06
  May  8 12:46:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename gc @ 05/08/23 12:46:57.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:46:57.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:46:57.074
  STEP: create the deployment @ 05/08/23 12:46:57.078
  W0508 12:46:57.082914      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/08/23 12:46:57.083
  STEP: delete the deployment @ 05/08/23 12:46:57.587
  STEP: wait for all rs to be garbage collected @ 05/08/23 12:46:57.592
  STEP: expected 0 rs, got 1 rs @ 05/08/23 12:46:57.593
  STEP: expected 0 pods, got 2 pods @ 05/08/23 12:46:57.596
  STEP: Gathering metrics @ 05/08/23 12:46:58.104
  W0508 12:46:58.108654      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  8 12:46:58.108: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  8 12:46:58.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2622" for this suite. @ 05/08/23 12:46:58.112
• [1.056 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/08/23 12:46:58.117
  May  8 12:46:58.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:46:58.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:46:58.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:46:58.13
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:46:58.133
  STEP: Saw pod success @ 05/08/23 12:47:02.149
  May  8 12:47:02.151: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-4863c70f-cdf4-41f7-8a61-7066ea3075db container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:47:02.162
  May  8 12:47:02.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3494" for this suite. @ 05/08/23 12:47:02.173
• [4.060 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/08/23 12:47:02.178
  May  8 12:47:02.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubelet-test @ 05/08/23 12:47:02.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:47:02.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:47:02.191
  STEP: Waiting for pod completion @ 05/08/23 12:47:02.199
  May  8 12:47:04.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3267" for this suite. @ 05/08/23 12:47:04.211
• [2.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/08/23 12:47:04.218
  May  8 12:47:04.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:47:04.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:47:04.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:47:04.229
  STEP: Creating configMap with name cm-test-opt-del-579fa014-de38-401b-8e24-35bdfc459835 @ 05/08/23 12:47:04.233
  STEP: Creating configMap with name cm-test-opt-upd-ab8b2945-3a1c-46da-b722-6577da2e4298 @ 05/08/23 12:47:04.238
  STEP: Creating the pod @ 05/08/23 12:47:04.241
  STEP: Deleting configmap cm-test-opt-del-579fa014-de38-401b-8e24-35bdfc459835 @ 05/08/23 12:47:06.268
  STEP: Updating configmap cm-test-opt-upd-ab8b2945-3a1c-46da-b722-6577da2e4298 @ 05/08/23 12:47:06.271
  STEP: Creating configMap with name cm-test-opt-create-9d818d82-6a3f-4ccb-acc6-21ed0775bbc3 @ 05/08/23 12:47:06.274
  STEP: waiting to observe update in volume @ 05/08/23 12:47:06.279
  May  8 12:48:22.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4246" for this suite. @ 05/08/23 12:48:22.577
• [78.362 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/08/23 12:48:22.581
  May  8 12:48:22.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 12:48:22.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:22.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:22.591
  May  8 12:48:22.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/08/23 12:48:23.859
  May  8 12:48:23.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 create -f -'
  May  8 12:48:24.470: INFO: stderr: ""
  May  8 12:48:24.470: INFO: stdout: "e2e-test-crd-publish-openapi-3982-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May  8 12:48:24.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 delete e2e-test-crd-publish-openapi-3982-crds test-foo'
  May  8 12:48:24.532: INFO: stderr: ""
  May  8 12:48:24.532: INFO: stdout: "e2e-test-crd-publish-openapi-3982-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May  8 12:48:24.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 apply -f -'
  May  8 12:48:24.739: INFO: stderr: ""
  May  8 12:48:24.739: INFO: stdout: "e2e-test-crd-publish-openapi-3982-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May  8 12:48:24.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 delete e2e-test-crd-publish-openapi-3982-crds test-foo'
  May  8 12:48:24.800: INFO: stderr: ""
  May  8 12:48:24.800: INFO: stdout: "e2e-test-crd-publish-openapi-3982-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/08/23 12:48:24.8
  May  8 12:48:24.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 create -f -'
  May  8 12:48:24.989: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/08/23 12:48:24.989
  May  8 12:48:24.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 create -f -'
  May  8 12:48:25.177: INFO: rc: 1
  May  8 12:48:25.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 apply -f -'
  May  8 12:48:25.385: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/08/23 12:48:25.385
  May  8 12:48:25.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 create -f -'
  May  8 12:48:25.594: INFO: rc: 1
  May  8 12:48:25.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 --namespace=crd-publish-openapi-4347 apply -f -'
  May  8 12:48:25.795: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/08/23 12:48:25.795
  May  8 12:48:25.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 explain e2e-test-crd-publish-openapi-3982-crds'
  May  8 12:48:25.989: INFO: stderr: ""
  May  8 12:48:25.989: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3982-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/08/23 12:48:25.989
  May  8 12:48:25.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 explain e2e-test-crd-publish-openapi-3982-crds.metadata'
  May  8 12:48:26.200: INFO: stderr: ""
  May  8 12:48:26.200: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3982-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May  8 12:48:26.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 explain e2e-test-crd-publish-openapi-3982-crds.spec'
  May  8 12:48:26.394: INFO: stderr: ""
  May  8 12:48:26.394: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3982-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May  8 12:48:26.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 explain e2e-test-crd-publish-openapi-3982-crds.spec.bars'
  May  8 12:48:26.591: INFO: stderr: ""
  May  8 12:48:26.591: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3982-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/08/23 12:48:26.591
  May  8 12:48:26.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=crd-publish-openapi-4347 explain e2e-test-crd-publish-openapi-3982-crds.spec.bars2'
  May  8 12:48:26.782: INFO: rc: 1
  May  8 12:48:28.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4347" for this suite. @ 05/08/23 12:48:28.044
• [5.467 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/08/23 12:48:28.049
  May  8 12:48:28.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename svcaccounts @ 05/08/23 12:48:28.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:28.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:28.061
  STEP: Creating a pod to test service account token:  @ 05/08/23 12:48:28.064
  STEP: Saw pod success @ 05/08/23 12:48:32.077
  May  8 12:48:32.079: INFO: Trying to get logs from node worker-0 pod test-pod-ab5e5627-2a76-426a-94b1-3c5ee321535b container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 12:48:32.084
  May  8 12:48:32.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1337" for this suite. @ 05/08/23 12:48:32.093
• [4.049 seconds]
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/08/23 12:48:32.099
  May  8 12:48:32.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:48:32.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:32.108
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:32.11
  STEP: creating a collection of services @ 05/08/23 12:48:32.112
  May  8 12:48:32.112: INFO: Creating e2e-svc-a-tvlf9
  May  8 12:48:32.125: INFO: Creating e2e-svc-b-lsftn
  May  8 12:48:32.134: INFO: Creating e2e-svc-c-5hc6f
  STEP: deleting service collection @ 05/08/23 12:48:32.145
  May  8 12:48:32.165: INFO: Collection of services has been deleted
  May  8 12:48:32.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4612" for this suite. @ 05/08/23 12:48:32.167
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/08/23 12:48:32.174
  May  8 12:48:32.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:48:32.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:32.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:32.187
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7311 @ 05/08/23 12:48:32.189
  STEP: changing the ExternalName service to type=ClusterIP @ 05/08/23 12:48:32.193
  STEP: creating replication controller externalname-service in namespace services-7311 @ 05/08/23 12:48:32.206
  I0508 12:48:32.212353      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7311, replica count: 2
  I0508 12:48:35.263675      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 12:48:35.263: INFO: Creating new exec pod
  May  8 12:48:38.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7311 exec execpod5mbc8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  8 12:48:38.398: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  8 12:48:38.398: INFO: stdout: "externalname-service-cnzmc"
  May  8 12:48:38.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7311 exec execpod5mbc8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.20.212 80'
  May  8 12:48:38.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.20.212 80\nConnection to 10.101.20.212 80 port [tcp/http] succeeded!\n"
  May  8 12:48:38.517: INFO: stdout: ""
  May  8 12:48:39.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7311 exec execpod5mbc8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.20.212 80'
  May  8 12:48:39.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.20.212 80\nConnection to 10.101.20.212 80 port [tcp/http] succeeded!\n"
  May  8 12:48:39.641: INFO: stdout: "externalname-service-cnzmc"
  May  8 12:48:39.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:48:39.643: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-7311" for this suite. @ 05/08/23 12:48:39.654
• [7.484 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/08/23 12:48:39.661
  May  8 12:48:39.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-webhook @ 05/08/23 12:48:39.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:39.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:39.673
  STEP: Setting up server cert @ 05/08/23 12:48:39.675
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/08/23 12:48:39.966
  STEP: Deploying the custom resource conversion webhook pod @ 05/08/23 12:48:39.97
  STEP: Wait for the deployment to be ready @ 05/08/23 12:48:39.979
  May  8 12:48:39.984: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/08/23 12:48:41.99
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 12:48:41.999
  May  8 12:48:42.999: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May  8 12:48:43.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Creating a v1 custom resource @ 05/08/23 12:48:45.559
  STEP: v2 custom resource should be converted @ 05/08/23 12:48:45.563
  May  8 12:48:45.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-5292" for this suite. @ 05/08/23 12:48:46.104
• [6.448 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/08/23 12:48:46.112
  May  8 12:48:46.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 12:48:46.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:46.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:46.126
  STEP: creating Agnhost RC @ 05/08/23 12:48:46.129
  May  8 12:48:46.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-4742 create -f -'
  May  8 12:48:46.856: INFO: stderr: ""
  May  8 12:48:46.856: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/08/23 12:48:46.856
  May  8 12:48:47.859: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 12:48:47.859: INFO: Found 0 / 1
  May  8 12:48:48.859: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 12:48:48.859: INFO: Found 1 / 1
  May  8 12:48:48.859: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/08/23 12:48:48.859
  May  8 12:48:48.861: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 12:48:48.861: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  8 12:48:48.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-4742 patch pod agnhost-primary-qd9jg -p {"metadata":{"annotations":{"x":"y"}}}'
  May  8 12:48:48.925: INFO: stderr: ""
  May  8 12:48:48.925: INFO: stdout: "pod/agnhost-primary-qd9jg patched\n"
  STEP: checking annotations @ 05/08/23 12:48:48.925
  May  8 12:48:48.927: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 12:48:48.927: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  8 12:48:48.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4742" for this suite. @ 05/08/23 12:48:48.929
• [2.821 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/08/23 12:48:48.933
  May  8 12:48:48.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename daemonsets @ 05/08/23 12:48:48.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:48.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:48.943
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/08/23 12:48:48.957
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/08/23 12:48:48.96
  May  8 12:48:48.964: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 12:48:48.964: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 12:48:49.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 12:48:49.969: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 12:48:50.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 12:48:50.969: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/08/23 12:48:50.971
  May  8 12:48:50.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 12:48:50.985: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 12:48:51.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 12:48:51.990: INFO: Node worker-0 is running 0 daemon pod, expected 1
  May  8 12:48:52.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 12:48:52.989: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/08/23 12:48:52.989
  STEP: Deleting DaemonSet "daemon-set" @ 05/08/23 12:48:52.993
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2043, will wait for the garbage collector to delete the pods @ 05/08/23 12:48:52.993
  May  8 12:48:53.048: INFO: Deleting DaemonSet.extensions daemon-set took: 3.135678ms
  May  8 12:48:53.149: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.055246ms
  May  8 12:48:54.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 12:48:54.353: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  8 12:48:54.354: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25392"},"items":null}

  May  8 12:48:54.356: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25392"},"items":null}

  May  8 12:48:54.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2043" for this suite. @ 05/08/23 12:48:54.364
• [5.434 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/08/23 12:48:54.368
  May  8 12:48:54.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:48:54.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:54.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:54.381
  STEP: Creating configMap configmap-2757/configmap-test-2139898c-1015-465b-9e0d-5b962b9700af @ 05/08/23 12:48:54.385
  STEP: Creating a pod to test consume configMaps @ 05/08/23 12:48:54.388
  STEP: Saw pod success @ 05/08/23 12:48:58.401
  May  8 12:48:58.403: INFO: Trying to get logs from node worker-0 pod pod-configmaps-3c9102b7-6205-47e8-97c6-e6d7791fb13a container env-test: <nil>
  STEP: delete the pod @ 05/08/23 12:48:58.409
  May  8 12:48:58.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2757" for this suite. @ 05/08/23 12:48:58.42
• [4.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/08/23 12:48:58.432
  May  8 12:48:58.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 12:48:58.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:48:58.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:48:58.446
  STEP: creating the pod @ 05/08/23 12:48:58.449
  STEP: setting up watch @ 05/08/23 12:48:58.449
  STEP: submitting the pod to kubernetes @ 05/08/23 12:48:58.552
  STEP: verifying the pod is in kubernetes @ 05/08/23 12:48:58.557
  STEP: verifying pod creation was observed @ 05/08/23 12:48:58.559
  STEP: deleting the pod gracefully @ 05/08/23 12:49:00.568
  STEP: verifying pod deletion was observed @ 05/08/23 12:49:00.573
  May  8 12:49:01.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9797" for this suite. @ 05/08/23 12:49:01.371
• [2.943 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/08/23 12:49:01.378
  May  8 12:49:01.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replicaset @ 05/08/23 12:49:01.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:49:01.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:49:01.388
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/08/23 12:49:01.39
  STEP: When a replicaset with a matching selector is created @ 05/08/23 12:49:03.403
  STEP: Then the orphan pod is adopted @ 05/08/23 12:49:03.406
  STEP: When the matched label of one of its pods change @ 05/08/23 12:49:04.411
  May  8 12:49:04.413: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/08/23 12:49:04.421
  May  8 12:49:05.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1011" for this suite. @ 05/08/23 12:49:05.428
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/08/23 12:49:05.432
  May  8 12:49:05.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 12:49:05.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:49:05.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:49:05.445
  STEP: creating all guestbook components @ 05/08/23 12:49:05.447
  May  8 12:49:05.447: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May  8 12:49:05.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 create -f -'
  May  8 12:49:06.177: INFO: stderr: ""
  May  8 12:49:06.177: INFO: stdout: "service/agnhost-replica created\n"
  May  8 12:49:06.177: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May  8 12:49:06.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 create -f -'
  May  8 12:49:06.434: INFO: stderr: ""
  May  8 12:49:06.434: INFO: stdout: "service/agnhost-primary created\n"
  May  8 12:49:06.434: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May  8 12:49:06.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 create -f -'
  May  8 12:49:06.686: INFO: stderr: ""
  May  8 12:49:06.687: INFO: stdout: "service/frontend created\n"
  May  8 12:49:06.687: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May  8 12:49:06.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 create -f -'
  May  8 12:49:06.909: INFO: stderr: ""
  May  8 12:49:06.909: INFO: stdout: "deployment.apps/frontend created\n"
  May  8 12:49:06.909: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May  8 12:49:06.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 create -f -'
  May  8 12:49:07.148: INFO: stderr: ""
  May  8 12:49:07.148: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May  8 12:49:07.148: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May  8 12:49:07.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 create -f -'
  May  8 12:49:07.478: INFO: stderr: ""
  May  8 12:49:07.478: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/08/23 12:49:07.478
  May  8 12:49:07.478: INFO: Waiting for all frontend pods to be Running.
  May  8 12:49:12.529: INFO: Waiting for frontend to serve content.
  May  8 12:49:12.537: INFO: Trying to add a new entry to the guestbook.
  May  8 12:49:12.542: INFO: Verifying that added entry can be retrieved.
  May  8 12:49:12.550: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
  STEP: using delete to clean up resources @ 05/08/23 12:49:17.559
  May  8 12:49:17.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 delete --grace-period=0 --force -f -'
  May  8 12:49:17.628: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 12:49:17.628: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/08/23 12:49:17.628
  May  8 12:49:17.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 delete --grace-period=0 --force -f -'
  May  8 12:49:17.702: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 12:49:17.702: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/08/23 12:49:17.702
  May  8 12:49:17.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 delete --grace-period=0 --force -f -'
  May  8 12:49:17.771: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 12:49:17.771: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/08/23 12:49:17.771
  May  8 12:49:17.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 delete --grace-period=0 --force -f -'
  May  8 12:49:17.829: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 12:49:17.829: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/08/23 12:49:17.83
  May  8 12:49:17.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 delete --grace-period=0 --force -f -'
  May  8 12:49:17.906: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 12:49:17.906: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/08/23 12:49:17.906
  May  8 12:49:17.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5813 delete --grace-period=0 --force -f -'
  May  8 12:49:17.975: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 12:49:17.975: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May  8 12:49:17.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5813" for this suite. @ 05/08/23 12:49:17.977
• [12.552 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/08/23 12:49:17.986
  May  8 12:49:17.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-runtime @ 05/08/23 12:49:17.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:49:17.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:49:17.999
  STEP: create the container @ 05/08/23 12:49:18.002
  W0508 12:49:18.008689      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/08/23 12:49:18.008
  STEP: get the container status @ 05/08/23 12:49:21.019
  STEP: the container should be terminated @ 05/08/23 12:49:21.021
  STEP: the termination message should be set @ 05/08/23 12:49:21.021
  May  8 12:49:21.021: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/08/23 12:49:21.021
  May  8 12:49:21.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7673" for this suite. @ 05/08/23 12:49:21.061
• [3.079 seconds]
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/08/23 12:49:21.064
  May  8 12:49:21.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/08/23 12:49:21.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:49:21.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:49:21.075
  STEP: getting /apis @ 05/08/23 12:49:21.078
  STEP: getting /apis/storage.k8s.io @ 05/08/23 12:49:21.082
  STEP: getting /apis/storage.k8s.io/v1 @ 05/08/23 12:49:21.083
  STEP: creating @ 05/08/23 12:49:21.083
  STEP: watching @ 05/08/23 12:49:21.147
  May  8 12:49:21.147: INFO: starting watch
  STEP: getting @ 05/08/23 12:49:21.152
  STEP: listing in namespace @ 05/08/23 12:49:21.154
  STEP: listing across namespaces @ 05/08/23 12:49:21.155
  STEP: patching @ 05/08/23 12:49:21.157
  STEP: updating @ 05/08/23 12:49:21.165
  May  8 12:49:21.169: INFO: waiting for watch events with expected annotations in namespace
  May  8 12:49:21.169: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/08/23 12:49:21.169
  STEP: deleting a collection @ 05/08/23 12:49:21.176
  May  8 12:49:21.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-7147" for this suite. @ 05/08/23 12:49:21.186
• [0.126 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/08/23 12:49:21.191
  May  8 12:49:21.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename ingress @ 05/08/23 12:49:21.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:49:21.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:49:21.201
  STEP: getting /apis @ 05/08/23 12:49:21.203
  STEP: getting /apis/networking.k8s.io @ 05/08/23 12:49:21.207
  STEP: getting /apis/networking.k8s.iov1 @ 05/08/23 12:49:21.207
  STEP: creating @ 05/08/23 12:49:21.208
  STEP: getting @ 05/08/23 12:49:21.219
  STEP: listing @ 05/08/23 12:49:21.221
  STEP: watching @ 05/08/23 12:49:21.223
  May  8 12:49:21.223: INFO: starting watch
  STEP: cluster-wide listing @ 05/08/23 12:49:21.224
  STEP: cluster-wide watching @ 05/08/23 12:49:21.226
  May  8 12:49:21.226: INFO: starting watch
  STEP: patching @ 05/08/23 12:49:21.227
  STEP: updating @ 05/08/23 12:49:21.23
  May  8 12:49:21.236: INFO: waiting for watch events with expected annotations
  May  8 12:49:21.236: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/08/23 12:49:21.236
  STEP: updating /status @ 05/08/23 12:49:21.24
  STEP: get /status @ 05/08/23 12:49:21.246
  STEP: deleting @ 05/08/23 12:49:21.249
  STEP: deleting a collection @ 05/08/23 12:49:21.256
  May  8 12:49:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-4716" for this suite. @ 05/08/23 12:49:21.266
• [0.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/08/23 12:49:21.271
  May  8 12:49:21.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename taint-single-pod @ 05/08/23 12:49:21.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:49:21.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:49:21.282
  May  8 12:49:21.284: INFO: Waiting up to 1m0s for all nodes to be ready
  May  8 12:50:21.296: INFO: Waiting for terminating namespaces to be deleted...
  May  8 12:50:21.298: INFO: Starting informer...
  STEP: Starting pod... @ 05/08/23 12:50:21.298
  May  8 12:50:21.508: INFO: Pod is running on worker-0. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/08/23 12:50:21.508
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/08/23 12:50:21.516
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/08/23 12:50:21.519
  May  8 12:50:21.519: INFO: Pod wasn't evicted. Proceeding
  May  8 12:50:21.519: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/08/23 12:50:21.528
  STEP: Waiting some time to make sure that toleration time passed. @ 05/08/23 12:50:21.532
  May  8 12:51:36.532: INFO: Pod wasn't evicted. Test successful
  May  8 12:51:36.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-3000" for this suite. @ 05/08/23 12:51:36.535
• [135.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/08/23 12:51:36.541
  May  8 12:51:36.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:51:36.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:51:36.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:51:36.553
  STEP: creating service multi-endpoint-test in namespace services-7579 @ 05/08/23 12:51:36.556
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7579 to expose endpoints map[] @ 05/08/23 12:51:36.564
  May  8 12:51:36.567: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  May  8 12:51:37.571: INFO: successfully validated that service multi-endpoint-test in namespace services-7579 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7579 @ 05/08/23 12:51:37.572
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7579 to expose endpoints map[pod1:[100]] @ 05/08/23 12:51:39.585
  May  8 12:51:39.591: INFO: successfully validated that service multi-endpoint-test in namespace services-7579 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-7579 @ 05/08/23 12:51:39.592
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7579 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/08/23 12:51:41.602
  May  8 12:51:41.610: INFO: successfully validated that service multi-endpoint-test in namespace services-7579 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/08/23 12:51:41.611
  May  8 12:51:41.611: INFO: Creating new exec pod
  May  8 12:51:44.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7579 exec execpoddv4d6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May  8 12:51:44.741: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May  8 12:51:44.741: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:51:44.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7579 exec execpoddv4d6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.51.127 80'
  May  8 12:51:44.861: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.51.127 80\nConnection to 10.110.51.127 80 port [tcp/http] succeeded!\n"
  May  8 12:51:44.861: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:51:44.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7579 exec execpoddv4d6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May  8 12:51:44.986: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May  8 12:51:44.986: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:51:44.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-7579 exec execpoddv4d6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.51.127 81'
  May  8 12:51:45.105: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.51.127 81\nConnection to 10.110.51.127 81 port [tcp/*] succeeded!\n"
  May  8 12:51:45.105: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7579 @ 05/08/23 12:51:45.106
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7579 to expose endpoints map[pod2:[101]] @ 05/08/23 12:51:45.115
  May  8 12:51:45.124: INFO: successfully validated that service multi-endpoint-test in namespace services-7579 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-7579 @ 05/08/23 12:51:45.124
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7579 to expose endpoints map[] @ 05/08/23 12:51:45.132
  May  8 12:51:45.139: INFO: successfully validated that service multi-endpoint-test in namespace services-7579 exposes endpoints map[]
  May  8 12:51:45.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7579" for this suite. @ 05/08/23 12:51:45.152
• [8.615 seconds]
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/08/23 12:51:45.156
  May  8 12:51:45.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:51:45.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:51:45.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:51:45.167
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:51:45.169
  STEP: Saw pod success @ 05/08/23 12:51:49.183
  May  8 12:51:49.185: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-70ece5ce-1157-4df4-8bbb-854d319366e1 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:51:49.197
  May  8 12:51:49.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9489" for this suite. @ 05/08/23 12:51:49.209
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/08/23 12:51:49.214
  May  8 12:51:49.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename watch @ 05/08/23 12:51:49.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:51:49.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:51:49.226
  STEP: creating a watch on configmaps with label A @ 05/08/23 12:51:49.229
  STEP: creating a watch on configmaps with label B @ 05/08/23 12:51:49.229
  STEP: creating a watch on configmaps with label A or B @ 05/08/23 12:51:49.23
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/08/23 12:51:49.232
  May  8 12:51:49.235: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1639  01ec991b-c152-4d39-b0d9-93563b55b383 26280 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:51:49.235: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1639  01ec991b-c152-4d39-b0d9-93563b55b383 26280 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/08/23 12:51:49.235
  May  8 12:51:49.240: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1639  01ec991b-c152-4d39-b0d9-93563b55b383 26281 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:51:49.240: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1639  01ec991b-c152-4d39-b0d9-93563b55b383 26281 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/08/23 12:51:49.24
  May  8 12:51:49.244: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1639  01ec991b-c152-4d39-b0d9-93563b55b383 26282 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:51:49.244: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1639  01ec991b-c152-4d39-b0d9-93563b55b383 26282 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/08/23 12:51:49.245
  May  8 12:51:49.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1639  01ec991b-c152-4d39-b0d9-93563b55b383 26283 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:51:49.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1639  01ec991b-c152-4d39-b0d9-93563b55b383 26283 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/08/23 12:51:49.248
  May  8 12:51:49.250: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1639  f1bac4f2-cbdf-4932-b207-52c0c108dc04 26284 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:51:49.250: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1639  f1bac4f2-cbdf-4932-b207-52c0c108dc04 26284 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/08/23 12:51:59.251
  May  8 12:51:59.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1639  f1bac4f2-cbdf-4932-b207-52c0c108dc04 26334 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:51:59.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1639  f1bac4f2-cbdf-4932-b207-52c0c108dc04 26334 0 2023-05-08 12:51:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-08 12:51:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:52:09.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1639" for this suite. @ 05/08/23 12:52:09.263
• [20.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/08/23 12:52:09.268
  May  8 12:52:09.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename namespaces @ 05/08/23 12:52:09.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:09.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:09.279
  STEP: Creating a test namespace @ 05/08/23 12:52:09.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:09.289
  STEP: Creating a service in the namespace @ 05/08/23 12:52:09.291
  STEP: Deleting the namespace @ 05/08/23 12:52:09.3
  STEP: Waiting for the namespace to be removed. @ 05/08/23 12:52:09.305
  STEP: Recreating the namespace @ 05/08/23 12:52:15.307
  STEP: Verifying there is no service in the namespace @ 05/08/23 12:52:15.315
  May  8 12:52:15.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8615" for this suite. @ 05/08/23 12:52:15.32
  STEP: Destroying namespace "nsdeletetest-324" for this suite. @ 05/08/23 12:52:15.323
  May  8 12:52:15.325: INFO: Namespace nsdeletetest-324 was already deleted
  STEP: Destroying namespace "nsdeletetest-8966" for this suite. @ 05/08/23 12:52:15.325
• [6.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/08/23 12:52:15.331
  May  8 12:52:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename namespaces @ 05/08/23 12:52:15.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:15.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:15.342
  STEP: Creating namespace "e2e-ns-276hw" @ 05/08/23 12:52:15.344
  May  8 12:52:15.353: INFO: Namespace "e2e-ns-276hw-4176" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-276hw-4176" @ 05/08/23 12:52:15.353
  May  8 12:52:15.358: INFO: Namespace "e2e-ns-276hw-4176" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-276hw-4176" @ 05/08/23 12:52:15.358
  May  8 12:52:15.363: INFO: Namespace "e2e-ns-276hw-4176" has []v1.FinalizerName{"kubernetes"}
  May  8 12:52:15.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7803" for this suite. @ 05/08/23 12:52:15.365
  STEP: Destroying namespace "e2e-ns-276hw-4176" for this suite. @ 05/08/23 12:52:15.371
• [0.043 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/08/23 12:52:15.374
  May  8 12:52:15.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 12:52:15.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:15.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:15.385
  STEP: creating service in namespace services-3640 @ 05/08/23 12:52:15.387
  STEP: creating service affinity-clusterip-transition in namespace services-3640 @ 05/08/23 12:52:15.387
  STEP: creating replication controller affinity-clusterip-transition in namespace services-3640 @ 05/08/23 12:52:15.396
  I0508 12:52:15.403504      23 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-3640, replica count: 3
  I0508 12:52:18.454944      23 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 12:52:18.459: INFO: Creating new exec pod
  May  8 12:52:21.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-3640 exec execpod-affinityxthsc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May  8 12:52:21.590: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May  8 12:52:21.590: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:52:21.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-3640 exec execpod-affinityxthsc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.52.70 80'
  May  8 12:52:21.725: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.52.70 80\nConnection to 10.107.52.70 80 port [tcp/http] succeeded!\n"
  May  8 12:52:21.725: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  8 12:52:21.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-3640 exec execpod-affinityxthsc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.52.70:80/ ; done'
  May  8 12:52:21.904: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n"
  May  8 12:52:21.904: INFO: stdout: "\naffinity-clusterip-transition-hvqf2\naffinity-clusterip-transition-6hpnn\naffinity-clusterip-transition-hvqf2\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-6hpnn\naffinity-clusterip-transition-hvqf2\naffinity-clusterip-transition-hvqf2\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-hvqf2\naffinity-clusterip-transition-6hpnn\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-6hpnn\naffinity-clusterip-transition-6hpnn\naffinity-clusterip-transition-6hpnn"
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-hvqf2
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-6hpnn
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-hvqf2
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-6hpnn
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-hvqf2
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-hvqf2
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-hvqf2
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-6hpnn
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-6hpnn
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-6hpnn
  May  8 12:52:21.904: INFO: Received response from host: affinity-clusterip-transition-6hpnn
  May  8 12:52:21.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=services-3640 exec execpod-affinityxthsc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.52.70:80/ ; done'
  May  8 12:52:22.081: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.52.70:80/\n"
  May  8 12:52:22.081: INFO: stdout: "\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm\naffinity-clusterip-transition-scpzm"
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.081: INFO: Received response from host: affinity-clusterip-transition-scpzm
  May  8 12:52:22.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:52:22.086: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3640, will wait for the garbage collector to delete the pods @ 05/08/23 12:52:22.093
  May  8 12:52:22.151: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.250736ms
  May  8 12:52:22.251: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.152923ms
  STEP: Destroying namespace "services-3640" for this suite. @ 05/08/23 12:52:23.863
• [8.492 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/08/23 12:52:23.87
  May  8 12:52:23.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 12:52:23.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:23.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:23.882
  STEP: Creating configMap with name configmap-test-volume-6c396447-26da-4332-8cdf-f00ac0d73a44 @ 05/08/23 12:52:23.884
  STEP: Creating a pod to test consume configMaps @ 05/08/23 12:52:23.887
  STEP: Saw pod success @ 05/08/23 12:52:27.902
  May  8 12:52:27.904: INFO: Trying to get logs from node worker-0 pod pod-configmaps-352c02e1-3a6a-4299-b631-324e50523fd5 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 12:52:27.909
  May  8 12:52:27.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6351" for this suite. @ 05/08/23 12:52:27.92
• [4.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/08/23 12:52:27.926
  May  8 12:52:27.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename deployment @ 05/08/23 12:52:27.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:27.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:27.937
  STEP: creating a Deployment @ 05/08/23 12:52:27.941
  STEP: waiting for Deployment to be created @ 05/08/23 12:52:27.945
  STEP: waiting for all Replicas to be Ready @ 05/08/23 12:52:27.946
  May  8 12:52:27.947: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  8 12:52:27.947: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  8 12:52:27.956: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  8 12:52:27.956: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  8 12:52:27.967: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  8 12:52:27.967: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  8 12:52:27.992: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  8 12:52:27.992: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  8 12:52:28.656: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May  8 12:52:28.656: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May  8 12:52:28.838: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/08/23 12:52:28.838
  W0508 12:52:28.848505      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  8 12:52:28.850: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/08/23 12:52:28.85
  May  8 12:52:28.851: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0
  May  8 12:52:28.851: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0
  May  8 12:52:28.851: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0
  May  8 12:52:28.851: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0
  May  8 12:52:28.851: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0
  May  8 12:52:28.851: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0
  May  8 12:52:28.852: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0
  May  8 12:52:28.852: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 0
  May  8 12:52:28.852: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:28.852: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:28.852: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:28.852: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:28.852: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:28.852: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:28.860: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:28.860: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:28.875: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:28.875: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:28.881: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:28.881: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:28.890: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:28.891: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:29.854: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:29.854: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:29.868: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  STEP: listing Deployments @ 05/08/23 12:52:29.868
  May  8 12:52:29.870: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/08/23 12:52:29.87
  May  8 12:52:29.882: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/08/23 12:52:29.882
  May  8 12:52:29.887: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:29.892: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:29.906: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:29.922: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:29.930: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:30.679: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:30.859: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:30.882: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:30.899: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May  8 12:52:31.679: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/08/23 12:52:31.69
  STEP: fetching the DeploymentStatus @ 05/08/23 12:52:31.696
  May  8 12:52:31.699: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:31.699: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:31.699: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:31.700: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:31.700: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 1
  May  8 12:52:31.700: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:31.700: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 3
  May  8 12:52:31.700: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:31.701: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 2
  May  8 12:52:31.701: INFO: observed Deployment test-deployment in namespace deployment-6473 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/08/23 12:52:31.701
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.707: INFO: observed event type MODIFIED
  May  8 12:52:31.709: INFO: Log out all the ReplicaSets if there is no deployment created
  May  8 12:52:31.713: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-6473  5a5a9591-43ed-4aae-96c1-a33e7ada9f5f 26652 3 2023-05-08 12:52:27 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d6ad3153-c792-4819-bae6-f4536130d1f6 0xc004524487 0xc004524488}] [] [{kube-controller-manager Update apps/v1 2023-05-08 12:52:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6ad3153-c792-4819-bae6-f4536130d1f6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:52:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004524560 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May  8 12:52:31.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6473" for this suite. @ 05/08/23 12:52:31.718
• [3.799 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/08/23 12:52:31.727
  May  8 12:52:31.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replication-controller @ 05/08/23 12:52:31.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:31.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:31.737
  STEP: creating a ReplicationController @ 05/08/23 12:52:31.742
  STEP: waiting for RC to be added @ 05/08/23 12:52:31.745
  STEP: waiting for available Replicas @ 05/08/23 12:52:31.746
  STEP: patching ReplicationController @ 05/08/23 12:52:32.855
  STEP: waiting for RC to be modified @ 05/08/23 12:52:32.864
  STEP: patching ReplicationController status @ 05/08/23 12:52:32.864
  STEP: waiting for RC to be modified @ 05/08/23 12:52:32.869
  STEP: waiting for available Replicas @ 05/08/23 12:52:32.869
  STEP: fetching ReplicationController status @ 05/08/23 12:52:32.875
  STEP: patching ReplicationController scale @ 05/08/23 12:52:32.877
  STEP: waiting for RC to be modified @ 05/08/23 12:52:32.884
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/08/23 12:52:32.885
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/08/23 12:52:33.672
  STEP: updating ReplicationController status @ 05/08/23 12:52:33.677
  STEP: waiting for RC to be modified @ 05/08/23 12:52:33.683
  STEP: listing all ReplicationControllers @ 05/08/23 12:52:33.683
  STEP: checking that ReplicationController has expected values @ 05/08/23 12:52:33.687
  STEP: deleting ReplicationControllers by collection @ 05/08/23 12:52:33.687
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/08/23 12:52:33.692
  May  8 12:52:33.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0508 12:52:33.726130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-2062" for this suite. @ 05/08/23 12:52:33.727
• [2.004 seconds]
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/08/23 12:52:33.731
  May  8 12:52:33.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 12:52:33.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:33.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:33.742
  May  8 12:52:33.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: creating the pod @ 05/08/23 12:52:33.744
  STEP: submitting the pod to kubernetes @ 05/08/23 12:52:33.744
  E0508 12:52:34.726814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:35.726985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:52:35.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8241" for this suite. @ 05/08/23 12:52:35.769
• [2.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/08/23 12:52:35.774
  May  8 12:52:35.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename dns @ 05/08/23 12:52:35.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:35.785
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:35.787
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/08/23 12:52:35.79
  May  8 12:52:35.795: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1582  9b850b46-4b4e-4971-93b4-c3964b87a2f8 26830 0 2023-05-08 12:52:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-08 12:52:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94925,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94925,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0508 12:52:36.727159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:37.727571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/08/23 12:52:37.801
  May  8 12:52:37.801: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1582 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:52:37.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:52:37.802: INFO: ExecWithOptions: Clientset creation
  May  8 12:52:37.802: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1582/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/08/23 12:52:37.872
  May  8 12:52:37.872: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1582 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 12:52:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 12:52:37.873: INFO: ExecWithOptions: Clientset creation
  May  8 12:52:37.873: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1582/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  8 12:52:37.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 12:52:37.951: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1582" for this suite. @ 05/08/23 12:52:37.958
• [2.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/08/23 12:52:37.965
  May  8 12:52:37.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename watch @ 05/08/23 12:52:37.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:37.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:37.975
  STEP: creating a watch on configmaps @ 05/08/23 12:52:37.977
  STEP: creating a new configmap @ 05/08/23 12:52:37.978
  STEP: modifying the configmap once @ 05/08/23 12:52:37.982
  STEP: closing the watch once it receives two notifications @ 05/08/23 12:52:37.986
  May  8 12:52:37.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6813  c06c17d1-dc27-4475-ac3e-7104f8619297 26899 0 2023-05-08 12:52:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-08 12:52:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:52:37.986: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6813  c06c17d1-dc27-4475-ac3e-7104f8619297 26900 0 2023-05-08 12:52:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-08 12:52:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/08/23 12:52:37.987
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/08/23 12:52:37.991
  STEP: deleting the configmap @ 05/08/23 12:52:37.992
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/08/23 12:52:37.995
  May  8 12:52:37.995: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6813  c06c17d1-dc27-4475-ac3e-7104f8619297 26901 0 2023-05-08 12:52:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-08 12:52:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:52:37.995: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6813  c06c17d1-dc27-4475-ac3e-7104f8619297 26902 0 2023-05-08 12:52:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-08 12:52:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  8 12:52:37.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6813" for this suite. @ 05/08/23 12:52:37.998
• [0.039 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/08/23 12:52:38.005
  May  8 12:52:38.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replicaset @ 05/08/23 12:52:38.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:38.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:38.015
  May  8 12:52:38.026: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0508 12:52:38.728330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:39.728911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:40.729705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:41.730138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:42.730440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:52:43.030: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/08/23 12:52:43.03
  STEP: Scaling up "test-rs" replicaset  @ 05/08/23 12:52:43.03
  May  8 12:52:43.038: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/08/23 12:52:43.038
  W0508 12:52:43.044772      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  8 12:52:43.046: INFO: observed ReplicaSet test-rs in namespace replicaset-1414 with ReadyReplicas 1, AvailableReplicas 1
  May  8 12:52:43.060: INFO: observed ReplicaSet test-rs in namespace replicaset-1414 with ReadyReplicas 1, AvailableReplicas 1
  May  8 12:52:43.072: INFO: observed ReplicaSet test-rs in namespace replicaset-1414 with ReadyReplicas 1, AvailableReplicas 1
  May  8 12:52:43.079: INFO: observed ReplicaSet test-rs in namespace replicaset-1414 with ReadyReplicas 1, AvailableReplicas 1
  E0508 12:52:43.730479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:52:43.895: INFO: observed ReplicaSet test-rs in namespace replicaset-1414 with ReadyReplicas 2, AvailableReplicas 2
  May  8 12:52:43.903: INFO: observed Replicaset test-rs in namespace replicaset-1414 with ReadyReplicas 3 found true
  May  8 12:52:43.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1414" for this suite. @ 05/08/23 12:52:43.905
• [5.905 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/08/23 12:52:43.913
  May  8 12:52:43.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename cronjob @ 05/08/23 12:52:43.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:52:43.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:52:43.932
  STEP: Creating a ForbidConcurrent cronjob @ 05/08/23 12:52:43.934
  STEP: Ensuring a job is scheduled @ 05/08/23 12:52:43.938
  E0508 12:52:44.731270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:45.731555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:46.732658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:47.732965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:48.733418      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:49.733554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:50.733940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:51.734442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:52.735544      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:53.735685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:54.735891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:55.736202      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:56.736496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:57.736819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:58.737484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:52:59.737678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:00.738572      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:01.738986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/08/23 12:53:01.941
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/08/23 12:53:01.943
  STEP: Ensuring no more jobs are scheduled @ 05/08/23 12:53:01.945
  E0508 12:53:02.738935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:03.739067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:04.740096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:05.740359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:06.740817      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:07.741102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:08.741576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:09.742640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:10.743386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:11.743900      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:12.744551      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:13.744814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:14.745518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:15.745793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:16.745874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:17.746645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:18.746818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:19.747069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:20.747727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:21.747891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:22.748437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:23.748714      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:24.749252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:25.749519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:26.749752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:27.750018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:28.750198      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:29.750472      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:30.751289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:31.751833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:32.752548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:33.752729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:34.753184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:35.753663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:36.753873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:37.754139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:38.754206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:39.754477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:40.754643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:41.755138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:42.755824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:43.756095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:44.756272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:45.756558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:46.756886      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:47.757153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:48.757516      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:49.758645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:50.759714      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:51.759873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:52.760049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:53.760321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:54.760759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:55.761031      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:56.761285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:57.761520      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:58.762627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:53:59.762889      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:00.763171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:01.764251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:02.764914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:03.765195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:04.765356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:05.765660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:06.766646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:07.766917      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:08.767076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:09.767339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:10.767500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:11.767752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:12.768210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:13.768480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:14.768651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:15.768934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:16.769074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:17.769347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:18.770073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:19.770344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:20.770930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:21.771485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:22.772185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:23.772352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:24.772675      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:25.773390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:26.773529      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:27.773794      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:28.774658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:29.774914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:30.775824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:31.776533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:32.777274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:33.777541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:34.778660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:35.778848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:36.778961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:37.779233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:38.779402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:39.779664      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:40.780126      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:41.780284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:42.780901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:43.781168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:44.781333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:45.781531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:46.781619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:47.781889      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:48.782071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:49.783143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:50.783292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:51.783907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:52.784685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:53.784854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:54.785222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:55.785503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:56.785540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:57.785808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:58.786650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:54:59.786908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:00.787671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:01.788137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:02.788053      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:03.788215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:04.788624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:05.788911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:06.789220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:07.789697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:08.790387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:09.790652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:10.791254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:11.791740      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:12.792684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:13.792950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:14.793121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:15.793495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:16.794584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:17.794845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:18.795911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:19.796182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:20.796402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:21.796984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:22.797517      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:23.797698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:24.797904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:25.798501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:26.798600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:27.799093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:28.799846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:29.800122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:30.800391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:31.800932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:32.801519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:33.802657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:34.802905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:35.803445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:36.803652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:37.803906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:38.804077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:39.804359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:40.804528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:41.805062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:42.805519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:43.806653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:44.807068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:45.807808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:46.807979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:47.808140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:48.808755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:49.808927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:50.809452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:51.809533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:52.810641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:53.810908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:54.811073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:55.811942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:56.812580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:57.812849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:58.813518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:55:59.813789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:00.813811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:01.814644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:02.815346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:03.815612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:04.816422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:05.817319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:06.817516      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:07.818645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:08.819464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:09.819734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:10.820223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:11.820487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:12.821507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:13.822653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:14.822939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:15.823326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:16.824136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:17.824399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:18.825126      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:19.825437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:20.826242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:21.826505      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:22.826670      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:23.826934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:24.827414      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:25.828189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:26.828377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:27.828668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:28.828852      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:29.829122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:30.829480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:31.829752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:32.829925      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:33.830192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:34.830375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:35.831158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:36.831708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:37.832305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:38.832496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:39.833104      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:40.833981      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:41.834494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:42.834667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:43.834955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:44.835401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:45.836280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:46.836614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:47.836886      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:48.837062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:49.837331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:50.837918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:51.838176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:52.838817      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:53.839091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:54.839271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:55.839633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:56.840254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:57.840557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:58.841415      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:56:59.841679      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:00.841813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:01.842315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:02.842575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:03.842856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:04.843018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:05.843809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:06.843982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:07.844266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:08.844872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:09.845143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:10.845757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:11.846039      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:12.846766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:13.847042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:14.847422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:15.848219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:16.848844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:17.849023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:18.849891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:19.850064      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:20.850876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:21.851161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:22.851419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:23.851700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:24.852228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:25.852899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:26.853619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:27.853876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:28.854667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:29.854929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:30.855938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:31.856209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:32.856372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:33.857511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:34.857681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:35.858731      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:36.859215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:37.859492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:38.860209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:39.860497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:40.861237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:41.861508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:42.862263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:43.862437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:44.863286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:45.864342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:46.864521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:47.864813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:48.865805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:49.866090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:50.866503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:51.866770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:52.867630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:53.867908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:54.868087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:55.868840      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:56.868996      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:57.869264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:58.869439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:57:59.869738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:00.869818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:01.870466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/08/23 12:58:01.949
  May  8 12:58:01.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2924" for this suite. @ 05/08/23 12:58:01.955
• [318.046 seconds]
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/08/23 12:58:01.959
  May  8 12:58:01.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sysctl @ 05/08/23 12:58:01.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:01.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:01.977
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/08/23 12:58:01.979
  STEP: Watching for error events or started pod @ 05/08/23 12:58:01.984
  E0508 12:58:02.870897      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:03.871201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 05/08/23 12:58:03.988
  E0508 12:58:04.871327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:05.872152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 05/08/23 12:58:05.995
  STEP: Getting logs from the pod @ 05/08/23 12:58:05.995
  STEP: Checking that the sysctl is actually updated @ 05/08/23 12:58:06.008
  May  8 12:58:06.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3031" for this suite. @ 05/08/23 12:58:06.011
• [4.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/08/23 12:58:06.016
  May  8 12:58:06.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename deployment @ 05/08/23 12:58:06.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:06.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:06.028
  May  8 12:58:06.031: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May  8 12:58:06.037: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0508 12:58:06.872331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:07.872802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:08.872990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:09.873300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:10.873845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:58:11.040: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/08/23 12:58:11.04
  May  8 12:58:11.040: INFO: Creating deployment "test-rolling-update-deployment"
  May  8 12:58:11.044: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May  8 12:58:11.048: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0508 12:58:11.874015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:12.874300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:58:13.053: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May  8 12:58:13.054: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May  8 12:58:13.060: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3094  7e36987d-328c-4892-a7a4-da3f2df68089 27815 1 2023-05-08 12:58:11 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-08 12:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006af0578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-08 12:58:11 +0000 UTC,LastTransitionTime:2023-05-08 12:58:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-08 12:58:12 +0000 UTC,LastTransitionTime:2023-05-08 12:58:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  8 12:58:13.061: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-3094  bfa12ca7-f0cd-4956-8e1b-bd1213d08da3 27805 1 2023-05-08 12:58:11 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7e36987d-328c-4892-a7a4-da3f2df68089 0xc006d189a7 0xc006d189a8}] [] [{kube-controller-manager Update apps/v1 2023-05-08 12:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e36987d-328c-4892-a7a4-da3f2df68089\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:58:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d18a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  8 12:58:13.062: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May  8 12:58:13.062: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3094  7e44ea80-387b-430b-9c3b-edb66e67c29a 27814 2 2023-05-08 12:58:06 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7e36987d-328c-4892-a7a4-da3f2df68089 0xc006d18877 0xc006d18878}] [] [{e2e.test Update apps/v1 2023-05-08 12:58:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e36987d-328c-4892-a7a4-da3f2df68089\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-08 12:58:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006d18938 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  8 12:58:13.064: INFO: Pod "test-rolling-update-deployment-656d657cd8-dkwqm" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-dkwqm test-rolling-update-deployment-656d657cd8- deployment-3094  36fc4b5f-c059-4ac2-92b7-bd6bde77ce85 27804 0 2023-05-08 12:58:11 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 bfa12ca7-f0cd-4956-8e1b-bd1213d08da3 0xc006af0967 0xc006af0968}] [] [{kube-controller-manager Update v1 2023-05-08 12:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bfa12ca7-f0cd-4956-8e1b-bd1213d08da3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-08 12:58:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stvwf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stvwf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:58:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-08 12:58:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.39.71,PodIP:10.244.1.148,StartTime:2023-05-08 12:58:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-08 12:58:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://65ed1c9428b05c95cfbfd90c60590bf1237515272ed056e83a3d47f7dc0f429d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.148,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  8 12:58:13.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3094" for this suite. @ 05/08/23 12:58:13.066
• [7.054 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/08/23 12:58:13.07
  May  8 12:58:13.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename endpointslice @ 05/08/23 12:58:13.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:13.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:13.083
  E0508 12:58:13.874931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:14.875216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:58:15.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1155" for this suite. @ 05/08/23 12:58:15.12
• [2.057 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/08/23 12:58:15.127
  May  8 12:58:15.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubelet-test @ 05/08/23 12:58:15.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:15.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:15.137
  E0508 12:58:15.876352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:16.876524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:17.876820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:18.877132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:58:19.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1434" for this suite. @ 05/08/23 12:58:19.153
• [4.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/08/23 12:58:19.157
  May  8 12:58:19.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename watch @ 05/08/23 12:58:19.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:19.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:19.168
  STEP: getting a starting resourceVersion @ 05/08/23 12:58:19.17
  STEP: starting a background goroutine to produce watch events @ 05/08/23 12:58:19.172
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/08/23 12:58:19.172
  E0508 12:58:19.877319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:20.877692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:21.878577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:58:21.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5314" for this suite. @ 05/08/23 12:58:22.012
• [2.906 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/08/23 12:58:22.064
  May  8 12:58:22.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 12:58:22.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:22.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:22.075
  STEP: Creating a pod to test downward api env vars @ 05/08/23 12:58:22.078
  E0508 12:58:22.878787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:23.878954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:24.879773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:25.880347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 12:58:26.091
  May  8 12:58:26.092: INFO: Trying to get logs from node worker-0 pod downward-api-67765cd0-9486-4d7c-a0f1-dc0f9c9c2bd4 container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 12:58:26.098
  May  8 12:58:26.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8722" for this suite. @ 05/08/23 12:58:26.11
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/08/23 12:58:26.12
  May  8 12:58:26.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename svcaccounts @ 05/08/23 12:58:26.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:26.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:26.132
  E0508 12:58:26.881317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:27.881559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/08/23 12:58:28.148
  May  8 12:58:28.148: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1054 pod-service-account-fe53bdf7-f152-4b22-a28c-b05b6e1202ab -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/08/23 12:58:28.269
  May  8 12:58:28.269: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1054 pod-service-account-fe53bdf7-f152-4b22-a28c-b05b6e1202ab -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/08/23 12:58:28.397
  May  8 12:58:28.397: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1054 pod-service-account-fe53bdf7-f152-4b22-a28c-b05b6e1202ab -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May  8 12:58:28.515: INFO: Got root ca configmap in namespace "svcaccounts-1054"
  May  8 12:58:28.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1054" for this suite. @ 05/08/23 12:58:28.519
• [2.403 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/08/23 12:58:28.524
  May  8 12:58:28.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 12:58:28.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:28.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:28.534
  STEP: validating api versions @ 05/08/23 12:58:28.536
  May  8 12:58:28.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-4147 api-versions'
  May  8 12:58:28.599: INFO: stderr: ""
  May  8 12:58:28.599: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautopilot.k0sproject.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.k0sproject.io/v1beta1\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  May  8 12:58:28.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4147" for this suite. @ 05/08/23 12:58:28.603
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/08/23 12:58:28.609
  May  8 12:58:28.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-watch @ 05/08/23 12:58:28.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:58:28.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:58:28.62
  May  8 12:58:28.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  E0508 12:58:28.882017      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:29.882499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:30.883578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 05/08/23 12:58:31.151
  May  8 12:58:31.157: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-08T12:58:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-08T12:58:31Z]] name:name1 resourceVersion:28078 uid:1d5d29bc-3676-4716-b8c4-527d8c441b77] num:map[num1:9223372036854775807 num2:1000000]]}
  E0508 12:58:31.883941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:32.884700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:33.884963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:34.885246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:35.886267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:36.886595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:37.886890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:38.887178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:39.887473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:40.888049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 05/08/23 12:58:41.158
  May  8 12:58:41.163: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-08T12:58:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-08T12:58:41Z]] name:name2 resourceVersion:28117 uid:acf3c1bb-b0b5-4602-a07d-3d30f6ae1bcc] num:map[num1:9223372036854775807 num2:1000000]]}
  E0508 12:58:41.889060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:42.889346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:43.889635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:44.889808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:45.890752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:46.890913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:47.891194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:48.891463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:49.891720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:50.891854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 05/08/23 12:58:51.163
  May  8 12:58:51.168: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-08T12:58:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-08T12:58:51Z]] name:name1 resourceVersion:28138 uid:1d5d29bc-3676-4716-b8c4-527d8c441b77] num:map[num1:9223372036854775807 num2:1000000]]}
  E0508 12:58:51.892281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:52.892418      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:53.892709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:54.892862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:55.893674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:56.894638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:57.894931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:58.895220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:58:59.895476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:00.896023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 05/08/23 12:59:01.169
  May  8 12:59:01.174: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-08T12:58:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-08T12:59:01Z]] name:name2 resourceVersion:28159 uid:acf3c1bb-b0b5-4602-a07d-3d30f6ae1bcc] num:map[num1:9223372036854775807 num2:1000000]]}
  E0508 12:59:01.896546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:02.896435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:03.896934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:04.897226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:05.897285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:06.897584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:07.898643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:08.898913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:09.899177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:10.899915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 05/08/23 12:59:11.175
  May  8 12:59:11.180: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-08T12:58:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-08T12:58:51Z]] name:name1 resourceVersion:28180 uid:1d5d29bc-3676-4716-b8c4-527d8c441b77] num:map[num1:9223372036854775807 num2:1000000]]}
  E0508 12:59:11.900093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:12.900364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:13.901042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:14.901125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:15.901902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:16.902107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:17.902662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:18.902930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:19.903108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:20.903995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 05/08/23 12:59:21.181
  May  8 12:59:21.186: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-08T12:58:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-08T12:59:01Z]] name:name2 resourceVersion:28201 uid:acf3c1bb-b0b5-4602-a07d-3d30f6ae1bcc] num:map[num1:9223372036854775807 num2:1000000]]}
  E0508 12:59:21.904436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:22.904709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:23.904820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:24.905093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:25.906122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:26.906379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:27.906658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:28.906821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:29.907087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:30.908021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 12:59:31.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-7129" for this suite. @ 05/08/23 12:59:31.7
• [63.096 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/08/23 12:59:31.706
  May  8 12:59:31.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 12:59:31.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:59:31.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:59:31.717
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 12:59:31.72
  E0508 12:59:31.908842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:32.909935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:33.910657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:34.910944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 12:59:35.734
  May  8 12:59:35.736: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-5bbb23a6-f08a-40d1-9671-ec38545e254e container client-container: <nil>
  STEP: delete the pod @ 05/08/23 12:59:35.74
  May  8 12:59:35.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1107" for this suite. @ 05/08/23 12:59:35.752
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/08/23 12:59:35.759
  May  8 12:59:35.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-preemption @ 05/08/23 12:59:35.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 12:59:35.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 12:59:35.769
  May  8 12:59:35.781: INFO: Waiting up to 1m0s for all nodes to be ready
  E0508 12:59:35.911874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:36.912184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:37.912264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:38.912527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:39.912640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:40.913175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:41.914212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:42.914518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:43.915411      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:44.915713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:45.915875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:46.916171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:47.916238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:48.916527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:49.917292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:50.918038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:51.918442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:52.918714      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:53.919496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:54.919775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:55.920189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:56.920463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:57.921263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:58.921587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 12:59:59.922308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:00.922458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:01.923582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:02.923745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:03.923818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:04.924106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:05.924240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:06.924509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:07.925506      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:08.925803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:09.925896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:10.925955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:11.926408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:12.926581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:13.926779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:14.927084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:15.927290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:16.927576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:17.928357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:18.929023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:19.929472      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:20.930019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:21.930824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:22.931121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:23.931289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:24.931601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:25.931745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:26.932054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:27.932984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:28.933268      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:29.934228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:30.934561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:31.935428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:32.935717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:33.936217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:34.936524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:00:35.796: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/08/23 13:00:35.798
  May  8 13:00:35.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/08/23 13:00:35.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:00:35.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:00:35.81
  May  8 13:00:35.821: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May  8 13:00:35.823: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May  8 13:00:35.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  8 13:00:35.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-7114" for this suite. @ 05/08/23 13:00:35.87
  STEP: Destroying namespace "sched-preemption-7827" for this suite. @ 05/08/23 13:00:35.873
• [60.119 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/08/23 13:00:35.879
  May  8 13:00:35.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename disruption @ 05/08/23 13:00:35.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:00:35.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:00:35.889
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/08/23 13:00:35.891
  STEP: Waiting for the pdb to be processed @ 05/08/23 13:00:35.894
  E0508 13:00:35.937014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:36.937519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/08/23 13:00:37.905
  STEP: Waiting for all pods to be running @ 05/08/23 13:00:37.905
  May  8 13:00:37.907: INFO: pods: 0 < 3
  E0508 13:00:37.937726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:38.937880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/08/23 13:00:39.911
  STEP: Updating the pdb to allow a pod to be evicted @ 05/08/23 13:00:39.916
  STEP: Waiting for the pdb to be processed @ 05/08/23 13:00:39.921
  E0508 13:00:39.938445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:40.939576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/08/23 13:00:41.926
  STEP: Waiting for all pods to be running @ 05/08/23 13:00:41.926
  STEP: Waiting for the pdb to observed all healthy pods @ 05/08/23 13:00:41.928
  E0508 13:00:41.940121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/08/23 13:00:41.944
  STEP: Waiting for the pdb to be processed @ 05/08/23 13:00:41.956
  E0508 13:00:42.940967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:43.941110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 05/08/23 13:00:43.964
  STEP: locating a running pod @ 05/08/23 13:00:43.966
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/08/23 13:00:43.972
  STEP: Waiting for the pdb to be deleted @ 05/08/23 13:00:43.975
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/08/23 13:00:43.976
  STEP: Waiting for all pods to be running @ 05/08/23 13:00:43.976
  May  8 13:00:43.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1906" for this suite. @ 05/08/23 13:00:43.99
• [8.118 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/08/23 13:00:43.997
  May  8 13:00:43.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 13:00:43.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:00:44.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:00:44.012
  STEP: Creating the pod @ 05/08/23 13:00:44.014
  E0508 13:00:44.941184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:45.942067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:00:46.543: INFO: Successfully updated pod "annotationupdated5dc1bdd-31b5-47ef-a32d-6f534a7acf06"
  E0508 13:00:46.942515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:47.942845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:00:48.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4697" for this suite. @ 05/08/23 13:00:48.559
• [4.566 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/08/23 13:00:48.565
  May  8 13:00:48.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 13:00:48.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:00:48.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:00:48.576
  STEP: Creating secret with name secret-test-map-5bf843d5-84e3-4847-94f2-18c296528f33 @ 05/08/23 13:00:48.578
  STEP: Creating a pod to test consume secrets @ 05/08/23 13:00:48.581
  E0508 13:00:48.943764      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:49.944121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:50.944547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:51.944581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:00:52.594
  May  8 13:00:52.596: INFO: Trying to get logs from node worker-1 pod pod-secrets-4d0b8f3a-c229-4d24-a77c-28ed2a175ee5 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 13:00:52.61
  May  8 13:00:52.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6335" for this suite. @ 05/08/23 13:00:52.622
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/08/23 13:00:52.627
  May  8 13:00:52.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 13:00:52.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:00:52.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:00:52.639
  STEP: Creating a pod to test downward api env vars @ 05/08/23 13:00:52.642
  E0508 13:00:52.944758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:53.945042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:54.945668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:55.946491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:00:56.659
  May  8 13:00:56.661: INFO: Trying to get logs from node worker-1 pod downward-api-7175362e-033c-4c6a-8684-ac4382978071 container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 13:00:56.667
  May  8 13:00:56.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7635" for this suite. @ 05/08/23 13:00:56.678
• [4.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/08/23 13:00:56.684
  May  8 13:00:56.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename endpointslice @ 05/08/23 13:00:56.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:00:56.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:00:56.694
  May  8 13:00:56.702: INFO: Endpoints addresses: [10.0.39.11] , ports: [6443]
  May  8 13:00:56.702: INFO: EndpointSlices addresses: [10.0.39.11] , ports: [6443]
  May  8 13:00:56.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8662" for this suite. @ 05/08/23 13:00:56.704
• [0.024 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/08/23 13:00:56.708
  May  8 13:00:56.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 13:00:56.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:00:56.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:00:56.719
  STEP: Creating secret with name secret-test-54e34d93-ab53-4198-973d-8d301f633181 @ 05/08/23 13:00:56.721
  STEP: Creating a pod to test consume secrets @ 05/08/23 13:00:56.724
  E0508 13:00:56.947461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:57.947757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:58.948369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:00:59.948652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:01:00.736
  May  8 13:01:00.738: INFO: Trying to get logs from node worker-0 pod pod-secrets-c0594320-b2cc-40af-9e6e-fdcbdfe6a69d container secret-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 13:01:00.743
  May  8 13:01:00.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6348" for this suite. @ 05/08/23 13:01:00.754
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/08/23 13:01:00.759
  May  8 13:01:00.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename ingressclass @ 05/08/23 13:01:00.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:00.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:00.771
  STEP: getting /apis @ 05/08/23 13:01:00.773
  STEP: getting /apis/networking.k8s.io @ 05/08/23 13:01:00.776
  STEP: getting /apis/networking.k8s.iov1 @ 05/08/23 13:01:00.777
  STEP: creating @ 05/08/23 13:01:00.778
  STEP: getting @ 05/08/23 13:01:00.787
  STEP: listing @ 05/08/23 13:01:00.789
  STEP: watching @ 05/08/23 13:01:00.79
  May  8 13:01:00.790: INFO: starting watch
  STEP: patching @ 05/08/23 13:01:00.791
  STEP: updating @ 05/08/23 13:01:00.794
  May  8 13:01:00.799: INFO: waiting for watch events with expected annotations
  May  8 13:01:00.799: INFO: saw patched and updated annotations
  STEP: deleting @ 05/08/23 13:01:00.799
  STEP: deleting a collection @ 05/08/23 13:01:00.807
  May  8 13:01:00.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-9156" for this suite. @ 05/08/23 13:01:00.817
• [0.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/08/23 13:01:00.822
  May  8 13:01:00.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 13:01:00.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:00.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:00.832
  STEP: Creating configMap configmap-4985/configmap-test-a205132a-699b-49d6-920c-30eda33110fb @ 05/08/23 13:01:00.835
  STEP: Creating a pod to test consume configMaps @ 05/08/23 13:01:00.838
  E0508 13:01:00.949528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:01.950004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:02.950541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:03.950820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:01:04.85
  May  8 13:01:04.852: INFO: Trying to get logs from node worker-0 pod pod-configmaps-d8052525-15c2-464c-b456-8a64e9acc9f0 container env-test: <nil>
  STEP: delete the pod @ 05/08/23 13:01:04.857
  May  8 13:01:04.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4985" for this suite. @ 05/08/23 13:01:04.868
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/08/23 13:01:04.872
  May  8 13:01:04.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename init-container @ 05/08/23 13:01:04.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:04.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:04.883
  STEP: creating the pod @ 05/08/23 13:01:04.885
  May  8 13:01:04.885: INFO: PodSpec: initContainers in spec.initContainers
  E0508 13:01:04.951798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:05.952953      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:06.953267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:07.954204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:08.954367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:01:09.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9874" for this suite. @ 05/08/23 13:01:09.042
• [4.173 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/08/23 13:01:09.046
  May  8 13:01:09.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename certificates @ 05/08/23 13:01:09.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:09.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:09.058
  E0508 13:01:09.955356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 05/08/23 13:01:10.199
  STEP: getting /apis/certificates.k8s.io @ 05/08/23 13:01:10.202
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/08/23 13:01:10.203
  STEP: creating @ 05/08/23 13:01:10.204
  STEP: getting @ 05/08/23 13:01:10.218
  STEP: listing @ 05/08/23 13:01:10.219
  STEP: watching @ 05/08/23 13:01:10.221
  May  8 13:01:10.221: INFO: starting watch
  STEP: patching @ 05/08/23 13:01:10.222
  STEP: updating @ 05/08/23 13:01:10.226
  May  8 13:01:10.230: INFO: waiting for watch events with expected annotations
  May  8 13:01:10.230: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/08/23 13:01:10.231
  STEP: patching /approval @ 05/08/23 13:01:10.233
  STEP: updating /approval @ 05/08/23 13:01:10.236
  STEP: getting /status @ 05/08/23 13:01:10.241
  STEP: patching /status @ 05/08/23 13:01:10.243
  STEP: updating /status @ 05/08/23 13:01:10.249
  STEP: deleting @ 05/08/23 13:01:10.255
  STEP: deleting a collection @ 05/08/23 13:01:10.261
  May  8 13:01:10.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-1588" for this suite. @ 05/08/23 13:01:10.273
• [1.230 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/08/23 13:01:10.277
  May  8 13:01:10.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename runtimeclass @ 05/08/23 13:01:10.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:10.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:10.288
  STEP: Deleting RuntimeClass runtimeclass-8568-delete-me @ 05/08/23 13:01:10.293
  STEP: Waiting for the RuntimeClass to disappear @ 05/08/23 13:01:10.296
  May  8 13:01:10.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8568" for this suite. @ 05/08/23 13:01:10.304
• [0.031 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/08/23 13:01:10.308
  May  8 13:01:10.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 13:01:10.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:10.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:10.321
  STEP: Setting up server cert @ 05/08/23 13:01:10.333
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 13:01:10.769
  STEP: Deploying the webhook pod @ 05/08/23 13:01:10.775
  STEP: Wait for the deployment to be ready @ 05/08/23 13:01:10.783
  May  8 13:01:10.786: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0508 13:01:10.955749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:11.956580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/08/23 13:01:12.793
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 13:01:12.801
  E0508 13:01:12.956719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:01:13.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/08/23 13:01:13.804
  STEP: create a configmap that should be updated by the webhook @ 05/08/23 13:01:13.82
  May  8 13:01:13.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3464" for this suite. @ 05/08/23 13:01:13.87
  STEP: Destroying namespace "webhook-markers-3072" for this suite. @ 05/08/23 13:01:13.874
• [3.569 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/08/23 13:01:13.879
  May  8 13:01:13.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename endpointslice @ 05/08/23 13:01:13.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:13.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:13.892
  E0508 13:01:13.957350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:14.957482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:15.958431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:16.958594      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:17.958783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 05/08/23 13:01:18.948
  E0508 13:01:18.959457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:19.959749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:20.959951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:21.960292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:22.960599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 05/08/23 13:01:23.953
  E0508 13:01:23.960616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:24.960828      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:25.961487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:26.961994      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:27.962277      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/08/23 13:01:28.958
  E0508 13:01:28.962591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:29.962857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:30.963503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:31.963822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:32.964091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 05/08/23 13:01:33.963
  E0508 13:01:33.964521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:01:33.975: INFO: EndpointSlice for Service endpointslice-6892/example-named-port not found
  E0508 13:01:34.965264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:35.966034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:36.966347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:37.966627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:38.966810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:39.967263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:40.968051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:41.968367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:42.968546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:43.968857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:01:43.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6892" for this suite. @ 05/08/23 13:01:43.984
• [30.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/08/23 13:01:43.99
  May  8 13:01:43.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replication-controller @ 05/08/23 13:01:43.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:43.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:44.001
  May  8 13:01:44.003: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0508 13:01:44.969023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/08/23 13:01:45.011
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/08/23 13:01:45.015
  E0508 13:01:45.969518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/08/23 13:01:46.02
  May  8 13:01:46.026: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/08/23 13:01:46.026
  E0508 13:01:46.969704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:01:47.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4498" for this suite. @ 05/08/23 13:01:47.034
• [3.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/08/23 13:01:47.041
  May  8 13:01:47.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-probe @ 05/08/23 13:01:47.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:01:47.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:01:47.052
  STEP: Creating pod liveness-07720ef2-7046-4124-bfb5-af0fcc81638b in namespace container-probe-9384 @ 05/08/23 13:01:47.054
  E0508 13:01:47.970369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:48.970548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:01:49.072: INFO: Started pod liveness-07720ef2-7046-4124-bfb5-af0fcc81638b in namespace container-probe-9384
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/08/23 13:01:49.072
  May  8 13:01:49.075: INFO: Initial restart count of pod liveness-07720ef2-7046-4124-bfb5-af0fcc81638b is 0
  E0508 13:01:49.971153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:50.971610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:51.972542      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:52.972822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:53.973634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:54.973924      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:55.974528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:56.974676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:57.975337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:58.975622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:01:59.975896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:00.976561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:01.977521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:02.977826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:03.978649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:04.978965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:05.979728      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:06.980029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:07.980173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:08.980347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:02:09.109: INFO: Restart count of pod container-probe-9384/liveness-07720ef2-7046-4124-bfb5-af0fcc81638b is now 1 (20.034727532s elapsed)
  E0508 13:02:09.980870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:10.981541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:11.982654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:12.982951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:13.983579      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:14.983896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:15.984349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:16.984494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:17.985130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:18.985279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:19.985427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:20.985987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:21.986774      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:22.987085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:23.987234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:24.987409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:25.987897      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:26.988071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:27.988798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:28.988943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:02:29.141: INFO: Restart count of pod container-probe-9384/liveness-07720ef2-7046-4124-bfb5-af0fcc81638b is now 2 (40.066023952s elapsed)
  E0508 13:02:29.989245      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:30.990076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:31.990655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:32.990800      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:33.991468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:34.991618      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:35.992103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:36.992402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:37.993201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:38.993493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:39.993893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:40.993970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:41.994574      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:42.994870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:43.994979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:44.995271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:45.995434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:46.995763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:47.996577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:48.996859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:02:49.172: INFO: Restart count of pod container-probe-9384/liveness-07720ef2-7046-4124-bfb5-af0fcc81638b is now 3 (1m0.096999506s elapsed)
  E0508 13:02:49.997037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:50.998162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:51.998333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:52.998617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:53.999225      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:54.999361      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:56.000484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:57.000759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:58.001407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:02:59.001697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:00.001976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:01.001992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:02.002437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:03.002755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:04.003822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:05.004014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:06.004235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:07.004545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:08.005322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:09.005609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:03:09.202: INFO: Restart count of pod container-probe-9384/liveness-07720ef2-7046-4124-bfb5-af0fcc81638b is now 4 (1m20.126755509s elapsed)
  E0508 13:03:10.005846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:11.005977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:12.006879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:13.007150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:14.007982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:15.008154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:16.008270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:17.008538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:18.009659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:19.009827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:20.010000      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:21.011100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:22.011748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:23.012448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:24.013115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:25.013389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:26.013815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:27.014644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:28.015518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:29.015815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:30.016423      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:31.016940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:32.017966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:33.018231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:34.018773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:35.018945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:36.019172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:37.019439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:38.020464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:39.020740      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:40.020921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:41.021009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:42.021177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:43.021503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:44.022289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:45.022653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:46.023506      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:47.024330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:48.025121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:49.025418      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:50.025843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:51.026350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:52.027044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:53.027402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:54.028084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:55.028270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:56.029095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:57.029776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:58.030785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:03:59.031069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:00.031528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:01.032301      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:02.032762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:03.033059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:04.033223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:05.033707      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:06.033927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:07.034200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:08.034793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:09.035060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:10.035488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:11.036005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:12.037101      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:13.037279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:14.037449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:15.037626      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:16.038549      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:17.038817      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:18.039619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:19.039908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:04:19.313: INFO: Restart count of pod container-probe-9384/liveness-07720ef2-7046-4124-bfb5-af0fcc81638b is now 5 (2m30.238380135s elapsed)
  May  8 13:04:19.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 13:04:19.315
  STEP: Destroying namespace "container-probe-9384" for this suite. @ 05/08/23 13:04:19.322
• [152.285 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/08/23 13:04:19.327
  May  8 13:04:19.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename events @ 05/08/23 13:04:19.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:04:19.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:04:19.34
  STEP: creating a test event @ 05/08/23 13:04:19.342
  STEP: listing all events in all namespaces @ 05/08/23 13:04:19.347
  STEP: patching the test event @ 05/08/23 13:04:19.349
  STEP: fetching the test event @ 05/08/23 13:04:19.353
  STEP: updating the test event @ 05/08/23 13:04:19.355
  STEP: getting the test event @ 05/08/23 13:04:19.36
  STEP: deleting the test event @ 05/08/23 13:04:19.361
  STEP: listing all events in all namespaces @ 05/08/23 13:04:19.365
  May  8 13:04:19.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9141" for this suite. @ 05/08/23 13:04:19.369
• [0.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/08/23 13:04:19.375
  May  8 13:04:19.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename gc @ 05/08/23 13:04:19.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:04:19.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:04:19.388
  May  8 13:04:19.416: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"03308632-9b38-4f62-818a-b27dd6de94b5", Controller:(*bool)(0xc004a1d416), BlockOwnerDeletion:(*bool)(0xc004a1d417)}}
  May  8 13:04:19.422: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"546a6344-689a-46b2-968b-61401525418a", Controller:(*bool)(0xc004f6e0e6), BlockOwnerDeletion:(*bool)(0xc004f6e0e7)}}
  May  8 13:04:19.429: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d2db30be-8b3d-4dec-bfb0-6c8e89a73fc5", Controller:(*bool)(0xc004bf5d86), BlockOwnerDeletion:(*bool)(0xc004bf5d87)}}
  E0508 13:04:20.040248      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:21.040615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:22.040910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:23.041185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:24.041495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:04:24.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2390" for this suite. @ 05/08/23 13:04:24.439
• [5.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/08/23 13:04:24.445
  May  8 13:04:24.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename security-context @ 05/08/23 13:04:24.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:04:24.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:04:24.459
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/08/23 13:04:24.462
  E0508 13:04:25.041627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:26.042522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:04:26.476
  May  8 13:04:26.478: INFO: Trying to get logs from node worker-0 pod security-context-019c60f4-aa61-4d23-9d7f-45471c033a55 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 13:04:26.491
  May  8 13:04:26.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-1975" for this suite. @ 05/08/23 13:04:26.501
• [2.059 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/08/23 13:04:26.504
  May  8 13:04:26.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename subpath @ 05/08/23 13:04:26.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:04:26.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:04:26.515
  STEP: Setting up data @ 05/08/23 13:04:26.517
  STEP: Creating pod pod-subpath-test-configmap-z7gg @ 05/08/23 13:04:26.522
  STEP: Creating a pod to test atomic-volume-subpath @ 05/08/23 13:04:26.522
  E0508 13:04:27.043266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:28.043572      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:29.043632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:30.043927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:31.044681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:32.044769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:33.045687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:34.045818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:35.046296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:36.047079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:37.048030      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:38.048338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:39.049280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:40.049602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:41.050160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:42.050451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:43.051430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:44.051738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:45.052841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:46.053678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:47.054480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:48.054784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:04:48.566
  May  8 13:04:48.567: INFO: Trying to get logs from node worker-0 pod pod-subpath-test-configmap-z7gg container test-container-subpath-configmap-z7gg: <nil>
  STEP: delete the pod @ 05/08/23 13:04:48.573
  STEP: Deleting pod pod-subpath-test-configmap-z7gg @ 05/08/23 13:04:48.58
  May  8 13:04:48.580: INFO: Deleting pod "pod-subpath-test-configmap-z7gg" in namespace "subpath-8324"
  May  8 13:04:48.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8324" for this suite. @ 05/08/23 13:04:48.584
• [22.085 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/08/23 13:04:48.59
  May  8 13:04:48.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 13:04:48.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:04:48.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:04:48.6
  STEP: Creating configMap with name configmap-test-volume-map-f2c862a9-ed3d-4035-be49-655d861360b1 @ 05/08/23 13:04:48.602
  STEP: Creating a pod to test consume configMaps @ 05/08/23 13:04:48.607
  E0508 13:04:49.055501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:50.055800      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:04:50.617
  May  8 13:04:50.619: INFO: Trying to get logs from node worker-0 pod pod-configmaps-0b5b7e53-fee2-4864-abe9-94e974ac24c0 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 13:04:50.624
  May  8 13:04:50.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7932" for this suite. @ 05/08/23 13:04:50.636
• [2.050 seconds]
------------------------------
S
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/08/23 13:04:50.64
  May  8 13:04:50.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename prestop @ 05/08/23 13:04:50.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:04:50.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:04:50.651
  STEP: Creating server pod server in namespace prestop-7920 @ 05/08/23 13:04:50.653
  STEP: Waiting for pods to come up. @ 05/08/23 13:04:50.659
  E0508 13:04:51.056018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:52.056716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-7920 @ 05/08/23 13:04:52.665
  E0508 13:04:53.057825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:54.058638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 05/08/23 13:04:54.673
  E0508 13:04:55.059576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:56.060522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:57.060823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:58.060980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:04:59.061272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:04:59.684: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May  8 13:04:59.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/08/23 13:04:59.686
  STEP: Destroying namespace "prestop-7920" for this suite. @ 05/08/23 13:04:59.694
• [9.057 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/08/23 13:04:59.698
  May  8 13:04:59.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 13:04:59.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:04:59.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:04:59.709
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/08/23 13:04:59.712
  May  8 13:04:59.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-327 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May  8 13:04:59.779: INFO: stderr: ""
  May  8 13:04:59.779: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/08/23 13:04:59.779
  E0508 13:05:00.061810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:01.062054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:02.062471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:03.062563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:04.062644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/08/23 13:05:04.831
  May  8 13:05:04.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-327 get pod e2e-test-httpd-pod -o json'
  May  8 13:05:04.890: INFO: stderr: ""
  May  8 13:05:04.890: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-05-08T13:04:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-327\",\n        \"resourceVersion\": \"29732\",\n        \"uid\": \"2f586071-75a3-4cb6-a7ab-6b6f157b1f70\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-bn56r\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-0\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-bn56r\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-08T13:04:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-08T13:05:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-08T13:05:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-08T13:04:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ca1708cc9f8bf6ef0848987dc9906ca670a31cd4c80d265a4d43732f8bba7849\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-08T13:05:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.39.71\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.170\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.170\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-08T13:04:59Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/08/23 13:05:04.89
  May  8 13:05:04.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-327 replace -f -'
  E0508 13:05:05.063148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:05:05.214: INFO: stderr: ""
  May  8 13:05:05.214: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/08/23 13:05:05.214
  May  8 13:05:05.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-327 delete pods e2e-test-httpd-pod'
  E0508 13:05:06.064154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:05:06.450: INFO: stderr: ""
  May  8 13:05:06.450: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  8 13:05:06.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-327" for this suite. @ 05/08/23 13:05:06.453
• [6.759 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/08/23 13:05:06.457
  May  8 13:05:06.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 13:05:06.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:05:06.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:05:06.469
  STEP: Creating a ResourceQuota @ 05/08/23 13:05:06.471
  STEP: Getting a ResourceQuota @ 05/08/23 13:05:06.474
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/08/23 13:05:06.476
  STEP: Patching the ResourceQuota @ 05/08/23 13:05:06.478
  STEP: Deleting a Collection of ResourceQuotas @ 05/08/23 13:05:06.483
  STEP: Verifying the deleted ResourceQuota @ 05/08/23 13:05:06.489
  May  8 13:05:06.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6177" for this suite. @ 05/08/23 13:05:06.493
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/08/23 13:05:06.498
  May  8 13:05:06.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename field-validation @ 05/08/23 13:05:06.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:05:06.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:05:06.51
  May  8 13:05:06.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  E0508 13:05:07.064244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:08.064600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0508 13:05:09.048242      23 warnings.go:70] unknown field "alpha"
  W0508 13:05:09.048267      23 warnings.go:70] unknown field "beta"
  W0508 13:05:09.048273      23 warnings.go:70] unknown field "delta"
  W0508 13:05:09.048278      23 warnings.go:70] unknown field "epsilon"
  W0508 13:05:09.048284      23 warnings.go:70] unknown field "gamma"
  May  8 13:05:09.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0508 13:05:09.065085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "field-validation-7618" for this suite. @ 05/08/23 13:05:09.065
• [2.570 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/08/23 13:05:09.07
  May  8 13:05:09.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replication-controller @ 05/08/23 13:05:09.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:05:09.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:05:09.081
  STEP: Given a ReplicationController is created @ 05/08/23 13:05:09.083
  STEP: When the matched label of one of its pods change @ 05/08/23 13:05:09.086
  May  8 13:05:09.088: INFO: Pod name pod-release: Found 0 pods out of 1
  E0508 13:05:10.065807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:11.066638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:12.066845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:13.067046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:14.067734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:05:14.092: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/08/23 13:05:14.102
  E0508 13:05:15.067921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:05:15.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6945" for this suite. @ 05/08/23 13:05:15.112
• [6.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/08/23 13:05:15.119
  May  8 13:05:15.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/08/23 13:05:15.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:05:15.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:05:15.132
  STEP: fetching the /apis discovery document @ 05/08/23 13:05:15.135
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/08/23 13:05:15.136
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/08/23 13:05:15.136
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/08/23 13:05:15.136
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/08/23 13:05:15.138
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/08/23 13:05:15.138
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/08/23 13:05:15.139
  May  8 13:05:15.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5457" for this suite. @ 05/08/23 13:05:15.143
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/08/23 13:05:15.151
  May  8 13:05:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 13:05:15.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:05:15.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:05:15.177
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/08/23 13:05:15.18
  E0508 13:05:16.068583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:17.069086      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:18.069976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:19.070140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:05:19.199
  May  8 13:05:19.200: INFO: Trying to get logs from node worker-1 pod pod-180d8d5a-9a6a-4507-811f-1e24694995fa container test-container: <nil>
  STEP: delete the pod @ 05/08/23 13:05:19.213
  May  8 13:05:19.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2539" for this suite. @ 05/08/23 13:05:19.224
• [4.077 seconds]
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/08/23 13:05:19.228
  May  8 13:05:19.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename hostport @ 05/08/23 13:05:19.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:05:19.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:05:19.24
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/08/23 13:05:19.244
  E0508 13:05:20.070300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:21.071054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:22.071538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:23.071698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:24.071945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:25.072233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:26.072458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:27.072756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:28.073585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:29.073884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:30.074708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:31.075279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.53.117 on the node which pod1 resides and expect scheduled @ 05/08/23 13:05:31.271
  E0508 13:05:32.076084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:33.076379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.53.117 but use UDP protocol on the node which pod2 resides @ 05/08/23 13:05:33.281
  E0508 13:05:34.076794      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:35.077080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:36.078015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:37.078320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:38.078599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:39.078770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:40.079566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:41.080166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:42.081175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:43.081410      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:44.081417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:45.081557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:46.082079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:47.082746      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/08/23 13:05:47.317
  May  8 13:05:47.317: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.53.117 http://127.0.0.1:54323/hostname] Namespace:hostport-8223 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 13:05:47.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 13:05:47.318: INFO: ExecWithOptions: Clientset creation
  May  8 13:05:47.318: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8223/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.53.117+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.53.117, port: 54323 @ 05/08/23 13:05:47.39
  May  8 13:05:47.390: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.53.117:54323/hostname] Namespace:hostport-8223 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 13:05:47.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 13:05:47.391: INFO: ExecWithOptions: Clientset creation
  May  8 13:05:47.391: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8223/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.53.117%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.53.117, port: 54323 UDP @ 05/08/23 13:05:47.451
  May  8 13:05:47.452: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.53.117 54323] Namespace:hostport-8223 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 13:05:47.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 13:05:47.452: INFO: ExecWithOptions: Clientset creation
  May  8 13:05:47.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8223/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.53.117+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0508 13:05:48.083849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:49.084343      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:50.084630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:51.085720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:52.086020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:05:52.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-8223" for this suite. @ 05/08/23 13:05:52.515
• [33.291 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/08/23 13:05:52.519
  May  8 13:05:52.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename subpath @ 05/08/23 13:05:52.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:05:52.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:05:52.532
  STEP: Setting up data @ 05/08/23 13:05:52.534
  STEP: Creating pod pod-subpath-test-secret-2w9h @ 05/08/23 13:05:52.54
  STEP: Creating a pod to test atomic-volume-subpath @ 05/08/23 13:05:52.54
  E0508 13:05:53.086835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:54.087115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:55.088239      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:56.089042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:57.089379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:58.089528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:05:59.090697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:00.090873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:01.091963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:02.092147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:03.093257      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:04.093541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:05.094350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:06.095158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:07.095329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:08.095640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:09.095809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:10.096972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:11.097108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:12.097399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:13.098171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:14.098430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:15.099068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:16.100041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:06:16.585
  May  8 13:06:16.587: INFO: Trying to get logs from node worker-0 pod pod-subpath-test-secret-2w9h container test-container-subpath-secret-2w9h: <nil>
  STEP: delete the pod @ 05/08/23 13:06:16.593
  STEP: Deleting pod pod-subpath-test-secret-2w9h @ 05/08/23 13:06:16.603
  May  8 13:06:16.603: INFO: Deleting pod "pod-subpath-test-secret-2w9h" in namespace "subpath-3555"
  May  8 13:06:16.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3555" for this suite. @ 05/08/23 13:06:16.606
• [24.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/08/23 13:06:16.612
  May  8 13:06:16.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename podtemplate @ 05/08/23 13:06:16.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:16.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:16.625
  May  8 13:06:16.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2690" for this suite. @ 05/08/23 13:06:16.646
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/08/23 13:06:16.652
  May  8 13:06:16.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename svcaccounts @ 05/08/23 13:06:16.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:16.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:16.662
  STEP: Creating ServiceAccount "e2e-sa-qkhw7"  @ 05/08/23 13:06:16.664
  May  8 13:06:16.667: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-qkhw7"  @ 05/08/23 13:06:16.667
  May  8 13:06:16.673: INFO: AutomountServiceAccountToken: true
  May  8 13:06:16.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5305" for this suite. @ 05/08/23 13:06:16.675
• [0.026 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/08/23 13:06:16.679
  May  8 13:06:16.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename job @ 05/08/23 13:06:16.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:16.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:16.688
  STEP: Creating Indexed job @ 05/08/23 13:06:16.69
  STEP: Ensuring job reaches completions @ 05/08/23 13:06:16.695
  E0508 13:06:17.101037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:18.101198      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:19.102205      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:20.102518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:21.103268      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:22.103429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:23.103603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:24.103882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 05/08/23 13:06:24.698
  May  8 13:06:24.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8685" for this suite. @ 05/08/23 13:06:24.702
• [8.028 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/08/23 13:06:24.707
  May  8 13:06:24.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 13:06:24.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:24.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:24.718
  STEP: creating pod @ 05/08/23 13:06:24.72
  E0508 13:06:25.104055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:26.104652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:06:26.735: INFO: Pod pod-hostip-61d94916-a6b5-4a04-b1b3-406fc5cd6f41 has hostIP: 10.0.39.71
  May  8 13:06:26.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9480" for this suite. @ 05/08/23 13:06:26.737
• [2.033 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/08/23 13:06:26.741
  May  8 13:06:26.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename statefulset @ 05/08/23 13:06:26.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:26.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:26.753
  STEP: Creating service test in namespace statefulset-9738 @ 05/08/23 13:06:26.756
  May  8 13:06:26.765: INFO: Found 0 stateful pods, waiting for 1
  E0508 13:06:27.105242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:28.105417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:29.105523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:30.106640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:31.107169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:32.107484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:33.107636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:34.107947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:35.108229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:36.109005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:06:36.768: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/08/23 13:06:36.772
  W0508 13:06:36.782640      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  8 13:06:36.786: INFO: Found 1 stateful pods, waiting for 2
  E0508 13:06:37.109719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:38.109881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:39.110608      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:40.110911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:41.111088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:42.111377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:43.111686      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:44.111975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:45.112146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:46.112905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:06:46.790: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  8 13:06:46.791: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/08/23 13:06:46.794
  STEP: Delete all of the StatefulSets @ 05/08/23 13:06:46.796
  STEP: Verify that StatefulSets have been deleted @ 05/08/23 13:06:46.8
  May  8 13:06:46.802: INFO: Deleting all statefulset in ns statefulset-9738
  May  8 13:06:46.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9738" for this suite. @ 05/08/23 13:06:46.81
• [20.077 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/08/23 13:06:46.818
  May  8 13:06:46.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir @ 05/08/23 13:06:46.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:46.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:46.833
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/08/23 13:06:46.836
  E0508 13:06:47.113884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:48.113870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:06:48.846
  May  8 13:06:48.848: INFO: Trying to get logs from node worker-0 pod pod-457460a7-88f1-4f1f-8d3a-3cf4ec324d23 container test-container: <nil>
  STEP: delete the pod @ 05/08/23 13:06:48.853
  May  8 13:06:48.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9848" for this suite. @ 05/08/23 13:06:48.862
• [2.049 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/08/23 13:06:48.868
  May  8 13:06:48.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename job @ 05/08/23 13:06:48.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:48.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:48.879
  STEP: Creating a job @ 05/08/23 13:06:48.881
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/08/23 13:06:48.886
  E0508 13:06:49.114057      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:50.114622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/08/23 13:06:50.889
  STEP: updating /status @ 05/08/23 13:06:50.893
  STEP: get /status @ 05/08/23 13:06:50.898
  May  8 13:06:50.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2151" for this suite. @ 05/08/23 13:06:50.904
• [2.041 seconds]
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/08/23 13:06:50.909
  May  8 13:06:50.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename containers @ 05/08/23 13:06:50.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:50.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:50.919
  STEP: Creating a pod to test override arguments @ 05/08/23 13:06:50.921
  E0508 13:06:51.114935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:52.115692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:53.116327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:06:54.116600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:06:54.935
  May  8 13:06:54.936: INFO: Trying to get logs from node worker-1 pod client-containers-0794b1d7-66a2-4ca0-b46c-456cf138a163 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 13:06:54.95
  May  8 13:06:54.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5885" for this suite. @ 05/08/23 13:06:54.96
• [4.057 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/08/23 13:06:54.966
  May  8 13:06:54.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 13:06:54.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:54.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:54.976
  May  8 13:06:54.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-804 create -f -'
  E0508 13:06:55.117602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:06:55.690: INFO: stderr: ""
  May  8 13:06:55.690: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May  8 13:06:55.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-804 create -f -'
  May  8 13:06:55.956: INFO: stderr: ""
  May  8 13:06:55.956: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/08/23 13:06:55.956
  E0508 13:06:56.118719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:06:56.959: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 13:06:56.959: INFO: Found 1 / 1
  May  8 13:06:56.959: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May  8 13:06:56.961: INFO: Selector matched 1 pods for map[app:agnhost]
  May  8 13:06:56.961: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  8 13:06:56.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-804 describe pod agnhost-primary-pncmb'
  May  8 13:06:57.027: INFO: stderr: ""
  May  8 13:06:57.027: INFO: stdout: "Name:             agnhost-primary-pncmb\nNamespace:        kubectl-804\nPriority:         0\nService Account:  default\nNode:             worker-1/10.0.53.117\nStart Time:       Mon, 08 May 2023 13:06:55 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.0.22\nIPs:\n  IP:           10.244.0.22\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d1d160901399382ed6503aaf406a77a1e33cb1b700382597578b91be4ac07e8a\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 08 May 2023 13:06:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9knf2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-9knf2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-804/agnhost-primary-pncmb to worker-1\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  May  8 13:06:57.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-804 describe rc agnhost-primary'
  May  8 13:06:57.092: INFO: stderr: ""
  May  8 13:06:57.092: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-804\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-pncmb\n"
  May  8 13:06:57.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-804 describe service agnhost-primary'
  E0508 13:06:57.119849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:06:57.155: INFO: stderr: ""
  May  8 13:06:57.155: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-804\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.111.134.73\nIPs:               10.111.134.73\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.0.22:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May  8 13:06:57.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-804 describe node worker-0'
  May  8 13:06:57.232: INFO: stderr: ""
  May  8 13:06:57.232: INFO: stdout: "Name:               worker-0\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=worker-0\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 08 May 2023 11:37:56 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  worker-0\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 08 May 2023 13:06:56 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 08 May 2023 13:02:17 +0000   Mon, 08 May 2023 11:37:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 08 May 2023 13:02:17 +0000   Mon, 08 May 2023 11:37:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 08 May 2023 13:02:17 +0000   Mon, 08 May 2023 11:37:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 08 May 2023 13:02:17 +0000   Mon, 08 May 2023 11:38:09 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.39.71\n  Hostname:    worker-0\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    50620216Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               7621156Ki\n  pods:                 110\nAllocatable:\n  cpu:                  4\n  ephemeral-storage:    46651590989\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               7518756Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 119ea29c864b46a2b0d1c5d2254b397b\n  System UUID:                ec2a3e8b-0bfb-a3b1-bf49-48fe2a666416\n  Boot ID:                    6759fb9a-7f3b-4da7-a554-074b5132de8f\n  Kernel Version:             5.19.0-1024-aws\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.0\n  Kubelet Version:            v1.27.1+k0s\n  Kube-Proxy Version:         v1.27.1+k0s\nPodCIDR:                      10.244.1.0/24\nPodCIDRs:                     10.244.1.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  job-2151                    suspend-false-to-true-pxshz                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         9s\n  job-2151                    suspend-false-to-true-tgr7t                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         9s\n  kube-system                 coredns-878bb57ff-lw4w7                                    100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     16m\n  kube-system                 konnectivity-agent-j9zk2                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                 kube-proxy-gx5cm                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                 kube-router-ckb57                                          250m (6%)     0 (0%)      16Mi (0%)        0 (0%)         89m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-bf38d49c1cac438c-zf5gq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests   Limits\n  --------             --------   ------\n  cpu                  350m (8%)  0 (0%)\n  memory               86Mi (1%)  170Mi (2%)\n  ephemeral-storage    0 (0%)     0 (0%)\n  hugepages-1Gi        0 (0%)     0 (0%)\n  hugepages-2Mi        0 (0%)     0 (0%)\n  example.com/fakecpu  0          0\nEvents:                <none>\n"
  May  8 13:06:57.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-804 describe namespace kubectl-804'
  May  8 13:06:57.296: INFO: stderr: ""
  May  8 13:06:57.296: INFO: stdout: "Name:         kubectl-804\nLabels:       e2e-framework=kubectl\n              e2e-run=928fc166-9554-4a46-90b9-1821fb86d69c\n              kubernetes.io/metadata.name=kubectl-804\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May  8 13:06:57.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-804" for this suite. @ 05/08/23 13:06:57.299
• [2.336 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/08/23 13:06:57.303
  May  8 13:06:57.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename daemonsets @ 05/08/23 13:06:57.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:06:57.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:06:57.315
  STEP: Creating simple DaemonSet "daemon-set" @ 05/08/23 13:06:57.326
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/08/23 13:06:57.329
  May  8 13:06:57.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:06:57.335: INFO: Node worker-0 is running 0 daemon pod, expected 1
  E0508 13:06:58.120187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:06:58.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:06:58.340: INFO: Node worker-0 is running 0 daemon pod, expected 1
  E0508 13:06:59.120346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:06:59.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 13:06:59.340: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/08/23 13:06:59.342
  May  8 13:06:59.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 13:06:59.353: INFO: Node worker-1 is running 0 daemon pod, expected 1
  E0508 13:07:00.121507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:07:00.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 13:07:00.358: INFO: Node worker-1 is running 0 daemon pod, expected 1
  E0508 13:07:01.121641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:07:01.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 13:07:01.358: INFO: Node worker-1 is running 0 daemon pod, expected 1
  E0508 13:07:02.122506      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:07:02.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 13:07:02.358: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/08/23 13:07:02.36
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7199, will wait for the garbage collector to delete the pods @ 05/08/23 13:07:02.36
  May  8 13:07:02.417: INFO: Deleting DaemonSet.extensions daemon-set took: 4.149318ms
  May  8 13:07:02.517: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.107265ms
  E0508 13:07:03.122160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:07:03.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:07:03.419: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  8 13:07:03.421: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30657"},"items":null}

  May  8 13:07:03.425: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30658"},"items":null}

  May  8 13:07:03.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7199" for this suite. @ 05/08/23 13:07:03.435
• [6.135 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/08/23 13:07:03.438
  May  8 13:07:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename services @ 05/08/23 13:07:03.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:07:03.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:07:03.449
  May  8 13:07:03.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1841" for this suite. @ 05/08/23 13:07:03.455
• [0.020 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/08/23 13:07:03.461
  May  8 13:07:03.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 13:07:03.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:07:03.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:07:03.472
  STEP: Setting up server cert @ 05/08/23 13:07:03.483
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 13:07:04.006
  STEP: Deploying the webhook pod @ 05/08/23 13:07:04.011
  STEP: Wait for the deployment to be ready @ 05/08/23 13:07:04.02
  May  8 13:07:04.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0508 13:07:04.122645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:05.123019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/08/23 13:07:06.031
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 13:07:06.041
  E0508 13:07:06.123843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:07:07.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  8 13:07:07.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  E0508 13:07:07.123955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/08/23 13:07:07.551
  STEP: Creating a custom resource that should be denied by the webhook @ 05/08/23 13:07:07.574
  E0508 13:07:08.124972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:09.125140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/08/23 13:07:09.586
  STEP: Updating the custom resource with disallowed data should be denied @ 05/08/23 13:07:09.592
  STEP: Deleting the custom resource should be denied @ 05/08/23 13:07:09.597
  STEP: Remove the offending key and value from the custom resource data @ 05/08/23 13:07:09.602
  STEP: Deleting the updated custom resource should be successful @ 05/08/23 13:07:09.61
  May  8 13:07:09.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0508 13:07:10.125528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3668" for this suite. @ 05/08/23 13:07:10.157
  STEP: Destroying namespace "webhook-markers-1131" for this suite. @ 05/08/23 13:07:10.161
• [6.704 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/08/23 13:07:10.166
  May  8 13:07:10.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/08/23 13:07:10.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:07:10.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:07:10.178
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/08/23 13:07:10.18
  May  8 13:07:10.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  E0508 13:07:11.126645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:12.127106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:13.127714      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:14.128499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:15.129093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/08/23 13:07:15.528
  May  8 13:07:15.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  E0508 13:07:16.129937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:07:16.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  E0508 13:07:17.130114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:18.130168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:19.130595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:20.131539      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:21.132230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:07:22.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6668" for this suite. @ 05/08/23 13:07:22.052
• [11.890 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/08/23 13:07:22.056
  May  8 13:07:22.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 13:07:22.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:07:22.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:07:22.068
  STEP: Creating configMap with name configmap-test-volume-map-675c2b2b-3298-44af-8a65-aa4eef269dbb @ 05/08/23 13:07:22.07
  STEP: Creating a pod to test consume configMaps @ 05/08/23 13:07:22.073
  E0508 13:07:22.132894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:23.134058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:24.134638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:25.134912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:07:26.086
  May  8 13:07:26.088: INFO: Trying to get logs from node worker-1 pod pod-configmaps-5c36a727-3e99-470b-b263-2cce31b8afb3 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 13:07:26.093
  May  8 13:07:26.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5428" for this suite. @ 05/08/23 13:07:26.105
• [4.053 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/08/23 13:07:26.109
  May  8 13:07:26.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename job @ 05/08/23 13:07:26.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:07:26.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:07:26.12
  STEP: Creating a job @ 05/08/23 13:07:26.122
  STEP: Ensuring active pods == parallelism @ 05/08/23 13:07:26.126
  E0508 13:07:26.135189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:27.135614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:28.136016      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:29.136308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 05/08/23 13:07:30.129
  STEP: deleting Job.batch foo in namespace job-1305, will wait for the garbage collector to delete the pods @ 05/08/23 13:07:30.129
  E0508 13:07:30.137165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:07:30.186: INFO: Deleting Job.batch foo took: 4.117304ms
  May  8 13:07:30.287: INFO: Terminating Job.batch foo pods took: 100.402202ms
  E0508 13:07:31.137272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:32.138048      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:33.138636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:34.139393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:35.140056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:36.141193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:37.141710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:38.142481      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:39.143181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:40.143831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:41.144761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:42.145472      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:43.146080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:44.147016      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:45.147689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:46.148755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:47.149462      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:48.150091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:49.150796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:50.151782      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:51.152767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:52.153373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:53.154080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:54.154622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:55.155468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:56.156392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:57.156990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:58.157673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:07:59.158473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:00.159066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 05/08/23 13:08:00.887
  May  8 13:08:00.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1305" for this suite. @ 05/08/23 13:08:00.892
• [34.787 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/08/23 13:08:00.898
  May  8 13:08:00.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 13:08:00.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:00.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:00.909
  STEP: Creating configMap with name projected-configmap-test-volume-b6a05ce6-2881-461e-9370-a43333181f91 @ 05/08/23 13:08:00.911
  STEP: Creating a pod to test consume configMaps @ 05/08/23 13:08:00.913
  E0508 13:08:01.159279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:02.159562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:03.159760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:04.160079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:08:04.927
  May  8 13:08:04.929: INFO: Trying to get logs from node worker-0 pod pod-projected-configmaps-7981c6eb-d3a1-4be4-8fcb-f9be8a919955 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 13:08:04.934
  May  8 13:08:04.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8196" for this suite. @ 05/08/23 13:08:04.949
• [4.054 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/08/23 13:08:04.953
  May  8 13:08:04.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename webhook @ 05/08/23 13:08:04.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:04.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:04.965
  STEP: Setting up server cert @ 05/08/23 13:08:04.979
  E0508 13:08:05.160796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/08/23 13:08:05.525
  STEP: Deploying the webhook pod @ 05/08/23 13:08:05.531
  STEP: Wait for the deployment to be ready @ 05/08/23 13:08:05.539
  May  8 13:08:05.545: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0508 13:08:06.161676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:07.161825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/08/23 13:08:07.551
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 13:08:07.565
  E0508 13:08:08.162659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:08:08.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/08/23 13:08:08.568
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/08/23 13:08:08.584
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/08/23 13:08:08.593
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/08/23 13:08:08.6
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/08/23 13:08:08.607
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/08/23 13:08:08.613
  May  8 13:08:08.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9337" for this suite. @ 05/08/23 13:08:08.646
  STEP: Destroying namespace "webhook-markers-8552" for this suite. @ 05/08/23 13:08:08.651
• [3.703 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/08/23 13:08:08.657
  May  8 13:08:08.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename gc @ 05/08/23 13:08:08.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:08.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:08.669
  STEP: create the deployment @ 05/08/23 13:08:08.671
  W0508 13:08:08.675247      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/08/23 13:08:08.675
  E0508 13:08:09.162825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 05/08/23 13:08:09.18
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/08/23 13:08:09.185
  STEP: Gathering metrics @ 05/08/23 13:08:09.697
  W0508 13:08:09.700426      23 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  8 13:08:09.700: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  8 13:08:09.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6040" for this suite. @ 05/08/23 13:08:09.702
• [1.049 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/08/23 13:08:09.707
  May  8 13:08:09.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 13:08:09.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:09.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:09.719
  STEP: Creating configMap with name projected-configmap-test-volume-map-8ae3b421-4503-4df3-bd7e-5f33a1bdbf2e @ 05/08/23 13:08:09.721
  STEP: Creating a pod to test consume configMaps @ 05/08/23 13:08:09.724
  E0508 13:08:10.162856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:11.163614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:12.163904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:13.164191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:08:13.738
  May  8 13:08:13.742: INFO: Trying to get logs from node worker-0 pod pod-projected-configmaps-8930cc22-faf3-4cb1-a57b-cbf6a578d412 container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 13:08:13.749
  May  8 13:08:13.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-453" for this suite. @ 05/08/23 13:08:13.763
• [4.061 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/08/23 13:08:13.768
  May  8 13:08:13.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 13:08:13.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:13.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:13.784
  STEP: Creating configMap with name projected-configmap-test-volume-ec509ea6-6736-4dec-9fa4-0983ed72fae1 @ 05/08/23 13:08:13.787
  STEP: Creating a pod to test consume configMaps @ 05/08/23 13:08:13.792
  E0508 13:08:14.164810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:15.164984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:16.165742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:17.166028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:08:17.808
  May  8 13:08:17.809: INFO: Trying to get logs from node worker-0 pod pod-projected-configmaps-15e98454-b9c2-4689-9f6b-2a83ecbb1e9f container agnhost-container: <nil>
  STEP: delete the pod @ 05/08/23 13:08:17.814
  May  8 13:08:17.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1133" for this suite. @ 05/08/23 13:08:17.827
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/08/23 13:08:17.838
  May  8 13:08:17.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename cronjob @ 05/08/23 13:08:17.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:17.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:17.849
  STEP: Creating a cronjob @ 05/08/23 13:08:17.851
  STEP: creating @ 05/08/23 13:08:17.851
  STEP: getting @ 05/08/23 13:08:17.856
  STEP: listing @ 05/08/23 13:08:17.857
  STEP: watching @ 05/08/23 13:08:17.859
  May  8 13:08:17.859: INFO: starting watch
  STEP: cluster-wide listing @ 05/08/23 13:08:17.86
  STEP: cluster-wide watching @ 05/08/23 13:08:17.862
  May  8 13:08:17.862: INFO: starting watch
  STEP: patching @ 05/08/23 13:08:17.862
  STEP: updating @ 05/08/23 13:08:17.867
  May  8 13:08:17.873: INFO: waiting for watch events with expected annotations
  May  8 13:08:17.873: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/08/23 13:08:17.873
  STEP: updating /status @ 05/08/23 13:08:17.876
  STEP: get /status @ 05/08/23 13:08:17.881
  STEP: deleting @ 05/08/23 13:08:17.882
  STEP: deleting a collection @ 05/08/23 13:08:17.89
  May  8 13:08:17.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4178" for this suite. @ 05/08/23 13:08:17.898
• [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/08/23 13:08:17.904
  May  8 13:08:17.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename configmap @ 05/08/23 13:08:17.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:17.911
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:17.914
  STEP: Creating configMap with name configmap-test-volume-b5283c5f-e1b3-418f-bdf6-7ef67d71cb66 @ 05/08/23 13:08:17.916
  STEP: Creating a pod to test consume configMaps @ 05/08/23 13:08:17.922
  E0508 13:08:18.166743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:19.167292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:20.168215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:21.169283      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:08:21.935
  May  8 13:08:21.936: INFO: Trying to get logs from node worker-0 pod pod-configmaps-b67fb00e-2b5d-484d-ac49-c02eac20254a container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/08/23 13:08:21.941
  May  8 13:08:21.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2806" for this suite. @ 05/08/23 13:08:21.953
• [4.053 seconds]
------------------------------
S
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/08/23 13:08:21.958
  May  8 13:08:21.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename downward-api @ 05/08/23 13:08:21.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:21.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:21.971
  STEP: Creating a pod to test downward api env vars @ 05/08/23 13:08:21.973
  E0508 13:08:22.170050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:23.170215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:24.170357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:25.170553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:08:25.986
  May  8 13:08:25.988: INFO: Trying to get logs from node worker-0 pod downward-api-60fd30c8-d9be-41f8-b00b-6be2a536bb7e container dapi-container: <nil>
  STEP: delete the pod @ 05/08/23 13:08:25.994
  May  8 13:08:26.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9233" for this suite. @ 05/08/23 13:08:26.006
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/08/23 13:08:26.014
  May  8 13:08:26.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 13:08:26.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:26.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:26.027
  STEP: create deployment with httpd image @ 05/08/23 13:08:26.03
  May  8 13:08:26.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-716 create -f -'
  E0508 13:08:26.170646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:08:26.247: INFO: stderr: ""
  May  8 13:08:26.247: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/08/23 13:08:26.247
  May  8 13:08:26.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-716 diff -f -'
  May  8 13:08:26.455: INFO: rc: 1
  May  8 13:08:26.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-716 delete -f -'
  May  8 13:08:26.514: INFO: stderr: ""
  May  8 13:08:26.514: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May  8 13:08:26.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-716" for this suite. @ 05/08/23 13:08:26.517
• [0.506 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/08/23 13:08:26.521
  May  8 13:08:26.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename resourcequota @ 05/08/23 13:08:26.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:26.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:26.531
  STEP: Counting existing ResourceQuota @ 05/08/23 13:08:26.534
  E0508 13:08:27.170961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:28.171123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:29.171432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:30.172332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:31.172792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/08/23 13:08:31.536
  STEP: Ensuring resource quota status is calculated @ 05/08/23 13:08:31.54
  E0508 13:08:32.172955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:33.173235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 05/08/23 13:08:33.543
  STEP: Ensuring resource quota status captures replication controller creation @ 05/08/23 13:08:33.552
  E0508 13:08:34.173345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:35.173522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 05/08/23 13:08:35.555
  STEP: Ensuring resource quota status released usage @ 05/08/23 13:08:35.558
  E0508 13:08:36.174155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:37.174430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:08:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1052" for this suite. @ 05/08/23 13:08:37.563
• [11.046 seconds]
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/08/23 13:08:37.567
  May  8 13:08:37.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pod-network-test @ 05/08/23 13:08:37.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:08:37.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:08:37.579
  STEP: Performing setup for networking test in namespace pod-network-test-7128 @ 05/08/23 13:08:37.581
  STEP: creating a selector @ 05/08/23 13:08:37.581
  STEP: Creating the service pods in kubernetes @ 05/08/23 13:08:37.581
  May  8 13:08:37.581: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0508 13:08:38.174588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:39.174902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:40.175869      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:41.176178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:42.176745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:43.177032      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:44.177108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:45.177263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:46.177296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:47.177602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:48.178586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:49.178876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:50.179513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:51.179973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:52.180891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:53.181174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:54.181252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:55.181406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:56.182471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:57.182780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:58.183100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:08:59.183225      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/08/23 13:08:59.634
  E0508 13:09:00.184112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:01.184218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:01.644: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May  8 13:09:01.644: INFO: Breadth first check of 10.244.1.193 on host 10.0.39.71...
  May  8 13:09:01.645: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.194:9080/dial?request=hostname&protocol=http&host=10.244.1.193&port=8083&tries=1'] Namespace:pod-network-test-7128 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 13:09:01.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 13:09:01.646: INFO: ExecWithOptions: Clientset creation
  May  8 13:09:01.646: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7128/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.194%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.193%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  8 13:09:01.710: INFO: Waiting for responses: map[]
  May  8 13:09:01.710: INFO: reached 10.244.1.193 after 0/1 tries
  May  8 13:09:01.710: INFO: Breadth first check of 10.244.0.29 on host 10.0.53.117...
  May  8 13:09:01.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.194:9080/dial?request=hostname&protocol=http&host=10.244.0.29&port=8083&tries=1'] Namespace:pod-network-test-7128 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 13:09:01.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 13:09:01.713: INFO: ExecWithOptions: Clientset creation
  May  8 13:09:01.713: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7128/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.194%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.29%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  8 13:09:01.775: INFO: Waiting for responses: map[]
  May  8 13:09:01.775: INFO: reached 10.244.0.29 after 0/1 tries
  May  8 13:09:01.775: INFO: Going to retry 0 out of 2 pods....
  May  8 13:09:01.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7128" for this suite. @ 05/08/23 13:09:01.777
• [24.214 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/08/23 13:09:01.782
  May  8 13:09:01.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 13:09:01.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:01.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:01.793
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 13:09:01.795
  E0508 13:09:02.184894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:03.185072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:04.185860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:05.186033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:09:05.809
  May  8 13:09:05.811: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-c841305d-8cf6-49dd-90ff-0033a46774d3 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 13:09:05.816
  May  8 13:09:05.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6835" for this suite. @ 05/08/23 13:09:05.828
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/08/23 13:09:05.833
  May  8 13:09:05.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replication-controller @ 05/08/23 13:09:05.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:05.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:05.844
  STEP: Creating replication controller my-hostname-basic-e50e8c25-e38d-4bf8-8040-dad0bcdf24db @ 05/08/23 13:09:05.846
  May  8 13:09:05.851: INFO: Pod name my-hostname-basic-e50e8c25-e38d-4bf8-8040-dad0bcdf24db: Found 0 pods out of 1
  E0508 13:09:06.186358      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:07.186765      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:08.187053      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:09.187190      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:10.187482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:10.854: INFO: Pod name my-hostname-basic-e50e8c25-e38d-4bf8-8040-dad0bcdf24db: Found 1 pods out of 1
  May  8 13:09:10.854: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e50e8c25-e38d-4bf8-8040-dad0bcdf24db" are running
  May  8 13:09:10.856: INFO: Pod "my-hostname-basic-e50e8c25-e38d-4bf8-8040-dad0bcdf24db-dqnpv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-08 13:09:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-08 13:09:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-08 13:09:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-08 13:09:05 +0000 UTC Reason: Message:}])
  May  8 13:09:10.856: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/08/23 13:09:10.856
  May  8 13:09:10.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6160" for this suite. @ 05/08/23 13:09:10.869
• [5.039 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/08/23 13:09:10.873
  May  8 13:09:10.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/08/23 13:09:10.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:10.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:10.884
  May  8 13:09:10.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  E0508 13:09:11.187869      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:12.187954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:13.188654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:14.189610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:15.190413      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:16.190569      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:17.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3081" for this suite. @ 05/08/23 13:09:17.038
• [6.169 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/08/23 13:09:17.043
  May  8 13:09:17.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename crd-webhook @ 05/08/23 13:09:17.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:17.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:17.056
  STEP: Setting up server cert @ 05/08/23 13:09:17.058
  E0508 13:09:17.191207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/08/23 13:09:17.622
  STEP: Deploying the custom resource conversion webhook pod @ 05/08/23 13:09:17.627
  STEP: Wait for the deployment to be ready @ 05/08/23 13:09:17.634
  May  8 13:09:17.638: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0508 13:09:18.191327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:19.191486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:19.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0508 13:09:20.192604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:21.193022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:21.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0508 13:09:22.193978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:23.194661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:23.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0508 13:09:24.194807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:25.195299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:25.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0508 13:09:26.195451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:27.195607      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:27.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 8, 13, 9, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0508 13:09:28.195768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:29.196025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/08/23 13:09:29.648
  STEP: Verifying the service has paired with the endpoint @ 05/08/23 13:09:29.662
  E0508 13:09:30.196280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:30.662: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May  8 13:09:30.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  E0508 13:09:31.196551      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:32.197350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:33.197667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/08/23 13:09:33.222
  STEP: Create a v2 custom resource @ 05/08/23 13:09:33.236
  STEP: List CRs in v1 @ 05/08/23 13:09:33.272
  STEP: List CRs in v2 @ 05/08/23 13:09:33.276
  May  8 13:09:33.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-1678" for this suite. @ 05/08/23 13:09:33.814
• [16.775 seconds]
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/08/23 13:09:33.818
  May  8 13:09:33.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename proxy @ 05/08/23 13:09:33.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:33.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:33.832
  May  8 13:09:33.834: INFO: Creating pod...
  E0508 13:09:34.198493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:35.198907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:35.844: INFO: Creating service...
  May  8 13:09:35.854: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/pods/agnhost/proxy?method=DELETE
  May  8 13:09:35.860: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  8 13:09:35.860: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/pods/agnhost/proxy?method=OPTIONS
  May  8 13:09:35.862: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  8 13:09:35.862: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/pods/agnhost/proxy?method=PATCH
  May  8 13:09:35.865: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  8 13:09:35.865: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/pods/agnhost/proxy?method=POST
  May  8 13:09:35.868: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  8 13:09:35.868: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/pods/agnhost/proxy?method=PUT
  May  8 13:09:35.870: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  8 13:09:35.870: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/services/e2e-proxy-test-service/proxy?method=DELETE
  May  8 13:09:35.873: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  8 13:09:35.873: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May  8 13:09:35.876: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  8 13:09:35.876: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/services/e2e-proxy-test-service/proxy?method=PATCH
  May  8 13:09:35.879: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  8 13:09:35.879: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/services/e2e-proxy-test-service/proxy?method=POST
  May  8 13:09:35.882: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  8 13:09:35.882: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/services/e2e-proxy-test-service/proxy?method=PUT
  May  8 13:09:35.885: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  8 13:09:35.885: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/pods/agnhost/proxy?method=GET
  May  8 13:09:35.887: INFO: http.Client request:GET StatusCode:301
  May  8 13:09:35.887: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/services/e2e-proxy-test-service/proxy?method=GET
  May  8 13:09:35.889: INFO: http.Client request:GET StatusCode:301
  May  8 13:09:35.889: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/pods/agnhost/proxy?method=HEAD
  May  8 13:09:35.891: INFO: http.Client request:HEAD StatusCode:301
  May  8 13:09:35.891: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8703/services/e2e-proxy-test-service/proxy?method=HEAD
  May  8 13:09:35.893: INFO: http.Client request:HEAD StatusCode:301
  May  8 13:09:35.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-8703" for this suite. @ 05/08/23 13:09:35.895
• [2.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/08/23 13:09:35.899
  May  8 13:09:35.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 13:09:35.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:35.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:35.911
  STEP: creating the pod @ 05/08/23 13:09:35.913
  May  8 13:09:35.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5777 create -f -'
  E0508 13:09:36.199183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:36.277: INFO: stderr: ""
  May  8 13:09:36.277: INFO: stdout: "pod/pause created\n"
  E0508 13:09:37.200163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:38.200449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/08/23 13:09:38.283
  May  8 13:09:38.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5777 label pods pause testing-label=testing-label-value'
  May  8 13:09:38.349: INFO: stderr: ""
  May  8 13:09:38.349: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/08/23 13:09:38.349
  May  8 13:09:38.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5777 get pod pause -L testing-label'
  May  8 13:09:38.412: INFO: stderr: ""
  May  8 13:09:38.412: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/08/23 13:09:38.412
  May  8 13:09:38.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5777 label pods pause testing-label-'
  May  8 13:09:38.479: INFO: stderr: ""
  May  8 13:09:38.479: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/08/23 13:09:38.479
  May  8 13:09:38.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5777 get pod pause -L testing-label'
  May  8 13:09:38.540: INFO: stderr: ""
  May  8 13:09:38.540: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 05/08/23 13:09:38.541
  May  8 13:09:38.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5777 delete --grace-period=0 --force -f -'
  May  8 13:09:38.605: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  8 13:09:38.605: INFO: stdout: "pod \"pause\" force deleted\n"
  May  8 13:09:38.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5777 get rc,svc -l name=pause --no-headers'
  May  8 13:09:38.672: INFO: stderr: "No resources found in kubectl-5777 namespace.\n"
  May  8 13:09:38.672: INFO: stdout: ""
  May  8 13:09:38.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-5777 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  8 13:09:38.733: INFO: stderr: ""
  May  8 13:09:38.733: INFO: stdout: ""
  May  8 13:09:38.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5777" for this suite. @ 05/08/23 13:09:38.735
• [2.841 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/08/23 13:09:38.741
  May  8 13:09:38.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename secrets @ 05/08/23 13:09:38.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:38.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:38.751
  STEP: Creating secret with name secret-test-8c605db5-0ab9-427a-b405-e0d7cdfb3b27 @ 05/08/23 13:09:38.754
  STEP: Creating a pod to test consume secrets @ 05/08/23 13:09:38.758
  E0508 13:09:39.200452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:40.200760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:41.200991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:42.201305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:09:42.771
  May  8 13:09:42.773: INFO: Trying to get logs from node worker-0 pod pod-secrets-f13b5d69-4ec4-4153-8c01-f00c71ef4b17 container secret-env-test: <nil>
  STEP: delete the pod @ 05/08/23 13:09:42.78
  May  8 13:09:42.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9063" for this suite. @ 05/08/23 13:09:42.79
• [4.054 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/08/23 13:09:42.796
  May  8 13:09:42.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename daemonsets @ 05/08/23 13:09:42.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:42.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:42.806
  May  8 13:09:42.819: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/08/23 13:09:42.822
  May  8 13:09:42.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:09:42.824: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/08/23 13:09:42.824
  May  8 13:09:42.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:09:42.836: INFO: Node worker-1 is running 0 daemon pod, expected 1
  E0508 13:09:43.202191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:43.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 13:09:43.839: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/08/23 13:09:43.841
  May  8 13:09:43.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 13:09:43.852: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0508 13:09:44.203195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:44.855: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:09:44.855: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/08/23 13:09:44.855
  May  8 13:09:44.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:09:44.862: INFO: Node worker-1 is running 0 daemon pod, expected 1
  E0508 13:09:45.203445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:45.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:09:45.865: INFO: Node worker-1 is running 0 daemon pod, expected 1
  E0508 13:09:46.203912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:46.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 13:09:46.864: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/08/23 13:09:46.868
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8435, will wait for the garbage collector to delete the pods @ 05/08/23 13:09:46.868
  May  8 13:09:46.923: INFO: Deleting DaemonSet.extensions daemon-set took: 3.371889ms
  May  8 13:09:47.023: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.256136ms
  E0508 13:09:47.204322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:48.204825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:09:48.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:09:48.826: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  8 13:09:48.828: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31927"},"items":null}

  May  8 13:09:48.830: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31927"},"items":null}

  May  8 13:09:48.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8435" for this suite. @ 05/08/23 13:09:48.845
• [6.053 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/08/23 13:09:48.852
  May  8 13:09:48.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 13:09:48.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:09:48.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:09:48.866
  STEP: Creating secret with name s-test-opt-del-d08e3e9a-07c9-41c0-b8f3-b21a3e6bd45b @ 05/08/23 13:09:48.87
  STEP: Creating secret with name s-test-opt-upd-868571a5-a0f1-451b-9f4c-034f2597d455 @ 05/08/23 13:09:48.873
  STEP: Creating the pod @ 05/08/23 13:09:48.877
  E0508 13:09:49.204936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:50.205196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-d08e3e9a-07c9-41c0-b8f3-b21a3e6bd45b @ 05/08/23 13:09:50.903
  STEP: Updating secret s-test-opt-upd-868571a5-a0f1-451b-9f4c-034f2597d455 @ 05/08/23 13:09:50.907
  STEP: Creating secret with name s-test-opt-create-b3c1e12b-8a02-41af-bbfb-11f22d5108bf @ 05/08/23 13:09:50.91
  STEP: waiting to observe update in volume @ 05/08/23 13:09:50.913
  E0508 13:09:51.205739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:52.206038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:53.206692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:54.206979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:55.207018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:56.207708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:57.208252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:58.208742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:09:59.209673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:00.210636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:01.210839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:02.211179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:03.211415      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:04.211694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:05.212135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:06.212280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:07.213223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:08.213535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:09.214501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:10.214787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:11.215914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:12.216103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:13.217028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:14.217289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:15.218293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:16.219072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:17.219814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:18.219996      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:19.220065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:20.220555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:21.221435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:22.221614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:23.222624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:24.222887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:25.223150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:26.223906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:27.224702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:28.224967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:29.225669      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:30.226641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:31.227759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:32.228249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:33.228934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:34.229111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:35.229682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:36.230571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:37.231504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:38.231683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:39.232773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:40.233049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:41.233203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:42.233265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:43.234234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:44.234507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:45.234649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:46.235615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:47.236499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:48.236777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:49.237426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:50.237787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:51.238901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:52.239063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:53.239867      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:54.240159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:55.240712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:56.241524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:57.241653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:58.241914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:10:59.241983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:00.242273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:01.243298      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:02.243576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:03.244500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:04.244821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:05.245093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:06.245607      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:07.245903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:08.246170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:09.246194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:10.246485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:11:11.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3791" for this suite. @ 05/08/23 13:11:11.222
• [82.375 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/08/23 13:11:11.227
  May  8 13:11:11.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename namespaces @ 05/08/23 13:11:11.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:11:11.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:11:11.24
  STEP: Read namespace status @ 05/08/23 13:11:11.242
  May  8 13:11:11.244: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/08/23 13:11:11.244
  E0508 13:11:11.246485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:11:11.249: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/08/23 13:11:11.249
  May  8 13:11:11.254: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May  8 13:11:11.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8414" for this suite. @ 05/08/23 13:11:11.257
• [0.033 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/08/23 13:11:11.261
  May  8 13:11:11.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename namespaces @ 05/08/23 13:11:11.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:11:11.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:11:11.273
  STEP: Creating a test namespace @ 05/08/23 13:11:11.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:11:11.282
  STEP: Creating a pod in the namespace @ 05/08/23 13:11:11.284
  STEP: Waiting for the pod to have running status @ 05/08/23 13:11:11.289
  E0508 13:11:12.247045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:13.247610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 05/08/23 13:11:13.295
  STEP: Waiting for the namespace to be removed. @ 05/08/23 13:11:13.298
  E0508 13:11:14.248092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:15.248698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:16.248746      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:17.249377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:18.249899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:19.250830      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:20.251656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:21.252394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:22.253187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:23.253355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:24.253532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/08/23 13:11:24.302
  STEP: Verifying there are no pods in the namespace @ 05/08/23 13:11:24.31
  May  8 13:11:24.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-508" for this suite. @ 05/08/23 13:11:24.315
  STEP: Destroying namespace "nsdeletetest-3982" for this suite. @ 05/08/23 13:11:24.318
  May  8 13:11:24.320: INFO: Namespace nsdeletetest-3982 was already deleted
  STEP: Destroying namespace "nsdeletetest-6668" for this suite. @ 05/08/23 13:11:24.32
• [13.063 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/08/23 13:11:24.324
  May  8 13:11:24.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename statefulset @ 05/08/23 13:11:24.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:11:24.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:11:24.335
  STEP: Creating service test in namespace statefulset-8294 @ 05/08/23 13:11:24.337
  STEP: Creating statefulset ss in namespace statefulset-8294 @ 05/08/23 13:11:24.342
  May  8 13:11:24.349: INFO: Found 0 stateful pods, waiting for 1
  E0508 13:11:25.254412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:26.255538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:27.255856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:28.256125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:29.256299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:30.256619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:31.257015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:32.257320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:33.257619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:34.257769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:11:34.353: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/08/23 13:11:34.357
  STEP: Getting /status @ 05/08/23 13:11:34.361
  May  8 13:11:34.363: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/08/23 13:11:34.363
  May  8 13:11:34.370: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/08/23 13:11:34.37
  May  8 13:11:34.372: INFO: Observed &StatefulSet event: ADDED
  May  8 13:11:34.372: INFO: Found Statefulset ss in namespace statefulset-8294 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  8 13:11:34.372: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/08/23 13:11:34.372
  May  8 13:11:34.372: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  8 13:11:34.377: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/08/23 13:11:34.377
  May  8 13:11:34.378: INFO: Observed &StatefulSet event: ADDED
  May  8 13:11:34.378: INFO: Deleting all statefulset in ns statefulset-8294
  May  8 13:11:34.380: INFO: Scaling statefulset ss to 0
  E0508 13:11:35.258558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:36.259600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:37.259873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:38.260180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:39.260480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:40.260761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:41.260790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:42.261089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:43.261431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:44.261703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:11:44.392: INFO: Waiting for statefulset status.replicas updated to 0
  May  8 13:11:44.394: INFO: Deleting statefulset ss
  May  8 13:11:44.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8294" for this suite. @ 05/08/23 13:11:44.403
• [20.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/08/23 13:11:44.41
  May  8 13:11:44.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/08/23 13:11:44.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:11:44.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:11:44.42
  STEP: creating a target pod @ 05/08/23 13:11:44.422
  E0508 13:11:45.262657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:46.263736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/08/23 13:11:46.435
  E0508 13:11:47.263753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:48.264041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/08/23 13:11:48.452
  May  8 13:11:48.453: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6231 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  8 13:11:48.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  May  8 13:11:48.453: INFO: ExecWithOptions: Clientset creation
  May  8 13:11:48.453: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-6231/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May  8 13:11:48.521: INFO: Exec stderr: ""
  May  8 13:11:48.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-6231" for this suite. @ 05/08/23 13:11:48.528
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/08/23 13:11:48.533
  May  8 13:11:48.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename field-validation @ 05/08/23 13:11:48.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:11:48.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:11:48.544
  STEP: apply creating a deployment @ 05/08/23 13:11:48.546
  May  8 13:11:48.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2261" for this suite. @ 05/08/23 13:11:48.556
• [0.027 seconds]
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/08/23 13:11:48.56
  May  8 13:11:48.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename sysctl @ 05/08/23 13:11:48.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:11:48.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:11:48.571
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/08/23 13:11:48.573
  May  8 13:11:48.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-5525" for this suite. @ 05/08/23 13:11:48.578
• [0.021 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/08/23 13:11:48.585
  May  8 13:11:48.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename cronjob @ 05/08/23 13:11:48.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:11:48.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:11:48.594
  STEP: Creating a suspended cronjob @ 05/08/23 13:11:48.596
  STEP: Ensuring no jobs are scheduled @ 05/08/23 13:11:48.6
  E0508 13:11:49.265153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:50.265518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:51.265778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:52.265947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:53.266299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:54.266478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:55.266806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:56.267704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:57.268443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:58.268593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:11:59.269253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:00.269527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:01.269588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:02.269878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:03.270206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:04.270527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:05.271119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:06.271851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:07.272216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:08.272375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:09.272739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:10.273021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:11.273490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:12.273761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:13.274649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:14.274823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:15.275376      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:16.276133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:17.276753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:18.277010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:19.277959      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:20.278646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:21.279347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:22.279872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:23.280168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:24.280452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:25.281163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:26.281516      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:27.282488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:28.282654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:29.283662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:30.283841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:31.284799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:32.285061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:33.285408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:34.285716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:35.285874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:36.286634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:37.287191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:38.287454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:39.288398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:40.288570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:41.289100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:42.289376      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:43.290249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:44.290513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:45.291682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:46.292613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:47.292776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:48.293038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:49.293513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:50.294638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:51.295660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:52.295920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:53.296434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:54.296574      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:55.297495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:56.298388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:57.299161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:58.299442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:12:59.299968      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:00.300239      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:01.300447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:02.300733      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:03.300850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:04.301041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:05.301214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:06.301507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:07.302640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:08.302806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:09.303421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:10.303908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:11.304804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:12.305063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:13.305317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:14.306426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:15.307047      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:16.307783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:17.307952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:18.308306      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:19.308489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:20.308648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:21.308761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:22.308915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:23.309083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:24.309349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:25.310005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:26.310636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:27.311577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:28.311846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:29.312488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:30.312753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:31.313839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:32.314132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:33.314600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:34.314863      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:35.315978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:36.316743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:37.317395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:38.317531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:39.317851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:40.318107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:41.318139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:42.318409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:43.318574      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:44.318753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:45.319330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:46.320377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:47.320835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:48.321096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:49.321864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:50.322123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:51.322814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:52.323079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:53.323236      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:54.323501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:55.324062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:56.324825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:57.325477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:58.325736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:13:59.325933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:00.326672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:01.327257      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:02.327801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:03.328297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:04.328580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:05.328971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:06.329356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:07.329971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:08.330647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:09.331380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:10.331555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:11.331668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:12.331944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:13.332200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:14.332467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:15.332627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:16.333507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:17.334168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:18.334447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:19.334621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:20.334896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:21.335317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:22.335606      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:23.336063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:24.336340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:25.337472      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:26.338420      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:27.338595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:28.338855      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:29.339407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:30.339675      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:31.340252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:32.340421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:33.340716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:34.340763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:35.341344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:36.342303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:37.342942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:38.343207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:39.344275      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:40.344447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:41.345459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:42.345732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:43.346640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:44.346899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:45.347074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:46.347839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:47.348064      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:48.348326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:49.349481      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:50.349743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:51.350643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:52.350807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:53.350967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:54.351228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:55.351431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:56.352275      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:57.352403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:58.352690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:14:59.353224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:00.353439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:01.353535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:02.354651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:03.355062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:04.355189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:05.355346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:06.355995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:07.356491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:08.356768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:09.357741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:10.358029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:11.358844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:12.359121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:13.359698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:14.359967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:15.360499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:16.361383      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:17.361561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:18.362631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:19.362803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:20.363086      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:21.363995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:22.364359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:23.364786      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:24.364956      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:25.365511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:26.366428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:27.366959      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:28.367242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:29.367568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:30.367735      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:31.368828      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:32.369128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:33.369511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:34.369605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:35.370395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:36.371364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:37.371964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:38.372224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:39.372417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:40.372684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:41.373311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:42.373492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:43.374187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:44.374488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:45.374660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:46.375562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:47.376240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:48.376986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:49.377510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:50.377798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:51.378328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:52.378613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:53.379137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:54.379456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:55.380260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:56.380813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:57.381529      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:58.381828      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:15:59.382518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:00.382798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:01.383894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:02.384066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:03.384841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:04.385162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:05.385589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:06.385639      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:07.386630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:08.386911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:09.387462      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:10.387762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:11.388843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:12.389132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:13.389439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:14.389720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:15.389806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:16.390645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:17.390803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:18.391076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:19.391728      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:20.391998      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:21.392930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:22.393194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:23.393513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:24.394650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:25.394890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:26.395747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:27.395909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:28.396199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:29.396768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:30.397330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:31.398468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:32.398633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:33.399016      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:34.399281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:35.399459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:36.400439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:37.401507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:38.401771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:39.402635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:40.402891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:41.403856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:42.404023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:43.404201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:44.404490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:45.405521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:46.406017      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:47.406583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:48.407427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/08/23 13:16:48.605
  STEP: Removing cronjob @ 05/08/23 13:16:48.606
  May  8 13:16:48.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3479" for this suite. @ 05/08/23 13:16:48.612
• [300.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/08/23 13:16:48.617
  May  8 13:16:48.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename dns @ 05/08/23 13:16:48.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:16:48.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:16:48.638
  STEP: Creating a test headless service @ 05/08/23 13:16:48.64
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5860 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5860;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5860 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5860;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5860.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5860.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5860.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5860.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5860.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5860.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5860.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5860.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5860.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5860.svc;check="$$(dig +notcp +noall +answer +search 156.200.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.200.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.200.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.200.156_tcp@PTR;sleep 1; done
   @ 05/08/23 13:16:48.652
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5860 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5860;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5860 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5860;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5860.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5860.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5860.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5860.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5860.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5860.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5860.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5860.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5860.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5860.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5860.svc;check="$$(dig +notcp +noall +answer +search 156.200.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.200.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.200.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.200.156_tcp@PTR;sleep 1; done
   @ 05/08/23 13:16:48.652
  STEP: creating a pod to probe DNS @ 05/08/23 13:16:48.652
  STEP: submitting the pod to kubernetes @ 05/08/23 13:16:48.652
  E0508 13:16:49.408207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:50.408385      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/08/23 13:16:50.668
  STEP: looking for the results for each expected name from probers @ 05/08/23 13:16:50.67
  May  8 13:16:50.675: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.678: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.680: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.683: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.685: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.687: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.690: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.693: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.706: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.708: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.711: INFO: Unable to read jessie_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.713: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.716: INFO: Unable to read jessie_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.723: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.725: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:50.735: INFO: Lookups using dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5860 wheezy_tcp@dns-test-service.dns-5860 wheezy_udp@dns-test-service.dns-5860.svc wheezy_tcp@dns-test-service.dns-5860.svc wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5860 jessie_tcp@dns-test-service.dns-5860 jessie_udp@dns-test-service.dns-5860.svc jessie_tcp@dns-test-service.dns-5860.svc jessie_udp@_http._tcp.dns-test-service.dns-5860.svc jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc]

  E0508 13:16:51.408571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:52.408872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:53.409338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:54.409650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:55.409824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:16:55.740: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.748: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.751: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.756: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.758: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.771: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.774: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.776: INFO: Unable to read jessie_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.779: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.782: INFO: Unable to read jessie_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.785: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.788: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.790: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:16:55.801: INFO: Lookups using dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5860 wheezy_tcp@dns-test-service.dns-5860 wheezy_udp@dns-test-service.dns-5860.svc wheezy_tcp@dns-test-service.dns-5860.svc wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5860 jessie_tcp@dns-test-service.dns-5860 jessie_udp@dns-test-service.dns-5860.svc jessie_tcp@dns-test-service.dns-5860.svc jessie_udp@_http._tcp.dns-test-service.dns-5860.svc jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc]

  E0508 13:16:56.410390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:57.410538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:58.410836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:16:59.410973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:00.411247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:00.740: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.748: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.756: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.758: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.771: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.773: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.776: INFO: Unable to read jessie_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.778: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.781: INFO: Unable to read jessie_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.783: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.786: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.788: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:00.798: INFO: Lookups using dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5860 wheezy_tcp@dns-test-service.dns-5860 wheezy_udp@dns-test-service.dns-5860.svc wheezy_tcp@dns-test-service.dns-5860.svc wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5860 jessie_tcp@dns-test-service.dns-5860 jessie_udp@dns-test-service.dns-5860.svc jessie_tcp@dns-test-service.dns-5860.svc jessie_udp@_http._tcp.dns-test-service.dns-5860.svc jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc]

  E0508 13:17:01.411891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:02.412057      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:03.412334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:04.412502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:05.412675      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:05.739: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.742: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.748: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.755: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.758: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.771: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.774: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.776: INFO: Unable to read jessie_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.779: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.782: INFO: Unable to read jessie_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.784: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.787: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.789: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:05.800: INFO: Lookups using dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5860 wheezy_tcp@dns-test-service.dns-5860 wheezy_udp@dns-test-service.dns-5860.svc wheezy_tcp@dns-test-service.dns-5860.svc wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5860 jessie_tcp@dns-test-service.dns-5860 jessie_udp@dns-test-service.dns-5860.svc jessie_tcp@dns-test-service.dns-5860.svc jessie_udp@_http._tcp.dns-test-service.dns-5860.svc jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc]

  E0508 13:17:06.413679      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:07.413958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:08.414262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:09.414545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:10.414701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:10.740: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.748: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.755: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.758: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.771: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.773: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.776: INFO: Unable to read jessie_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.778: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.781: INFO: Unable to read jessie_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.784: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.786: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.789: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:10.799: INFO: Lookups using dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5860 wheezy_tcp@dns-test-service.dns-5860 wheezy_udp@dns-test-service.dns-5860.svc wheezy_tcp@dns-test-service.dns-5860.svc wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5860 jessie_tcp@dns-test-service.dns-5860 jessie_udp@dns-test-service.dns-5860.svc jessie_tcp@dns-test-service.dns-5860.svc jessie_udp@_http._tcp.dns-test-service.dns-5860.svc jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc]

  E0508 13:17:11.415300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:12.415597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:13.415934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:14.416212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:15.416547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:15.739: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.742: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.747: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.752: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.755: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.757: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.770: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.773: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.775: INFO: Unable to read jessie_udp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.778: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860 from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.781: INFO: Unable to read jessie_udp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.783: INFO: Unable to read jessie_tcp@dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.786: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.788: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc from pod dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40: the server could not find the requested resource (get pods dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40)
  May  8 13:17:15.798: INFO: Lookups using dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5860 wheezy_tcp@dns-test-service.dns-5860 wheezy_udp@dns-test-service.dns-5860.svc wheezy_tcp@dns-test-service.dns-5860.svc wheezy_udp@_http._tcp.dns-test-service.dns-5860.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5860.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5860 jessie_tcp@dns-test-service.dns-5860 jessie_udp@dns-test-service.dns-5860.svc jessie_tcp@dns-test-service.dns-5860.svc jessie_udp@_http._tcp.dns-test-service.dns-5860.svc jessie_tcp@_http._tcp.dns-test-service.dns-5860.svc]

  E0508 13:17:16.416747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:17.417028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:18.417320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:19.417629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:20.417777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:20.799: INFO: DNS probes using dns-5860/dns-test-737d8c3c-efec-4d78-b3d2-1ae57b6a0e40 succeeded

  May  8 13:17:20.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/08/23 13:17:20.801
  STEP: deleting the test service @ 05/08/23 13:17:20.812
  STEP: deleting the test headless service @ 05/08/23 13:17:20.829
  STEP: Destroying namespace "dns-5860" for this suite. @ 05/08/23 13:17:20.837
• [32.225 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/08/23 13:17:20.843
  May  8 13:17:20.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename security-context-test @ 05/08/23 13:17:20.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:17:20.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:17:20.854
  E0508 13:17:21.418156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:22.418781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:23.419592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:24.419876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7465" for this suite. @ 05/08/23 13:17:24.873
• [4.035 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/08/23 13:17:24.879
  May  8 13:17:24.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 13:17:24.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:17:24.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:17:24.889
  STEP: Creating a pod to test downward API volume plugin @ 05/08/23 13:17:24.891
  E0508 13:17:25.420674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:26.420769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:27.420913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:28.421229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/08/23 13:17:28.905
  May  8 13:17:28.906: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-96706d08-7d5c-4eeb-85ad-92b3cb30c041 container client-container: <nil>
  STEP: delete the pod @ 05/08/23 13:17:28.92
  May  8 13:17:28.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5638" for this suite. @ 05/08/23 13:17:28.933
• [4.058 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/08/23 13:17:28.937
  May  8 13:17:28.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename svc-latency @ 05/08/23 13:17:28.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:17:28.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:17:28.947
  May  8 13:17:28.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-9136 @ 05/08/23 13:17:28.95
  I0508 13:17:28.953954      23 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9136, replica count: 1
  E0508 13:17:29.422151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0508 13:17:30.006041      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0508 13:17:30.423301      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0508 13:17:31.006312      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  8 13:17:31.129: INFO: Created: latency-svc-sx9v5
  May  8 13:17:31.134: INFO: Got endpoints: latency-svc-sx9v5 [27.417004ms]
  May  8 13:17:31.143: INFO: Created: latency-svc-tt65f
  May  8 13:17:31.150: INFO: Got endpoints: latency-svc-tt65f [16.574362ms]
  May  8 13:17:31.152: INFO: Created: latency-svc-76nkl
  May  8 13:17:31.154: INFO: Got endpoints: latency-svc-76nkl [20.04702ms]
  May  8 13:17:31.161: INFO: Created: latency-svc-j2dqr
  May  8 13:17:31.164: INFO: Got endpoints: latency-svc-j2dqr [29.565985ms]
  May  8 13:17:31.170: INFO: Created: latency-svc-2bdc6
  May  8 13:17:31.176: INFO: Got endpoints: latency-svc-2bdc6 [42.328515ms]
  May  8 13:17:31.178: INFO: Created: latency-svc-28nkp
  May  8 13:17:31.184: INFO: Got endpoints: latency-svc-28nkp [50.10359ms]
  May  8 13:17:31.186: INFO: Created: latency-svc-744jh
  May  8 13:17:31.194: INFO: Got endpoints: latency-svc-744jh [59.525935ms]
  May  8 13:17:31.197: INFO: Created: latency-svc-v8wxb
  May  8 13:17:31.199: INFO: Got endpoints: latency-svc-v8wxb [65.02042ms]
  May  8 13:17:31.208: INFO: Created: latency-svc-jc6jb
  May  8 13:17:31.212: INFO: Got endpoints: latency-svc-jc6jb [77.63507ms]
  May  8 13:17:31.215: INFO: Created: latency-svc-nmhf8
  May  8 13:17:31.216: INFO: Got endpoints: latency-svc-nmhf8 [81.757133ms]
  May  8 13:17:31.224: INFO: Created: latency-svc-tdv5p
  May  8 13:17:31.227: INFO: Got endpoints: latency-svc-tdv5p [92.158088ms]
  May  8 13:17:31.238: INFO: Created: latency-svc-4725l
  May  8 13:17:31.244: INFO: Got endpoints: latency-svc-4725l [109.258956ms]
  May  8 13:17:31.244: INFO: Created: latency-svc-7mgbb
  May  8 13:17:31.252: INFO: Got endpoints: latency-svc-7mgbb [117.206065ms]
  May  8 13:17:31.255: INFO: Created: latency-svc-b6gzv
  May  8 13:17:31.260: INFO: Got endpoints: latency-svc-b6gzv [125.178317ms]
  May  8 13:17:31.264: INFO: Created: latency-svc-gjb7b
  May  8 13:17:31.271: INFO: Got endpoints: latency-svc-gjb7b [136.163164ms]
  May  8 13:17:31.273: INFO: Created: latency-svc-wl79j
  May  8 13:17:31.282: INFO: Got endpoints: latency-svc-wl79j [148.271073ms]
  May  8 13:17:31.284: INFO: Created: latency-svc-x65nw
  May  8 13:17:31.287: INFO: Got endpoints: latency-svc-x65nw [136.867985ms]
  May  8 13:17:31.295: INFO: Created: latency-svc-2l8mk
  May  8 13:17:31.298: INFO: Got endpoints: latency-svc-2l8mk [144.241158ms]
  May  8 13:17:31.301: INFO: Created: latency-svc-cz8jf
  May  8 13:17:31.304: INFO: Got endpoints: latency-svc-cz8jf [140.370548ms]
  May  8 13:17:31.311: INFO: Created: latency-svc-tqnzr
  May  8 13:17:31.317: INFO: Got endpoints: latency-svc-tqnzr [140.606802ms]
  May  8 13:17:31.320: INFO: Created: latency-svc-kzwq7
  May  8 13:17:31.323: INFO: Got endpoints: latency-svc-kzwq7 [138.42375ms]
  May  8 13:17:31.328: INFO: Created: latency-svc-58w9l
  May  8 13:17:31.331: INFO: Got endpoints: latency-svc-58w9l [137.478806ms]
  May  8 13:17:31.337: INFO: Created: latency-svc-gqzsc
  May  8 13:17:31.340: INFO: Got endpoints: latency-svc-gqzsc [141.099084ms]
  May  8 13:17:31.347: INFO: Created: latency-svc-97s9g
  May  8 13:17:31.350: INFO: Got endpoints: latency-svc-97s9g [138.171711ms]
  May  8 13:17:31.357: INFO: Created: latency-svc-pmj2g
  May  8 13:17:31.362: INFO: Got endpoints: latency-svc-pmj2g [146.013861ms]
  May  8 13:17:31.365: INFO: Created: latency-svc-7nsq6
  May  8 13:17:31.372: INFO: Got endpoints: latency-svc-7nsq6 [144.910856ms]
  May  8 13:17:31.375: INFO: Created: latency-svc-l9qrq
  May  8 13:17:31.390: INFO: Got endpoints: latency-svc-l9qrq [146.379752ms]
  May  8 13:17:31.392: INFO: Created: latency-svc-rvzkq
  May  8 13:17:31.397: INFO: Got endpoints: latency-svc-rvzkq [145.280611ms]
  May  8 13:17:31.403: INFO: Created: latency-svc-fdkfx
  May  8 13:17:31.408: INFO: Got endpoints: latency-svc-fdkfx [147.750228ms]
  May  8 13:17:31.413: INFO: Created: latency-svc-ngv64
  May  8 13:17:31.421: INFO: Got endpoints: latency-svc-ngv64 [149.858251ms]
  E0508 13:17:31.423295      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:31.423: INFO: Created: latency-svc-nlkl5
  May  8 13:17:31.430: INFO: Got endpoints: latency-svc-nlkl5 [148.232188ms]
  May  8 13:17:31.433: INFO: Created: latency-svc-6pqkv
  May  8 13:17:31.440: INFO: Got endpoints: latency-svc-6pqkv [152.490302ms]
  May  8 13:17:31.445: INFO: Created: latency-svc-r4hlg
  May  8 13:17:31.449: INFO: Got endpoints: latency-svc-r4hlg [151.083603ms]
  May  8 13:17:31.453: INFO: Created: latency-svc-7t5nc
  May  8 13:17:31.460: INFO: Got endpoints: latency-svc-7t5nc [155.892229ms]
  May  8 13:17:31.463: INFO: Created: latency-svc-lss5f
  May  8 13:17:31.467: INFO: Got endpoints: latency-svc-lss5f [149.962999ms]
  May  8 13:17:31.475: INFO: Created: latency-svc-72jdn
  May  8 13:17:31.481: INFO: Got endpoints: latency-svc-72jdn [158.094313ms]
  May  8 13:17:31.484: INFO: Created: latency-svc-49n8b
  May  8 13:17:31.485: INFO: Got endpoints: latency-svc-49n8b [153.874162ms]
  May  8 13:17:31.501: INFO: Created: latency-svc-4jt5t
  May  8 13:17:31.509: INFO: Created: latency-svc-zwcsc
  May  8 13:17:31.519: INFO: Created: latency-svc-knqhn
  May  8 13:17:31.527: INFO: Created: latency-svc-zb9dg
  May  8 13:17:31.535: INFO: Created: latency-svc-rdcnw
  May  8 13:17:31.535: INFO: Got endpoints: latency-svc-4jt5t [194.922123ms]
  May  8 13:17:31.540: INFO: Created: latency-svc-95j5g
  May  8 13:17:31.550: INFO: Created: latency-svc-l8fbg
  May  8 13:17:31.558: INFO: Created: latency-svc-4db6z
  May  8 13:17:31.567: INFO: Created: latency-svc-gzc8m
  May  8 13:17:31.575: INFO: Created: latency-svc-rvdx2
  May  8 13:17:31.587: INFO: Got endpoints: latency-svc-zwcsc [236.182975ms]
  May  8 13:17:31.590: INFO: Created: latency-svc-vqjs4
  May  8 13:17:31.597: INFO: Created: latency-svc-c9g7n
  May  8 13:17:31.608: INFO: Created: latency-svc-jcw7q
  May  8 13:17:31.615: INFO: Created: latency-svc-4x4jt
  May  8 13:17:31.623: INFO: Created: latency-svc-2lvvv
  May  8 13:17:31.631: INFO: Created: latency-svc-jrwqg
  May  8 13:17:31.637: INFO: Got endpoints: latency-svc-knqhn [274.936962ms]
  May  8 13:17:31.640: INFO: Created: latency-svc-r774x
  May  8 13:17:31.648: INFO: Created: latency-svc-bbktb
  May  8 13:17:31.685: INFO: Got endpoints: latency-svc-zb9dg [313.556161ms]
  May  8 13:17:31.697: INFO: Created: latency-svc-wdwdf
  May  8 13:17:31.733: INFO: Got endpoints: latency-svc-rdcnw [343.393084ms]
  May  8 13:17:31.744: INFO: Created: latency-svc-frfpd
  May  8 13:17:31.784: INFO: Got endpoints: latency-svc-95j5g [387.217888ms]
  May  8 13:17:31.796: INFO: Created: latency-svc-chwkr
  May  8 13:17:31.835: INFO: Got endpoints: latency-svc-l8fbg [427.153832ms]
  May  8 13:17:31.846: INFO: Created: latency-svc-bhxt9
  May  8 13:17:31.883: INFO: Got endpoints: latency-svc-4db6z [461.905864ms]
  May  8 13:17:31.894: INFO: Created: latency-svc-8pmj5
  May  8 13:17:31.935: INFO: Got endpoints: latency-svc-gzc8m [503.968671ms]
  May  8 13:17:31.957: INFO: Created: latency-svc-s2wds
  May  8 13:17:31.986: INFO: Got endpoints: latency-svc-rvdx2 [545.663194ms]
  May  8 13:17:31.999: INFO: Created: latency-svc-4s6gz
  May  8 13:17:32.034: INFO: Got endpoints: latency-svc-vqjs4 [584.347081ms]
  May  8 13:17:32.044: INFO: Created: latency-svc-t75nb
  May  8 13:17:32.084: INFO: Got endpoints: latency-svc-c9g7n [624.455733ms]
  May  8 13:17:32.096: INFO: Created: latency-svc-h6hdt
  May  8 13:17:32.138: INFO: Got endpoints: latency-svc-jcw7q [670.740041ms]
  May  8 13:17:32.148: INFO: Created: latency-svc-946dn
  May  8 13:17:32.183: INFO: Got endpoints: latency-svc-4x4jt [701.801334ms]
  May  8 13:17:32.195: INFO: Created: latency-svc-t8vds
  May  8 13:17:32.234: INFO: Got endpoints: latency-svc-2lvvv [748.943574ms]
  May  8 13:17:32.246: INFO: Created: latency-svc-tglbn
  May  8 13:17:32.284: INFO: Got endpoints: latency-svc-jrwqg [748.92136ms]
  May  8 13:17:32.296: INFO: Created: latency-svc-f8nj9
  May  8 13:17:32.333: INFO: Got endpoints: latency-svc-r774x [746.774842ms]
  May  8 13:17:32.343: INFO: Created: latency-svc-hcbfs
  May  8 13:17:32.383: INFO: Got endpoints: latency-svc-bbktb [746.099441ms]
  May  8 13:17:32.393: INFO: Created: latency-svc-cczzc
  E0508 13:17:32.423826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:32.433: INFO: Got endpoints: latency-svc-wdwdf [748.04453ms]
  May  8 13:17:32.443: INFO: Created: latency-svc-v8gnk
  May  8 13:17:32.483: INFO: Got endpoints: latency-svc-frfpd [749.431021ms]
  May  8 13:17:32.495: INFO: Created: latency-svc-t9pqq
  May  8 13:17:32.533: INFO: Got endpoints: latency-svc-chwkr [748.270113ms]
  May  8 13:17:32.543: INFO: Created: latency-svc-4xmhz
  May  8 13:17:32.584: INFO: Got endpoints: latency-svc-bhxt9 [749.505909ms]
  May  8 13:17:32.594: INFO: Created: latency-svc-fcx54
  May  8 13:17:32.633: INFO: Got endpoints: latency-svc-8pmj5 [750.021653ms]
  May  8 13:17:32.643: INFO: Created: latency-svc-sjblh
  May  8 13:17:32.685: INFO: Got endpoints: latency-svc-s2wds [750.529499ms]
  May  8 13:17:32.695: INFO: Created: latency-svc-842hh
  May  8 13:17:32.733: INFO: Got endpoints: latency-svc-4s6gz [747.6549ms]
  May  8 13:17:32.743: INFO: Created: latency-svc-khssx
  May  8 13:17:32.785: INFO: Got endpoints: latency-svc-t75nb [750.875467ms]
  May  8 13:17:32.795: INFO: Created: latency-svc-wvs2f
  May  8 13:17:32.835: INFO: Got endpoints: latency-svc-h6hdt [750.248081ms]
  May  8 13:17:32.847: INFO: Created: latency-svc-7hhgx
  May  8 13:17:32.883: INFO: Got endpoints: latency-svc-946dn [745.160271ms]
  May  8 13:17:32.894: INFO: Created: latency-svc-8l9lb
  May  8 13:17:32.933: INFO: Got endpoints: latency-svc-t8vds [750.364784ms]
  May  8 13:17:32.945: INFO: Created: latency-svc-7nwbp
  May  8 13:17:32.983: INFO: Got endpoints: latency-svc-tglbn [749.078892ms]
  May  8 13:17:32.997: INFO: Created: latency-svc-c4lm2
  May  8 13:17:33.032: INFO: Got endpoints: latency-svc-f8nj9 [747.882997ms]
  May  8 13:17:33.046: INFO: Created: latency-svc-g4k5p
  May  8 13:17:33.083: INFO: Got endpoints: latency-svc-hcbfs [749.764009ms]
  May  8 13:17:33.099: INFO: Created: latency-svc-rrkpr
  May  8 13:17:33.133: INFO: Got endpoints: latency-svc-cczzc [749.833844ms]
  May  8 13:17:33.143: INFO: Created: latency-svc-j7xlb
  May  8 13:17:33.182: INFO: Got endpoints: latency-svc-v8gnk [749.004845ms]
  May  8 13:17:33.193: INFO: Created: latency-svc-w9qrp
  May  8 13:17:33.233: INFO: Got endpoints: latency-svc-t9pqq [749.896742ms]
  May  8 13:17:33.243: INFO: Created: latency-svc-9m4jm
  May  8 13:17:33.285: INFO: Got endpoints: latency-svc-4xmhz [752.258033ms]
  May  8 13:17:33.296: INFO: Created: latency-svc-skvpt
  May  8 13:17:33.334: INFO: Got endpoints: latency-svc-fcx54 [749.370548ms]
  May  8 13:17:33.345: INFO: Created: latency-svc-qr5nr
  May  8 13:17:33.383: INFO: Got endpoints: latency-svc-sjblh [749.707419ms]
  May  8 13:17:33.392: INFO: Created: latency-svc-tdm7b
  E0508 13:17:33.424842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:33.434: INFO: Got endpoints: latency-svc-842hh [748.677227ms]
  May  8 13:17:33.447: INFO: Created: latency-svc-m88ww
  May  8 13:17:33.484: INFO: Got endpoints: latency-svc-khssx [750.707253ms]
  May  8 13:17:33.494: INFO: Created: latency-svc-mglp9
  May  8 13:17:33.533: INFO: Got endpoints: latency-svc-wvs2f [748.160854ms]
  May  8 13:17:33.543: INFO: Created: latency-svc-pfl8t
  May  8 13:17:33.585: INFO: Got endpoints: latency-svc-7hhgx [750.002868ms]
  May  8 13:17:33.595: INFO: Created: latency-svc-sjn5z
  May  8 13:17:33.635: INFO: Got endpoints: latency-svc-8l9lb [751.977954ms]
  May  8 13:17:33.649: INFO: Created: latency-svc-bl6gf
  May  8 13:17:33.683: INFO: Got endpoints: latency-svc-7nwbp [749.453529ms]
  May  8 13:17:33.693: INFO: Created: latency-svc-tfbxm
  May  8 13:17:33.736: INFO: Got endpoints: latency-svc-c4lm2 [752.735023ms]
  May  8 13:17:33.748: INFO: Created: latency-svc-shglr
  May  8 13:17:33.783: INFO: Got endpoints: latency-svc-g4k5p [750.563863ms]
  May  8 13:17:33.792: INFO: Created: latency-svc-rm9ml
  May  8 13:17:33.832: INFO: Got endpoints: latency-svc-rrkpr [749.029319ms]
  May  8 13:17:33.844: INFO: Created: latency-svc-bp59g
  May  8 13:17:33.883: INFO: Got endpoints: latency-svc-j7xlb [749.515532ms]
  May  8 13:17:33.894: INFO: Created: latency-svc-pfngb
  May  8 13:17:33.932: INFO: Got endpoints: latency-svc-w9qrp [749.931359ms]
  May  8 13:17:33.950: INFO: Created: latency-svc-gj5gg
  May  8 13:17:33.985: INFO: Got endpoints: latency-svc-9m4jm [752.374336ms]
  May  8 13:17:33.996: INFO: Created: latency-svc-6zdzf
  May  8 13:17:34.034: INFO: Got endpoints: latency-svc-skvpt [749.146858ms]
  May  8 13:17:34.047: INFO: Created: latency-svc-th4v4
  May  8 13:17:34.085: INFO: Got endpoints: latency-svc-qr5nr [751.07972ms]
  May  8 13:17:34.094: INFO: Created: latency-svc-7mtlz
  May  8 13:17:34.133: INFO: Got endpoints: latency-svc-tdm7b [750.590473ms]
  May  8 13:17:34.143: INFO: Created: latency-svc-j2nv6
  May  8 13:17:34.184: INFO: Got endpoints: latency-svc-m88ww [750.584133ms]
  May  8 13:17:34.194: INFO: Created: latency-svc-nzqwr
  May  8 13:17:34.235: INFO: Got endpoints: latency-svc-mglp9 [750.534828ms]
  May  8 13:17:34.249: INFO: Created: latency-svc-6j2vc
  May  8 13:17:34.282: INFO: Got endpoints: latency-svc-pfl8t [749.345398ms]
  May  8 13:17:34.293: INFO: Created: latency-svc-5dn8f
  May  8 13:17:34.335: INFO: Got endpoints: latency-svc-sjn5z [749.858073ms]
  May  8 13:17:34.352: INFO: Created: latency-svc-57wtg
  May  8 13:17:34.383: INFO: Got endpoints: latency-svc-bl6gf [747.944994ms]
  May  8 13:17:34.394: INFO: Created: latency-svc-8dvnb
  E0508 13:17:34.425370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:34.432: INFO: Got endpoints: latency-svc-tfbxm [749.675268ms]
  May  8 13:17:34.443: INFO: Created: latency-svc-q78wd
  May  8 13:17:34.484: INFO: Got endpoints: latency-svc-shglr [747.652931ms]
  May  8 13:17:34.494: INFO: Created: latency-svc-crj79
  May  8 13:17:34.533: INFO: Got endpoints: latency-svc-rm9ml [750.181406ms]
  May  8 13:17:34.545: INFO: Created: latency-svc-xhvvp
  May  8 13:17:34.589: INFO: Got endpoints: latency-svc-bp59g [756.666131ms]
  May  8 13:17:34.599: INFO: Created: latency-svc-44gpp
  May  8 13:17:34.635: INFO: Got endpoints: latency-svc-pfngb [751.821571ms]
  May  8 13:17:34.646: INFO: Created: latency-svc-xgzvv
  May  8 13:17:34.685: INFO: Got endpoints: latency-svc-gj5gg [752.879277ms]
  May  8 13:17:34.701: INFO: Created: latency-svc-bmgg8
  May  8 13:17:34.733: INFO: Got endpoints: latency-svc-6zdzf [747.646951ms]
  May  8 13:17:34.757: INFO: Created: latency-svc-h4dv7
  May  8 13:17:34.787: INFO: Got endpoints: latency-svc-th4v4 [752.962854ms]
  May  8 13:17:34.802: INFO: Created: latency-svc-x87sf
  May  8 13:17:34.834: INFO: Got endpoints: latency-svc-7mtlz [748.709759ms]
  May  8 13:17:34.847: INFO: Created: latency-svc-gn4w9
  May  8 13:17:34.883: INFO: Got endpoints: latency-svc-j2nv6 [750.220419ms]
  May  8 13:17:34.894: INFO: Created: latency-svc-fc2gb
  May  8 13:17:34.934: INFO: Got endpoints: latency-svc-nzqwr [749.83047ms]
  May  8 13:17:34.944: INFO: Created: latency-svc-lm42p
  May  8 13:17:34.983: INFO: Got endpoints: latency-svc-6j2vc [748.085068ms]
  May  8 13:17:34.993: INFO: Created: latency-svc-g2xkl
  May  8 13:17:35.034: INFO: Got endpoints: latency-svc-5dn8f [751.425497ms]
  May  8 13:17:35.043: INFO: Created: latency-svc-bcqj7
  May  8 13:17:35.083: INFO: Got endpoints: latency-svc-57wtg [748.437087ms]
  May  8 13:17:35.093: INFO: Created: latency-svc-fr8gw
  May  8 13:17:35.134: INFO: Got endpoints: latency-svc-8dvnb [750.835531ms]
  May  8 13:17:35.145: INFO: Created: latency-svc-s8dmb
  May  8 13:17:35.183: INFO: Got endpoints: latency-svc-q78wd [750.916718ms]
  May  8 13:17:35.197: INFO: Created: latency-svc-tmlk9
  May  8 13:17:35.234: INFO: Got endpoints: latency-svc-crj79 [750.226283ms]
  May  8 13:17:35.243: INFO: Created: latency-svc-xctzn
  May  8 13:17:35.282: INFO: Got endpoints: latency-svc-xhvvp [749.159317ms]
  May  8 13:17:35.294: INFO: Created: latency-svc-8tz7s
  May  8 13:17:35.336: INFO: Got endpoints: latency-svc-44gpp [747.017603ms]
  May  8 13:17:35.346: INFO: Created: latency-svc-khcfn
  May  8 13:17:35.382: INFO: Got endpoints: latency-svc-xgzvv [747.733939ms]
  May  8 13:17:35.396: INFO: Created: latency-svc-77f89
  E0508 13:17:35.426486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:35.433: INFO: Got endpoints: latency-svc-bmgg8 [747.179441ms]
  May  8 13:17:35.447: INFO: Created: latency-svc-w4vd4
  May  8 13:17:35.483: INFO: Got endpoints: latency-svc-h4dv7 [750.14436ms]
  May  8 13:17:35.493: INFO: Created: latency-svc-525mk
  May  8 13:17:35.535: INFO: Got endpoints: latency-svc-x87sf [748.112116ms]
  May  8 13:17:35.550: INFO: Created: latency-svc-vd4hd
  May  8 13:17:35.583: INFO: Got endpoints: latency-svc-gn4w9 [749.439305ms]
  May  8 13:17:35.595: INFO: Created: latency-svc-gz274
  May  8 13:17:35.633: INFO: Got endpoints: latency-svc-fc2gb [749.604321ms]
  May  8 13:17:35.649: INFO: Created: latency-svc-jx89x
  May  8 13:17:35.683: INFO: Got endpoints: latency-svc-lm42p [748.46584ms]
  May  8 13:17:35.693: INFO: Created: latency-svc-b9v7q
  May  8 13:17:35.733: INFO: Got endpoints: latency-svc-g2xkl [750.634697ms]
  May  8 13:17:35.743: INFO: Created: latency-svc-4df9b
  May  8 13:17:35.783: INFO: Got endpoints: latency-svc-bcqj7 [748.917782ms]
  May  8 13:17:35.795: INFO: Created: latency-svc-kjmd5
  May  8 13:17:35.833: INFO: Got endpoints: latency-svc-fr8gw [750.268486ms]
  May  8 13:17:35.846: INFO: Created: latency-svc-l6b44
  May  8 13:17:35.885: INFO: Got endpoints: latency-svc-s8dmb [750.988063ms]
  May  8 13:17:35.895: INFO: Created: latency-svc-tjffq
  May  8 13:17:35.932: INFO: Got endpoints: latency-svc-tmlk9 [749.012726ms]
  May  8 13:17:35.942: INFO: Created: latency-svc-lgbv4
  May  8 13:17:35.982: INFO: Got endpoints: latency-svc-xctzn [748.342764ms]
  May  8 13:17:35.992: INFO: Created: latency-svc-qblfg
  May  8 13:17:36.035: INFO: Got endpoints: latency-svc-8tz7s [752.813659ms]
  May  8 13:17:36.045: INFO: Created: latency-svc-fspbk
  May  8 13:17:36.083: INFO: Got endpoints: latency-svc-khcfn [747.060281ms]
  May  8 13:17:36.093: INFO: Created: latency-svc-5hccn
  May  8 13:17:36.137: INFO: Got endpoints: latency-svc-77f89 [754.791473ms]
  May  8 13:17:36.147: INFO: Created: latency-svc-7htqx
  May  8 13:17:36.186: INFO: Got endpoints: latency-svc-w4vd4 [753.401717ms]
  May  8 13:17:36.197: INFO: Created: latency-svc-nwjdf
  May  8 13:17:36.233: INFO: Got endpoints: latency-svc-525mk [749.974408ms]
  May  8 13:17:36.244: INFO: Created: latency-svc-45hph
  May  8 13:17:36.285: INFO: Got endpoints: latency-svc-vd4hd [750.018385ms]
  May  8 13:17:36.300: INFO: Created: latency-svc-tq2pg
  May  8 13:17:36.333: INFO: Got endpoints: latency-svc-gz274 [750.139797ms]
  May  8 13:17:36.344: INFO: Created: latency-svc-gn7cx
  May  8 13:17:36.384: INFO: Got endpoints: latency-svc-jx89x [750.953691ms]
  May  8 13:17:36.398: INFO: Created: latency-svc-w28l2
  E0508 13:17:36.427178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:36.435: INFO: Got endpoints: latency-svc-b9v7q [752.522283ms]
  May  8 13:17:36.445: INFO: Created: latency-svc-mhngj
  May  8 13:17:36.484: INFO: Got endpoints: latency-svc-4df9b [750.121594ms]
  May  8 13:17:36.495: INFO: Created: latency-svc-l5n58
  May  8 13:17:36.533: INFO: Got endpoints: latency-svc-kjmd5 [750.313581ms]
  May  8 13:17:36.544: INFO: Created: latency-svc-r85cq
  May  8 13:17:36.585: INFO: Got endpoints: latency-svc-l6b44 [751.850158ms]
  May  8 13:17:36.596: INFO: Created: latency-svc-zhxnk
  May  8 13:17:36.634: INFO: Got endpoints: latency-svc-tjffq [748.863139ms]
  May  8 13:17:36.649: INFO: Created: latency-svc-qmm72
  May  8 13:17:36.683: INFO: Got endpoints: latency-svc-lgbv4 [751.050904ms]
  May  8 13:17:36.694: INFO: Created: latency-svc-8tcxj
  May  8 13:17:36.734: INFO: Got endpoints: latency-svc-qblfg [751.927894ms]
  May  8 13:17:36.744: INFO: Created: latency-svc-m4jpx
  May  8 13:17:36.783: INFO: Got endpoints: latency-svc-fspbk [747.912901ms]
  May  8 13:17:36.796: INFO: Created: latency-svc-l69bp
  May  8 13:17:36.833: INFO: Got endpoints: latency-svc-5hccn [750.358956ms]
  May  8 13:17:36.847: INFO: Created: latency-svc-kw482
  May  8 13:17:36.885: INFO: Got endpoints: latency-svc-7htqx [747.59725ms]
  May  8 13:17:36.895: INFO: Created: latency-svc-4vzvw
  May  8 13:17:36.933: INFO: Got endpoints: latency-svc-nwjdf [746.809862ms]
  May  8 13:17:36.946: INFO: Created: latency-svc-tbmwg
  May  8 13:17:36.983: INFO: Got endpoints: latency-svc-45hph [749.384602ms]
  May  8 13:17:36.993: INFO: Created: latency-svc-g27qm
  May  8 13:17:37.034: INFO: Got endpoints: latency-svc-tq2pg [748.953398ms]
  May  8 13:17:37.045: INFO: Created: latency-svc-xfnjm
  May  8 13:17:37.083: INFO: Got endpoints: latency-svc-gn7cx [749.892741ms]
  May  8 13:17:37.093: INFO: Created: latency-svc-jfwtv
  May  8 13:17:37.142: INFO: Got endpoints: latency-svc-w28l2 [758.05804ms]
  May  8 13:17:37.151: INFO: Created: latency-svc-8gzd2
  May  8 13:17:37.183: INFO: Got endpoints: latency-svc-mhngj [747.915801ms]
  May  8 13:17:37.193: INFO: Created: latency-svc-p4q42
  May  8 13:17:37.234: INFO: Got endpoints: latency-svc-l5n58 [749.999632ms]
  May  8 13:17:37.251: INFO: Created: latency-svc-9ks8g
  May  8 13:17:37.285: INFO: Got endpoints: latency-svc-r85cq [751.729712ms]
  May  8 13:17:37.295: INFO: Created: latency-svc-kdkpt
  May  8 13:17:37.333: INFO: Got endpoints: latency-svc-zhxnk [747.540459ms]
  May  8 13:17:37.344: INFO: Created: latency-svc-vq96r
  May  8 13:17:37.384: INFO: Got endpoints: latency-svc-qmm72 [749.678354ms]
  May  8 13:17:37.395: INFO: Created: latency-svc-f5dk9
  E0508 13:17:37.427913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:37.434: INFO: Got endpoints: latency-svc-8tcxj [750.383814ms]
  May  8 13:17:37.444: INFO: Created: latency-svc-svskb
  May  8 13:17:37.483: INFO: Got endpoints: latency-svc-m4jpx [748.750973ms]
  May  8 13:17:37.494: INFO: Created: latency-svc-sqc6m
  May  8 13:17:37.533: INFO: Got endpoints: latency-svc-l69bp [749.579065ms]
  May  8 13:17:37.544: INFO: Created: latency-svc-59pqn
  May  8 13:17:37.584: INFO: Got endpoints: latency-svc-kw482 [750.705413ms]
  May  8 13:17:37.595: INFO: Created: latency-svc-24p2j
  May  8 13:17:37.633: INFO: Got endpoints: latency-svc-4vzvw [748.500872ms]
  May  8 13:17:37.644: INFO: Created: latency-svc-24vwm
  May  8 13:17:37.684: INFO: Got endpoints: latency-svc-tbmwg [750.778686ms]
  May  8 13:17:37.694: INFO: Created: latency-svc-hlzkl
  May  8 13:17:37.734: INFO: Got endpoints: latency-svc-g27qm [751.458776ms]
  May  8 13:17:37.745: INFO: Created: latency-svc-rx88q
  May  8 13:17:37.784: INFO: Got endpoints: latency-svc-xfnjm [749.01834ms]
  May  8 13:17:37.795: INFO: Created: latency-svc-27pql
  May  8 13:17:37.833: INFO: Got endpoints: latency-svc-jfwtv [749.934297ms]
  May  8 13:17:37.844: INFO: Created: latency-svc-f2mp9
  May  8 13:17:37.884: INFO: Got endpoints: latency-svc-8gzd2 [742.076818ms]
  May  8 13:17:37.895: INFO: Created: latency-svc-lt4vt
  May  8 13:17:37.935: INFO: Got endpoints: latency-svc-p4q42 [751.655512ms]
  May  8 13:17:37.945: INFO: Created: latency-svc-9k6w7
  May  8 13:17:37.990: INFO: Got endpoints: latency-svc-9ks8g [756.260952ms]
  May  8 13:17:38.036: INFO: Got endpoints: latency-svc-kdkpt [750.655788ms]
  May  8 13:17:38.043: INFO: Created: latency-svc-v7d7d
  May  8 13:17:38.051: INFO: Created: latency-svc-7xw22
  May  8 13:17:38.084: INFO: Got endpoints: latency-svc-vq96r [751.024187ms]
  May  8 13:17:38.097: INFO: Created: latency-svc-qsm4s
  May  8 13:17:38.135: INFO: Got endpoints: latency-svc-f5dk9 [750.912013ms]
  May  8 13:17:38.144: INFO: Created: latency-svc-swzvs
  May  8 13:17:38.183: INFO: Got endpoints: latency-svc-svskb [749.074554ms]
  May  8 13:17:38.195: INFO: Created: latency-svc-6lh5m
  May  8 13:17:38.233: INFO: Got endpoints: latency-svc-sqc6m [749.749741ms]
  May  8 13:17:38.243: INFO: Created: latency-svc-2jqzt
  May  8 13:17:38.285: INFO: Got endpoints: latency-svc-59pqn [752.578717ms]
  May  8 13:17:38.295: INFO: Created: latency-svc-gdjxr
  May  8 13:17:38.333: INFO: Got endpoints: latency-svc-24p2j [749.062136ms]
  May  8 13:17:38.345: INFO: Created: latency-svc-sgv7s
  May  8 13:17:38.384: INFO: Got endpoints: latency-svc-24vwm [750.673392ms]
  May  8 13:17:38.394: INFO: Created: latency-svc-l9x7l
  E0508 13:17:38.428928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:38.435: INFO: Got endpoints: latency-svc-hlzkl [751.337632ms]
  May  8 13:17:38.445: INFO: Created: latency-svc-wch7s
  May  8 13:17:38.483: INFO: Got endpoints: latency-svc-rx88q [748.566248ms]
  May  8 13:17:38.492: INFO: Created: latency-svc-l5pxn
  May  8 13:17:38.534: INFO: Got endpoints: latency-svc-27pql [750.146219ms]
  May  8 13:17:38.543: INFO: Created: latency-svc-qjt8z
  May  8 13:17:38.583: INFO: Got endpoints: latency-svc-f2mp9 [749.299918ms]
  May  8 13:17:38.593: INFO: Created: latency-svc-l7jlh
  May  8 13:17:38.633: INFO: Got endpoints: latency-svc-lt4vt [748.740421ms]
  May  8 13:17:38.643: INFO: Created: latency-svc-wsp2l
  May  8 13:17:38.685: INFO: Got endpoints: latency-svc-9k6w7 [750.13029ms]
  May  8 13:17:38.695: INFO: Created: latency-svc-4hqh6
  May  8 13:17:38.733: INFO: Got endpoints: latency-svc-v7d7d [743.249976ms]
  May  8 13:17:38.743: INFO: Created: latency-svc-5twhx
  May  8 13:17:38.783: INFO: Got endpoints: latency-svc-7xw22 [747.462222ms]
  May  8 13:17:38.798: INFO: Created: latency-svc-swh8m
  May  8 13:17:38.835: INFO: Got endpoints: latency-svc-qsm4s [750.79412ms]
  May  8 13:17:38.845: INFO: Created: latency-svc-8m46s
  May  8 13:17:38.885: INFO: Got endpoints: latency-svc-swzvs [750.687605ms]
  May  8 13:17:38.895: INFO: Created: latency-svc-4c54v
  May  8 13:17:38.933: INFO: Got endpoints: latency-svc-6lh5m [750.048056ms]
  May  8 13:17:38.948: INFO: Created: latency-svc-hm77l
  May  8 13:17:38.984: INFO: Got endpoints: latency-svc-2jqzt [751.375618ms]
  May  8 13:17:39.033: INFO: Got endpoints: latency-svc-gdjxr [748.027564ms]
  May  8 13:17:39.084: INFO: Got endpoints: latency-svc-sgv7s [750.929656ms]
  May  8 13:17:39.133: INFO: Got endpoints: latency-svc-l9x7l [748.293683ms]
  May  8 13:17:39.184: INFO: Got endpoints: latency-svc-wch7s [749.172365ms]
  May  8 13:17:39.233: INFO: Got endpoints: latency-svc-l5pxn [749.909845ms]
  May  8 13:17:39.285: INFO: Got endpoints: latency-svc-qjt8z [750.887601ms]
  May  8 13:17:39.333: INFO: Got endpoints: latency-svc-l7jlh [750.276991ms]
  May  8 13:17:39.383: INFO: Got endpoints: latency-svc-wsp2l [750.081677ms]
  E0508 13:17:39.429783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:39.432: INFO: Got endpoints: latency-svc-4hqh6 [746.921696ms]
  May  8 13:17:39.484: INFO: Got endpoints: latency-svc-5twhx [750.723123ms]
  May  8 13:17:39.534: INFO: Got endpoints: latency-svc-swh8m [751.238806ms]
  May  8 13:17:39.584: INFO: Got endpoints: latency-svc-8m46s [748.596592ms]
  May  8 13:17:39.635: INFO: Got endpoints: latency-svc-4c54v [749.999811ms]
  May  8 13:17:39.683: INFO: Got endpoints: latency-svc-hm77l [749.736736ms]
  May  8 13:17:39.683: INFO: Latencies: [16.574362ms 20.04702ms 29.565985ms 42.328515ms 50.10359ms 59.525935ms 65.02042ms 77.63507ms 81.757133ms 92.158088ms 109.258956ms 117.206065ms 125.178317ms 136.163164ms 136.867985ms 137.478806ms 138.171711ms 138.42375ms 140.370548ms 140.606802ms 141.099084ms 144.241158ms 144.910856ms 145.280611ms 146.013861ms 146.379752ms 147.750228ms 148.232188ms 148.271073ms 149.858251ms 149.962999ms 151.083603ms 152.490302ms 153.874162ms 155.892229ms 158.094313ms 194.922123ms 236.182975ms 274.936962ms 313.556161ms 343.393084ms 387.217888ms 427.153832ms 461.905864ms 503.968671ms 545.663194ms 584.347081ms 624.455733ms 670.740041ms 701.801334ms 742.076818ms 743.249976ms 745.160271ms 746.099441ms 746.774842ms 746.809862ms 746.921696ms 747.017603ms 747.060281ms 747.179441ms 747.462222ms 747.540459ms 747.59725ms 747.646951ms 747.652931ms 747.6549ms 747.733939ms 747.882997ms 747.912901ms 747.915801ms 747.944994ms 748.027564ms 748.04453ms 748.085068ms 748.112116ms 748.160854ms 748.270113ms 748.293683ms 748.342764ms 748.437087ms 748.46584ms 748.500872ms 748.566248ms 748.596592ms 748.677227ms 748.709759ms 748.740421ms 748.750973ms 748.863139ms 748.917782ms 748.92136ms 748.943574ms 748.953398ms 749.004845ms 749.012726ms 749.01834ms 749.029319ms 749.062136ms 749.074554ms 749.078892ms 749.146858ms 749.159317ms 749.172365ms 749.299918ms 749.345398ms 749.370548ms 749.384602ms 749.431021ms 749.439305ms 749.453529ms 749.505909ms 749.515532ms 749.579065ms 749.604321ms 749.675268ms 749.678354ms 749.707419ms 749.736736ms 749.749741ms 749.764009ms 749.83047ms 749.833844ms 749.858073ms 749.892741ms 749.896742ms 749.909845ms 749.931359ms 749.934297ms 749.974408ms 749.999632ms 749.999811ms 750.002868ms 750.018385ms 750.021653ms 750.048056ms 750.081677ms 750.121594ms 750.13029ms 750.139797ms 750.14436ms 750.146219ms 750.181406ms 750.220419ms 750.226283ms 750.248081ms 750.268486ms 750.276991ms 750.313581ms 750.358956ms 750.364784ms 750.383814ms 750.529499ms 750.534828ms 750.563863ms 750.584133ms 750.590473ms 750.634697ms 750.655788ms 750.673392ms 750.687605ms 750.705413ms 750.707253ms 750.723123ms 750.778686ms 750.79412ms 750.835531ms 750.875467ms 750.887601ms 750.912013ms 750.916718ms 750.929656ms 750.953691ms 750.988063ms 751.024187ms 751.050904ms 751.07972ms 751.238806ms 751.337632ms 751.375618ms 751.425497ms 751.458776ms 751.655512ms 751.729712ms 751.821571ms 751.850158ms 751.927894ms 751.977954ms 752.258033ms 752.374336ms 752.522283ms 752.578717ms 752.735023ms 752.813659ms 752.879277ms 752.962854ms 753.401717ms 754.791473ms 756.260952ms 756.666131ms 758.05804ms]
  May  8 13:17:39.683: INFO: 50 %ile: 749.146858ms
  May  8 13:17:39.683: INFO: 90 %ile: 751.458776ms
  May  8 13:17:39.683: INFO: 99 %ile: 756.666131ms
  May  8 13:17:39.683: INFO: Total sample count: 200
  May  8 13:17:39.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-9136" for this suite. @ 05/08/23 13:17:39.686
• [10.753 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/08/23 13:17:39.691
  May  8 13:17:39.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/08/23 13:17:39.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:17:39.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:17:39.704
  STEP: Creating 50 configmaps @ 05/08/23 13:17:39.706
  STEP: Creating RC which spawns configmap-volume pods @ 05/08/23 13:17:39.945
  May  8 13:17:40.081: INFO: Pod name wrapped-volume-race-01efa63e-0c53-4dc4-9aed-69523426d10e: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/08/23 13:17:40.081
  E0508 13:17:40.430612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:41.431263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/08/23 13:17:42.109
  May  8 13:17:42.120: INFO: Pod name wrapped-volume-race-6965582a-1f86-4016-a453-9913b41bd866: Found 0 pods out of 5
  E0508 13:17:42.432416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:43.432693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:44.432982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:45.433284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:46.433540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:47.128: INFO: Pod name wrapped-volume-race-6965582a-1f86-4016-a453-9913b41bd866: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/08/23 13:17:47.128
  STEP: Creating RC which spawns configmap-volume pods @ 05/08/23 13:17:47.161
  May  8 13:17:47.175: INFO: Pod name wrapped-volume-race-be97ea2e-3720-443c-9f24-567959ef0ed0: Found 0 pods out of 5
  E0508 13:17:47.434473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:48.434780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:49.434980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:50.435299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:51.435398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:17:52.183: INFO: Pod name wrapped-volume-race-be97ea2e-3720-443c-9f24-567959ef0ed0: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/08/23 13:17:52.183
  May  8 13:17:52.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-be97ea2e-3720-443c-9f24-567959ef0ed0 in namespace emptydir-wrapper-1746, will wait for the garbage collector to delete the pods @ 05/08/23 13:17:52.196
  May  8 13:17:52.252: INFO: Deleting ReplicationController wrapped-volume-race-be97ea2e-3720-443c-9f24-567959ef0ed0 took: 3.741593ms
  May  8 13:17:52.353: INFO: Terminating ReplicationController wrapped-volume-race-be97ea2e-3720-443c-9f24-567959ef0ed0 pods took: 100.891281ms
  E0508 13:17:52.435833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:53.436632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-6965582a-1f86-4016-a453-9913b41bd866 in namespace emptydir-wrapper-1746, will wait for the garbage collector to delete the pods @ 05/08/23 13:17:53.754
  May  8 13:17:53.811: INFO: Deleting ReplicationController wrapped-volume-race-6965582a-1f86-4016-a453-9913b41bd866 took: 3.775439ms
  May  8 13:17:53.912: INFO: Terminating ReplicationController wrapped-volume-race-6965582a-1f86-4016-a453-9913b41bd866 pods took: 101.033327ms
  E0508 13:17:54.437423      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-01efa63e-0c53-4dc4-9aed-69523426d10e in namespace emptydir-wrapper-1746, will wait for the garbage collector to delete the pods @ 05/08/23 13:17:54.913
  May  8 13:17:54.971: INFO: Deleting ReplicationController wrapped-volume-race-01efa63e-0c53-4dc4-9aed-69523426d10e took: 3.892241ms
  May  8 13:17:55.071: INFO: Terminating ReplicationController wrapped-volume-race-01efa63e-0c53-4dc4-9aed-69523426d10e pods took: 100.194576ms
  E0508 13:17:55.437981      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/08/23 13:17:56.372
  E0508 13:17:56.438529      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-1746" for this suite. @ 05/08/23 13:17:56.537
• [16.849 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/08/23 13:17:56.544
  May  8 13:17:56.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename projected @ 05/08/23 13:17:56.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:17:56.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:17:56.557
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-9faf744c-44ba-4ee9-b0c9-777b09a79797 @ 05/08/23 13:17:56.562
  STEP: Creating the pod @ 05/08/23 13:17:56.565
  E0508 13:17:57.439355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:17:58.440395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-9faf744c-44ba-4ee9-b0c9-777b09a79797 @ 05/08/23 13:17:58.583
  STEP: waiting to observe update in volume @ 05/08/23 13:17:58.587
  E0508 13:17:59.441223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:00.441418      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:01.441916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:02.442203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:03.442394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:04.442723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:05.443200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:06.443242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:07.443435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:08.443724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:09.443904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:10.444179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:11.444346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:12.444643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:13.445465      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:14.445644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:15.445976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:16.446663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:17.446766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:18.446928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:19.447295      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:20.447570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:21.447987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:22.448250      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:23.448856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:24.449117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:25.449434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:26.450397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:27.450779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:28.451045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:29.451221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:30.451488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:31.452136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:32.452397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:33.453246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:34.453532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:35.454224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:36.454885      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:37.455703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:38.455966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:39.456354      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:40.456624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:41.456768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:42.457033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:43.457817      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:44.458092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:45.459152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:46.459806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:47.459868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:48.460137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:49.460292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:50.460565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:51.460600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:52.460879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:53.461397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:54.461656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:55.462632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:56.463520      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:57.463588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:58.463861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:18:59.464952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:00.465155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:01.465263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:02.465537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:03.465712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:04.465979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:05.466140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:06.466871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:07.467461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:08.467740      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:09.468109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:10.468392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:11.468930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:12.469041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:13.469381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:14.469697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:15.470411      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:16.471088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:16.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6081" for this suite. @ 05/08/23 13:19:16.87
• [80.329 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/08/23 13:19:16.876
  May  8 13:19:16.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename runtimeclass @ 05/08/23 13:19:16.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:16.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:16.887
  May  8 13:19:16.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8635" for this suite. @ 05/08/23 13:19:16.896
• [0.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/08/23 13:19:16.901
  May  8 13:19:16.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename daemonsets @ 05/08/23 13:19:16.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:16.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:16.911
  STEP: Creating simple DaemonSet "daemon-set" @ 05/08/23 13:19:16.922
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/08/23 13:19:16.925
  May  8 13:19:16.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:19:16.929: INFO: Node worker-0 is running 0 daemon pod, expected 1
  E0508 13:19:17.471325      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:17.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 13:19:17.934: INFO: Node worker-0 is running 0 daemon pod, expected 1
  E0508 13:19:18.472145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:18.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 13:19:18.934: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/08/23 13:19:18.936
  STEP: DeleteCollection of the DaemonSets @ 05/08/23 13:19:18.938
  STEP: Verify that ReplicaSets have been deleted @ 05/08/23 13:19:18.942
  May  8 13:19:18.948: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35631"},"items":null}

  May  8 13:19:18.950: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35631"},"items":[{"metadata":{"name":"daemon-set-bm5bc","generateName":"daemon-set-","namespace":"daemonsets-3196","uid":"829bccc5-5281-436c-8d92-68515b7d1c83","resourceVersion":"35625","creationTimestamp":"2023-05-08T13:19:16Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"38976a04-b883-485e-ab5e-d2846727db2e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-08T13:19:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"38976a04-b883-485e-ab5e-d2846727db2e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-08T13:19:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9cl8h","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9cl8h","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-08T13:19:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-08T13:19:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-08T13:19:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-08T13:19:16Z"}],"hostIP":"10.0.53.117","podIP":"10.244.0.44","podIPs":[{"ip":"10.244.0.44"}],"startTime":"2023-05-08T13:19:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-08T13:19:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a144608f1e97ac03970f7e1b1830d387c52bfe4c50a56813cf9fe065e830e3a5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gtcpm","generateName":"daemon-set-","namespace":"daemonsets-3196","uid":"6c26876a-16fd-4750-b18b-206aab13106a","resourceVersion":"35629","creationTimestamp":"2023-05-08T13:19:16Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"38976a04-b883-485e-ab5e-d2846727db2e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-08T13:19:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"38976a04-b883-485e-ab5e-d2846727db2e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-08T13:19:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tgkpp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tgkpp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-08T13:19:16Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-08T13:19:18Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-08T13:19:18Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-08T13:19:16Z"}],"hostIP":"10.0.39.71","podIP":"10.244.1.213","podIPs":[{"ip":"10.244.1.213"}],"startTime":"2023-05-08T13:19:16Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-08T13:19:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://c512b7a37986d1df4fd85fcfac2d5f1913e09fee466aa2c1a188efc9ecfffeb3","started":true}],"qosClass":"BestEffort"}}]}

  May  8 13:19:18.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3196" for this suite. @ 05/08/23 13:19:18.961
• [2.063 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/08/23 13:19:18.965
  May  8 13:19:18.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename deployment @ 05/08/23 13:19:18.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:18.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:18.977
  May  8 13:19:18.985: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0508 13:19:19.473042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:20.473172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:21.473477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:22.473535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:23.474669      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:23.987: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/08/23 13:19:23.987
  May  8 13:19:23.987: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/08/23 13:19:23.994
  May  8 13:19:24.000: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7738  ea3ccb21-1c2c-4809-b424-bf6df8a04362 35687 1 2023-05-08 13:19:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-08 13:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006b33928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  May  8 13:19:24.002: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  May  8 13:19:24.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7738" for this suite. @ 05/08/23 13:19:24.007
• [5.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/08/23 13:19:24.015
  May  8 13:19:24.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename proxy @ 05/08/23 13:19:24.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:24.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:24.026
  May  8 13:19:24.029: INFO: Creating pod...
  E0508 13:19:24.475101      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:25.476165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:26.040: INFO: Creating service...
  May  8 13:19:26.049: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/pods/agnhost/proxy/some/path/with/DELETE
  May  8 13:19:26.055: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  8 13:19:26.055: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/pods/agnhost/proxy/some/path/with/GET
  May  8 13:19:26.058: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May  8 13:19:26.058: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/pods/agnhost/proxy/some/path/with/HEAD
  May  8 13:19:26.060: INFO: http.Client request:HEAD | StatusCode:200
  May  8 13:19:26.060: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/pods/agnhost/proxy/some/path/with/OPTIONS
  May  8 13:19:26.066: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  8 13:19:26.066: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/pods/agnhost/proxy/some/path/with/PATCH
  May  8 13:19:26.069: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  8 13:19:26.069: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/pods/agnhost/proxy/some/path/with/POST
  May  8 13:19:26.072: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  8 13:19:26.072: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/pods/agnhost/proxy/some/path/with/PUT
  May  8 13:19:26.074: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  8 13:19:26.074: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/services/test-service/proxy/some/path/with/DELETE
  May  8 13:19:26.077: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  8 13:19:26.077: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/services/test-service/proxy/some/path/with/GET
  May  8 13:19:26.080: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May  8 13:19:26.080: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/services/test-service/proxy/some/path/with/HEAD
  May  8 13:19:26.083: INFO: http.Client request:HEAD | StatusCode:200
  May  8 13:19:26.083: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/services/test-service/proxy/some/path/with/OPTIONS
  May  8 13:19:26.086: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  8 13:19:26.086: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/services/test-service/proxy/some/path/with/PATCH
  May  8 13:19:26.089: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  8 13:19:26.089: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/services/test-service/proxy/some/path/with/POST
  May  8 13:19:26.091: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  8 13:19:26.091: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-307/services/test-service/proxy/some/path/with/PUT
  May  8 13:19:26.094: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  8 13:19:26.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-307" for this suite. @ 05/08/23 13:19:26.096
• [2.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/08/23 13:19:26.103
  May  8 13:19:26.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename runtimeclass @ 05/08/23 13:19:26.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:26.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:26.114
  E0508 13:19:26.476567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:27.476857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:28.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-343" for this suite. @ 05/08/23 13:19:28.135
• [2.038 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/08/23 13:19:28.142
  May  8 13:19:28.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/08/23 13:19:28.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:28.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:28.153
  STEP: create the container to handle the HTTPGet hook request. @ 05/08/23 13:19:28.158
  E0508 13:19:28.477842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:29.478652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/08/23 13:19:30.17
  E0508 13:19:30.479642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:31.480802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/08/23 13:19:32.18
  STEP: delete the pod with lifecycle hook @ 05/08/23 13:19:32.185
  E0508 13:19:32.480914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:33.481210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:34.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3706" for this suite. @ 05/08/23 13:19:34.198
• [6.060 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/08/23 13:19:34.203
  May  8 13:19:34.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename replication-controller @ 05/08/23 13:19:34.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:34.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:34.215
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/08/23 13:19:34.217
  E0508 13:19:34.482200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:35.482398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 05/08/23 13:19:36.228
  STEP: Then the orphan pod is adopted @ 05/08/23 13:19:36.232
  E0508 13:19:36.482927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:37.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7814" for this suite. @ 05/08/23 13:19:37.239
• [3.040 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/08/23 13:19:37.244
  May  8 13:19:37.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename pods @ 05/08/23 13:19:37.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:37.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:37.254
  STEP: creating the pod @ 05/08/23 13:19:37.257
  STEP: submitting the pod to kubernetes @ 05/08/23 13:19:37.257
  E0508 13:19:37.483082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:38.483381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 05/08/23 13:19:39.269
  STEP: updating the pod @ 05/08/23 13:19:39.272
  E0508 13:19:39.484446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:39.779: INFO: Successfully updated pod "pod-update-807e41e1-9d6e-4bed-8334-07cef7771848"
  STEP: verifying the updated pod is in kubernetes @ 05/08/23 13:19:39.781
  May  8 13:19:39.783: INFO: Pod update OK
  May  8 13:19:39.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5992" for this suite. @ 05/08/23 13:19:39.786
• [2.546 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/08/23 13:19:39.79
  May  8 13:19:39.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename runtimeclass @ 05/08/23 13:19:39.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:39.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:39.802
  E0508 13:19:40.484636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0508 13:19:41.485311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:41.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3625" for this suite. @ 05/08/23 13:19:41.825
• [2.041 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/08/23 13:19:41.831
  May  8 13:19:41.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename kubectl @ 05/08/23 13:19:41.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:41.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:41.841
  May  8 13:19:41.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1703741274 --namespace=kubectl-9573 version'
  May  8 13:19:41.899: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May  8 13:19:41.899: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1+k0s\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-22T13:33:55Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May  8 13:19:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9573" for this suite. @ 05/08/23 13:19:41.902
• [0.075 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/08/23 13:19:41.906
  May  8 13:19:41.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1703741274
  STEP: Building a namespace api object, basename daemonsets @ 05/08/23 13:19:41.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/08/23 13:19:41.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/08/23 13:19:41.918
  May  8 13:19:41.929: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/08/23 13:19:41.934
  May  8 13:19:41.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:19:41.938: INFO: Node worker-0 is running 0 daemon pod, expected 1
  E0508 13:19:42.486042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:42.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:19:42.942: INFO: Node worker-0 is running 0 daemon pod, expected 1
  E0508 13:19:43.486179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:43.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 13:19:43.943: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/08/23 13:19:43.95
  STEP: Check that daemon pods images are updated. @ 05/08/23 13:19:43.958
  May  8 13:19:43.960: INFO: Wrong image for pod: daemon-set-57kf2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  8 13:19:43.960: INFO: Wrong image for pod: daemon-set-5xt2j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0508 13:19:44.486426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:44.966: INFO: Wrong image for pod: daemon-set-57kf2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0508 13:19:45.487729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:45.965: INFO: Wrong image for pod: daemon-set-57kf2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  8 13:19:45.965: INFO: Pod daemon-set-qf8pz is not available
  E0508 13:19:46.488113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:46.966: INFO: Pod daemon-set-zgqlm is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/08/23 13:19:46.968
  May  8 13:19:46.972: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  8 13:19:46.972: INFO: Node worker-0 is running 0 daemon pod, expected 1
  E0508 13:19:47.488241      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:47.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  8 13:19:47.978: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/08/23 13:19:47.987
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5561, will wait for the garbage collector to delete the pods @ 05/08/23 13:19:47.987
  May  8 13:19:48.043: INFO: Deleting DaemonSet.extensions daemon-set took: 3.699957ms
  May  8 13:19:48.144: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.850338ms
  E0508 13:19:48.488988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  8 13:19:49.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  8 13:19:49.447: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  8 13:19:49.448: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36037"},"items":null}

  May  8 13:19:49.450: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36037"},"items":null}

  May  8 13:19:49.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5561" for this suite. @ 05/08/23 13:19:49.458
• [7.555 seconds]
------------------------------
SSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May  8 13:19:49.463: INFO: Running AfterSuite actions on node 1
  May  8 13:19:49.463: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
  E0508 13:19:49.488975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
[ReportAfterSuite] PASSED [0.050 seconds]
------------------------------

Ran 378 of 7207 Specs in 5993.462 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h39m53.791273784s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

